{"3395": [], "3388": [], "3383": [], "3377": ["I think that's a good idea. Not the highest priority though \ud83d\ude04 "], "3365": [], "3360": [], "3350": [], "3347": [], "3346": [], "3340": [], "3311": [], "3283": [], "3261": [], "3260": ["The ctrl+c is already implemented in the downloadclient:  but there it is easy since it is multithreaded. If I understand correctly, we want to apply something similar for the uploadclient which is not mult-threaded. What about the other clients? Do we need it there? "], "3254": [], "3250": [], "3249": [], "3235": [], "3200": [], "3199": [], "3164": ["Yes the 12h default timeout is too short, given the fact that we expect to always do bulky staging submissions to sites. A longer default timeout setting in FTS, plus respecting the timeout passed by client (e.g. rucio), may just do it. Hope it helps, Xin ", "Since we are discussing this also via mail: We can increase this by changing the rucio.cfg `bring_online` setting, so we do not need to change the default in the Rucio code as well. "], "3148": [], "3147": [], "3135": [], "3124": [], "3115": ["Adding a `None` ssh identity (in an attempt to avoid using default values in the bootstrap section) also results in padding error. "], "3103": [], "3101": ["That is because traces were not meant to be used as return values for some time. They were just the data for the monitoring system. Actually I don't like that we start to expand this even more. Some time ago I discussed another approach with @bari12 were it would be possible to set callback functions for different steps of the download client. E.g., one for failed downloads, one for successful downloads, one for failed attemps... For each event the download client would call this function and give the user the opportunity to react to the event. ", "This needs some more discussion next week. I am not sure if the callbacks alone solves this well enough. ", "In meanwhile, we discussed with Tobi whether current downloadclient is threadsafe or not. The problem is in the list reference that I created in pilot for adding the traces:  According to the following article about locking:  appending to a list is threadsafe operation. "], "3097": [], "3096": [], "3084": ["Tests performed on the latest version. All tests have been performed on CERN-PROD_SCRATCHDISK and UNI-FREIBURG_SCRATCHDISK 1) upload with registration in several attempts 2) upload without registration to the same rses 3) upload of existing file that is not registered 4) upload of a file that is already registered 5) upload of non-existing file All these tests performed well. Although it is not all we should be checking, e.g. uploading corrupted file, uploading to not-existing rse etc. ", "Following the comments from @TWAtGH I dropped the summary string form the NotAllFilesUploaded exception - #3158 The suggestion from Tobi is to add attributes directly to the exception:  whether as a dict or standalone attributes corresponding to the quantities in the summary, needs to be discussed and decided. "], "3076": [], "3042": [], "3031": ["You skipped the 'modifications' section in the description :P So how should this be resolved? Is it related to #1393 ? ", "Using urllib.quote_plus() for all but containers? ", "I don't think thjis is related to #1393 - There it is specifically if the trailing `/` is mentioned in the did name, like ATLAS prodsys was doing this in the past. Here the error must be something else. ", "I think this is rather an list_replicas issue then. But if the physical filename or the dataset name contains a \"/\" it will be problematic because it's the directory separator. We could change the download client to replace it by another character or create a subdirectory for each \"/\" but I think the best thing would be not to have \"/\" in the LFNs :sweat_smile: ", "Creating subdirectories is fine. This is in fact how these files are stored on storage. The LFN just includes these structural directories. There is nothing wrong with having `/` in the LFN. I would suggest we just create subdirectories. @TWAtGH can you please try and confirm the exact issue. If it is in list_replicas then this goes over to @mlassnig - but we should narrow down what the actual issue is. ", "Looks like the download is already implemented like this. But it's difficult to try because the upload doesn't support this. I assume that most client functions that do an REST API call won't work using a DID containing a slash. A little bit more info about the exact issue would be nice @cserf and how can I create a DID containing a slash? Also how would double slashes be resolved? Just a single directory I guess. ", "Ok in fact it was the server which rejected the upload due to a missing `AllowEncodedSlashes On` But the server still don't allow to insert slashed DIDs. I think it happens in the `add_replicas` call: ``` 2019-11-19 17:25:17,488 DEBUG File DID does not exist 2019-11-19 17:25:19,032 ERROR Provided object does not match schema. Details: Problem validating dids : u'slash/lfn01' does not match '^[A-Za-z0-9][A-Za-z0-9\\\\.\\\\-\\\\_]{1,250}$' Failed validating 'pattern' in schema['items']['properties']['name']: {'description': 'Data Identifier name', 'pattern': '^[A-Za-z0-9][A-Za-z0-9\\\\.\\\\-\\\\_]{1,250}$', 'type': 'string'} On instance[0]['name']: u'slash/lfn01' ``` ", "Yes, you have to try this with a schema which allows slashes. Eg. the `belleii` schema. ", "@TWAtGH is there any news on this? ", "Traceback : ``` rucio -v download test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst 2020-02-11 05:54:32,806 INFO Processing 1 item(s) for input 2020-02-11 05:54:32,806 DEBUG num_unmerged_items=1; num_dids=1; num_merged_items=1 2020-02-11 05:54:32,806 INFO Getting sources of DIDs 2020-02-11 05:54:32,806 DEBUG schemes: None 2020-02-11 05:54:32,806 DEBUG rse_expression: *\\istape=true 2020-02-11 05:54:32,806 DEBUG num DIDs for list_replicas call: 1 2020-02-11 05:54:34,649 DEBUG num resolved files: 2 2020-02-11 05:54:34,655 DEBUG \"unzip -v\" returned with exitcode 0 2020-02-11 05:54:34,661 DEBUG \"tar --version\" returned with exitcode 0 2020-02-11 05:54:34,661 DEBUG num list_replicas calls: 1 2020-02-11 05:54:34,661 DEBUG Queueing file: test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst/bf55cdc6f73e473396c3476ca7047384 2020-02-11 05:54:34,661 DEBUG real parents: set(['test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst']) 2020-02-11 05:54:34,661 DEBUG options: {'test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst': {'ignore_checksum': False, 'transfer_timeout': 3600, 'destinations': set([('.', False)])}} 2020-02-11 05:54:34,663 DEBUG Traceback (most recent call last): File \"/usr/bin/rucio\", line 159, in new_funct return function(*args, **kwargs) File \"/usr/bin/rucio\", line 975, in download result = download_client.download_dids(items, args.ndownloader, trace_pattern) File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 280, in download_dids input_items = self._prepare_items_for_download(did_to_options, merged_items_with_sources, resolve_archives=resolve_archives) File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 1158, in _prepare_items_for_download paths = [os.path.join(self._prepare_dest_dir(dest[0], dataset_name, file_did_name, dest[1]), file_did_name) for dest in destinations] File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 1395, in _prepare_dest_dir os.makedirs(dest_dir_path) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs makedirs(head, mode) File \"/usr/lib64/python2.7/os.py\", line 157, in makedirs mkdir(name, mode) OSError: [Errno 13] Permission denied: '/grid' 2020-02-11 05:54:34,663 ERROR [Errno 13] Permission denied: '/grid' 2020-02-11 05:54:34,663 ERROR Rucio exited with an unexpected/unknown error. ``` "], "3023": ["@cserf: The columns are added to the table by @jwackito You can add the filling of the values to the poller. ", "transfertool/fts3.py bulk_query method wasn't querying staging info, that's why staging_start and staging_finished columns in the requests were not populated... ", "@jwackito this still doesn't seem to work. We have to check if we are losing this info somewhere in the workflow or if we just don't get the value from FTS. @jwackito please try to debug this on one of the integration pollers. "], "2983": [], "2973": [], "2961": [], "2926": [], "2915": [], "2881": ["Thanks for the report. I'm having a look. "], "2830": [], "2827": [], "2770": [], "2714": [], "2706": [], "2703": ["For what it's worth, I would also find this very useful, so I'll add my support! ", "This would indeed be very useful and much appreciated. "], "2686": ["i wonder what the most appropriate way to do this is. We can basically do a json dump of the post/put data to the logfile or even ES. It still wouldn't be in the apache access log, but it would be easy to correlate the access with the line in the logfile/ES. ", "dumping the request json to the logfile seems the right thing to do to me. i've had a look at the different parameters in the mod_wsgi documentation, but there's just no way you can write to the access_log. we can however dump the access itself also in the error_log, and silence the access_log, so everything would be in a single file? "], "2639": [], "2637": [], "2636": [], "2635": ["1st part #2500 2nd part #2553 ", "Main schema change - #2727 "], "2634": ["Yet another non-existing web page referred to in \"rucio list-rses\" help for --expressions option:  "], "2631": ["[Google  to follow this activity. ", "Also related to #2630 "], "2630": ["[Google  to follow this activity. "], "2621": ["Is it still valid ? ", "I don't think it is valid. There is a loop on activity, so I don't understand what's wrong. Martin, can you double check ? ", "I think the issue is that `activity` is not passed through to `get_next` It loops over the activities but does nothing with it. "], "2613": ["See also  ", "Please link from a menu tab, as I only find the url in gmail. I expect my earlier comments are still relevant. "], "2582": ["I think we discussed this on Slack. I'll make a patch for this ", "Hi Eric, this is still an issue. Did you figure out why this is happening? ", "Oh, I've complete forgotten about this. And I don't see a patch in CMSRucio for it (where the script resides). I'll at least make an issue there. "], "2543": [], "2542": ["There should be also an easy way to configure the throttler in the config table . For example there could be a throttler mode in the config like `throttler_mode`: - throttler_mode, SRC_PER_ACT (to throttle per source RSE and per activity) - throttler_mode, DEST_PER_ACT - throttler_mode, SRC_DEST_PER_ACT - throttler_mode, SRC_ALL_ACT - throttler_mode, DEST_ALL_ACT - throttler_mode, SRC_DEST_ALL_ACT (to throttle per source and destination RSE and over all activites) Then each RSE would need a threshold value in combination with the mode to allow switching between modes like: - 'All Activities, DEST_ALL_ACT, MOCK', 10000 - 'User Subscription, SRC_DEST_ALL_ACT, MOCK', 20000 This would add the modes to values in the rse_transfer_limits table. Also each RSE should be configured to use FIFO or grouped FIFO depending on the mode: - 'DEST_ALL_ACT, MOCK', FIFO ", "Yes, we would need some kind of mode specifier in the table. I wonder if this should be an additional column though. ", "Also there are some special cases to be considered - if the throttler uses destination and source, it could be that the destination RSE is configured to use FIFO and the source RSE grouped FIFO, then its not clear what to use - if the throttler uses destination and source, it could be that a RSE is used as source and destination, then the configuration would also need a field to specify that like `'All Activities, DEST_ALL_ACT, MOCK, DEST', 10000` if MOCK is used as destination RSE ", "> Yes, we would need some kind of mode specifier in the table. I wonder if this should be an additional column though. @bari12 You mean in the rse_transfer_limits table? I thought that the throttler can only have one mode, so I would set the value only once ", "Yes, in `rse_transfer_limits`. It's just not super nice that all the \"configuration\" is part of a long string. For some things it's not possible otherwise, but we should try to put as many things as possible into separate columns. ", "Ah yes makes sense. We should be maybe also check why the throttler thresholds are stored in the config table. I think the ones in the `rse_transfer_limits` table are unused and it would be better to use them and then to use seperate colums, like you said ", "I think this schema for `rse_transfer_limits` would fit. | RSE ID | Activity | Max transfers | Transfers | Waiting | Direction | Mode | Volumn | Deadline | ------------- |:-------------:| -----:|-----:|-----:| -----:| -----:| -----:| -----:| | MOCK | ALL | 10000 | 10 | 10 | DEST | FIFO | 10000 | - | MOCK | User Subscription | 500 | 10 | 10 | SRC | FIFO | - | - | MOCK | User Subscription | 500 | 10 | 10 | DEST | GFIFO | 500 | - | MOCK | User Subscription | 500 | 10 | 10 | SRC | GFIFO | - | 3 Then depending on the throttler mode the thresholds and the release mode can be read out, e.g the mode `DEST_PER_ACT` would read the third row, `DEST_ALL_ACT` would read the first row and `SRC_PER_ACT ` would read the second row ", "Yes, I think that would work. (Although Volum**e** instead of Volum**n** \ud83d\ude04 Right now there is also a column `rse_expression` in the table. I am not quite sure why, maybe this was just put there in preparation when the throttler was created. ", "In the new implementation the throttler lost the ability to set a limit for `all_rses`, only limits for individual rses are possible now. This needs to be added again. "], "2517": [], "2459": ["Is this a possibility? ", "Yes, this shouldn't be too complicated to do for the `deep=False` case. For the `deep=True` case it is a bit more complicated, as several tables are joined here, but I assume that this one is not relevant for you anyway. I think we can extend the API with giving a list of scope/names, or add a new call - but it wouldn't allow the `deep=True` case. ", "And what's the difference between deep=True and deep=False? ", "There is a table storing all dataset replicas (called `collection_replicas`) which is queried when doing the query with `deep=False` - this is a very efficient query (just a PK lookup) With `deep=True` this is not resolved via the collection_replica table, but the result is generated based on the location of every single file replica in the dataset. This is an expensive query, which is not needed in general, as mostly you create replication rules on the datasets you are querying for. But there are certain workflows which do not result in a collection replica being created for the dataset (as no rule was created for the dataset) in this case `deep=True` must be used. Just to illustrate a case where this might be necessary. You have a dataset `A`. 50% of the files of dataset `A` are in dataset `B` and 50% in dataset `C`. If you don't create a replication rule for dataset `A`, but only for `B` and `C`, no collection replica was created for dataset `A`. For the simple query (`deep=False`), it will look like that the dataset has no full replica there, although all files actually have a replica. In that case the deep query needs to be used. ", "I see, thanks for this explanation. Indeed I believe we won't have need for using deep=True, but Eric knows those use cases better than me. ", "@bari12 Something on our setup is not working correctly, I think. Even making a rule directly on a dataset, we don't really get replicas without the deep flag. From the CLI ``` # rucio list-dataset-replicas \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\" DATASET: cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396 +----------------------+---------+---------+ | RSE | FOUND | TOTAL | |----------------------+---------+---------| | T2_US_Wisconsin_Test | 0 | 0 | | T2_DE_DESY | 0 | 0 | | T2_US_Nebraska_Test | 0 | 0 | +----------------------+---------+---------+ # rucio list-dataset-replicas --deep \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\" DATASET: cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396 +----------------------+---------+---------+ | RSE | FOUND | TOTAL | |----------------------+---------+---------| | T2_US_Wisconsin_Test | 20 | 20 | | T2_DE_DESY | 20 | 20 | | T2_US_Nebraska_Test | 20 | 20 | +----------------------+---------+---------+ ``` While the rules are on the dataset: ``` # rucio list-rules \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\" ID ACCOUNT SCOPE:NAME STATE[OK/REPL/STUCK] RSE_EXPRESSION COPIES EXPIRES (UTC) CREATED (UTC) -------------------------------- --------------- -------------------------------------------------------------------------------------- ---------------------- -------------------- -------- --------------- ------------------- d0d3e85fc1954b51b5b623e4fadea08b root cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396 OK[20/0/0] T2_US_Nebraska_Test 1 2019-04-25 21:17:13 18548137e8a745a09262c227a842c655 root cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396 OK[20/0/0] T2_US_Wisconsin_Test 1 2019-04-25 20:37:59 1ba5e5caf72a46b4bc8b7ea5b17ae40a sync_t2_de_desy cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396 OK[20/0/0] rse=T2_DE_DESY 1 2019-04-18 09:02:49 ``` Are we missing a trigger or procedure in Oracle that is supposed to fill in the collection_replicas table? ", "Yes, there is the `COLL_UPDATED_REPLICAS` procedure in  But this works with virtual scopes which Gancho set up (Basically there is a virtual scope for MC, one for DATA, one for Panda and one for everything else. (Would need to check with Gancho how to exactly setup these virtual scopes in oracle) But alternatively you can also use the `abacus-collection-replica` daemon  This one does the same as the oracle procedure. ", "After I tried posted this, I found that daemon. Running it \"does not work\" in that this line  basically takes minutes and consumes >2.5 GB of RAM and causes not just my pod but my k8s node to crash. Maybe because this is because we've got a lot built up there and have never done this? Is there an easy way to limit it? ", "Yes, the daemon really seems to rely on the table being small. Possibly easiest would be to do a cleanup with the PL/SQL first. I have a version without the virtual scopes, which you could just execute once. Let me check ", "Hi @bari12 , getting back to the original issue. In case you don't need any further information from my side, would you have a rough ETA for considering this ticket? Even by the end of 2019 would be helpful ;) ", "Hi @amaltaro I can't give you an exact ETA, but it should be before the end of 2019 :-) This is not a difficult development, I just need to find time for it, or find someone who has time to do it \ud83d\ude04 "], "2417": [], "2414": ["Config file was changed according to David's proposal. Documentation needs to be done still. "], "2410": ["Alternatively, a json column? I don't like string parsing too much "], "2393": ["Hi Rod, At one point we were discussing of looking into third_party_copy support of zip constituents. xroot/FTS would obviously have to support that. But if this is the case, we could essentially create the replicas via transfer requests and just nee some logic around that. The timescale of this is a different question though. "], "2387": [], "2356": [], "2330": ["For the last point: There are many scripts in there which do not belong in our core repo. We should think about collecting the useful ones in a rucio/tools or rucio/utils repo. Something to discuss today. "], "2325": [], "2318": [], "2312": [], "2311": [], "2310": ["- Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again. - Add a column which gives the reason of the failure if all accesses fail with same error. I am interested in 'Checksum issue' since it does not require further check (in contrary to problem to access file) ", "- When files are declared lost/corrupted, we reach a page almost empty. It would be usefull to have a button to go back to the same configuration (period, type) to continue to check and declare other files. ", "All the requests can be implemented relatively easily but this one \"Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again.\". It will require some significant changes. I need to evaluate how much work will be needed. "], "2282": [], "2264": [], "2263": [], "2258": ["The problem is with the value '1.0' that is not handled properly by `tabulate` : ``` >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1.0')] >>> print(tabulate.tabulate(table, tablefmt='plain')) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.5/lib/python2.7/site-packages/tabulate.py\", line 1301, in tabulate for c, ct, fl_fmt, miss_v in zip(cols, coltypes, float_formats, missing_vals)] File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.5/lib/python2.7/site-packages/tabulate.py\", line 768, in _format return format(float(val), floatfmt) ValueError: could not convert string to float: True ``` vs : ``` >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1')] >>> print(tabulate.tabulate(table, tablefmt='plain')) TAIWAN-LCG2_TAPE-STAGING: True is_stagingarea: True stresstestweight: 1 ``` or : ``` >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1,0')] >>> print(tabulate.tabulate(table, tablefmt='plain')) TAIWAN-LCG2_TAPE-STAGING: True is_stagingarea: True stresstestweight: 1,0 ``` ", "Sorry forgot to mention that I've debugged it already to this point. But I won't have time to look further into it ( @bari12 ) ", "@nikmagini @TomasJavurek can you please have a look on this issue. (See @cserf comment) ", "BTW, the `Strange error: No section: 'policy'` will be fixed here :  "], "2254": ["Testing a xrootd implementation: [root@rucio-nagios-prod-02 rse]# rucio-admin rse delete-protocol --scheme root RAL-LCG2-ECHO_SCRATCHDISK [root@rucio-nagios-prod-02 rse]# rucio-admin rse add-protocol --hostname xrootd.echo.stfc.ac.uk --scheme root --prefix /atlas:scratchdisk/rucio/ --port 1094 --impl rucio.rse.protocols.gfal.Default --domain-json '{\"wan\": {\"read\": 2, \"write\": 2, \"third_party_copy\": 2, \"delete\": 2}, \"lan\": {\"read\": 1, \"write\": 2, \"delete\": 2}}' RAL-LCG2-ECHO_SCRATCHDISK "], "2232": [], "2218": [], "2216": ["How many files is this list long? ", "Just following up on old tickets: The chunking should be done on the auditor side. The list_replicas method here is a core function, which should not need protection from other core/daemon calls - only on the API level. Thus please chunk this in the daemon itself. "], "2213": [], "2159": [], "2143": [], "2104": [], "2063": [], "2029": ["@mlassnig please have a look if this is still relevant; Close if not. "], "1923": [], "1911": ["Note that importing the client at the top of the `rucio` CLI causes an authentication to be attempted even when you're just doing `rucio --help`. ", "That doesn't seem to be problem only with importing downloadclient but Clients in genereal. The config is imported in baseclient. ", "Just importing the module should not read the config? Only when constructing the client object; Then indeed it does read the config and even trys to authenticate against the rucio server. @TomasJavurek can you please check the code path why this is the case even when doing `rucio --help`? In my opinion no client object needs to be constructed in that case. "], "1895": [], "1867": ["### Example On an RSE there are 100 TB free, dataset `A` is 10 TB and is already available on the RSE, dataset `B` is 5 TB and not available on the RSE, container `C` contains `A` and `B`. The e-mail sent when a user requests to replicate `C` should report that the expected free space after the approval is 95 TB. Currently, the e-mail will report 85 TB. "], "1860": [], "1839": ["This should be consolidated. I would not be too afraid of backwards-compatibility, but maybe we should still add the new parameter, leave the old one in, but if it is used display some kind of message. A few (feature) versions later we then take it out. We used this concept already for several other CLI changes. ", "Much of it is historical due to the API itself, but we can start changing the clients and bring the API, REST, and core slowly up to speed. At least for the RSE parameter I would suggest for conciseness to always use `--rse` but always parse RSE expressions, and get rid of the `--rse-expression` or `--expression` extra arguments. ", "As rse expression can give a back a list, i.e. `--rses` looks more appropriate imo. starting with the same name for both `rse-expression` and `expression` would be already a good step. ", "Sounds good to me: - `--rse` if it is really only 1 RSE - `--rses` if it is an expression. Although --rses suggests a bit that you can enumerate them, which is not really true (`RSEA|RSEB|RSEC` would work but not `RSEA,RSEB,RSEC`) ", "But shouldn't we handle single RSE or multiple RSEs automatically? `--rse` is a basically a subset of `--rses` and having two different parameters is confusing. ", "We would have to go through it, in most cases `--rses` is ok. But in cases where you really only want one RSE, e.g. marking a replica as bad/lost, --rses doesn't really make sense, as it suggests a list, which doesn't work. ", "I would do `--rse` everywhere, and in case a list comes back instead of a single RSE fail to the client and tell \"hey you selected more than one\", instead of having two parameters. (In a way, like SQL cursors) ", "ah but are you proposing to always resolve RSE by expression ? Most of the time the RSE expression corresponds to the RSE name. It might be worth to check the implementation as this will be the most popular call then... ", "From a performance that should be fine, the expression is cached server-side, so it's not even going to the DB too much. ", "it's worth to check that even without the caching, it does a simple lookup first against the rse table. Not sure the caching is enabled if memcached is not there and the memoization fallback will be most likely in memory per thread/process, which won't reduce that much the query rate. "], "1834": [], "1830": [], "1829": [], "1817": ["It seems that it is not possible to catch exceptions inside the streaming. To reproduce, run this minimal flask server. The exception gets raised on flask side but does not get catched. An internal server error is generated instead of the 'error' string. ```python from flask import Flask, Response, jsonify app = Flask(__name__) @app.route(\"/\") def hello(): def generate(): for i in range(0, 100): raise Exception yield '1' + '\\n' try: return Response(generate()) except Exception as error: return jsonify({'error': error}) if __name__ == '__main__': app.run(passthrough_errors=True) ``` ``` FLASK_APP=file.py flask run ``` One solution would be to rework the logic and check for possible problems like non existing account before querying the database. Another solution would be to call the same function inside a for loop but with a break after one loop, where we check for exceptions. Afterward the streaming can start if there was no exception catched. ", "To me the accurate path looks like the one where we change the logic (and checking) of the APIs which use streaming. Thus we do all the conflict checking first and then just iterate the stream. Conceptually, the error handling we do right now with streaming is wrong and possibly leads to malformed replies. Because the HTTP return code is calculated (and sent!) at the header anyway. So let's say this is a `200`, whatever you stream afterwards (and throws an exception) is not properly reflected in the error code. But this will take time, we probably have to more carefully select which APIs we offer as a streaming API (list-contents, list-replicas, ...) and not be so generous like we are now with them. ", "Worth mentioning here to think about proper support for HTTP Chunked Encoding. "], "1808": ["Just a small note as I've been working with LIGO to utilize OSG and EGI-based computing centers for multiple years now. While \"proprietary HPC clusters\" characterizes at least one LIGO resource, there's actually a wide range of resources available. Regardless, that's neither here nor there. The rest of the post looks good! ", "Hi Brian, Thanks for the head up. Indeed I was just intending to highlight the differences between our two computing infrastructures, not to be \"detailed\"! :) Cheers! Gabriele ", "Hi guys, fascinating discussion, this is something that we have mulled in @EGI-Foundation for a bit as well. It would be out of place to make sweeping statements about DIRAC without the developers involved, so maybe this issue could be pointed out to them if that hasn't already been done. My 2c is that there there are two patterns right now in developing these platforms 1. Build a core product, discover need for some other functionality, tack it on, ???, Profit!!! 1. Build a core product, discover need for some other functionality, **set up a contract with another set of services** to do that, ??? Profit We have often looked at DIRAC as an HTC solution, but it's way more than that and just using it as an HTC solution is actually quite hard. I hazard to say that it works best when it's the _primary_ interface for users and applications. Rucio on the other hand is (forgive me for projecting my own perception here), a fantastic data management system. It could (is?) tack on compute management as well. As a product, we (say, EGI), would like it to interoperate with other services like cloud compute, HTC, HPC, etc, via stable APIs and do it's data management thing. It would be nice to know if Rucio could be used as a drop-in replacement data catalogue for DIRAC, and more interesting to know if DIRAC could be used as a drop-in compute orchestration service for Rucio. My personal feeling is that something that does compute orchestration only would be a better fit (maybe, HTCondor, I don't have a great answer here, sorry). Thanks! (usual disclaimer of \"these opinions are mine and mine alone\", \"this does not represent the position of EGI, EGI Foundation _etc_\" apply here :wink: ) ", "Hi Bruce, I am pointing some DIRAC people to this issue! Cheers, Gabriele ", "Hi, I am DIRAC technical coordinator, and right now its main developer. I've been pointed here, I will try to give some advice. As mentioned above DIRAC give you the possibility to work with different, and even multiple, catalogs. Just to mention some real-life use case, which are the ones working best: - CLIC uses DIRAC with the  (DIRAC File Catalog) both as a \"Replica Catalog\" and as a \"Metadata Catalog\" - Belle2 uses DIRAC with the LFC (LCG File Catalog) as \"Replica Catalog\" and AMGA as \"Metadata Catalog\" - LHCb uses DIRAC with the DFC as \"Replica Catalog\" and the LHCb Bookkeeping as \"Metadata and provenance Catalog\". Some years ago LHCb moved its production replica catalog from the LFC to the DFC - all the other users that I know either use the DFC (the majority) or the LFC or some other combination. The DFC, the LFC, AMGA, the LHCb Bookkeeping are all  In DIRAC terminology, in fact, they are all Catalog plug-ins, as a DIRAC Catalog is such if it implements the same interface (e.g. add file, remove, etc). All catalogs implement the same interface and inherit from  You can have more than 1 catalog at the same time, as obvious from the examples above. In this case, the operations will be executed on all of them. So, for example you can register files on BOTH the LFC and DFC at the same time. Basically, each catalog plug-in implements a certain operation (e.g. the `addFile` operation) following its own \"interpretation\" of what, e.g. adding a file means for a certain catalog. So, what may be interesting for you, is implementing a RucioCatalogClient.py. The rest is purely configuration. ", "Hi @fstagni, thank you for joining the conversation. Indeed that was the solution thought about at first, but I have some questions to ask: 1. How is the synchronization between two separate catalogs (the DIRAC one and the external one) dealt with? 2. What are the required keys for the external catalog to implement to make it fully compatible with DIRAC? Is case the external catalog doesn't provide topological information, how does DIRAC retrieves such details? 3. Is there a list of the write and read methods to be implemented in the `FileCatalogClientBase.py` custom derived interface? 4. What are the methods that are needed by DIRAC to make it efficient at least as with the native catalog? Thank you, Gabriele ", "1. There's always a Master catalog, this is defined in the DIRAC CS (Configuration System). Just to be sure: you don't need to necessarily have the \"DIRAC one\". I repeat: a catalog is a catalog as long as it behaves like one. 2. What's a topological information for you? The location of the replicas? 3. Each catalog plugin \"announce\" what it can do. Otherwise there's a really basic list of required methods, just check the code for that. 4. DIRAC needs nothing at all in that sense. The DFC is just another service. ", "1. Awesome. I think the Rucio catalog can generate the missing info on the fly using functions, hence it should be possible to plug it to DIRAC; 2. yes, for example. As far as I have understood, DIRAC tries to perform the computation as close as possible to data. How does it figures out how to do that? Is the catalog providing some information or is it doe using some external metrics? 3. I saw the list of the mandatory `READ_METHODS = [ 'hasAccess', 'exists', 'getPathPermissions' ]`. It seems to me (and makes totally sense) that DIRAC requires to be able to read data, but the persistent output of data is not strictly required for the computing functionalities. 4. Several times in your publications there is an indication of a link between DIRAC's computing efficiency and the catalog information. Can you clarify a bit about that? In addition I would like to ask you if there is any example of a custom implementation of `FileCatalogClientBase.py` (e.g. for LFC) to read and understand more the integration process. Thank you Gabriele ", "1. I am not sure which are the \"missing info\"... ? Can you elaborate? 2. This is not fully correct. DIRAC CAN make the computation \"close\" to the data, but this is not a requirement. In fact you can run productions even in \"full mesh\" mode, meaning jobs can in theory go everywhere independently of the location of the input files. 2a. A \"replica catalog\" at least provides you with the location of the replicas. This location is a DIRAC [Storage  This info is used in several places, but a DIRAC SE can simply be \"RucioSE\" if this is something you want. 3. DIRAC jobs decide if and where to store 0/1/N of their outputs. The  object is what links the functionalities of FC and SE, and it's often the starting point for simple DM operations 4. Well... the DIRAC DFC is fast, efficient, practical, customizable, widely used, and it's already there. I can't compare it to any other solution apart the LFC, and I am not in a position of comparing it with the Rucio Catalog because I don't know much about it. 5. For examples just look in  : LcgFileCatalogClient.py is the LFC, FileCatalogClient is the DFC (and I would not suggest to look at the others because they are a bit less obvious). For LHCb  is the LHCb Bookkeeping. ", "1. and 2.: I see that DIRAC can operate in \"fully mesh\" mode, but in any other case the locality of data and computing resources should be described. The 1. question is about that: what information is needed for not to operate in \"fully mesh\". 3. Correct me if I am wrong: if I get to use Rucio catalog from DIRAC I should be able to publish output files to Rucio BY HAND using direct calls to Rucio APIs. To perform a basic test of the eventual `FileCatalogClientRucio.py` implementation it should be enough to use direct calls instead of a custom DIRAC plugin. 4. The question was more about \"how much of the DIRAC efficiency is due to custom information stored in/computed from the DFC?\" 5. Thanks for the references ", "1. DIRAC would need to know where the input data is to make a proper job scheduling. Data is always in at least one \"Storage Element\". The SE(s) needs to be described in the DIRAC CS. 2. Of course 3. TLDR: nothing. DIRAC is a (not small) set of components. Each component has its own life, and there's no need to install all of them. In fact many installations only install a small subset of them. The DFC is just a DIRAC component. In fact, from a informed user's perspective it's just a URL. The interrogations and answers that it gives need to follow a certain contract, nothing else. If another catalog respects that contract, then you're done. The `RucioFileCatalogClient.py` file should be the only one of the whole DIRAC where you do `import rucio`. "], "1806": [], "1801": [], "1797": ["FTS is thinking about adding this to FTS; On hold until confirmation "], "1773": [], "1771": ["For reference :  ", "Just came across the same problem, any chance it will be fixed? ", "@tbeerman , what can we do ? "], "1760": [], "1759": ["Happens on ``` Name : MariaDB-server Arch : x86_64 Version : 10.2.18 ``` with `MySQL-python 1.2.5` Will have to try to reproduce this. ", "Still not able to reproduce. Abandon? ", "ping @astroclark ", "Closing. ", "We're also seeing similar behavior with MariaDB 10.3.15 and MySQL-python 1.2.5 In judge-cleaner.log: ``` 2019-07-05 08:51:04,856 490 DEBUG Deleting lock x1t_SR001_170320_1116_tpc:records-943c2b3857b2a913583557452451b5049362ecbd-000000 for rule 2da476bb1d854febbf28b76deddc78a6 (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170320_1116_tpc', 'records-943c2b3857b2a913583557452451b5049362ecbd-000000', '-\\xa4v\\xbb\\x1d\\x85O\\xeb\\xbf(\\xb7m\\xed\\xdcx\\xa6', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at:  2019-07-05 08:51:04,861 490 ERROR Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/judge/cleaner.py\", line 107, in rule_cleaner delete_rule(rule_id=rule_id, nowait=True) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170320_1116_tpc', 'records-943c2b3857b2a913583557452451b5049362ecbd-000000', '-\\xa4v\\xbb\\x1d\\x85O\\xeb\\xbf(\\xb7m\\xed\\xdcx\\xa6', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at:  ``` In undertaker.log: ```2019-07-05 08:50:09,628 488 INFO Undertaker(1): Receive 1 dids to delete 2019-07-05 08:50:09,628 488 INFO Removing did x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30 (DATASET) Data identifier not found. Details: Data identifier 'archive:raw_records-58340a130a541c95997fd1c442930427b04eac30' not found 2019-07-05 08:50:09,646 488 DEBUG Removing rule d0b05b80dee24094b2a9f7b146412c97 for did x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30 on RSE-Expression UC_OSG_USERDISK 2019-07-05 08:50:09,671 488 DEBUG Deleting lock x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30-000000 for rule d0b05b80dee24094b2a9f7b146412c97 (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170319_1011_tpc', 'raw_records-58340a130a541c95997fd1c442930427b04eac30-000000', '\\xd0\\xb0[\\x80\\xde\\xe2@\\x94\\xb2\\xa9\\xf7\\xb1FA,\\x97', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at:  2019-07-05 08:50:09,678 488 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/undertaker/undertaker.py\", line 109, in undertaker logging.error('Undertaker(%s): Got database error %s.', worker_number, str(error)) UnboundLocalError: local variable 'error' referenced before assignment ``` ", "@bari12 can we come up with a reproducable test for this? ", "I haven't looked into this personally. It would be good to understand under which conditions this shows up. Running the daemons with multiple threads, or also one thread etc. @astroclark @jlstephen Then we could try to reproduce this condition. ", "From Handson: > Hi rucio-team, > I\u2019m DDM admin from ASGC(Taiwan site) > We have a rucio server to manger out data. > Here is our version information > Rucio 1.13.3 > MariaDB 10.1.37 > Python package SQLAlchemy-1.1.13 > > When I run rucio-judge-cleaner, I got error like: > > 2019-10-17 10:37:52,738 3265 INFO rule_cleaner[0/0]: Deleting rule bb60ed7f80ad43059984e5a8663f5c29 with expression TW-EOS02_AMS02SCRATCHDISK > 2019-10-17 10:38:53,390 3265 DEBUG Deleting lock ams-user-felixlee:amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz for rule bb60ed7f80ad43059984e5a8663f5c29 > Exception _mysql_exceptions.InterfaceError: (0, '') in <bound method SSCursor.__del__ of <MySQLdb.cursors.SSCursor object at 0x31a5c50>> ignored > (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('ams-user-felixlee', 'amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz', '\\xbb`\\xed\\x7f\\x80\\xadC\\x05\\x99\\x84\\xe5\\xa8f?\\\\)', 'b?\\xc5q\\xb4\\xbfA\\xbf\\x94\\x93\\xe4)\\x86\\x92\\xed\\x1a')] > > 2019-10-17 10:38:53,397 3265 ERROR Traceback (most recent call last): > File \"/opt/rucio/lib/rucio/daemons/judge/cleaner.py\", line 93, in rule_cleaner > delete_rule(rule_id=rule_id, nowait=True) > File \"/opt/rucio/lib/rucio/db/sqla/session.py\", line 358, in new_funct > raise DatabaseException(str(error)) > DatabaseException: Database exception. > Details: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('ams-user-felixlee', 'amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz', '\\xbb`\\xed\\x7f\\x80\\xadC\\x05\\x99\\x84\\xe5\\xa8f?\\\\)', 'b?\\xc5q\\xb4\\xbfA\\xbf\\x94\\x93\\xe4)\\x86\\x92\\xed\\x1a')] > > I google it and found this issue  > I do some test and found in the rucio/lib/core/rule.py line 823(I use the Rucio 1.13.3 on git hub to be the stander of line number) > locks = session.query(models.ReplicaLock).filter(models.ReplicaLock.rule_id == rule_id).with_for_update(nowait=nowait).yield_per(100) > If there was a rule that contain over 100 locks (which the number is the same with yield_per input), then the error will show up. > I guess the session query ReplicaLock table get over 100 row, but function yield_per only return 100 for once and the rows which satisfy ReplicaLock.rule_id == rule_id are still lock by function with_for_update. > So on the rucio/lib/core/rule.py line 830 > if __delete_lock_and_update_replica(lock=lock, purge_replicas=rule.purge_replicas, nowait=nowait, session=session): > rucio/lib/core/rule.py line 2711 > lock.delete(session=session, flush=False) > > it failed to delete the row on the ReplicaLock table. > I\u2019m not sure my guess is right or not. > Increase the number of yield_per input can fix the problem temporarily. > But I think that is not a good way. > Is there a better way to fix it? > > Best regards, > Handson Peng ", "To my understanding, the most likely reason why this is happening is that due to the `yield_per` parameter, the query is actually still running and the delete call (line 2711) changes the table thus somewhat conflicting the query (since it is still running). This is not an issue on Oracle, but it appears this brings MySQL into troubles. When you are changing the `yield_per` to something bigger than the amount of locks, this is not an issue as the query already ended at the moment we start deleting the locks. The only solution I see is to change the `yield_per(...)` to an `all()`, so the query finishes at the beginning of the loop. The downside of this is that the entire result has to be loaded into memory, which for big rules might be an issue. Maybe we can do this conditional to the DB implementation. ", "The problem solved by changing the yield_per(...) to an all(). The rule stuck before is around 60K locks with the same rule_id. And there is no other problems. So I still don't know the limit of rule size. ", "There is no principal limit of the rule size, however, the repairer might load the entire rule into memory, which for bigger rules requires a bit more. Thus if you run the repairer with a lot of threads in this uses all memory already under normal conditions, these large rule peaks might bring it over the limit. "], "1752": ["I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though. We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way. ", "Martin, the transformation implies full knowledge of all keys. Since it is not possible in this structure such process can be very tedious (not only on code writing) but on parsing level too. For instance, how many keys exists, will these keys stay the same over long time (I don't think so since we may add discard RSEs for instance), how many structure have this issue in all Rucio APIs, what is complete schema of all APIs. The list can go on. Not to mention that custom transformation cost time and resources. I outlined the issue and presented use-cases. Anyone/tool who need to deal with aggregation or analytics of the results will face this issue. When you need to deal with dozens of services in real time it is a big issue. But of course I understand backward compatibility issue and ideally it should be fixed both on server and client sides. If is it feasible it is another story. Of course I can work around such cases, but I'm in favor that this issue should be fixed eventually. On 0, Martin Barisits <notifications@github.com> wrote: > I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. > > This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though. > We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way. > > -- > You are receiving this because you authored the thread. > Reply to this email directly or view it on GitHub: >  ", "Hi Martin, Aren\u2019t at least a few of the APIs passed through a JSON schema verification? Or maybe I\u2019m just recalling some internal data structures? It\u2019d probably be useful to maintain some sort of published schema as new users of the API come along. Success breeds these sort of questions, after all! Thanks, Brian Sent from my iPhone > On Nov 5, 2018, at 12:13 PM, Valentin Kuznetsov <notifications@github.com> wrote: > > Martin, > the transformation implies full knowledge of all keys. Since it is not possible > in this structure such process can be very tedious (not only on code writing) > but on parsing level too. For instance, how many keys exists, will these keys > stay the same over long time (I don't think so since we may add discard RSEs for > instance), how many structure have this issue in all Rucio APIs, what is > complete schema of all APIs. The list can go on. Not to mention that > custom transformation cost time and resources. > > I outlined the issue and presented use-cases. Anyone/tool who need to deal > with aggregation or analytics of the results will face this issue. When you need > to deal with dozens of services in real time it is a big issue. > > But of course I understand backward compatibility issue and ideally it should be > fixed both on server and client sides. If is it feasible it is another story. > > Of course I can work around such cases, but I'm in favor that this issue should > be fixed eventually. > > On 0, Martin Barisits <notifications@github.com> wrote: > > I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. > > > > This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though. > > We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way. > > > > -- > > You are receiving this because you authored the thread. > > Reply to this email directly or view it on GitHub: > >  > \u2014 > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread. ", "We only do the schema verification for the incoming data structures  not for the outgoing one. But I fully agree, we should maybe have something for the return structures too. Not so much a verification, but more documentation. @vkuznet I get your point, I didn't mean to do the transformation for all eternity, just for now; The data structures right now are very much prepared in a way so that the client (The rucio client that is) can immediately deal with them efficiently. (That's why the rses are keys, so the client can quickly get all replica states for a certain rse in a long list of replicas) - I think what might be most appropriate is an option for the call to prepare the return object in an \"analytics ready\" way. We would have to look at all REST calls you are making and see if such an option is needed and keep it in mind if there are API changes in the future. ", "Martin it is not a stopper for me since I deal with these issues in other data-services, but eventually I'll be glad if this issue will be fixed one way or another. ", "So far I identified the following dynamic structures in /replicas/scope/name API: - states ``` \"states\":{\"T2_US_Nebraska\":\"AVAILABLE\"}} ``` - rses ``` \"rses\":{\"T2_US_Nebraska\":[\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\"]} ``` - pfns ``` \"pfns\":{\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\":{\"client_extract\":false,\"domain\":\"wan\",\"priority\":1,\"rse\":\"T2_US_Nebraska\",\"type\":\"DISK\",\"volatile\":false}} ``` ", "The pfns example is extremely bad if such records will be placed in DB and indexed on these keys since (a) pfn names are long strings, (b) we have extremely long list of pfns in GRID universe. ", "Hi Valentin, I'm not really following you. What is your concern with the pfns? They are not stored in the database at all? ", "Yes, but if we will dump data to HDFS (as is) and then create a dataframe the pfns will become attributes (column names) rather then values. On 0, Mario Lassnig <notifications@github.com> wrote: > Hi Valentin, I'm not really following you. What is your concern with the pfns? They are not stored in the database at all? > > -- > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub: >  ", "Hi @vkuznet - I think we've determined that this ticket is really about having a JSON schema for the Rucio APIs, no? Since that comes down to a pretty major redesign of the APIs (which, FWIW, is being discussed), what should we do with this ticket? Brian ", "I think it is up to Rucio team. We can use it for progress or further examples (as I did today), or it can be turned into actions/milestones. ", "Discussed in the development meeting on  We decided to leave it like this until we have a V2 API (Long term project) ", "Should this become an real problem for wanting to dump data to HDFS, it should be possible as an interim step to define the desired schema(s) for v2 and build a simple translator which would precede v2, right? ", "yes, that's definitely an option. "], "1746": ["This are actually two different methods, but there seem to be a problem with the regular expression of the rest endpoint in combination with the CMS SCOPE_NAME regular expression, so in both cases, it calls list_replicas.  `/([^/]*)(?=/)(.*)/datasets$` resolves to the list_dataset_replicas method and `/([^/]*)(?=/)(.*)/?$` to the list_replicas method But the latter one seems to cover anything with .../datasets too, so it should probably be changed to: `/([^/]*)(?=/)(.*)/$` So in conclusion: It should be two different methods, but in your case it is calling wrongly the same method. ", "Yet, another mis-behavior: this call  returns ``` {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\": {\"domain\": \"wan\", \"rse\": \"T2_US_Nebraska\", \"priority\": 1, \"volatile\": false, \"client_extract\": false, \"type\": \"DISK\"}}, \"adler32\": \"881ec8cf\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\", \"rses\": {\"T2_US_Nebraska\": [\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\"]}, \"scope\": \"cms\", \"bytes\": 3890770828, \"md5\": null} ``` while this call  returns nothing. Is it the same regex issue? Please note that this time I used as a name a (CMS dataset) /a/b/c pattern rather then (CMS block) /a/b/c#123 one. ", "I think in this case it is correct; The first query (without /datasets) returns the file replicas. The second query (/datasets) returns dataset replicas, which essentially are just a more efficient way to get the replica status of all files of a dataset (in the database we create a dataset replica which has synchronised counters) - However, dataset replicas are not created in all workflows, thus in this case everything seems correct. I think you are right, in your previous query including the # this seems to be somehow problematic with the REST regex. I will have a closer look on this in the next days. "], "1667": [], "1648": ["Should not be an issue with #716 "], "1636": ["I'm not sure what the problem is here (maybe related to the double `/` in xrootd path). But `add_files_to_dataset` definitely works without SRM. We tried with `gsiftp` at T0 and it works fine. "], "1583": ["In the transfer request the core creates, we already create a `ds_scope` and `ds_name` attribute. This is mostly done for tape endpoints, as it is used in the PFN generation. I think we can add this to the `file_metadata` dict which is part of the fts request. However, the ds_name/ds_scope is usually the dataset the rule is defined on. E.g. if the file is also in different datasets, only one will be given. Also if the rule is on a file directly, none will be given, even if the file is in a dataset. "], "1558": [], "1504": [], "1431": [], "1393": ["Beware that `%2F` is a really tricky character to work with, especially in APIs that include the DID in the URL path itself. For example, `mod_wsgi` always converts these back to `/`. How good is the unit test coverage in this area? ", "Test coverage is bad in this. Essentially we never tested with any / in there (not even the trailing one) as for us the container names (in ATLAS) do not include the trailing /. So we need to add some tests with \"/\" or other special characters in the names. Solution will be most likely that we ask external applications in ATLAS not to submit the trailing /, since the other solutions impact communities using slashes in their names (which is valid). "], "1382": ["Replicas controller (== GET calls) is unused by the Rucio clients. We should deprecate it in the next feature release. ", "It's actually used by the ARC clients, cannot deprecate. Still needs deep-check if there are functionality differences between GET and POST ", "We keep it as it is, but should put comment in code. "], "1351": ["We looked at this and it looks like a pure statically linked binary is not possible, but perhaps we can provide a stripped down RPM that installs this binary. Note though that this will require some additional dependencies (ROOT's libCore, boost, etc) ", "another option would be to add a configuration option that lists the extensions for which GUIDs are extracted. the default could be ``` extractGUID_extensions: [\"*.root\"] ``` which could be set to ``` extractGUID_extensions: [] ``` "], "1304": ["We discussed this couple of times and it's still not clear if this is a bug or not. Can we discuss this on Thursday @cserf @TomasJavurek @bari12 ? ", "Could be an upload of a replica on a different rse (did exists) - This should add a rule on the RSEx "], "1248": [], "1245": ["Hi Alessandra, I think that's possible, but maybe @cserf or @TomasJavurek can comment. ", "Hi, any answer here? It seems that after declaring those 300k files lost 3 times, rucio still thinks it has them [1]. Unfortunately more than keeping on declaring all of them I cannot do in these conditions. thanks [1]  ", "Hi, That's possible. I'll try to implement it next week. Cheers, Cedric "], "1213": ["After checking the code, the necromancer do it the proper way. The problem is actually due to the fact that the ignore_availabilty flag that is in the signature of the list_replicas method is not working, so reassign the ticket to core. "], "1202": [], "1157": ["Hmm, it's technically not mandatory to use alembic to install the database, so looking for the alembic_version might not be the right approach. What we could do is to test if it is there and compare then, I am not so sure if this is really helpful though. ", "It is a real pain to recover from this issues. I would really prefer that there is only one way to install and upgrade the DB. If we want to maintain the ability to install the DB without alembic, you shouldn't be able to use alembic to upgrade the database. On Tue, 22 May 2018 at 15:06, Martin Barisits <notifications@github.com> wrote: > Hmm, it's technically not mandatory to use alembic to install the > database, so looking for the alembic_version might not be the right > approach. What we could do is to test if it is there and compare then, I am > not so sure if this is really helpful though. > > \u2014 > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub >  or mute > the thread >  > . > -- Benedikt Riedel Scientific Programmer University of Chicago Computation Institute ", "We discussed this in the meeting today. We will add a check for this in all daemons on startup. - If the alembic table is present, check if the right schema version is installed, if not fail. - If the alembic table is not present, do nothing for the moment. (This line we will remove eventually) "], "1152": ["Up ", "Some observations I made with a simluated timeout in the API: 1. Using a 60s TimeOut: Apache times out after 60s and returns a 502 Proxy error. ``` <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"> <html> <head> <title>502 Proxy Error</title> </head> <body> <h1>Proxy Error</h1> <p>The proxy server received an invalid response from an upstream server. <br /> The proxy server could not handle the request <em> <a href=\"/proxy/rses/MOCK5\">GET&nbsp;/proxy/rses/MOCK5</a> </em>. <p> Reason: <strong>Error reading from remote server</strong> </p> </p> </body> </html> ``` Strangely this occurred only with postman in my test but not with the client. The client received the correct response. 2. Using a 600s TimeOut: The default timeout in the base client is 600s. The client raises a requests TimeOut exception which does not get catched. I think it would make sense to change the default timeout in the base client to 60s and to catch the TimeOut exception to then raise an own TimeOut exception ? But I could not reproduce a case where the client interprets a request successfully when there was a timeout. ", "Just to follow up on this in the github-a-thon. Could you maybe try to generate a case where HTTP streaming is used? I could imagine a workflow where: - Client and server communicate via python requests with the streaming parameter. - As the header (of a stream) is sent first, it might be a HTTP 200 - During the stream (even if is just one line) an exception might occur and actually kill the DB session. - This could still be interpreted as a HTTP 200, as this was sent in the header first. Now, most POST calls where a timeout matters do not really stream much, but we should check if this can still happen if it is requested via python requests. ", "@mlassnig @TomasJavurek please have a look if we can still do anything with this one. If you thik it's not relevant anymore, please close "], "1114": [], "1109": [], "1091": ["I am a bit concerned that the only info so far is a list of files inside a zip file. In order to see if a file is in a zip file, you`d need to search this table - also when the file is not archived. It seems to me you need a special replica, which is just the name of the zipfile. Then list-replicas would also show the locations of the zip file. Adding files to the zip, would mean adding a special replica to all the content files. Anyway, I`m glad some else has to worry about this stuff. Cheers, Rod. ", "Without entering into all the details, listing replicas without any additional entries is doable. About the format of the url replica for a file contained in a zip, you proposed `<scheme>://zipfile#constituent1` I'm wondering if it's a well adopted semantic for this. like in web browser, etc. ", "We have to see how we do this. Adding a \"special\" replica for each zip-content is a possibility, but this adds workflow complexity to keep everything consistent. I prefer the way we discussed in the dev meeting today by just incorporating the information we already have. The implication there though is performance, as each list-replicas call gets more complicated and we really have to make sure that this doesn't slow down things in general. ", "So, any progress/thoughts? ", "For #46 @TomasJavurek will work on this, no update yet. The transparent list-replica support we discussed and we think it should be fine with the information which is already there, without creating some fake replicas. Which would be better for consistency reasons. Essentially, when you list the replicas for a file the information that it is also a constituent in a zip file is there, thus we can list the parent replicas then. It might have some performance implications for the general workflow, so this needs to be done carefully. We didn't discuss a development plan specifically, but I was hoping that @vingar could work on this? Maybe he can comment? ", "Hi @rodwalker, So we discussed the timeline of the the transparent archive support (list-replicas of files also exposes parent archives) and we are aiming to have this by end of June (Hopefully in the 1.17.0 release). ", "Just to follow up on the results so far (There will be a presentation in the ATLAS S&C week this Thursday about this as well): - `rucio download <constituent> --archive-did <archive>` issues an xrdcp streamed download of the constituent in the archive; However, the archive has to be named specifically for this to be possible. - list_replicas has been adapted if you do list_replicas <constituent> that it also outputs the archive with the streaming option. This should enable the client to do a download of a constituent transparently. This will need an update of root as far as I know (@mlassnig ?), as the streaming is currently only possible via xrdcp and not gfal. What is not there: - `rucio add-rule <constituent>` makes a rule on the archive. This will be discussed on Thursday. - With the list_replicas change @mlassnig did, if the constituent is not on a root enabled storage, the replica dictionary has a `client_extract=True` flag, which tells the client to download the archive and extract the file. This functionality needs to be implemented. (@TWAtGH or @TomasJavurek ?) ", "I have updated the missing features/bugs to the overview on the top. Please comment here in this ticket if anything else is missing. ", "I think we can remove the clients label from this issue @bari12 "], "1026": [], "1021": [], "857": [], "781": ["For 1. this needs a little bit of work as the error is not propagated to the locks. It's only the last error stored at rule level. 2. this information is in the requests_history, just needs to be added to the dictionary. "], "736": [], "713": [], "704": ["Up "], "689": [], "630": [], "609": ["More info:  ", "@bbockelm Won't the workflow be similar with openId ? Something like: Client, rucio auth servers, ext auth. service (openId/scytoken) ask token --------> forward token request -----> token request store token <---- store token in db and answer with it to the clients <---- give token then interacts with Rucio & co ", "Not quite, because the Rucio server itself will be the issuer. If Rucio allows you to get an X-Rucio-Auth-Token you can request a SciToken instead (with the appropriate domains set based on VO policy). ", "in X-Rucio-Auth-Token you expect to have the SciToken one ? ", "The client can request it in addition and use it to go to storage (If the storage allows the domain). We can also think to use it as a replacement for X-Rucio-Auth-Token with a custom Rucio-Auth domain. ", "I'd suggest starting out by having them alongside each other (to understand how the library works), then eventually replacing `X-Rucio-Auth-Token`. ", "The clients will have to store both tokens and put in addition the sciToken one in the request header `Authorization: Bearer <token>` for the next interactions (rucio, storage, etc). Am I correct ? ", "Yeah - at the beginning, one would just use the SciToken for interaction with storage. In fact, this is a strong argument for keeping multiple tokens -- you don't want the storage authorization token to additionally interact with Rucio. So, your Rucio token would gain a new format (which is convenient because it doesn't have to be serialized by Rucio) and you'd still separately handle a storage-related token. While Rucio and the storage service would need to understand how to verify / parse the token, the client can continue to treat it as an opaque string. "], "595": [], "583": [], "536": ["During Rucio Community Workshop 2020 this was also identified by DUNE as being very useful to handle rules on larger datasets/containers. ", "+1000 ", "+1M Finally a proper alternative of updating \u00b4updated_at\u00b4 :) "], "534": ["The docstring headers are not handled properly by sphinx for the API documentation. The clients source files will be updated to: ``` # Copyright (c) 20XX-2018 CERN for the benefit of the ATLAS collaboration. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # You may not use this file except in compliance with the License. # You may obtain a copy of the License at #  # # Authors: # - ... ``` ", "Back to block comments ;-) Correct dates according to git are 2012-2018. I'd also modify the first to lines to be a bit more explicit (no need for (c) for example) ``` # Copyright 2012-2018 European Organization for Nuclear Research (CERN) # For the benefit of the ATLAS Collaboration # ```... (also makes it nicer to modify the second line later on, if necessary) ", "yes, back to block comments and we can hope that pylint will offer a way to mute the missing docstring at the top of the file I was thinking having the starting date based on the file creation date. According to  The format must be: (c) Copyright [year] CERN [for the benefit of the [Name of appropriate group] Collaboration] ", "Right, then that's how it is ;-) ", "this one is not true everywhere ... ", "Ah I thought with your recent patches you iterated all files. ", "only the ones I've modified to get the doc generation working "], "528": [], "527": [], "526": [], "485": ["However, this does not add a second leading slash when supplied with one /. Considering that all CMS datasets are /primary/secondary/Tier this won't impact CMS. ", "yes, I knew this but this was the best option I quickly managed to come with to satisfy the CMS naming convention. The proper behavior here would have been to reject the name as all CMS names start with a slash. ", "Which my python API did for something that did not start with / but the CLI did not for some reason. Anyhow, this is low/no priority for CMS, but may be useful to keep around. "], "479": [], "461": ["The `verify=False` I vaguely remember was a fix due to a temporary issue, but I might be wrong. Maybe @mlassnig remembers. ", "verify=False: Indeed, this looks like a remnant. For the session handling, yes, you're right. For the delegation, this is done via a periodic cron outside the conveyor. We didn't want to couple this as a failure of one could affect the other: - tools/delegate_proxy_fts.py (btw, once FTS3 releases their CLI which can set the delegation lifetime longer than 4 hours this script can be made much more simple. With this script we get a safety buffer of 96 hours.) ", "@mlassnig - we really need to come back to this one (but, unfortunately, I don't think I'll personally have time to tackle it). `verify=False` is bad news. ", "We need to add a separate ca_cert chain per FTS host (eg. BNL hostcert vs UKeS vs CERN). Shouldn't be too difficult. I'll have a look next week. ", "I'm not entirely sure that's necessary - current version of the `requests` module (really, it's probably `urllib3` that does this...) allows you to point at a directory (such as `/etc/grid-security/certificates`). Additionally, even older versions allowed multiple CAs per file (simply concatenate them all together). In other projects that use clients that don't understand directory-based CA layouts, we've just written scripts to do the concatenation at startup. ", "That makes it even easier then. Thanks for the heads-up. ", "Hi, can you take a look at (*) and see if that can be of any use in this regards? I mean that as a starting point to get your feedback on how to proceed for the integration of fts delegation, if that can be useful. (*)  ", "I have pushed this part (*) of the changes needed for transfers in CMS, but that is more general for FTS proxy delegation. I splitted it because I think that can be safely introduced and utilized for other purposes not CMS related. So I was thinking that for the other changes, more CMS specific, I could open an issue to better follow up without delaying further the proxy delegation stuff. How does it sound @bbockelm @mlassnig @vgaronne @bari12 ? (*)  ", "Hi Diego, I only had a quick look (will look more in detail later), but it looks good to me like that. I would propose you open a separate issue (even for the proxy delegation) and submit the PR already to rucio so we can comment there. No need to just keep it in your private repo. ", "Agreed, I'm doing that. Thanks "], "421": ["@vingar mentioned that there is already signed URL support in Rucio and will be updating the ticket with some example code. Additionally, here is some sample code for getting a macaroon from a dCache endpoint:  ", "Creating bearer tokens is already the basic authn/authz scheme in Rucio, where we delegate the verification of the requestor to separate entities (Kerberos Servers, VO CAs, SSH pubkeys, ...). Creating Rucio-issued bearer tokens (e.g., based on the SciTokens proposal) which can then be given to rucio clients and transfertools should be doable by adding a new token generator which takes account, identity, and optionally RSEs as input. (In case a challenge-response handshake is needed, there's a workflow in place as we need to do this for SSH-based auth already and which can be adapted.) This could even supersede the X-Rucio-Auth-Token. To auth with storage, generating signed URLs follows naturally from this, as instead asking a third party to sign the URL Rucio could do it internally by passing the optional RSE to the token generator. Few things to take care about: - thrash rate on the token table - length of the bearer tokens - attributes of the bearer tokens which are not part of the token itself ", "Length can be a legitimate problem. Apache's limit tends to be around 4KB. A narrowly scoped token (including a full PFN) would be around 1KB. So, if you issued a token to download a specific file, it would not be a problem. The problem would crop up if you wanted to do something like a token for a long list of files -- in such a case, you'd probably have to do a more coarse-grained scope (e.g., at the directory level). Since the tokens are signed with a public key, they don't need to be recorded in a manner analogous to the existing table. It is a good idea to have a log activity of tokens recently (no need to record the tokens themselves) issued for auditing purposes, but that's more a log file than a DB table. For attributes - for the pure file transfer use case, that's fairly well specified already. If we wanted to include internal Rucio authorizations, you'd probably just embed the authorization name in the `scp` (scope) claim. ", "Scheduled for 1.19.1 "], "372": [], "366": ["2017-12-15 11:16:31,919 auditor-worker DEBUG [PID 21268] Checking \"PIC_MCTAPE\" 2017-12-15 11:16:33,957 auditor-worker ERROR [PID 21268] Check of \"PIC_MCTAPE\" failed in 0 minutes, 1 remaining attemps: (GError: srm-ifce err: Communication error on send, err: [SE][Ls][]  CGSI-gSOAP running on rucio-daemon-prod-02.cern.ch reports Could NOT import client credentials How auditor can suffer from 'erroron send'? "], "354": [], "112": ["Needs to be discussed with Paul ", "Try to figure out where the earliest possible exit in the codepath is. ", "For the upload one of the first things we do is query the rse information:  So directly afterwards we could check if any protocol implementation is available "], "57": [], "54": ["For combining weights we could do some kind of simple language. eg. .1freespace+.9network "], "53": ["We could apply the same logic Thomas das for monitoring to asynchronously prefill the sources in WAITING state. ", "Needs to be evaluated; Maybe use Thomas algorithm for monitoring. "], "43": [], "38": [], "36": ["scope:name for fake_dataset: other:other ", "I checked the code, and it looks like what is done in the conveyor is OK. But, there might be a problem if the file has a parent DID with the wrong number of `.`. It will be fixed. In addition this part is very ATLAS specific : ``` # DQ2 path always starts with /, but prefix might not end with / naming_convention = rse_attrs[dest_rse_id].get('naming_convention', None) dest_path = construct_surl(dsn, name, naming_convention) ``` We should have a way to have different conventions for different collaboration. "], "35": [], "31": []}