{"3395": [], "3388": [], "3383": [], "3377": ["I think that's a good idea. Not the highest priority though \ud83d\ude04 "], "3365": [], "3360": [], "3350": [], "3347": [], "3346": [], "3340": [], "3311": [], "3283": [], "3261": [], "3260": ["The ctrl+c is already implemented in the downloadclient:  https://github.com/rucio/rucio/blob/master/lib/rucio/client/downloadclient.py#L388  but there it is easy since it is multithreaded. If I understand correctly, we want to apply something similar for the uploadclient which is not mult-threaded. What about the other clients? Do we need it there? "], "3254": [], "3250": [], "3249": [], "3235": [], "3200": [], "3199": [], "3164": ["Yes the 12h default timeout is too short, given the fact that we expect to always do bulky staging submissions to sites.    A longer default timeout setting in FTS, plus respecting the timeout passed by client (e.g. rucio), may just do it.   Hope it helps,  Xin  ", "Since we are discussing this also via mail:  We can increase this by changing the rucio.cfg `bring_online` setting, so we do not need to change the default in the Rucio code as well."], "3148": [], "3147": [], "3135": [], "3124": [], "3115": ["Adding a `None` ssh identity (in an attempt to avoid using default values in the bootstrap section) also results in padding error."], "3103": [], "3101": ["That is because traces were not meant to be used as return values for some time. They were just the data for the monitoring system. Actually I don't like that we start to expand this even more.  Some time ago I discussed another approach with @bari12 were it would be possible to set callback functions for different steps of the download client. E.g., one for failed downloads, one for successful downloads, one for failed attemps...  For each event the download client would call this function and give the user the opportunity to react to the event.", "This needs some more discussion next week. I am not sure if the callbacks alone solves this well enough.", "In meanwhile, we discussed with Tobi whether current downloadclient is threadsafe or not. The problem is in the list reference that I created in pilot for adding the traces:  https://github.com/rucio/rucio/blob/master/lib/rucio/client/downloadclient.py#L454    According to the following article about locking:  http://effbot.org/zone/thread-synchronization.htm    appending to a list is threadsafe operation. "], "3097": [], "3096": [], "3084": ["Tests performed on the latest version. All tests have been performed on CERN-PROD_SCRATCHDISK and UNI-FREIBURG_SCRATCHDISK  1) upload with registration in several attempts  2) upload without registration to the same rses  3) upload of existing file that is not registered  4) upload of a file that is already registered  5) upload of non-existing file  All these tests performed well. Although it is not all we should be checking, e.g. uploading corrupted file, uploading to not-existing rse etc.", "Following the comments from @TWAtGH I dropped the summary string form the NotAllFilesUploaded exception - #3158     The suggestion from Tobi is to add attributes directly to the exception:  https://github.com/rucio/rucio/blob/master/lib/rucio/common/exception.py#L847  whether as a dict or standalone attributes corresponding to the quantities in the summary, needs to be discussed and decided."], "3076": [], "3042": [], "3031": ["You skipped the 'modifications' section in the description :P  So how should this be resolved?  Is it related to #1393 ?", "Using urllib.quote_plus() for all but containers?  ", "I don't think thjis is related to #1393 - There it is specifically if the trailing `/` is mentioned in the did name, like ATLAS prodsys was doing this in the past. Here the error must be something else. ", "I think this is rather an list_replicas issue then. But if the physical filename or the dataset name contains a \"/\" it will be problematic because it's the directory separator. We could change the download client to replace it by another character or create a subdirectory for each \"/\" but I think the best thing would be not to have \"/\" in the LFNs :sweat_smile: ", "Creating subdirectories is fine. This is in fact how these files are stored on storage. The LFN just includes these structural directories. There is nothing wrong with having `/` in the LFN.  I would suggest we just create subdirectories. @TWAtGH can you please try and confirm the exact issue. If it is in list_replicas then this goes over to @mlassnig - but we should narrow down what the actual issue is.", "Looks like the download is already implemented like this. But it's difficult to try because the upload doesn't support this. I assume that most client functions that do an REST API call won't work using a DID containing a slash.  A little bit more info about the exact issue would be nice @cserf and how can I create a DID containing a slash?  Also how would double slashes be resolved? Just a single directory I guess.", "Ok in fact it was the server which rejected the upload due to a missing `AllowEncodedSlashes On`  But the server still don't allow to insert slashed DIDs. I think it happens in the `add_replicas` call:  ```  2019-11-19 17:25:17,488\tDEBUG\tFile DID does not exist  2019-11-19 17:25:19,032\tERROR\tProvided object does not match schema.  Details: Problem validating dids : u'slash/lfn01' does not match '^[A-Za-z0-9][A-Za-z0-9\\\\.\\\\-\\\\_]{1,250}$'    Failed validating 'pattern' in schema['items']['properties']['name']:      {'description': 'Data Identifier name',       'pattern': '^[A-Za-z0-9][A-Za-z0-9\\\\.\\\\-\\\\_]{1,250}$',       'type': 'string'}    On instance[0]['name']:      u'slash/lfn01'  ```", "Yes, you have to try this with a schema which allows slashes. Eg. the `belleii` schema.", "@TWAtGH is there any news on this? ", "Traceback :  ```  rucio -v download test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst  2020-02-11 05:54:32,806 INFO    Processing 1 item(s) for input  2020-02-11 05:54:32,806 DEBUG   num_unmerged_items=1; num_dids=1; num_merged_items=1  2020-02-11 05:54:32,806 INFO    Getting sources of DIDs  2020-02-11 05:54:32,806 DEBUG   schemes: None  2020-02-11 05:54:32,806 DEBUG   rse_expression: *\\istape=true  2020-02-11 05:54:32,806 DEBUG   num DIDs for list_replicas call: 1  2020-02-11 05:54:34,649 DEBUG   num resolved files: 2  2020-02-11 05:54:34,655 DEBUG   \"unzip -v\" returned with exitcode 0  2020-02-11 05:54:34,661 DEBUG   \"tar --version\" returned with exitcode 0  2020-02-11 05:54:34,661 DEBUG   num list_replicas calls: 1  2020-02-11 05:54:34,661 DEBUG   Queueing file: test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst/bf55cdc6f73e473396c3476ca7047384  2020-02-11 05:54:34,661 DEBUG   real parents: set(['test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst'])  2020-02-11 05:54:34,661 DEBUG   options: {'test:/grid/belle/ddm/functional_tests/release-0x-0y-0z/DB00000xxx/2020-02-11/MCyy/26903/dst': {'ignore_checksum': False, 'transfer_timeout': 3600, 'destinations': set([('.', False)])}}  2020-02-11 05:54:34,663 DEBUG   Traceback (most recent call last):    File \"/usr/bin/rucio\", line 159, in new_funct      return function(*args, **kwargs)    File \"/usr/bin/rucio\", line 975, in download      result = download_client.download_dids(items, args.ndownloader, trace_pattern)    File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 280, in download_dids      input_items = self._prepare_items_for_download(did_to_options, merged_items_with_sources, resolve_archives=resolve_archives)    File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 1158, in _prepare_items_for_download      paths = [os.path.join(self._prepare_dest_dir(dest[0], dataset_name, file_did_name, dest[1]), file_did_name) for dest in destinations]    File \"/usr/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 1395, in _prepare_dest_dir      os.makedirs(dest_dir_path)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs      makedirs(head, mode)    File \"/usr/lib64/python2.7/os.py\", line 157, in makedirs      mkdir(name, mode)  OSError: [Errno 13] Permission denied: '/grid'    2020-02-11 05:54:34,663 ERROR   [Errno 13] Permission denied: '/grid'  2020-02-11 05:54:34,663 ERROR  Rucio exited with an unexpected/unknown error.  ```"], "3023": ["@cserf: The columns are added to the table by @jwackito You can add the filling of the values to the poller. ", "transfertool/fts3.py bulk_query method wasn't querying staging info, that's why staging_start and staging_finished columns in the requests were not populated...", "@jwackito this still doesn't seem to work. We have to check if we are losing this info somewhere in the workflow or if we just don't get the value from FTS. @jwackito please try to debug this on one of the integration pollers."], "2983": [], "2973": [], "2961": [], "2926": [], "2915": [], "2881": ["Thanks for the report. I'm having a look."], "2830": [], "2827": [], "2770": [], "2714": [], "2706": [], "2703": ["For what it's worth, I would also find this very useful, so I'll add my support!", "This would indeed be very useful and much appreciated."], "2686": ["i wonder what the most appropriate way to do this is. We can basically do a json dump of the post/put data to the logfile or even ES. It still wouldn't be in the apache access log, but it would be easy to correlate the access with the line in the logfile/ES.", "dumping the request json to the logfile seems the right thing to do to me. i've had a look at the different parameters in the mod_wsgi documentation, but there's just no way you can write to the access_log. we can however dump the access itself also in the error_log, and silence the access_log, so everything would be in a single file?"], "2639": [], "2637": [], "2636": [], "2635": ["1st part #2500   2nd part #2553 ", "Main schema change - #2727 "], "2634": ["Yet another non-existing web page referred to in \"rucio list-rses\" help for --expressions option:  http://rucio.cern.ch/client_tutorial.html#adding-rules-for-replication  "], "2631": ["[Google doc](https://docs.google.com/document/d/1ZJwmP971SGo1X3lS8gbJEMrpgQU0FzEUiJF01yK68MA/edit#heading=h.3lnruokyb4eg) to follow this activity.", "Also related to #2630 "], "2630": ["[Google doc](https://docs.google.com/document/d/1ZJwmP971SGo1X3lS8gbJEMrpgQU0FzEUiJF01yK68MA/edit#heading=h.3lnruokyb4eg) to follow this activity."], "2621": ["Is it still valid ?", "I don't think it is valid. There is a loop on activity, so I don't understand what's wrong.  Martin, can you double check ?", "I think the issue is that `activity` is not passed through to `get_next`  It loops over the activities but does nothing with it. "], "2613": ["See also https://github.com/rucio/rucio/issues/2310", "Please link from a menu tab, as I only find the url in gmail. I expect my earlier comments are still relevant."], "2582": ["I think we discussed this on Slack. I'll make a patch for this", "Hi Eric, this is still an issue. Did you figure out why this is happening?", "Oh, I've complete forgotten about this. And I don't see a patch in CMSRucio for it (where the script resides). I'll at least make an issue there.   "], "2543": [], "2542": ["There should be also an easy way to configure the throttler in the config table .  For example there could be a throttler mode in the config like `throttler_mode`:  - throttler_mode, SRC_PER_ACT (to throttle per source RSE and per activity)  - throttler_mode, DEST_PER_ACT  - throttler_mode, SRC_DEST_PER_ACT  - throttler_mode, SRC_ALL_ACT  - throttler_mode, DEST_ALL_ACT  - throttler_mode, SRC_DEST_ALL_ACT (to throttle per source and destination RSE and over all activites)    Then each RSE would need a threshold value in combination with the mode to allow switching between modes like:  - 'All Activities, DEST_ALL_ACT, MOCK', 10000  - 'User Subscription, SRC_DEST_ALL_ACT, MOCK', 20000    This would add the modes to values in the rse_transfer_limits table.    Also each RSE should be configured to use FIFO or grouped FIFO depending on the mode:  - 'DEST_ALL_ACT, MOCK', FIFO  ", "Yes, we would need some kind of mode specifier in the table. I wonder if this should be an additional column though.", "Also there are some special cases to be considered  - if the throttler uses destination and source, it could be that the destination RSE is configured to use FIFO and the source RSE grouped FIFO, then its not clear what to use  - if the throttler uses destination and source, it could be that a RSE is used as source and destination, then the configuration would also need a field to specify that like `'All Activities, DEST_ALL_ACT, MOCK, DEST', 10000` if MOCK is used as destination RSE", "> Yes, we would need some kind of mode specifier in the table. I wonder if this should be an additional column though.    @bari12 You mean in the rse_transfer_limits table? I thought that the throttler can only have one mode, so I would set the value only once", "Yes, in `rse_transfer_limits`. It's just not super nice that all the \"configuration\" is part of a long string. For some things it's not possible otherwise, but we should try to put as many things as possible into separate columns.", "Ah yes makes sense. We should be maybe also check why the throttler thresholds are stored in the config table. I think the ones in the `rse_transfer_limits` table are unused and it would be better to use them and then to use seperate colums, like you said", "I think this schema for `rse_transfer_limits` would fit.    | RSE ID        | Activity           | Max transfers  | Transfers | Waiting | Direction | Mode | Volumn | Deadline  | ------------- |:-------------:| -----:|-----:|-----:| -----:|  -----:| -----:| -----:|  | MOCK      | ALL | 10000 | 10 | 10 | DEST | FIFO | 10000 | -  | MOCK      | User Subscription      |   500 | 10 | 10 | SRC | FIFO | - | -  | MOCK      | User Subscription      |   500 | 10 | 10 | DEST | GFIFO | 500 | -  | MOCK      | User Subscription      |   500 | 10 | 10 | SRC | GFIFO | - | 3      Then depending on the throttler mode the thresholds and the release mode can be read out, e.g the mode `DEST_PER_ACT` would read the third row, `DEST_ALL_ACT` would read the first row and `SRC_PER_ACT ` would read the second row  ", "Yes, I think that would work. (Although Volum**e** instead of Volum**n** \ud83d\ude04     Right now there is also a column `rse_expression` in the table. I am not quite sure why, maybe this was just put there in preparation when the throttler was created.", "In the new implementation the throttler lost the ability to set a limit for `all_rses`, only limits for individual rses are possible now. This needs to be added again."], "2517": [], "2459": ["Is this a possibility? ", "Yes, this shouldn't be too complicated to do for the `deep=False` case. For the `deep=True` case it is a bit more complicated, as several tables are joined here, but I assume that this one is not relevant for you anyway.    I think we can extend the API with giving a list of scope/names, or add a new call - but it wouldn't allow the `deep=True` case.", "And what's the difference between deep=True and deep=False?", "There is a table storing all dataset replicas (called `collection_replicas`) which is queried when doing the query with `deep=False` - this is a very efficient query (just a PK lookup)  With `deep=True` this is not resolved via the collection_replica table, but the result is generated based on the location of every single file replica in the dataset. This is an expensive query, which is not needed in general, as mostly you create replication rules on the datasets you are querying for. But there are certain workflows which do not result in a collection replica being created for the dataset (as no rule was created for the dataset) in this case `deep=True` must be used.    Just to illustrate a case where this might be necessary.   You have a dataset `A`. 50% of the files of dataset `A` are in dataset `B` and 50% in dataset `C`. If you don't create a replication rule for dataset `A`, but only for `B` and `C`, no collection replica was created for dataset `A`. For the simple query (`deep=False`), it will look like that the dataset has no full replica there, although all files actually have a replica. In that case the deep query needs to be used.", "I see, thanks for this explanation. Indeed I believe we won't have need for using deep=True, but Eric knows those use cases better than me.", "@bari12 Something on our setup is not working correctly, I think. Even making a rule directly on a dataset, we don't really get replicas without the deep flag. From the CLI    ```  # rucio list-dataset-replicas \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\"    DATASET: cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396  +----------------------+---------+---------+  | RSE                  |   FOUND |   TOTAL |  |----------------------+---------+---------|  | T2_US_Wisconsin_Test |       0 |       0 |  | T2_DE_DESY           |       0 |       0 |  | T2_US_Nebraska_Test  |       0 |       0 |  +----------------------+---------+---------+  # rucio list-dataset-replicas --deep \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\"    DATASET: cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396  +----------------------+---------+---------+  | RSE                  |   FOUND |   TOTAL |  |----------------------+---------+---------|  | T2_US_Wisconsin_Test |      20 |      20 |  | T2_DE_DESY           |      20 |      20 |  | T2_US_Nebraska_Test  |      20 |      20 |  +----------------------+---------+---------+  ```    While the rules are on the dataset:    ```  # rucio list-rules \"cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396\"  ID                                ACCOUNT          SCOPE:NAME                                                                              STATE[OK/REPL/STUCK]    RSE_EXPRESSION          COPIES  EXPIRES (UTC)    CREATED (UTC)  --------------------------------  ---------------  --------------------------------------------------------------------------------------  ----------------------  --------------------  --------  ---------------  -------------------  d0d3e85fc1954b51b5b623e4fadea08b  root             cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396  OK[20/0/0]              T2_US_Nebraska_Test          1                   2019-04-25 21:17:13  18548137e8a745a09262c227a842c655  root             cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396  OK[20/0/0]              T2_US_Wisconsin_Test         1                   2019-04-25 20:37:59  1ba5e5caf72a46b4bc8b7ea5b17ae40a  sync_t2_de_desy  cms:/SingleElectron/Run2017F-17Nov2017-v1/MINIAOD#f924e248-e029-11e7-aa2a-02163e01b396  OK[20/0/0]              rse=T2_DE_DESY               1                   2019-04-18 09:02:49  ```    Are we missing a trigger or procedure in Oracle that is supposed to fill in the collection_replicas table?  ", "Yes, there is the `COLL_UPDATED_REPLICAS` procedure in https://github.com/rucio/rucio/blob/master/etc/sql/oracle/procedures.sql  But this works with virtual scopes which Gancho set up (Basically there is a virtual scope for MC, one for DATA, one for Panda and one for everything else. (Would need to check with Gancho how to exactly setup these virtual scopes in oracle)    But alternatively you can also use the `abacus-collection-replica` daemon https://github.com/rucio/rucio/blob/master/bin/rucio-abacus-collection-replica  This one does the same as the oracle procedure.", "After I tried posted this, I found that daemon. Running it \"does not work\" in that this line https://github.com/rucio/rucio/blob/5c0258ba5064029f796c5c3d79681826dbae1778/lib/rucio/core/replica.py#L2371 basically takes minutes and consumes >2.5 GB of RAM and causes not just my pod but my k8s node to crash.    Maybe because this is because we've got a lot built up there and have never done this? Is there an easy way to limit it?", "Yes, the daemon really seems to rely on the table being small.  Possibly easiest would be to do a cleanup with the PL/SQL first. I have a version without the virtual scopes, which you could just execute once. Let me check", "Hi @bari12 , getting back to the original issue. In case you don't need any further information from my side, would you have a rough ETA for considering this ticket? Even by the end of 2019 would be helpful ;)", "Hi @amaltaro   I can't give you an exact ETA, but it should be before the end of 2019 :-)  This is not a difficult development, I just need to find time for it, or find someone who has time to do it \ud83d\ude04 "], "2417": [], "2414": ["Config file was changed according to David's proposal. Documentation needs to be done still. "], "2410": ["Alternatively, a json column? I don't like string parsing too much"], "2393": ["Hi Rod,  At one point we were discussing of looking into third_party_copy support of zip constituents. xroot/FTS would obviously have to support that. But if this is the case, we could essentially create the replicas via transfer requests and just nee some logic around that.  The timescale of this is a different question though."], "2387": [], "2356": [], "2330": ["For the last point: There are many scripts in there which do not belong in our core repo. We should think about collecting the useful ones in a rucio/tools or rucio/utils repo. Something to discuss today."], "2325": [], "2318": [], "2312": [], "2311": [], "2310": ["- Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again.    - Add a column which gives the reason of the failure if all accesses fail with same error. I am interested in 'Checksum issue' since it does not require further check (in contrary to problem to access file)    ", "- When files are declared lost/corrupted, we  reach a page almost empty. It would be usefull to have a button to go back to the same configuration (period, type) to continue to check and declare other files. ", "All the requests can be implemented relatively easily but this one \"Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again.\". It will require some significant changes. I need to evaluate how much work will be needed. "], "2282": [], "2264": [], "2263": [], "2258": ["  The problem is with the value '1.0' that is not handled properly by `tabulate` :   ```  >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1.0')]  >>> print(tabulate.tabulate(table, tablefmt='plain'))  Traceback (most recent call last):    File \"<stdin>\", line 1, in <module>    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.5/lib/python2.7/site-packages/tabulate.py\", line 1301, in tabulate      for c, ct, fl_fmt, miss_v in zip(cols, coltypes, float_formats, missing_vals)]    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.5/lib/python2.7/site-packages/tabulate.py\", line 768, in _format      return format(float(val), floatfmt)  ValueError: could not convert string to float: True  ```  vs :  ```  >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1')]  >>> print(tabulate.tabulate(table, tablefmt='plain'))  TAIWAN-LCG2_TAPE-STAGING:  True  is_stagingarea:            True  stresstestweight:             1  ```    or :  ```  >>> table = [(u'TAIWAN-LCG2_TAPE-STAGING:', 'True'), (u'is_stagingarea:', 'True'), (u'stresstestweight:', '1,0')]  >>> print(tabulate.tabulate(table, tablefmt='plain'))  TAIWAN-LCG2_TAPE-STAGING:  True  is_stagingarea:            True  stresstestweight:          1,0  ```     ", "Sorry forgot to mention that I've debugged it already to this point. But I won't have time to look further into it ( @bari12 )", "@nikmagini @TomasJavurek can you please have a look on this issue. (See @cserf comment)", "BTW, the `Strange error: No section: 'policy'` will be fixed here :  https://github.com/rucio/rucio/issues/2514"], "2254": ["Testing a xrootd implementation:  [root@rucio-nagios-prod-02 rse]# rucio-admin rse delete-protocol --scheme root RAL-LCG2-ECHO_SCRATCHDISK  [root@rucio-nagios-prod-02 rse]# rucio-admin rse add-protocol --hostname xrootd.echo.stfc.ac.uk --scheme root --prefix /atlas:scratchdisk/rucio/ --port 1094 --impl rucio.rse.protocols.gfal.Default --domain-json '{\"wan\": {\"read\": 2, \"write\": 2, \"third_party_copy\": 2, \"delete\": 2}, \"lan\": {\"read\": 1, \"write\": 2, \"delete\": 2}}'  RAL-LCG2-ECHO_SCRATCHDISK  "], "2232": [], "2218": [], "2216": ["How many files is this list long?", "Just following up on old tickets: The chunking should be done on the auditor side. The list_replicas method here is a core function, which should not need protection from other core/daemon calls - only on the API level. Thus please chunk this in the daemon itself."], "2213": [], "2159": [], "2143": [], "2104": [], "2063": [], "2029": ["@mlassnig please have a look if this is still relevant; Close if not."], "1923": [], "1911": ["Note that importing the client at the top of the `rucio` CLI causes an authentication to be attempted even when you're just doing `rucio --help`.", "That doesn't seem to be problem only with importing downloadclient but Clients in genereal. The config is imported in baseclient. ", "Just importing the module should not read the config? Only when constructing the client object; Then indeed it does read the config and even trys to authenticate against the rucio server.  @TomasJavurek can you please check the code path why this is the case even when doing `rucio --help`? In my opinion no client object needs to be constructed in that case."], "1895": [], "1867": ["### Example  On an RSE there are 100 TB free, dataset `A` is 10 TB and is already available on the RSE, dataset `B` is 5 TB and not available on the RSE, container `C` contains `A` and `B`. The e-mail sent when a user requests to replicate `C` should report that the expected free space after the approval is 95 TB. Currently, the e-mail will report 85 TB."], "1860": [], "1839": ["This should be consolidated. I would not be too afraid of backwards-compatibility, but maybe we should still add the new parameter, leave the old one in, but if it is used display some kind of message. A few (feature) versions later we then take it out. We used this concept already for several other CLI changes.", "Much of it is historical due to the API itself, but we can start changing the clients and bring the API, REST, and core slowly up to speed. At least for the RSE parameter I would suggest for conciseness to always use `--rse` but always parse RSE expressions, and get rid of the `--rse-expression` or `--expression` extra arguments.", "As rse expression can give a back a list, i.e. `--rses` looks more appropriate imo. starting with the same name for both `rse-expression` and `expression` would be already a good step.", "Sounds good to me:  - `--rse` if it is really only 1 RSE  - `--rses` if it is an expression. Although --rses suggests a bit that you can enumerate them, which is not really true (`RSEA|RSEB|RSEC` would work but not `RSEA,RSEB,RSEC`)", "But shouldn't we handle single RSE or multiple RSEs automatically? `--rse` is a basically a subset of `--rses` and having two different parameters is confusing.", "We would have to go through it, in most cases `--rses` is ok. But in cases where you really only want one RSE, e.g. marking a replica as bad/lost, --rses doesn't really make sense, as it suggests a list, which doesn't work.", "I would do `--rse` everywhere, and in case a list comes back instead of a single RSE fail to the client and tell \"hey you selected more than one\", instead of having two parameters. (In a way, like SQL cursors)", "ah but are you proposing to always resolve RSE by expression ? Most of the time the RSE expression corresponds to the RSE name. It might be worth to check the implementation as this will be the most popular call then...", "From a performance that should be fine, the expression is cached server-side, so it's not even going to the DB too much.", "it's worth to check that even without the caching,  it does a simple lookup first against the rse table. Not sure the caching is enabled if memcached is not there and the memoization fallback will be most likely in memory per thread/process, which won't reduce that much the query rate."], "1834": [], "1830": [], "1829": [], "1817": ["It seems that it is not possible to catch exceptions inside the streaming.  To reproduce, run this minimal flask server. The exception gets raised on flask side but does not get catched. An internal server error is generated instead of the 'error' string.    ```python  from flask import Flask, Response, jsonify    app = Flask(__name__)    @app.route(\"/\")  def hello():      def generate():           for i in range(0, 100):                 raise Exception              yield '1' + '\\n'                try:          return Response(generate())      except Exception as error:          return jsonify({'error': error})    if __name__ == '__main__':      app.run(passthrough_errors=True)  ```    ```  FLASK_APP=file.py flask run  ```    One solution would be to rework the logic and check for possible problems like non existing account before querying the database.   Another solution would be to call the same function inside a for loop but with a break after one loop, where we check for exceptions. Afterward the streaming can start if there was no exception catched.", "To me the accurate path looks like the one where we change the logic (and checking) of the APIs which use streaming. Thus we do all the conflict checking first and then just iterate the stream.    Conceptually, the error handling we do right now with streaming is wrong and possibly leads to malformed replies. Because the HTTP return code is calculated (and sent!) at the header anyway. So let's say this is a `200`, whatever you stream afterwards (and throws an exception) is not properly reflected in the error code.    But this will take time, we probably have to more carefully select which APIs we offer as a streaming API (list-contents, list-replicas, ...) and not be so generous like we are now with them.", "Worth mentioning here to think about proper support for HTTP Chunked Encoding."], "1808": ["Just a small note as I've been working with LIGO to utilize OSG and EGI-based computing centers for multiple years now.  While \"proprietary HPC clusters\" characterizes at least one LIGO resource, there's actually a wide range of resources available.    Regardless, that's neither here nor there.  The rest of the post looks good!", "Hi Brian,    Thanks for the head up.  Indeed I was just intending to highlight the differences between our two computing infrastructures, not to be \"detailed\"! :)  Cheers!    Gabriele", "Hi guys, fascinating discussion, this is something that we have mulled in @EGI-Foundation for a bit as well. It would be out of place to make sweeping statements about DIRAC without the developers involved, so maybe this issue could be pointed out to them if that hasn't already been done.    My 2c is that there there are two patterns right now in developing these platforms       1. Build a core product, discover need for some other functionality, tack it on, ???, Profit!!!    1. Build a core product, discover need for some other functionality, **set up a contract with another set of services** to do that, ??? Profit    We have often looked at DIRAC as an HTC solution, but it's way more than that and just using it as an HTC solution is actually quite hard. I hazard to say that it works best when it's the _primary_ interface for users and applications.    Rucio on the other hand is (forgive me for projecting my own perception here), a fantastic data management system. It could (is?) tack on compute management as well. As a product, we (say, EGI), would like it to interoperate with other services like cloud compute, HTC, HPC, etc, via stable APIs and do it's data management thing.     It would be nice to know if Rucio could be used as a drop-in replacement data catalogue for DIRAC, and more interesting to know if DIRAC could be used as a drop-in compute orchestration service for Rucio. My personal feeling is that something that does compute orchestration only would be a better fit (maybe, HTCondor, I don't have a great answer here, sorry).    Thanks!  (usual disclaimer of \"these opinions are mine and mine alone\", \"this does not represent the position of EGI, EGI Foundation _etc_\" apply here :wink: )", "Hi Bruce,    I am pointing some DIRAC people to this issue!  Cheers,    Gabriele", "Hi,  I am DIRAC technical coordinator, and right now its main developer. I've been pointed here, I will try to give some advice.    As mentioned above DIRAC give you the possibility to work with different, and even multiple, catalogs. Just to mention some real-life use case, which are the ones working best:  - CLIC uses DIRAC with the [DFC](https://dirac.readthedocs.io/en/latest/AdministratorGuide/Systems/DataManagement/dfc.html) (DIRAC File Catalog) both as a \"Replica Catalog\" and as a \"Metadata Catalog\"  - Belle2 uses DIRAC with the LFC (LCG File Catalog) as \"Replica Catalog\" and AMGA as \"Metadata Catalog\"  - LHCb uses DIRAC with the DFC as \"Replica Catalog\" and the LHCb Bookkeeping as \"Metadata and provenance Catalog\". Some years ago LHCb moved its production replica catalog from the LFC to the DFC  - all the other users that I know either use the DFC (the majority) or the LFC or some other combination.    The DFC, the LFC, AMGA, the LHCb Bookkeeping are all \"[Catalogs](https://dirac.readthedocs.io/en/latest/AdministratorGuide/Resources/Catalog/index.html)\". In DIRAC terminology, in fact, they are all Catalog plug-ins, as a DIRAC Catalog is such if it implements the same interface (e.g. add file, remove, etc). All catalogs implement the same interface and inherit from https://github.com/DIRACGrid/DIRAC/blob/integration/Resources/Catalog/FileCatalogClientBase.py  You can have more than 1 catalog at the same time, as obvious from the examples above. In this case, the operations will be executed on all of them. So, for example you can register files on BOTH the LFC and DFC at the same time.  Basically, each catalog plug-in implements a certain operation (e.g. the `addFile` operation) following its own \"interpretation\" of what, e.g. adding a file means for a certain catalog.    So, what may be interesting for you, is implementing a RucioCatalogClient.py. The rest is purely configuration.    ", "Hi @fstagni,    thank you for joining the conversation.  Indeed that was the solution thought about at first, but I have some questions to ask:  1. How is the synchronization between two separate catalogs (the DIRAC one and the external one) dealt with?  2. What are the required keys for the external catalog to implement to make it fully compatible with DIRAC? Is case the external catalog doesn't provide topological information, how does DIRAC retrieves such details?  3. Is there a list of the write and read methods to be implemented in the `FileCatalogClientBase.py` custom derived interface?  4. What are the methods that are needed by DIRAC to make it efficient at least as with the native catalog?    Thank you,    Gabriele", "1. There's always a Master catalog, this is defined in the DIRAC CS (Configuration System). Just to be sure: you don't need to necessarily have the \"DIRAC one\". I repeat: a catalog is a catalog as long as it behaves like one.  2. What's a topological information for you? The location of the replicas?  3. Each catalog plugin \"announce\" what it can do. Otherwise there's a really basic list of required methods, just check the code for that.  4. DIRAC needs nothing at all in that sense. The DFC is just another service.", "1. Awesome. I think the Rucio catalog can generate the missing info on the fly using functions, hence it should be possible to plug it to DIRAC;  2. yes, for example. As far as I have understood, DIRAC tries to perform the computation as close as possible to data. How does it figures out how to do that? Is the catalog providing some information or is it doe using some external metrics?  3. I saw the list of the mandatory `READ_METHODS = [ 'hasAccess', 'exists', 'getPathPermissions' ]`. It seems to me (and makes totally sense) that DIRAC requires to be able to read data, but the persistent output of data is not strictly required for the computing functionalities.  4. Several times in your publications there is an indication of a link between DIRAC's computing efficiency and the catalog information. Can you clarify a bit about that?    In addition I would like to ask you if there is any example of a custom implementation of `FileCatalogClientBase.py` (e.g. for LFC) to read and understand more the integration process.    Thank you    Gabriele", "1. I am not sure which are the \"missing info\"... ? Can you elaborate?  2. This is not fully correct. DIRAC CAN make the computation \"close\" to the data, but this is not a requirement. In fact you can run productions even in \"full mesh\" mode, meaning jobs can in theory go everywhere independently of the location of the input files.   2a. A \"replica catalog\" at least provides you with the location of the replicas. This location is a DIRAC [Storage Element](https://dirac.readthedocs.io/en/latest/AdministratorGuide/Resources/Storage/index.html). This info is used in several places, but a DIRAC SE can simply be \"RucioSE\" if this is something you want.  3. DIRAC jobs decide if and where to store 0/1/N of their outputs. The [DataManager](https://dirac.readthedocs.io/en/latest/CodeDocumentation/DataManagementSystem/Client/DataManager.html) object is what links the functionalities of FC and SE, and it's often the starting point for simple DM operations  4. Well... the DIRAC DFC is fast, efficient, practical, customizable, widely used, and it's already there. I can't compare it to any other solution apart the LFC, and I am not in a position of comparing it with the Rucio Catalog because I don't know much about it.  5. For examples just look in https://github.com/DIRACGrid/DIRAC/tree/integration/Resources/Catalog : LcgFileCatalogClient.py is the LFC, FileCatalogClient is the DFC (and I would not suggest to look at the others because they are a bit less obvious). For LHCb https://gitlab.cern.ch/lhcb-dirac/LHCbDIRAC/blob/master/LHCbDIRAC/Resources/Catalog/BookkeepingDBClient.py is the LHCb Bookkeeping.", "1. and 2.: I see that DIRAC can operate in \"fully mesh\" mode, but in any other case the locality of data and computing resources should be described. The 1. question is about that: what information is needed for not to operate in \"fully mesh\".  3. Correct me if I am wrong: if I get to use Rucio catalog from DIRAC I should be able to publish output files to Rucio BY HAND using direct calls to Rucio APIs. To perform a basic test of the eventual `FileCatalogClientRucio.py` implementation it should be enough to use direct calls instead of a custom DIRAC plugin.  4. The question was more about \"how much of the DIRAC efficiency is due to custom information stored in/computed from the DFC?\"  5. Thanks for the references ", "1. DIRAC would need to know where the input data is to make a proper job scheduling. Data is always in at least one \"Storage Element\". The SE(s) needs to be described in the DIRAC CS.  2. Of course  3. TLDR: nothing.   DIRAC is a (not small) set of components. Each component has its own life, and there's no need to install all of them. In fact many installations only install a small subset of them. The DFC is just a DIRAC component. In fact, from a informed user's perspective it's just a URL. The interrogations and answers that it gives need to follow a certain contract, nothing else. If another catalog respects that contract, then you're done. The `RucioFileCatalogClient.py` file should be the only one of the whole DIRAC where you do `import rucio`."], "1806": [], "1801": [], "1797": ["FTS is thinking about adding this to FTS; On hold until confirmation"], "1773": [], "1771": ["For reference : https://datatables.net/forums/discussion/43603/datatables-1-10-15-broke-date-sorting", "Just came across the same problem, any chance it will be fixed?", "@tbeerman , what can we do ?"], "1760": [], "1759": ["Happens on  ```  Name        : MariaDB-server  Arch        : x86_64  Version     : 10.2.18  ```  with `MySQL-python 1.2.5`    Will have to try to reproduce this.", "Still not able to reproduce. Abandon?", "ping @astroclark ", "Closing.", "We're also seeing similar behavior with MariaDB 10.3.15 and MySQL-python 1.2.5    In judge-cleaner.log:  ```  2019-07-05 08:51:04,856      490     DEBUG   Deleting lock x1t_SR001_170320_1116_tpc:records-943c2b3857b2a913583557452451b5049362ecbd-000000 for rule 2da476bb1d854febbf28b76deddc78a6  (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170320_1116_tpc', 'records-943c2b3857b2a913583557452451b5049362ecbd-000000', '-\\xa4v\\xbb\\x1d\\x85O\\xeb\\xbf(\\xb7m\\xed\\xdcx\\xa6', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at: http://sqlalche.me/e/f405)  2019-07-05 08:51:04,861 490     ERROR   Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/rucio/daemons/judge/cleaner.py\", line 107, in rule_cleaner      delete_rule(rule_id=rule_id, nowait=True)    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct      raise DatabaseException(str(error))  DatabaseException: Database exception.  Details: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170320_1116_tpc', 'records-943c2b3857b2a913583557452451b5049362ecbd-000000', '-\\xa4v\\xbb\\x1d\\x85O\\xeb\\xbf(\\xb7m\\xed\\xdcx\\xa6', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at: http://sqlalche.me/e/f405)  ```  In undertaker.log:  ```2019-07-05 08:50:09,628      488     INFO    Undertaker(1): Receive 1 dids to delete  2019-07-05 08:50:09,628 488     INFO    Removing did x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30 (DATASET)  Data identifier not found.  Details: Data identifier 'archive:raw_records-58340a130a541c95997fd1c442930427b04eac30' not found  2019-07-05 08:50:09,646 488     DEBUG   Removing rule d0b05b80dee24094b2a9f7b146412c97 for did x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30 on RSE-Expression UC_OSG_USERDISK  2019-07-05 08:50:09,671 488     DEBUG   Deleting lock x1t_SR001_170319_1011_tpc:raw_records-58340a130a541c95997fd1c442930427b04eac30-000000 for rule d0b05b80dee24094b2a9f7b146412c97  (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('x1t_SR001_170319_1011_tpc', 'raw_records-58340a130a541c95997fd1c442930427b04eac30-000000', '\\xd0\\xb0[\\x80\\xde\\xe2@\\x94\\xb2\\xa9\\xf7\\xb1FA,\\x97', 'jM\\x10)+\\xd2J\\xf5\\xb0\\x05\\x96,\\xafo\\x99\\x84')] (Background on this error at: http://sqlalche.me/e/f405)  2019-07-05 08:50:09,678 488     CRITICAL        Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/rucio/daemons/undertaker/undertaker.py\", line 109, in undertaker      logging.error('Undertaker(%s): Got database error %s.', worker_number, str(error))  UnboundLocalError: local variable 'error' referenced before assignment  ```", "@bari12 can we come up with a reproducable test for this?", "I haven't looked into this personally. It would be good to understand under which conditions this shows up. Running the daemons with multiple threads, or also one thread etc. @astroclark @jlstephen   Then we could try to reproduce this condition.", "From Handson:  > Hi rucio-team,  > I\u2019m DDM admin from ASGC(Taiwan site)  > We have a rucio server to manger out data.  > Here is our version information  > Rucio 1.13.3  > MariaDB 10.1.37  > Python package SQLAlchemy-1.1.13  >   > When I run rucio-judge-cleaner, I got error like:  >   > 2019-10-17 10:37:52,738 3265    INFO    rule_cleaner[0/0]: Deleting rule bb60ed7f80ad43059984e5a8663f5c29 with expression TW-EOS02_AMS02SCRATCHDISK  > 2019-10-17 10:38:53,390 3265    DEBUG   Deleting lock ams-user-felixlee:amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz for rule bb60ed7f80ad43059984e5a8663f5c29  > Exception _mysql_exceptions.InterfaceError: (0, '') in <bound method SSCursor.__del__ of <MySQLdb.cursors.SSCursor object at 0x31a5c50>> ignored  > (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('ams-user-felixlee', 'amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz', '\\xbb`\\xed\\x7f\\x80\\xadC\\x05\\x99\\x84\\xe5\\xa8f?\\\\)', 'b?\\xc5q\\xb4\\xbfA\\xbf\\x94\\x93\\xe4)\\x86\\x92\\xed\\x1a')]  >   > 2019-10-17 10:38:53,397 3265    ERROR   Traceback (most recent call last):  >   File \"/opt/rucio/lib/rucio/daemons/judge/cleaner.py\", line 93, in rule_cleaner  >     delete_rule(rule_id=rule_id, nowait=True)  >   File \"/opt/rucio/lib/rucio/db/sqla/session.py\", line 358, in new_funct  >     raise DatabaseException(str(error))  > DatabaseException: Database exception.  > Details: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('ams-user-felixlee', 'amsoutput2taiwan.873005283.pr.pl1.flux.l1o9.2016000.job.log.tgz', '\\xbb`\\xed\\x7f\\x80\\xadC\\x05\\x99\\x84\\xe5\\xa8f?\\\\)', 'b?\\xc5q\\xb4\\xbfA\\xbf\\x94\\x93\\xe4)\\x86\\x92\\xed\\x1a')]  >   > I google it and found this issue https://github.com/rucio/rucio/issues/1759  > I do some test and found in the rucio/lib/core/rule.py  line 823(I use the Rucio 1.13.3 on git hub to be the stander of line number)  > locks = session.query(models.ReplicaLock).filter(models.ReplicaLock.rule_id == rule_id).with_for_update(nowait=nowait).yield_per(100)  > If there was a rule that contain over 100 locks (which the number is the same with yield_per input), then the error will show up.  > I guess the session query ReplicaLock table get over 100 row, but function yield_per only return 100 for once and the rows which satisfy ReplicaLock.rule_id == rule_id are still lock by function with_for_update.  > So on the rucio/lib/core/rule.py  line 830  > if __delete_lock_and_update_replica(lock=lock, purge_replicas=rule.purge_replicas, nowait=nowait, session=session):  > rucio/lib/core/rule.py  line 2711  > lock.delete(session=session, flush=False)  >   > it failed to delete the row on the ReplicaLock table.  > I\u2019m not sure my guess is right or not.  > Increase the number of yield_per input can fix the problem temporarily.  > But I think that is not a good way.  > Is there a better way to fix it?  >   > Best regards,  > Handson Peng", "To my understanding, the most likely reason why this is happening is that due to the `yield_per` parameter, the query is actually still running and the delete call (line 2711) changes the table thus somewhat conflicting the query (since it is still running). This is not an issue on Oracle, but it appears this brings MySQL into troubles.    When you are changing the `yield_per` to something bigger than the amount of locks, this is not an issue as the query already ended at the moment we start deleting the locks.    The only solution I see is to change the `yield_per(...)` to an `all()`, so the query finishes at the beginning of the loop. The downside of this is that the entire result has to be loaded into memory, which for big rules might be an issue. Maybe we can do this conditional to the DB implementation.  ", "The problem solved by changing the yield_per(...) to an all().    The rule stuck before is around 60K locks with the same rule_id. And there is no other problems.  So I still don't know the limit of rule size.", "There is no principal limit of the rule size, however, the repairer might load the entire rule into memory, which for bigger rules requires a bit more. Thus if you run the repairer with a lot of threads in this uses all memory already under normal conditions, these large rule peaks might bring it over the limit."], "1752": ["I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data.    This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though.  We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way.", "Martin, the transformation implies full knowledge of all keys. Since it is not possible in this structure such process can be very tedious (not only on code writing) but on parsing level too. For instance, how many keys exists, will these keys stay the same over long time (I don't think so since we may add discard RSEs for instance), how many structure have this issue in all Rucio APIs, what is complete schema of all APIs. The list can go on. Not to mention that custom transformation cost time and resources.  I outlined the issue and presented use-cases. Anyone/tool who need to deal with aggregation or analytics of the results will face this issue. When you need to deal with dozens of services in real time it is a big issue.  But of course I understand backward compatibility issue and ideally it should be fixed both on server and client sides. If is it feasible it is another story.  Of course I can work around such cases, but I'm in favor that this issue should be fixed eventually.  On  0, Martin Barisits <notifications@github.com> wrote: > I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. >  > This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though. > We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way. >  > --  > You are receiving this because you authored the thread. > Reply to this email directly or view it on GitHub: > https://github.com/rucio/rucio/issues/1752#issuecomment-435965147 ", "Hi Martin,  Aren\u2019t at least a few of the APIs passed through a JSON schema verification?  Or maybe I\u2019m just recalling some internal data structures?  It\u2019d probably be useful to maintain some sort of published schema as new users of the API come along.  Success breeds these sort of questions, after all!  Thanks,  Brian  Sent from my iPhone  > On Nov 5, 2018, at 12:13 PM, Valentin Kuznetsov <notifications@github.com> wrote: >  > Martin, > the transformation implies full knowledge of all keys. Since it is not possible > in this structure such process can be very tedious (not only on code writing) > but on parsing level too. For instance, how many keys exists, will these keys > stay the same over long time (I don't think so since we may add discard RSEs for > instance), how many structure have this issue in all Rucio APIs, what is > complete schema of all APIs. The list can go on. Not to mention that > custom transformation cost time and resources. >  > I outlined the issue and presented use-cases. Anyone/tool who need to deal > with aggregation or analytics of the results will face this issue. When you need > to deal with dozens of services in real time it is a big issue. >  > But of course I understand backward compatibility issue and ideally it should be > fixed both on server and client sides. If is it feasible it is another story. >  > Of course I can work around such cases, but I'm in favor that this issue should > be fixed eventually. >  > On 0, Martin Barisits <notifications@github.com> wrote: > > I am not quite sure why you don't transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. > >  > > This pattern of key/attribute values is certainly used in more APIs, I would have to check which ones though. > > We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a v2 API, or some kind of option to return the result in this kind of way. > >  > > --  > > You are receiving this because you authored the thread. > > Reply to this email directly or view it on GitHub: > > https://github.com/rucio/rucio/issues/1752#issuecomment-435965147 > \u2014 > You are receiving this because you are subscribed to this thread. > Reply to this email directly, view it on GitHub, or mute the thread. ", "We only do the schema verification for the incoming data structures (https://github.com/rucio/rucio/blob/master/lib/rucio/common/schema/atlas.py) not for the outgoing one. But I fully agree, we should maybe have something for the return structures too. Not so much a verification, but more documentation.  @vkuznet I get your point, I didn't mean to do the transformation for all eternity, just for now; The data structures right now are very much prepared in a way so that the client (The rucio client that is) can immediately deal with them efficiently. (That's why the rses are keys, so the client can quickly get all replica states for a certain rse in a long list of replicas) - I think what might be most appropriate is an option for the call to prepare the return object in an \"analytics ready\" way. We would have to look at all REST calls you are making and see if such an option is needed and keep it in mind if there are API changes in the future.", "Martin it is not a stopper for me since I deal with these issues in other data-services, but eventually I'll be glad if this issue will be fixed one way or another.", "So far I identified the following dynamic structures in /replicas/scope/name API:  - states  ```  \"states\":{\"T2_US_Nebraska\":\"AVAILABLE\"}}  ```  - rses  ```  \"rses\":{\"T2_US_Nebraska\":[\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\"]}  ```  - pfns  ```  \"pfns\":{\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\":{\"client_extract\":false,\"domain\":\"wan\",\"priority\":1,\"rse\":\"T2_US_Nebraska\",\"type\":\"DISK\",\"volatile\":false}}  ```", "The pfns example is extremely bad if such records will be placed in DB and indexed on these keys since (a) pfn names are long strings, (b) we have extremely long list of pfns in GRID universe.", "Hi Valentin, I'm not really following you. What is your concern with the pfns? They are not stored in the database at all?", "Yes, but if we will dump data to HDFS (as is) and then create a dataframe the pfns will become attributes (column names) rather then values.  On  0, Mario Lassnig <notifications@github.com> wrote: > Hi Valentin, I'm not really following you. What is your concern with the pfns? They are not stored in the database at all? >  > --  > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub: > https://github.com/rucio/rucio/issues/1752#issuecomment-436307663 ", "Hi @vkuznet -    I think we've determined that this ticket is really about having a JSON schema for the Rucio APIs, no?  Since that comes down to a pretty major redesign of the APIs (which, FWIW, is being discussed), what should we do with this ticket?    Brian", "I think it is up to Rucio team. We can use it for progress or further examples (as I did today), or it can be turned into actions/milestones.", "Discussed in the development meeting on [2018-11-08](https://indico.cern.ch/event/766990/). We decided to leave it like this until we have a V2 API (Long term project)", "Should this become an real problem for wanting to dump data to HDFS, it should be possible as an interim step to define the desired schema(s) for v2 and build a simple translator which would precede v2, right?", "yes, that's definitely an option."], "1746": ["This are actually two different methods, but there seem to be a problem with the regular expression of the rest endpoint in combination with the CMS SCOPE_NAME regular expression, so in both cases, it calls list_replicas.   https://github.com/rucio/rucio/blob/ec11eeb6618cfa6a63587d0bf78f195a4b77b632/lib/rucio/web/rest/webpy/v1/replica.py#L56    `/([^/]*)(?=/)(.*)/datasets$` resolves to the list_dataset_replicas method   and `/([^/]*)(?=/)(.*)/?$` to the list_replicas method  But the latter one seems to cover anything with .../datasets too, so it should probably be changed to: `/([^/]*)(?=/)(.*)/$`    So in conclusion: It should be two different methods, but in your case it is calling wrongly the same method.", "Yet, another mis-behavior:    this call http://cms-rucio-test.cern.ch/replicas/cms/Charmonium/Run2017D-31Mar2018-v1/MINIAOD returns    ```  {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\": {\"domain\": \"wan\", \"rse\": \"T2_US_Nebraska\", \"priority\": 1, \"volatile\": false, \"client_extract\": false, \"type\": \"DISK\"}}, \"adler32\": \"881ec8cf\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\", \"rses\": {\"T2_US_Nebraska\": [\"gsiftp://red-gridftp.unl.edu/user/uscms01/pnfs/unl.edu/data4/cms//store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\"]}, \"scope\": \"cms\", \"bytes\": 3890770828, \"md5\": null}  ```  while this call  http://cms-rucio-test.cern.ch/replicas/cms/Charmonium/Run2017D-31Mar2018-v1/MINIAOD/datasets returns nothing.    Is it the same regex issue? Please note that this time I used as a name a (CMS dataset) /a/b/c pattern rather then (CMS block) /a/b/c#123 one.", "I think in this case it is correct; The first query (without /datasets) returns the file replicas. The second query (/datasets) returns dataset replicas, which essentially are just a more efficient way to get the replica status of all files of a dataset (in the database we create a dataset replica which has synchronised counters) - However, dataset replicas are not created in all workflows, thus in this case everything seems correct. I think you are right, in your previous query including the # this seems to be somehow problematic with the REST regex. I will have a closer look on this in the next days."], "1667": [], "1648": ["Should not be an issue with #716 "], "1636": ["I'm not sure what the problem is here (maybe related to the double `/` in xrootd path). But `add_files_to_dataset` definitely works without SRM. We tried with `gsiftp` at T0 and it works fine. "], "1583": ["In the transfer request the core creates, we already create a `ds_scope` and `ds_name` attribute. This is mostly done for tape endpoints, as it is used in the PFN generation. I think we can add this to the `file_metadata` dict which is part of the fts request.  However, the ds_name/ds_scope is usually the dataset the rule is defined on. E.g. if the file is also in different datasets, only one will be given. Also if the rule is on a file directly, none will be given, even if the file is in a dataset."], "1558": [], "1504": [], "1431": [], "1393": ["Beware that `%2F` is a really tricky character to work with, especially in APIs that include the DID in the URL path itself.  For example, `mod_wsgi` always converts these back to `/`.    How good is the unit test coverage in this area?", "Test coverage is bad in this. Essentially we never tested with any / in there (not even the trailing one) as for us the container names (in ATLAS) do not include the trailing /. So we need to add some tests with \"/\" or other special characters in the names.    Solution will be most likely that we ask external applications in ATLAS not to submit the trailing /, since the other solutions impact communities using slashes in their names (which is valid)."], "1382": ["Replicas controller (== GET calls) is unused by the Rucio clients. We should deprecate it in the next feature release.", "It's actually used by the ARC clients, cannot deprecate. Still needs deep-check if there are functionality differences between GET and POST", "We keep it as it is, but should put comment in code."], "1351": ["We looked at this and it looks like a pure statically linked binary is not possible, but perhaps we can provide a stripped down RPM that installs this binary.    Note though that this will require some additional dependencies (ROOT's libCore, boost, etc)", "another option would be to add a configuration option that lists the extensions for which GUIDs are extracted. the default could be     ```  extractGUID_extensions: [\"*.root\"]  ```    which could be set to       ```  extractGUID_extensions: []  ```  "], "1304": ["We discussed this couple of times and it's still not clear if this is a bug or not. Can we discuss this on Thursday @cserf @TomasJavurek @bari12 ?", "Could be an upload of a replica on a different rse (did exists) - This should add a rule on the RSEx"], "1248": [], "1245": ["Hi Alessandra, I think that's possible, but maybe @cserf or @TomasJavurek can comment.", "Hi,    any answer here? It seems that after declaring those 300k files lost 3 times, rucio still thinks it has them [1]. Unfortunately more than keeping on declaring all of them I cannot do in these conditions.    thanks    [1] https://its.cern.ch/jira/browse/ATLDDMOPS-5414?focusedCommentId=2135911&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-2135911", "Hi,    That's possible. I'll try to implement it next week.    Cheers,  Cedric"], "1213": ["After checking the code, the necromancer do it the proper way. The problem is actually due to the fact that the ignore_availabilty flag that is in the signature of the list_replicas method is not working, so reassign the ticket to core."], "1202": [], "1157": ["Hmm, it's technically not mandatory to use alembic to install the database, so looking for the alembic_version might not be the right approach. What we could do is to test if it is there and compare then, I am not so sure if this is really helpful though.", "It is a real pain to recover from this issues. I would really prefer that there is only one way to install and upgrade the DB. If we want to maintain the ability to install the DB without alembic, you shouldn't be able to use alembic to upgrade the database.  On Tue, 22 May 2018 at 15:06, Martin Barisits <notifications@github.com> wrote:  > Hmm, it's technically not mandatory to use alembic to install the > database, so looking for the alembic_version might not be the right > approach. What we could do is to test if it is there and compare then, I am > not so sure if this is really helpful though. > > \u2014 > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/1157#issuecomment-391025946>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/ABG4FjQsPhzEFytXcCp8e_d4S6WbdgMpks5t1CmRgaJpZM4T-Ps0> > . >   --  Benedikt Riedel Scientific Programmer University of Chicago Computation Institute ", "We discussed this in the meeting today. We will add a check for this in all daemons on startup.  - If the alembic table is present, check if the right schema version is installed, if not fail.  - If the alembic table is not present, do nothing for the moment. (This line we will remove eventually)"], "1152": ["Up", "Some observations I made with a simluated timeout in the API:    1. Using a 60s TimeOut:  Apache times out after 60s and returns a 502 Proxy error.   ```  <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">  <html>      <head>          <title>502 Proxy Error</title>      </head>      <body>          <h1>Proxy Error</h1>          <p>The proxy server received an invalid  response from an upstream server.              <br />  The proxy server could not handle the request              <em>                  <a href=\"/proxy/rses/MOCK5\">GET&nbsp;/proxy/rses/MOCK5</a>              </em>.              <p>  Reason:                   <strong>Error reading from remote server</strong>              </p>          </p>      </body>  </html>  ```  Strangely this occurred only with postman in my test but not with the client. The client received the correct response.    2. Using a 600s TimeOut:   The default timeout in the base client is 600s. The client raises a requests TimeOut exception which does not get catched.     I think it would make sense to change the default timeout in the base client to 60s and to catch the TimeOut exception to then raise an own TimeOut exception ?  But I could not reproduce a case where the client interprets a request successfully when there was a timeout.", "Just to follow up on this in the github-a-thon. Could you maybe try to generate a case where HTTP streaming is used?  I could imagine a workflow where:  - Client and server communicate via python requests with the streaming parameter.  - As the header (of a stream) is sent first, it might be a HTTP 200  - During the stream (even if is just one line) an exception might occur and actually kill the DB session.  - This could still be interpreted as a HTTP 200, as this was sent in the header first.  Now, most POST calls where a timeout matters do not really stream much, but we should check if this can still happen if it is requested via python requests. ", "@mlassnig @TomasJavurek please have a look if we can still do anything with this one. If you thik it's not relevant anymore, please close"], "1114": [], "1109": [], "1091": ["I am a bit concerned that the only info so far is a list of files inside a zip file. In order to see if a file is in a zip file, you`d need to search this table - also when the file is not archived.  It seems to me you need a special replica, which is just the name of the zipfile. Then list-replicas would also show the locations of the zip file.  Adding files to the zip, would mean adding a special replica to all the content files.    Anyway, I`m glad some else has to worry about this stuff.    Cheers,  Rod.  ", "Without entering into all the details, listing replicas without any additional entries is doable.    About the format of the url replica for a file contained in a zip, you proposed `<scheme>://zipfile#constituent1` I'm wondering if it's a well adopted semantic for this. like in web browser, etc.  ", "We have to see how we do this. Adding a \"special\" replica for each zip-content is a possibility, but this    adds workflow complexity to keep everything consistent. I prefer the way we discussed in the dev meeting today by just incorporating the information we already have. The implication there though is performance, as each list-replicas call gets more complicated and we really have to make sure that this doesn't slow down things in general.", "So, any progress/thoughts?", "For #46 @TomasJavurek will work on this, no update yet.  The transparent list-replica support we discussed and we think it should be fine with the information which is already there, without creating some fake replicas. Which would be better for consistency reasons. Essentially, when you list the replicas for a file the information that it is also a constituent in a zip file is there, thus we can list the parent replicas then. It might have some performance implications for the general workflow, so this needs to be done carefully. We didn't discuss a development plan specifically, but I was hoping that @vingar could work on this? Maybe he can comment?", "Hi @rodwalker,  So we discussed the timeline of the the transparent archive support (list-replicas of files also exposes parent archives) and we are aiming to have this by end of June (Hopefully in the 1.17.0 release).", "Just to follow up on the results so far (There will be a presentation in the ATLAS S&C week this Thursday about this as well):  - `rucio download <constituent> --archive-did <archive>` issues an xrdcp streamed download of the constituent in the archive; However, the archive has to be named specifically for this to be possible.  - list_replicas has been adapted if you do list_replicas <constituent> that it also outputs the archive with the streaming option. This should enable the client to do a download of a constituent transparently. This will need an update of root as far as I know (@mlassnig ?), as the streaming is currently only possible via xrdcp and not gfal.    What is not there:  - `rucio add-rule <constituent>` makes a rule on the archive. This will be discussed on Thursday.  - With the list_replicas change @mlassnig did, if the constituent is not on a root enabled storage, the replica dictionary has a `client_extract=True` flag, which tells the client to download the archive and extract the file. This functionality needs to be implemented. (@TWAtGH or @TomasJavurek ?)", "I have updated the missing features/bugs to the overview on the top. Please comment here in this ticket if anything else is missing.", "I think we can remove the clients label from this issue @bari12 "], "1026": [], "1021": [], "857": [], "781": ["For 1. this needs a little bit of work as the error is not propagated to the locks. It's only the last error stored at rule level.  2. this information is in the requests_history, just needs to be added to the dictionary."], "736": [], "713": [], "704": ["Up"], "689": [], "630": [], "609": ["More info: http://scitokens.org/", "@bbockelm Won't the workflow be similar with openId ?    Something like:  Client,          rucio auth servers,      ext auth. service (openId/scytoken)  ask token --------> forward token request -----> token request  store token <----  store token in db and answer with it to the clients   <---- give token    then interacts with Rucio & co  ", "Not quite, because the Rucio server itself will be the issuer. If Rucio allows you to get an X-Rucio-Auth-Token you can request a SciToken instead (with the appropriate domains set based on VO policy).", "in X-Rucio-Auth-Token you expect to have the SciToken one ?   ", "The client can request it in addition and use it to go to storage (If the storage allows the domain).    We can also think to use it as a replacement for X-Rucio-Auth-Token with a custom Rucio-Auth domain.", "I'd suggest starting out by having them alongside each other (to understand how the library works), then eventually replacing `X-Rucio-Auth-Token`.", "The clients will have to store both tokens and put in addition the sciToken one in the request header `Authorization: Bearer <token>` for the next interactions (rucio, storage, etc).  Am I correct ?  ", "Yeah - at the beginning, one would just use the SciToken for interaction with storage.    In fact, this is a strong argument for keeping multiple tokens -- you don't want the storage authorization token to additionally interact with Rucio.    So, your Rucio token would gain a new format (which is convenient because it doesn't have to be serialized by Rucio) and you'd still separately handle a storage-related token.    While Rucio and the storage service would need to understand how to verify / parse the token, the client can continue to treat it as an opaque string."], "595": [], "583": [], "536": ["During Rucio Community Workshop 2020 this was also identified by DUNE as being very useful to handle rules on larger datasets/containers.", "+1000", "+1M    Finally a proper alternative of updating \u00b4updated_at\u00b4  :)"], "534": ["The docstring headers are not handled properly by sphinx for the API documentation. The clients source files will be updated to:  ```  # Copyright (c) 20XX-2018 CERN for the benefit of the ATLAS collaboration.  #  # Licensed under the Apache License, Version 2.0 (the \"License\");  # You may not use this file except in compliance with the License.  # You may obtain a copy of the License at  # http://www.apache.org/licenses/LICENSE-2.0  #  # Authors:  # - ...  ```", "Back to block comments ;-)    Correct dates according to git are 2012-2018. I'd also modify the first to lines to be a bit more explicit (no need for (c) for example)      ```   # Copyright 2012-2018 European Organization for Nuclear Research (CERN)   # For the benefit of the ATLAS Collaboration   #     ```...    (also makes it nicer to modify the second line later on, if necessary)", "yes, back to block comments and we can hope that pylint will offer a way to mute the missing docstring at the top of the file    I was thinking having the starting date based on the file creation date.    According to http://hepsoftwarefoundation.org/notes/HSF-TN-2016-01.pdf  The format must be:    (c) Copyright [year] CERN [for the benefit of the [Name of appropriate  group] Collaboration]", "Right, then that's how it is ;-)", "this one is not true everywhere ...", "Ah I thought with your recent patches you iterated all files.", "only the ones I've modified to get the doc generation working"], "528": [], "527": [], "526": [], "485": ["However, this does not add a second leading slash when supplied with one /. Considering that all CMS datasets are /primary/secondary/Tier this won't impact CMS.   ", "yes, I knew this but this was the best option I quickly managed to come with to satisfy the CMS naming convention. The proper behavior here would have been to reject the name as all CMS names start with a slash.", "Which my python API did for something that did not start with / but the CLI did not for some reason.    Anyhow, this is low/no priority for CMS, but may be useful to keep around."], "479": [], "461": ["The `verify=False` I vaguely remember was a fix due to a temporary issue, but I might be wrong. Maybe @mlassnig remembers.", "verify=False: Indeed, this looks like a remnant.    For the session handling, yes, you're right.    For the delegation, this is done via a periodic cron outside the conveyor. We didn't want to couple this as a failure of one could affect the other:    - tools/delegate_proxy_fts.py    (btw, once FTS3 releases their CLI which can set the delegation lifetime longer than 4 hours this script can be made much more simple. With this script we get a safety buffer of 96 hours.)", "@mlassnig - we really need to come back to this one (but, unfortunately, I don't think I'll personally have time to tackle it).  `verify=False` is bad news.", "We need to add a separate ca_cert chain per FTS host (eg. BNL hostcert vs UKeS vs CERN). Shouldn't be too difficult. I'll have a look next week.", "I'm not entirely sure that's necessary - current version of the `requests` module (really, it's probably `urllib3` that does this...) allows you to point at a directory (such as `/etc/grid-security/certificates`).    Additionally, even older versions allowed multiple CAs per file (simply concatenate them all together).  In other projects that use clients that don't understand directory-based CA layouts, we've just written scripts to do the concatenation at startup.", "That makes it even easier then. Thanks for the heads-up.", "Hi,    can you take a look at (*) and see if that can be of any use in this regards?  I mean that as a starting point to get your feedback on how to proceed for the integration of fts delegation, if that can be useful.    (*)  https://github.com/dciangot/rucio/commit/00c012d6ae51c7125295ec3a25528270213021e9", "I have pushed this part (*) of the changes needed for transfers in CMS, but that is more general for FTS proxy delegation. I splitted it because I think that can be safely introduced and utilized for other purposes not CMS related.   So I was thinking that for the other changes, more CMS specific, I could open an issue to better follow up without delaying further the proxy delegation stuff.  How does it sound @bbockelm @mlassnig @vgaronne @bari12 ?    (*) https://github.com/dciangot/rucio/commit/f3095add17e980f544019fc571bc5a81f18778c4", "Hi Diego, I only had a quick look (will look more in detail later), but it looks good to me like that. I would propose you open a separate issue (even for the proxy delegation) and submit the PR already to rucio so we can comment there. No need to just keep it in your private repo.", "Agreed, I'm doing that. Thanks"], "421": ["@vingar mentioned that there is already signed URL support in Rucio and will be updating the ticket with some example code.    Additionally, here is some sample code for getting a macaroon from a dCache endpoint: https://github.com/bbockelm/x509-scitokens-issuer/blob/master/tools/macaroon-init  ", "Creating bearer tokens is already the basic authn/authz scheme in Rucio, where we delegate the verification of the requestor to separate entities (Kerberos Servers, VO CAs, SSH pubkeys, ...). Creating Rucio-issued bearer tokens (e.g., based on the SciTokens proposal) which can then be given to rucio clients and transfertools should be doable by adding a new token generator which takes account, identity, and optionally RSEs as input. (In case a challenge-response handshake is needed, there's a workflow in place as we need to do this for SSH-based auth already and which can be adapted.) This could even supersede the X-Rucio-Auth-Token.    To auth with storage, generating signed URLs follows naturally from this, as instead asking a third party to sign the URL Rucio could do it internally by passing the optional RSE to the token generator.    Few things to take care about:  - thrash rate on the token table  - length of the bearer tokens  - attributes of the bearer tokens which are not part of the token itself", "Length can be a legitimate problem.  Apache's limit tends to be around 4KB.  A narrowly scoped token (including a full PFN) would be around 1KB.  So, if you issued a token to download a specific file, it would not be a problem.  The problem would crop up if you wanted to do something like a token for a long list of files -- in such a case, you'd probably have to do a more coarse-grained scope (e.g., at the directory level).    Since the tokens are signed with a public key, they don't need to be recorded in a manner analogous to the existing table.  It is a good idea to have a log activity of tokens recently (no need to record the tokens themselves) issued for auditing purposes, but that's more a log file than a DB table.    For attributes - for the pure file transfer use case, that's fairly well specified already.  If we wanted to include internal Rucio authorizations, you'd probably just embed the authorization name in the `scp` (scope) claim.", "Scheduled for 1.19.1"], "372": [], "366": ["2017-12-15 11:16:31,919  auditor-worker          DEBUG    [PID    21268] Checking \"PIC_MCTAPE\"  2017-12-15 11:16:33,957  auditor-worker          ERROR    [PID    21268] Check of \"PIC_MCTAPE\" failed in 0 minutes, 1 remaining attemps: (GError: srm-ifce err: Communication error on send, err: [SE][Ls][] httpg://srmatlas.pic.es:8443/srm/managerv2: CGSI-gSOAP running on rucio-daemon-prod-02.cern.ch reports Could NOT import client credentials    How auditor can suffer from 'erroron send'?   "], "354": [], "112": ["Needs to be discussed with Paul", "Try to figure out where the earliest possible exit in the codepath is.", "For the upload one of the first things we do is query the rse information:  https://github.com/rucio/rucio/blob/master/lib/rucio/client/uploadclient.py#L111  So directly afterwards we could check if any protocol implementation is available"], "57": [], "54": ["For combining weights we could do some kind of simple language.  eg. .1freespace+.9network"], "53": ["We could apply the same logic Thomas das for monitoring to asynchronously prefill the sources in WAITING state.", "Needs to be evaluated; Maybe use Thomas algorithm for monitoring."], "43": [], "38": [], "36": ["scope:name for fake_dataset: other:other", "I checked the code, and it looks like what is done in the conveyor is OK.   But, there might be a problem if the file has a parent DID with the wrong number of `.`. It will be fixed. In addition this part is very ATLAS specific :  ```                      # DQ2 path always starts with /, but prefix might not end with /                      naming_convention = rse_attrs[dest_rse_id].get('naming_convention', None)                      dest_path = construct_surl(dsn, name, naming_convention)  ```  We should have a way to have different conventions for different collaboration."], "35": [], "31": [], "3389": [], "3382": ["I'll try to delete the storm protocol manually (which removes both I suppose) and add the correct one with the correct prefix to the attributes.", "After running:  rucio-admin delete-protocol --scheme storm INFN-T1_DATADISK  the correct protocol was removed but the wrong one remains.  At least this is shown in rucio-admin rse info  However, when I try to run the command for removing protocol again, it states that there is no storm protocol.  ", "`[root@rucio-nagios-prod-02 test_area]# rucio-admin rse delete-protocol --scheme storm INFN-T1_DATADISK  RSE does not support requested protocol.  Details: RSE 'INFN-T1_DATADISK' does not support protocol 'storm'  Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers.  Traceback (most recent call last):    File \"/usr/bin/rucio-admin\", line 126, in new_funct      return function(*args, **kwargs)    File \"/usr/bin/rucio-admin\", line 668, in del_protocol_rse      client.delete_protocols(args.rse, args.scheme, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/client/rseclient.py\", line 355, in delete_protocols      raise exc_cls(exc_msg)  RSEProtocolNotSupported: RSE does not support requested protocol.  Details: RSE 'INFN-T1_DATADISK' does not support protocol 'storm'    [root@rucio-nagios-prod-02 test_area]#   [root@rucio-nagios-prod-02 test_area]#   [root@rucio-nagios-prod-02 test_area]#   [root@rucio-nagios-prod-02 test_area]# rucio-admin rse add-protocol --hostname storm-fe.cr.cnaf.infn.it --scheme strom --prefix /storage/gpfs_atlas/atlasdatadisk/rucio/ --port 8443 --impl rucio.rse.protocols.storm.Default --domain-json '{\"wan\": {\"read\": 4, \"write\": 0, \"third_party_copy\": 0, \"delete\": 0}, \"lan\": {\"read\": 1, \"write\": 0, \"delete\": 0}}' INFN-T1_DATADISK  An object with the same identifier already exists.  Details: Protocol 'strom' on port 8443 already registered for  'INFN-T1_DATADISK' with hostname 'storm-fe.cr.cnaf.infn.it'.  This means that you are trying to add something that already exists.`", "From Cedric:  this is a known feature. It should disappear when we move to CRIC + new importer", "Additionally, there was a typo (`strom` instead of `storm`). Nothing to do here."], "3381": ["Hi @ericvaandering   Please go ahead with the changes. The code worked fine for us in our testing, there is other changes still to be done though since the link finder is sometimes a bit slow."], "3378": [], "3371": [], "3368": [], "3364": [], "3361": [], "3355": ["Oops, I should know better than that!    >>> 0 is not None  True  >>> bool(0)  False"], "3348": [], "3337": [], "3334": [], "3324": [], "3320": [], "3315": [], "3314": [], "3313": ["+ New naming convention"], "3312": ["I implemented that just in procedural style. I am thinking about something nicer. The build-blocks are:  1) We don't want to call protocols, if wan pfns already worked.  2) We check lan pfns, but registration should still be done with wan_pfns.  3) We raise an exception if nore wan neither lan pfns fit.  ", "Hmm,    why is this working correctly at OU_OSCER_ATLAS then? Here, we also have a wan_pfn that's different from the lan_pfn: root://se1.oscer.ou.edu/ (the external xrootd proxy / SE) vs. root://dms.oscer.ou.edu/ (the internal xrootd redirector). And as far as I can tell, write_lan works fine here and uses the internal one. Or am I misunderstanding the issue here?    I very occasionally see a failed panda job with an error related to rucio not understanding root://dms.oscer.ou.edu/, but those are very infrequent.    Thanks,      Horst  ", "I checked one job log and it looks like writes are using the write_wan host (se1)    https://aipanda184.cern.ch/condor_logs_2/20-03-04_06/grid.5156402.2.out    `2020-03-04 08:21:38,760 | INFO     | copytool_out        | gfal2                            | __gfal2_copy              | Event triggered: BOTH GFAL2:CORE:COPY LIST:ITEM file:///lscratch/31463202/atlas_djH56XPj/PanDA_Pilot-4661486006/HITS.20093197._121470.pool.root.1 => root://se1.oscer.ou.edu:1094//xrd/atlasdatadisk/rucio/mc16_13TeV/33/18/HITS.20093197._121470.pool.root.1.rucio.upload`    I don't know why they don't use the internal redirector. SITE_NAME is set correctly. But I think if they were using the internal redirector you would suffer the same problem.", "Thanks David.    I could've sworn we had this working correctly after Mario and Martin helped me with that last year some time, but maybe I'm just not remembering this correctly.    But we should get this working properly and automagically at any rate, since it will greatly improve our bandwidth, since using the internal redirector we basically have 70 Gbps to work with (7 storage servers), while anything that goes through the proxy is currently limited to its 10 Gbps 'through-flow'.    Thanks,      Horst  "], "3308": [], "3292": [], "3289": ["@TomasJavurek please have a look on this. I am not sure why this line was introduced (It did work before) Maybe it was just a mistake.", "Is this also the reason gridftp fails and thus the reason we can`t have a release with the wrote_wan fix?  Then it is urgent.    Cheers,  Rod.", "The line was introduced by @berghaus on our request, because @TWAtGH discovered volatile behaviour of protocol.stat, when it sometimes returned not correct answer. Thus we implemented retries we certain schedule, but it should not exceed few minutes.  "], "3286": ["gsiftp protocol is missing stat method:  https://github.com/rucio/rucio/blob/master/lib/rucio/rse/protocols/gsiftp.py  has it actually ever worked?", "This is still gsiftp via gfal. So the protocol (scheme) is the gfal.py file. It just routes the gsiftp calls through gfal.", "yeah I know, but gfal stat is anyway calling protocol implementation at the end, or not?  worked well for 1.20.2    [root@rucio-nagios-prod-02 test_area]# rucio upload --rse CERN-PROD_SCRATCHDISK --scope user.ddmadmin --protocol gsiftp DAOD.test.archive.zip.8  2020-02-06 11:18:01,632\tINFO\tPreparing upload for file DAOD.test.archive.zip.8  2020-02-06 11:18:01,750\tINFO\tFile DID already exists  2020-02-06 11:18:01,972\tINFO\tSuccessfully added replica in Rucio catalogue at CERN-PROD_SCRATCHDISK  2020-02-06 11:18:02,198\tINFO\tTrying upload with gsiftp to CERN-PROD_SCRATCHDISK  2020-02-06 11:18:02,198\t25255\tINFO\tTrying upload with gsiftp to CERN-PROD_SCRATCHDISK  2020-02-06 11:18:02,398\tINFO\tSuccessfully uploaded file DAOD.test.archive.zip.8  2020-02-06 11:18:02,398\t25255\tINFO\tSuccessfully uploaded file DAOD.test.archive.zip.8  2020-02-06 11:18:02,400\t25255\tDEBUG\tStarting new HTTPS connection (1): rucio-lb-prod.cern.ch:443  2020-02-06 11:18:02,426\t25255\tDEBUG\thttps://rucio-lb-prod.cern.ch:443 \"POST /traces/ HTTP/1.1\" 201 None  2020-02-06 11:18:05,537\t25255\tDEBUG\thttps://rucio-lb-prod.cern.ch:443 \"PUT /replicas HTTP/1.1\" 200 0  [root@rucio-nagios-prod-02 test_area]# rucio --version  rucio 1.20.2      ", "No this is somewhat hidden in the protocol definition of the RSE. It will specify the `scheme` somewhere (Which will be `gfal`) and then it will channel everything through the gfal.py. It will not touch the gsiftp.py one. (It's a bit misleading, I know)", "1.20.2 haven't had the GLOBALLY_SUPPORTED_CHECKSUMS implemented yet. This part of the stat method is different.", "I tried to verify checksum at CERN-PROD_SCRATCHDISK with Md5, that's one of the problems.", "second problem is, that .capitalize() method capitalize only first leter, who implemented that?", "with upper() instead, it works well. Patching it. But still, should really MD5 be used for CERN-PROD? I thought it is matter of few US sties.  ", "the md5 wasn't actually perferred, it was already the second attempt. First attempt failed, because of the capitalize()  "], "3285": ["In case file doesn't exist davix-http still returns 0 as exit code while returned data contains just HTML code with 404 error, e.g.  ```  [vokac@ui1 ~]$ davix-http --capath /cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/etc/grid-security-emi/certificates --cert $X509_USER_PROXY -X PROPFIND davs://gridhttps-storm-atlas.cr.cnaf.infn.it:8443/webdav/atlas/atlasscratchdisk/rucio/user/mphipps/36/a1/missing_file.txt  <html><body><h1>/atlas/atlasscratchdisk/rucio/user/mphipps/36/a1/missing_file.txt Not Found (404)</h1></body></html>  (Davix::HttpRequest) Error: HTTP 404 : File not found   [vokac@ui1 ~]$ echo $?  0  ```  Even though stdout is most probably valid XML (last line comes from stderr) even latest code just fails a bit later with Exception (ExpatError?), because you are trying to access \"d:getetag\" element that doesn't really exists in a query for non-existent file. Exception should be clear and davix-http is not optimal interface.    I also think ATLAS capath should not be hardcoded in Rucio sources."], "3284": [], "3282": ["Thanks David, I will have a look.", "I read the code again and I don't think this is an issue. What happens is that the weighting is skipped if any files in the dataset have replicas on any RSEs in the rule expression, which make sense in principle because it means less data movement. So I wonder if the bias in the data carousel was because some sites had RAW files lying around and so those sites were preferred even though they had less free space."], "3275": [], "3272": [], "3262": ["Only images can be attached, in-lining python code  ```python  #!/usr/bin/python  import os, sys, time  import subprocess  import select  import fcntl    def run_cmd_process(cmd, timeout=None):      \"\"\"      shell command parser with timeout        :param cmd: shell command as a string      :param timeout: in seconds        :return: stdout xor stderr, and errorcode      \"\"\"          # Note: PIPE hangs after filling 65K buffer      # continuously read stdout/stderr output with select      p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)        fd = p.stdout.fileno()      fl = fcntl.fcntl(fd, fcntl.F_GETFL)      fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)        fd = p.stderr.fileno()      fl = fcntl.fcntl(fd, fcntl.F_GETFL)      fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)        stdout = b''      stderr = b''      readfds = [ p.stdout.fileno(), p.stderr.fileno() ]      start = time.monotonic()      while True:          readable, writable, exceptional = select.select(readfds, [], [], timeout)            if timeout:              curr = time.monotonic()              if len(readable) == 0 and curr - start >= timeout:                  p.kill()                  raise Exception('timeout')              timeout -= curr-start              start = curr            for fd in readable:              if fd == p.stdout.fileno():                  stdout += p.stdout.read()              if fd == p.stderr.fileno():                  stderr += p.stderr.read()            # TODO: deal with situation when stdout/stderr is infinite          # otherwise this will fail witn OOM            if p.poll() != None:              break        return (p.returncode, stdout, stderr)      if __name__ == '__main__':      #handle = run_cmd_process(['find', '/proc'])      rcode, stdout, stderr = run_cmd_process('find /proc', timeout=0.001)      rcode, stdout, stderr = run_cmd_process('sleep 1')      rcode, stdout, stderr = run_cmd_process('sleep 10', timeout=0.1)      rcode, stdout, stderr = run_cmd_process('sleep 100')      print(\"CODE: %s\" % rcode)      print(\"STDOUT: %s\" % stdout)      print(\"STDERR: %s\" % stderr)  ```", "This is just super quick patch in order to get the output into the log. It will not stay like that.", "I think best/final solution would not use davix-http at all and should do something similar to https://github.com/rucio/rucio/blob/50e0c7a5b9830b433a14a3b0cec5564ae2a08f51/lib/rucio/rse/protocols/webdav.py#L489", "@vokac i tried it like this in the beginning, for some reason this was not behaving well with X.509, maybe it's fine these days, worth a try!", "Code as simple as following one works fine for me with StoRM endpoint from CentOS7  ```python  import requests  session = requests.Session()  res = session.request('PROPFIND', \"{}{}\".format(storm_base, lfn), verify=False, timeout=timeout, cert=(x509, x509))  print(res.status_code)  print(res.text)  session.close()  ```  works fine for me. The main issue seems to be with protocol selection https://github.com/rucio/rucio/blob/50e0c7a5b9830b433a14a3b0cec5564ae2a08f51/lib/rucio/rse/protocols/webdav.py#L41-L44 because StoRM storages works only if you specify TLSv1.2 or don't use special adapter at all.", "Comment from @davidgcameron   you have cmd = 'davix-http --capath ...  10:31  add in the next line something like logger.log(cmd)  10:33  can you also make the timeout configurable?"], "3258": ["this was fixed together with issue #3257"], "3257": [], "3253": [], "3236": [], "3228": [], "3225": [], "3209": ["Could be fixed with the no-exception parameter. But to check the client config in a pure server functionality is already wrong. As you said, the client config should not be necessary on a server. I think the right change is to do this similar to other optional (extras) modules. Check if they are on path, and only load them then.  Similar to https://github.com/rucio/rucio/blob/7e32dee2c3af1fd51417481483449ca3d9c3bf28/lib/rucio/client/baseclient.py#L67", "I am working on the issue. Just wanted to ask what should be done here:  https://github.com/rucio/rucio/blob/870d36024db27f709d66e9b7c12974bf0edc6fe2/lib/rucio/web/rest/webpy/v1/authentication.py#L539  Does this need to be changed too?", "Hi Ruturaj, no this one is fine. The server does also use the rucio config file. It just should not be assumed that there is a `[client]` section in the server config. This is somewhat contradictive. "], "3204": ["Hi @rptaylor   I don't think there are any concrete plans to do that. @tbeerman or @ericvaandering do you know any plans about that?", "Not that I know of.  ", "Ok I asked around a bit but nobody knows anyone planning to do this. I will close the ticket until we can find somebody interested in this development."], "3201": [], "3196": [], "3193": [], "3188": [], "3187": [], "3182": [], "3181": [], "3178": ["Of course, the right choice is hyphen-case!    Compare:    `tpc-commission-davs: True`  `tpc-commission-root: True`    vs.    `tpc_commission_davs: True`  `tpc_commission_root: True`    vs.    `TPCCommissionDAVS: True`  `TPCCommissionROOT: True`", "Let's finish the collection of all attributes in #2630 first. Then we can organize a rename of the mismatched ones"], "3175": [], "3165": [], "3161": [], "3160": ["This problem also applies to get-limits    ```  > rucio-admin account get-limits dcameron CERN-PROD_DATADISK  'Client' object has no attribute 'get_account_limit'  Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers.  Traceback (most recent call last):    File \"/home/dcameron/dev/ddm/rucio/bin/rucio-admin\", line 129, in new_funct      return function(*args, **kwargs)    File \"/home/dcameron/dev/ddm/rucio/bin/rucio-admin\", line 400, in get_limits      limits = client.get_account_limit(account=args.account, rse=args.rse)  AttributeError: 'Client' object has no attribute 'get_account_limit'  ```", "I didn't include a fix for getting the account limits _yet_, as that functionality still seems to be possible with `rucio list-account-limits`.", "I think David is referencing #3200   But this can be addressed in a separate PR. ", "Thanks for the patch @bjwhite-fnal!", "Still seeing this in Rucio-admin version 1.21.6. Is there something wrong with our setup?  ", "Let me be more clear. There is still no get_account_limit in accountlimitclient    ", "Yes, these methods are in the `accountclient`. It is a bit confusing, but I think historically they have always been there.  In `rucio-admin` we are still pointing to the wrong methods though I think. It should query the `get_local_account_limits` and `get_global_account_limits`. @bjwhite-fnal do you perhaps have time to have a look on this?", "This is partly fixed in @cserf PR #3293 but not fully, since we should still enable the querying of the global limits with `rucio-admin` as well."], "3153": ["I think we can just show the ones in `DONE` and `FAILED` already. But under normal conditions the requests will only be in this state very shortly..."], "3138": [], "3134": ["there currently is no replica attribute that corresponds with the time the replica is physically created (i.e. state = AVAILABLE);  the base class created_at attribute stores when the **record** in the database is created,  and hence is in particular for a tape stage-in scenario not helpful at all;  best possible workaround is to filter on the updated_at attribute instead;  this could lead to false positives for the motivation use case, but these might need to be handled in any case due to a safety overlap in the poll cycle;  -> add updated_after parameter to all list_replicas functions and nested functions down to the place where the SQL query is composed (list_replicas, _list_replicas,  _list_replicas_for_datasets,  _list_replicas_for_files, ...) also to the recursive call for archives;  add a .filter(models.RSEFileAssociation.updated_at >= updated_after) clause to the replica retrieval queries   (will need about 10 lines of code added/changed)"], "3129": [], "3128": [], "3127": [], "3121": [], "3118": [], "3111": [], "3108": [], "3102": [], "3093": [], "3089": [], "3086": [], "3085": [], "3081": [], "3078": [], "3074": [], "3071": [], "3070": [], "3054": [], "3053": [], "3050": ["Thanks, @TWAtGH can you please have a look."], "3039": [], "3038": ["You can run `tools/run_tests_docker.sh -i` to do just the database init again. I'll add the rest to the docs.     For logshow, you are missing a pseudo-terminal in docker, are you sure you did `-t` to `docker exec` ? Also, make sure you run a recent docker version (e.g., the default in CentOS is `v1.xx`, you need at least `v12.xx`, that's 11 major releases in between ;-) If you install the one from the linked repo in the docs you will get `v19.xx`.", "Indeed I was using the default CentOS 7 version 1.13. After updating to 19.03.4 it works. Maybe you can add the requirement for a newer version to the doc as well?    "], "3035": ["Reading through the thread the rule lifetime also seems appropriate to me. We are doing something similar for the pure STAGING requests for the STAGING_AREA rules.  The `copy_pin_lifetime` we are setting already (to -1). Do we need to add a parameter called `desideredLifetime` as well?", "No, the `copy_pin_lifetime` needs to be set to the rule lifetime, then everything is fine. In lieu of a rule lifetime, we can set a default of something like 48 hours. FTS will release the SRM pin after the transfer out of the tapebuffer is done.", "sorry i still i don't understand what are you referring to here as desideredLifetime as parameter for FTS. The only parameter to set FTS side is the copy_pin_lifetime, which is set as default to 8 hours now as Rucio fills it to -1 when submitting the job ( or it does not fill it at all).    The _srmBringOnline.desideredLifetime_ is set by FTS  and it takes as value the copy_pin_lifetime value passed by the client.    What is changing from FTS to FTS cause this is a parameter on gfal2 is the _srmPrepareToGet.desideredLifetime_   but  it does not affect the pin lifetime of the file on disk after the file has been staged via srmBringOnline though as this is taken from the copy_pin_lifetime.    Anyway by changing gfal2 to avoid to run a srmPrepareToGet if the file is not online we also avoid  to have different desideredLifetime if the file is then staged by the srmPrepareToGet.    i know it's a mess but everything around SRM is a mess:-)  ", "just to mention that Castor does not take into account the pin lifetime as the pinning is disabled.  But both Castor at CERN and RAL will be  decommissioned and ATM they have huge buffer space so they don't have garbage collector issues. ", "We can assume that desideredLifetime  == Copy_pin_lifetime. I have followed too much the srm spec... I've modified the title too.  ", "There is a pin leak in FTS: For the Staging + transfer case, FTS creates two pins. One for `bringonline` and another for `srmpreparetoget`. \u00a0On transfer completion, only the pin generated by `srmpreparetoget` is released. The one from `bringonline` is kept. https://its.cern.ch/jira/browse/FTS-1491\u00a0    For now, the pin lifetime from Rucio to FTS cannot be set to the rule lifetime\u00a0as it will keep files for too long on the buffer.     As the same time, the current default pin lifetime is too short and doesn't protect the files: Files are staged several times, garbaged collected, etc before the final transfer. This also impact site performance in general.\u00a0\u00a0https://its.cern.ch/jira/browse/FTS-1492", "Since this was just discussed in the data carousel meeting:  If we don't set it to the rule lifetime (since this might be too long) is it sufficient to set it to 48h?", "So specifically changing  https://github.com/rucio/rucio/blob/c2e73d4a4c5583d195a6bc78ecf41fc79f9a8d67/lib/rucio/core/transfer.py#L835  to 48h"], "3032": [], "3028": [], "3022": [], "3019": [], "3013": [], "3011": [], "3006": [], "2996": [], "2993": [], "2992": ["rucio list-files actually shows different outputs for an empty and non existing dataset. Issue dismissed. "], "2990": [], "2987": [], "2986": [], "2980": ["Actually this should already be implemented on Rucio side. There is a parameter called something like `copy_traces_out`. Or is there something missing?", "I think this can be closed"], "2970": [], "2969": ["Need discussion", "This issue was tagged erroneously. It is not related to the thread information contained in the messages."], "2968": [], "2960": [], "2957": [], "2954": [], "2947": [], "2943": [], "2931": ["I decided to use TransferToolWrongAnswer so as not to make a whole new exception for this. If you instead prefer a dedicated TransferToolNoID exception, let me know. :)"], "2925": ["Please assign to me :)", "avec plaisir ;-)", "I think there is a bit of duplication between #2408, #2168 and this. Would be good to clarify what is done where. \ud83d\ude04 ", "Proposed changes here: https://github.com/davidgcameron/rucio/commit/b97ac11958dc7746f34328d0faf4e4b431003b88    I didn't want to make a pull request yet, since we should discuss how these changes may affect other clients (if there are any?) using the download and upload clients. I've tested that the pilot (with my latest changes that were merged) and rucio download and upload CLI work ok.", "thanks!   @TWAtGH, can you please have a look", "Why do we need to do changes to the API at all? Are the log strings not enough?  I think it was implemented like this because Pilot expected Rucio to throw an exception if an error occurs.", "Removing the exceptions might be a bit problematic yes.  Why not put the info within the exception? Thus in the downloadclient line 1423 and 1425?  Adding it to the logging is also fine, but passing the error structurally on to the pilot is probably better within an exception?", "Logs are not enough for callers of the API which need to pass error messages up to other systems (i.e. pilot passing the error back to panda to display on the panda monitor).    Exceptions are bad because in a bulk call you may not want to stop transfers just because one file failed. The exception also does not tell you which file failed unless you do some error-prone string parsing. The doc for download_pfns even says that it returns a dictionary with file states including FAILED, so I suppose there were not meant to be exceptions originally. Also I don't think that a transfer failure is an exceptional or unexpected situation so throwing an exception is not really the right thing to do.", "Alright, but there needs to be an aggregated error message per API call that could be passed to other systems like panda? Or how will it be displayed if one file fails because of no source found, another one fails because of checksum validation and a third one succeeds? 'Not all files downloaded' sounds reasonable for me at this point.    For the download the API will try to download all given files even if one fails. As Martin mentioned it should be possible to put one error string per file into the exception and this looks to me like the best (and non API breaking) solution. And I think this should then be done for every error and not just in case the protocol gives an error like in your code changes currently.", "Summary of face to face discussion (please correct anything I got wrong): no change in API so as not to break backwards compatibility. There are two options for error propagation: pass the file status dictionary as a keyword argument in the exception, or use the information in the traces that is passed to download client (traces_copy_out) and filled with information.", "The commits above revert the previous changes and instead fill the 'stateReason' of the traces with error messages. I had to add a parameter to the uploadclient to support passing a reference to a traces list (it was already in the download client).", "Thanks a lot!  Looks fine to me except some small comments:    In the download client you have this unused variable `state_reason`:  https://github.com/davidgcameron/rucio/blob/8d0019545fe11cb626f0acd7e8a98fdbee1644b7/lib/rucio/client/downloadclient.py#L519    And I think you missed at least two points which raise errors quite frequently:  1. no sources available:  https://github.com/davidgcameron/rucio/blob/8d0019545fe11cb626f0acd7e8a98fdbee1644b7/lib/rucio/client/downloadclient.py#L475  2. failed checksum validation:  https://github.com/davidgcameron/rucio/blob/8d0019545fe11cb626f0acd7e8a98fdbee1644b7/lib/rucio/client/downloadclient.py#L590    Is there a reason the upload client uses the `state_reason` variable instead of writing it directly to the trace dict like the download client?    And one less important point: is `stateReason` as key name in the traces dict  already used somewhere or could it might be changed to e.g., `errorDescription` ? That would also make clear why it's empty on success :)", "Thanks for the comments. I fixed the minor issues.    > Is there a reason the upload client uses the state_reason variable instead of writing it directly to the trace dict like the download client?    The download client sends a trace for each attempt whereas the upload client only sends the trace after all attempts have finished, so this is the reason to keep `state_reason` outside the attempt loop. I'm not sure if this difference is intentional or not.    > And one less important point: is stateReason as key name in the traces dict already used somewhere or could it might be changed to e.g., errorDescription ? That would also make clear why it's empty on success :)    Maybe @tbeerman could answer that one, it could be used for example for the automatic declaration of lost files.  ", "Travis tests still fail due to:    ```  $ if [[ $SUITE == \"all\" ]]; then docker exec -it rucio /bin/sh -c \"/opt/rucio/tools/run_tests_docker.sh\" ; fi  Error: No such container: rucio  The command \"if [[ $SUITE == \"all\" ]]; then docker exec -it rucio /bin/sh -c \"/opt/rucio/tools/run_tests_docker.sh\" ; fi\" exited with 1.    ```  Is that expected?", "Did you try to restart or does it consistently fail? Sometimes this one shows up but I think this is more travis related.", "The tests also fail in the pull requests but with an Oracle error which I think is unrelated to my changes."], "2924": [], "2917": [], "2916": [], "2912": [], "2911": ["To me that makes sense, but @cserf is mostly an advocate of not making these commands too accessibly. \ud83d\ude04   But I think you can go ahead and implement that."], "2907": [], "2905": [], "2899": [], "2896": ["We have to make a bit of a policy decision here. Significant operations like deleting an RSE or re-creating it we always considered strictly manual operations, since it can have quite severe impacts. @cserf do you have an opinion how we should handle this with CRIC? Keep it manual?", "Well, as it is the importer right now it 'deletes' all RSEs that are not part of the JSON.", "i'm still in favour of my old proposal that we should have an RSE state, so not only exists/not_exists, but also something like enabled/suspended/decomissioning/removed_from_cric, etc... right now this can all be handled via RSE attributes, but I think it would make sense to have this as a dedicated RSEState.", "As discussed today, we should make the \"merge\" strategy configurable here.  - If not present in CRIC, delete RSE  - If not present in CRIC, keep RSE and make deletion only manual  @hahahannes, @arisfkiaras can one of you have a look on this please.", "Ok, I guess you mean soft delete the RSE?  Also, how do we deal with a \"new\" RSE with the same name in the first case? Should we reactivate and then update it"], "2895": ["In the rebalancing script we need to update scopes/account strings to InternalScope/InternalAccount. This should be not too difficult to fix. @dchristidis can you maybe have a look since @TomasJavurek is not available at the moment? Thanks!"], "2886": ["The conversion is done in the permission API, this is correct. The problem here is that sometimes we do comparisons such as:  `if issuer in rse_attr.get('rule_deleters').split(','):`  Which does not work, since the issuer object is looked for in a list of strings. This needs to be adapted to `issuer.external`.  See https://github.com/rucio/rucio/blob/525812b8f83f1069d38ab78aebedb732f21e77ec/lib/rucio/core/permission/atlas.py#L473  There might be other places in the permission code which do that as well.", "@hahahannes Can you please have a look in the permission APIs and adapt these lines? (Wherever issuer `in` is used. Thanks!"], "2882": [], "2878": [], "2877": [], "2874": [], "2872": [], "2866": [], "2865": [], "2861": ["@mlassnig Any progress on this one?    Please let me know if/how I can help to solve this.", "Hi Lionel, sorry, too much going on at the time, can't raise the priority on this ticket right now. If you want to help out fixing the code, there's a neat way now with the recently overhauled development dockers, you can have a look here: https://github.com/rucio/rucio/tree/master/etc/docker/dev", "I'm looking into this again. I might have ideas, finally.  ", "@ericvaandering This is great news. I confirm that the problem is still present."], "2858": ["This patch also includes a fix for an error reported by the Undertaker:  ```  2019-08-23 09:40:25,439 6947    CRITICAL        Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/rucio/daemons/undertaker/undertaker.py\", line 99, in undertaker      delete_dids(dids=chunk, account=InternalAccount('root'), expire_rules=True)    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 356, in new_funct      result = function(*args, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 584, in delete_dids      rucio.core.rule.archive_localgroupdisk_datasets(scope=did['scope'], name=did['name'], session=session)    File \"/usr/lib/python2.7/site-packages/rucio/common/policy.py\", line 96, in new_funct      return function(*args, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 372, in new_funct      result = function(*args, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 3113, in archive_localgroupdisk_datasets      rucio.core.did.set_status(scope='archive', name=name, open=False, session=session)    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 372, in new_funct      result = function(*args, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 1476, in set_status      values['bytes'], values['length'], values['events'] = __resolve_bytes_length_events_did(scope=scope, name=name, session=session)    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 372, in new_funct      result = function(*args, **kwargs)    File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 1734, in __resolve_bytes_length_events_did      did = session.query(models.DataIdentifier).filter_by(scope=scope, name=name).one()    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3275, in one      ret = self.one_or_none()    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3244, in one_or_none      ret = list(self)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3317, in __iter__      return self._execute_and_instances(context)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3342, in _execute_and_instances      result = conn.execute(querycontext.statement, self._params)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 988, in execute      return meth(self, multiparams, params)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 287, in _execute_on_connection      return connection._execute_clauseelement(self, multiparams, params)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1107, in _execute_clauseelement      distilled_params,    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1182, in _execute_context      e, util.text_type(statement), parameters, None, None    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1466, in _handle_dbapi_exception      util.raise_from_cause(sqlalchemy_exception, exc_info)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py\", line 383, in raise_from_cause      reraise(type(exception), exception, tb=exc_tb, cause=cause)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1179, in _execute_context      context = constructor(dialect, self, conn, *args)    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py\", line 745, in _init_compiled      for key in compiled_params    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py\", line 745, in <genexpr>      for key in compiled_params    File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/type_api.py\", line 1196, in process      return impl_processor(process_param(value, dialect))    File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/types.py\", line 191, in process_bind_param      raise InvalidType('Cannot insert to db. Expected InternalScope, got string type.')  ```"], "2855": [], "2841": [], "2840": ["Hi @arisfkiaras   Yes, please have a look! Thanks \ud83d\ude04 ", "Interesting enough the ```rucio add-did-meta``` seems not to be working at all on the rucio dev container.  Even though the alembic version is the same as my production server (2cbee484dcf9) on the rucio dev db the table did_meta does not include the 'updated_at' and 'created_at' columns while on my production db it does. ", "Yes, I also noticed that and already opened #2462. But couldnt find a fix so far", "Thanks Hannes, I had not seen that issue.    Another interesting finding, test_did_meta.py seem to succeed for any RucioException.  https://github.com/rucio/rucio/blob/master/lib/rucio/tests/test_did_meta.py#L55    I guess we can discuss this tomorrow in the meeting in conjunction with some generic metadata query times."], "2835": [], "2823": ["Hmmm, alembic runs fine for me in the container, both down and up.", "Or is this a problem with master/next being out of sync (when they shouldn't be?)", "I made some wrong changes on the next branch with the feature PR https://github.com/rucio/rucio/pull/2478 that removed the dots in some alembic files."], "2822": ["The thought we had was that we could set and get a value in the cache with the Dogpile interface to evaluate the status of the caching system when the regions are created. However, we were not sure where in the codebase to place such functionality. I was thinking in core/, but there is not an obvious core module for placement of such a utility.", "Maybe `lib/rucio/common/utils.py` is a good place, it has a couple other dogpile related functions in there already.", "In principle I think raising a warning is a good idea, so that there is at least a trace for this, but this might spam the log quite a bit. We always considered the caching backends as something optional, so we should not spam the users who decide not to run any.   Make the caching part of rucio.cfg, so we can track this caching decision? See #2812 ?"], "2819": [], "2812": ["Hopefully Travis doesn't break this time."], "2805": [], "2802": [], "2799": [], "2788": [], "2787": [], "2777": [], "2772": [], "2771": ["I would like to see a fix for this as well. I checked out the latest Rucio source this morning and found the bug with the /setup_demo.sh script to still be present.", "We are moving away from the `demo` containers in favor of the `dev` containers which are much more advanced. @mlassnig is the error above an issue for the dev ones? I don't think so, as it uses root containers for storage emulation?", "demo needs to be removed, it's unmaintained. see here: https://github.com/rucio/rucio/pull/2936", "Demo has been removed;"], "2769": ["After chatting with Stephane we figured out what is actually needed ;-) Since XCache supports http, we should allow xcache usage not only for root URLs but also for http URLs.", "(this also means forcefully mangling our typical davs:// into https://)"], "2764": [], "2763": [], "2760": [], "2755": ["Yes, there is already an ongoing discussion in #2690 about this. @hahahannes proposed to switch to statsd too.", "Great. Looks like that has progressed to a pull request even, so let's close this."], "2754": ["I just confirmed with GFAL people that GFAL indeed does **not** create any parent directories.", "I think this is just a mistake. I think my intention was to ensure that the the directories are not created for RSEs with URL signing enabled (in which case the protocol is `https` and `self.renaming` is true). So the condition should be `not (self.renaming and dest[:5] == 'https')` instead. I will submit a pull request to correct this."], "2747": [], "2737": [], "2736": [], "2732": [], "2731": ["Hi, can you maybe check if there is some exception output in /var/log/rucio/httpd_error_log.  Also try to rebuild the Docker image. I had the same problem when I build it on the next branch and then changed to another branch based on master where some dependency was missing.", "```  [Mon Jul 15 09:32:09.172307 2019] [:error] [pid 6] [client 127.0.0.1:55116] mod_wsgi (pid=6): Target WSGI script '/opt/rucio/lib/rucio/web/rest/authentication.py' cannot be loaded as Python module.  [Mon Jul 15 09:32:09.172346 2019] [:error] [pid 6] [client 127.0.0.1:55116] mod_wsgi (pid=6): Exception occurred processing WSGI script '/opt/rucio/lib/rucio/web/rest/authentication.py'.  [Mon Jul 15 09:32:09.172380 2019] [:error] [pid 6] [client 127.0.0.1:55116] Traceback (most recent call last):  [Mon Jul 15 09:32:09.172406 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/opt/rucio/lib/rucio/web/rest/authentication.py\", line 36, in <module>  [Mon Jul 15 09:32:09.172746 2019] [:error] [pid 6] [client 127.0.0.1:55116]     from rucio.api.authentication import (get_auth_token_user_pass,  [Mon Jul 15 09:32:09.172771 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/api/authentication.py\", line 22, in <module>  [Mon Jul 15 09:32:09.172981 2019] [:error] [pid 6] [client 127.0.0.1:55116]     from rucio.api import permission  [Mon Jul 15 09:32:09.173002 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/api/permission.py\", line 17, in <module>  [Mon Jul 15 09:32:09.173110 2019] [:error] [pid 6] [client 127.0.0.1:55116]     from rucio.core import permission  [Mon Jul 15 09:32:09.173130 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/permission/__init__.py\", line 38, in <module>  [Mon Jul 15 09:32:09.173266 2019] [:error] [pid 6] [client 127.0.0.1:55116]     from .atlas import *  # NOQA pylint:disable=wildcard-import  [Mon Jul 15 09:32:09.173287 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/permission/atlas.py\", line 25, in <module>  [Mon Jul 15 09:32:09.173896 2019] [:error] [pid 6] [client 127.0.0.1:55116]     import rucio.core.did  [Mon Jul 15 09:32:09.173942 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 46, in <module>  [Mon Jul 15 09:32:09.175187 2019] [:error] [pid 6] [client 127.0.0.1:55116]     import rucio.core.rule  [Mon Jul 15 09:32:09.175208 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 52, in <module>  [Mon Jul 15 09:32:09.177980 2019] [:error] [pid 6] [client 127.0.0.1:55116]     import rucio.core.replica  # import get_and_lock_file_replicas, get_and_lock_file_replicas_for_dataset  [Mon Jul 15 09:32:09.178005 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/replica.py\", line 54, in <module>  [Mon Jul 15 09:32:09.179967 2019] [:error] [pid 6] [client 127.0.0.1:55116]     from rucio.core.credential import get_signed_url  [Mon Jul 15 09:32:09.180003 2019] [:error] [pid 6] [client 127.0.0.1:55116]   File \"/usr/lib/python2.7/site-packages/rucio/core/credential.py\", line 43, in <module>  [Mon Jul 15 09:32:09.180205 2019] [:error] [pid 6] [client 127.0.0.1:55116]     import boto3  [Mon Jul 15 09:32:09.180233 2019] [:error] [pid 6] [client 127.0.0.1:55116] ImportError: No module named boto3  [Mon Jul 15 09:32:09.181110 2019] [ssl:debug] [pid 6] ssl_engine_io.c(993): [client 127.0.0.1:55116] AH02001: Connection closed to child 0 with standard shutdown (server localhost:443)  ```    So I pip-installed it and it runs through. It is still running so I can't report it to go through to the end yet, but the first issue seems to be tackled by the extra pip install inside the container (FYI: I run the container using docker-compose which basically pulls it from docker hub; I haven't tested building it using the Dockerfile in the dev subdirectory). ", "Cheered a bit too soon. I think one of the last tests seems to fail as well.   ```  ======================================================================  ERROR: REPLICA RECOVERER: Testing declaration of suspicious replicas as bad if they are found available on other RSEs.  ----------------------------------------------------------------------  Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/nose/case.py\", line 381, in setUp      try_run(self.inst, ('setup', 'setUp'))    File \"/usr/lib/python2.7/site-packages/nose/util.py\", line 471, in try_run      return func()    File \"/opt/rucio/lib/rucio/tests/test_replica_recoverer.py\", line 51, in setUp      assert_true(exitcode == 0)  AssertionError: False is not true    ======================================================================  FAIL: REAPER (DAEMON): Test the reaper daemon.  ----------------------------------------------------------------------  Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/nose/case.py\", line 197, in runTest      self.test(*self.arg)    File \"/opt/rucio/lib/rucio/tests/test_reaper.py\", line 47, in test_reaper      nose.tools.assert_equal(exitcode, 0)  AssertionError: 127 != 0  -------------------- >> begin captured stdout << ---------------------  ('rucio-reaper --run-once --rses MOCK', '', '/bin/sh: rucio-reaper: command not found\\n')    --------------------- >> end captured stdout << ----------------------  -------------------- >> begin captured logging << --------------------  root: INFO: main: starting processes  root: INFO: Reaper: This instance will work on RSEs: MOCK  root: INFO: Starting Reaper: Worker 0, child 1 will work on RSEs: MOCK  root: INFO: Reaper 0-1: Running on RSE MOCK None  root: DEBUG: RSE: MOCK, source_for_total_space: storage, source_for_used_space: storage  root: INFO: Reaper 0-1: Space usage for RSE {'rse_type': DISK, 'region_code': None, 'ASN': None, 'city': None, 'ISP': u'Brookhaven National Laboratory', 'rse': u'MOCK', 'created_at': datetime.datetime(2019, 7, 15, 9, 44, 3), 'deterministic': True, 'availability': 7L, 'time_zone': None, 'longitude': None, 'continent': u'NA', 'updated_at': datetime.datetime(2019, 7, 15, 9, 44, 3), 'staging_area': False, 'volatile': False, 'deleted': False, 'country_name': u'United States', 'latitude': None, 'deleted_at': None, 'id': 'd7c8e037e0a64b46b537a3ba57836b68'} - max_being_deleted_files: 10, needed_free_space: None, used: None, free: None  root: INFO: Reaper 0-1: free space is above minimum limit for MOCK  root: DEBUG: Reaper 0-1: list_unlocked_replicas on MOCK for 0 bytes in 0.03817486763 seconds: 0 replicas  root: INFO: Reaper 0-1: No replicas to delete MOCK. The next check will occur at 2019-07-15 10:41:03.842946  root: INFO: Graceful stop requested  root: INFO: Graceful stop done  --------------------- >> end captured logging << ---------------------  ```  Seems like rucio-reaper doesn't exist in this version.", "Confirming that using a dev container built from the repository itself solves the first issue. So adding the build instructions to the documentation, or pushing a new dev container to docker hub would fix that one.  Second (reaper) issue is still there.", "Yes, the new docker image should be already pushed. I will have a look for the reaper issue", "Now it complains that  ```  Sync rse_repository  Traceback (most recent call last):    File \"tools/sync_rses.py\", line 30, in <module>      json_data = open(rse_repo_file)  IOError: [Errno 2] No such file or directory: 'etc/rse_repository.json'  Failed to sync!  ```  Probably I can fix it by linking /etc in in the docker-compose, but I don't know if that's the intention.", "Sorry, I moved the dev container config to our container repository to setup automated builds on docker hub but I forgot this file. Should be fixed now if you pull again from docker hub.", "@ygrange Does it work for you?", "I\u2019m on leave now. Will have a look at it early next week!  -- Y. Grange  > Op 25 jul. 2019 om 16:00 heeft Hannes Hansen <notifications@github.com> het volgende geschreven: >  > @ygrange Does it work for you? >  > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub, or mute the thread.  ", "Still can't find `reaper`. I decided to updte the checked out version to 1.20.3 and I get another bunch of errors looking line this:  ```  ======================================================================  FAIL: SCOPE (REST): send a POST to create a already existing scope to test the error  ----------------------------------------------------------------------  Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/nose/case.py\", line 197, in runTest      self.test(*self.arg)    File \"/opt/rucio/lib/rucio/tests/test_scope.py\", line 98, in test_scope_duplicate      assert_equal(res1.status, 200)  AssertionError: 500 != 200  -------------------- >> begin captured stdout << ---------------------  ```", "I think the `reaper` error is related to it not having a `bin/` script like the other daemons. For the reaper we tried to build this on install of the package, which might not happen like this in the container. (We probably should go back to bin scripts)", "https://github.com/rucio/rucio/pull/2773 just passed and ready for merge, this fixes the reaper issue"], "2730": [], "2727": [], "2725": [], "2720": [], "2717": [], "2715": [], "2707": [], "2702": [], "2701": [], "2691": [], "2681": [], "2678": [], "2672": [], "2671": [], "2664": [], "2661": ["In case of cli I get:    [root@rucio-nagios-prod-02 ~]# rucio -v download file.that.does.not.exist.txt  2019-06-18 16:41:08,655\tINFO\tProcessing 1 item(s) for input  2019-06-18 16:41:08,655\tDEBUG\tProcessing item file.that.does.not.exist.txt  2019-06-18 16:41:08,655\tDEBUG\tRSE-Expression: istape=False  2019-06-18 16:41:08,655\tDEBUG\tSplitted DID: file:file.that.does.not.exist.txt  2019-06-18 16:41:08,782\tERROR\tData identifier not found.  Details: Data identifier 'file:file.that.does.not.exist.txt' not found  2019-06-18 16:41:08,782\tDEBUG\tThis means that the Data IDentifier you provided is not known by Rucio.    In case of pilot logs:  Not easily feasible with the Exceptions that we have. Imagine you have a bulk of files, some of them you download and some of them not. Than only thing, that you can raise at the end is: \"Not all of the files have been downloaded.\" We can introduce a new exception, where I list all the files that have not been downloaded and the reason for that.  ", "Hmmm... Can you check with Rod which release he uses ?", "My problem was interactive with the client, not from the pilot.", "Which release? Do you have an option to try with some of the newest ones? Like the testing release at ALRB?", "current one. try it with a non-existent file.  Cheers,  Rod."], "2656": [], "2653": [], "2652": [], "2649": [], "2640": [], "2638": ["submitted to probe repo"], "2633": [], "2632": [], "2625": [], "2622": [], "2620": ["This test is wrong:    https://github.com/rucio/rucio/blob/48f46fe34d11b8c34eedecb57df7054efe4340b6/lib/rucio/client/uploadclient.py#L159-L161    - it should be `if pfn and is_deterministic:`. But also, is it a good idea to switch to no_register here? - this might be more confusing than just failing.", "I think this is correct. In case of non-deterministic sites, pfn is obligatory option and only way to upload there. However, one sometimes still want to register such a dids.", "The problem could be with the is_deterministic flag, how it was set for given RSE that you want to upload to? In case of the ATLAS, it is defined in AGIS (CRIG).", "There's an earlier check that the pfn is defined for non-deterministic RSEs. The test I quoted says if pfn is provided and the RSE is non-deterministic, then set `no_register = True`. That's the opposite of what the warning message says. And it means that for a non-deterministic RSE the file will be uploaded but not registered.    (And the non-deterministic flag was set correctly - first thing I checked)", "I managed to reproduce the problem. It occurs only when:  --register-after-upload  is used.", "--register-after-upload is a slightly different problem - without it, on a non-deterministic RSE, the upload works, but the file isn't registered:  ```  $ rucio -v upload --rse FNAL_DCACHE_PERSISTENT --lifetime 3600 --scope np04_reco_keepup --pfn gsiftp://fndca1.fnal.gov:2811/pnfs/fnal.gov/usr/dune/persistent/RSE/dunepro/protodune/np04/beam/output/detector/full-reconstructed/11/54/96/00/np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root  2019-06-11 08:01:02,994\tINFO\tPreparing upload for file np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root  2019-06-11 08:01:02,994\tWARNING\tUpload with given pfn implies that no_register is True, except non-deterministic RSEs  2019-06-11 08:01:03,073\tINFO\tTrying upload with gsiftp to FNAL_DCACHE_PERSISTENT  2019-06-11 08:01:09,009\tINFO\tSuccessfully uploaded file np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root  Completed in 12.5664 sec.  $ rucio list-file-replicas --rse FNAL_DCACHE_PERSISTENT np04_reco_keepup:np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root  +---------+--------+------------+-----------+----------------+  | SCOPE   | NAME   | FILESIZE   | ADLER32   | RSE: REPLICA   |  |---------+--------+------------+-----------+----------------|  +---------+--------+------------+-----------+----------------+  ```  with --register-after-upload  ```  2019-06-11 08:00:10,845\tINFO\tPreparing upload for file np04_raw_run007301_0014_dl7_reco_17438455_0_20190326T164443.root  2019-06-11 08:00:10,846\tWARNING\tUpload with given pfn implies that no_register is True, except non-deterministic RSEs  2019-06-11 08:00:11,497\tERROR\tAn unknown exception occurred.  Details: no error information passed (http status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97'))  Completed in 17.1711 sec.  ```", "I've updated the pull request to fix the --register-after-upload issue as well. But I can still see a few potential issues    - On a deterministic RSE, if you provide a pfn it will automatically switch to no-register mode. This appears to be what was intended, only it was doing it for _non_-deterministic RSEs instead, but it seems dangerous to me; I think it'd be better to make these fail unless --no-register is explicitly given.  - With --register-after-upload, if the file exists it tests if it already registered and skips it if it is. But the check is if the DID exists, not if there's a replica at this RSE. So if the DID has been used before it won't be added to the RSE even though the file is there.  - This is getting off the scope of this ticket a bit, but rsemanager.exists() fails badly with an unknown error exception for non-deterministic RSEs if there is no file replica at the RSE. This is inconsistent with deterministic RSEs where it just returns false.  ", "I doesn't work even with api. Furthermore, the registration doesn't take into account name of the file, and rather attempt to register according to the name of the source file.     ", "> I've updated the pull request to fix the --register-after-upload issue as well. But I can still see a few potential issues  >   >     * On a deterministic RSE, if you provide a pfn it will automatically switch to no-register mode. This appears to be what was intended, only it was doing it for _non_-deterministic RSEs instead, but it seems dangerous to me; I think it'd be better to make these fail unless --no-register is explicitly given.  >   >     * With --register-after-upload, if the file exists it tests if it already registered and skips it if it is. But the check is if the DID exists, not if there's a replica at this RSE. So if the DID has been used before it won't be added to the RSE even though the file is there.  >   >     * This is getting off the scope of this ticket a bit, but rsemanager.exists() fails badly with an unknown error exception for non-deterministic RSEs if there is no file replica at the RSE. This is inconsistent with deterministic RSEs where it just returns false.    1) our WM system uses pfn without registration. If an user attempts to upload something with pfn, it will be soon deleted by the agent that deletes non-registered data.     2) yes. I think it is better to check DID. If we check replica only, it could attempt to register did, that is already present at different rses. We don't allow reuse of the same dids.    3) thx, to be fixed.   ", "> our WM system uses pfn without registration. If an user attempts to upload something with pfn, it will be soon deleted by the agent that deletes non-registered data.  Does it use --no-register (or the API equivalent)? Because I'm arguing that automatically changing between registration/no registration depending on the RSE type is potentially error prone. If it's explicit then there's no problem.", "> > our WM system uses pfn without registration. If an user attempts to upload something with pfn, it will be soon deleted by the agent that deletes non-registered data.  > > Does it use --no-register (or the API equivalent)? Because I'm arguing that automatically changing between registration/no registration depending on the RSE type is potentially error prone. If it's explicit then there's no problem.    yes, no_register flag in the API. But right, it doesn't sound generic enough. ", "excluding 'not' from: _if pfn and not is_deterministic_  solves the problem with registration.  the register_after_upload is however still failing. ", "ah, ok I am bit behind. register_after_upload doesn't work exactly because rsemgr.exists() is failing for non-deterministic sites, that @illingwo already mentioned above. That needs to be fixed first. I'll open separate ticket for that.", "was this the travis issue why it failed? I'll restart the pr from @illingwo as well, an that one would be preferred to approve."], "2616": ["Hi Boris, you can try to set the rule to stuck `rucio update-rule --stuck` which should issue a recount of the locks.    `x1t_SR001_170409_0531_tpc:raw` is a dataset with all 27 files part of the dataset? So not a container with multiple levels of containers/datasets?", "I tried the \"rucio update-rule --stuck <id>\" before.  And now it is shame on me, obviously it just took quite long to \"repair\" the file miss-count. But I can confirm that the observed bug from Rucio version 1.8 about the wrongly counted files for a certain rule is gone with 1.19."], "2614": [], "2612": ["This issue will be used only for modifications of rucio/core libraries. Second issue will be created for the IAM prototype script. "], "2611": [], "2607": [], "2606": ["Hi Lionel,  Thanks for reporting; We will have a look!"], "2601": [], "2600": ["The last commit is meant for discussion.  Especially, I am not sure whether it is not better to:  1) place and propagate external_traces directly in the methods of Download client in place of making it parameter of the class.   2) not to create separate method _update_traces, in place of adding it to _send_trace method. It might be enough just to rename it to _send_or_update_trace()    did is populated directly in the traces, but we still can make it dict: {did:trace} if needed for pilot. ", "The list is used as an object to which the reference is propagated from pilot to the client. I could propagate directly the trace objects from pilot, but that wouldn't be generic enought in my opinion. I don't see any issues with datasets/bulks/archives, but @TWAtGH might be of different opinion."], "2597": [], "2592": ["You still call new configuration third party read/write and I'm not completely sure what exactly does this mean. For XRootD it is necessary that destination endpoint supports TPC, because only pull mode is available, but for WebDAV you can either contact destination endpoint and use pull mode or it is also fine if you contact source endpoint and use push mode. Currently FTS use for WebDAV pull mode first and than automatically fallback to push (it is possible to configure FTS not to try push) ... this makes things even more interesting in case of multisource transfers and I have no idea how FTS currently deals with such TPC transfers in case of push fallback. I guess new third party read/write configuration doesn't try to cover all possible scenarios (push model)."], "2591": [], "2588": ["Looks like they are trying to get the oracle xe docker image back to dockerhub or to give travis access to it. I think that might be the only solution  https://github.com/oracle/docker-images/issues/1156"], "2579": [], "2576": [], "2560": [], "2556": [], "2553": [], "2548": ["AFAIK, the repo directory should be mounted with `-v `pwd`: ` to get the setup.py file available.", "You cannot mount anything during the build process. It is mounted when the container is actually started. For the setup.py to work during the build process the whole rucio directory would have to be copied into the container. It was working before without setup.py and I will revert it to that. If you need to run setup.py it can still be done after container has been started.", "yes, I realised my confusion afterwards. `COPY . /etc/rucio` should fix it otherwise.", "Doest it work with dockercompose: `etc/docker/dev/docker-compose.yml` ? I.e., `../../../../rucio/:/opt/rucio/`    This was how I tested and didn't see this error. `python setup develop` was also put there to deploy properly the package and dependency within the container.    ", "No, the mounting in docker-compose doesn't work. Also the change of the RUCIOHOME from `/opt/rucio`/ to `/etc/rucio` introduces some problems, e.g. the aliases for httpd are using `/opt/rucio`. You were able to build it and get it running?", "yes, it worked for me.  I might have changed the aliases as well. I should check the pull request.", "After some checks, `docker-compose --file etc/docker/dev/docker-compose.yml up -d` works for me and   ```  [root@35f4df04509c rucio]# more  /opt/rucio/etc/web/aliases-py27.conf   # Rucio REST  WSGIScriptAlias /accounts                /opt/rucio/.venv/lib/python2.7/site-packages/rucio/web/rest/account.py  ...  ``` points against my env. "], "2545": ["Another place with hardcoded protocols is here:  https://github.com/rucio/rucio/blob/master/lib/rucio/rse/protocols/gfal.py#L187  but it is perhaps irrelevant for this case, because storm won't get on protocol level. ", "Last place is here:  https://github.com/rucio/rucio/blob/master/tools/probes/common/check_sync_rses_with_agis#L137  introducing new protocol might fail the check. ", "It works now, but keeping the ticket open for following actions:  The requirements I remember now were  - to print the davix-http command and stdout/err to the log when it fails.  - check existence of the file to link to  - check using archive works. Make link to zip and extract required file to workdir.  - more generally, for all protocols, optimize common case where multiple files from the same archive are to be staged.  - allow configurable retries in API  - bulk download command for all inputs for job: parallel, retries, traces, return result object to pilot", "There is more concrete ticket now:  https://github.com/rucio/rucio/issues/3285  "], "2541": [], "2536": [], "2535": [], "2528": ["C/P from email:    \"\"\"  Hi,    Since this is R&D I don't like the idea of exposing this in the full API (I mean in the python replica client, server sided it's fine). Hence I like the (b) approach a bit more. Or alternatively we do not add a parameter to the method but make this configurable via a cfg parameter.    Server side it is more flexible, we can just do a wrapper around list_dataset_replicas which enhances the result with the vp query.    Cheers,  Martin  \"\"\"", "Just to elaborate on this: If it is just something we want to try out, let's either do a separate function (like Ilija suggested) or hide the activation of this somehow. We shouldn't pollute the public API with a parameter we use for testing (Even if it defaults to something) - Especially, since nobody else can use the VP service anyway.  Once this is production ready we can decide how to integrate it."], "2525": ["@tbeerman can you please du the `etc/docker` part of the cleanup"], "2521": [], "2520": ["I think @mlassnig did not add these on purpose, since mysql previously did not support check constraints. If this is supported now, we should add them again.", "Correct. MySQL did not support check constraints. I leave the glory of adding them to the migrate scripts to you ;-)", "btw, no need to edit the old scripts, just add a new one which adds them all in one go", "I would add them in the appropriate migrate scripts. It's more work, but this way we can roll back to a specific table version also with mysql.", "Seems like they should get created automatically by sqlalchemy as they are the enum types described in the models.py. Needs to be investigated why they dont get created."], "2519": [], "2518": ["Hi @dciangot   We had quite some issues the last days with the `fts3-rest-api` package, since it does quite a hacky workaround with including the `M2Crypto` package, which was killing the building of the entire rucio package.    This is now temporarily fixed with #2653, but it will for sure come up later, thus I would prefer if we can get the `fts3-rest-api` package out of the dependency tree completely. Right now it is used in the delegate function inside the `transfertool/fts3` and also used by some other methods in `transfertool/fts3myproxy`. Thus, if at all, this is only used by CMS, since ATLAS also doesn't use this kind of delegation. If you have some time on your hand it would be really great to rewrite this functionality, so we can remove the dependency. (Or remove it for now and add it back later, if not used)    Thanks!    Cheers,  Martin"], "2514": [], "2511": ["After some discussion, the approach will use the `core.config` module.", "Originally I wanted to support this as well:    `rucio-admin config set --section 'root-proxy-internal' --option 'MWT2' --value 'root://my.proxy.host:1094'`    but I could not figure out (even with AllowEncodedSlashes and the quote_plus trick) to make mod_wsgi keep two slashes together. So for now, we will have to do it like this:    `rucio-admin config set --section 'root-proxy-internal' --option 'MWT2' --value 'my.proxy.host:1094'`    and prepare the `root://` prepend in `list_replicas`."], "2505": ["I just noticed this as well. It seems there is a new version of M2Crypto since a few days (which is a dependency of fts3-rest-API. Need to check what the actual problem is. Some threads suggest to additionally install swig, but this doesn't seem a good solution to me.", "I fixed the demo build by adding this line in the Dockerfile, before the rucio package resolution starts:    RUN pip install --ignore-installed ipaddress  **RUN pip install M2Crypto==0.32**  RUN pip install rucio rucio-webui    This will trick pip into not installing the latest version. Whether it works or not runtime I still didn't check ", "I fixed this directly in our dependency file to fix M2Crypto to <0.33. This should also fix the demo issue."], "2502": [], "2501": [], "2500": [], "2498": ["It's not particularly relevant to the question but ...    > Standard gridftp do not support multiple checksum algorithms without the installation of additional plugins    This statement is incorrect.  Adler32 support was added in 2016 (see https://github.com/gridcf/gct/commit/e0cd71abf9bc491f75933044fb4401c5ef06662f#diff-2c3c5b72076f8c6e8b30031f83bee8b2).  Older GridFTP servers were also quite annoying in that they would provide the md5 sum regardless of what algorithm you requested.    (It's still good to handle multiple checksum algorithms better in Rucio for other reasons...)", "To me it feels logical to add the possibility to specify the checksum algorithm(s) to be used for a destination RSE. This could just be an RSE attribute with an enumerated list. Right now Rucio only supports adler32 and md5 and basically prefers adler32 over md5, but it is more flexible to make this decision per RSE. This somewhat overlaps with the discussion we had in #2410 where we discussed to basically offer arbitrary checksum support, if communities want to import their historic crc checksums.", "> It's not particularly relevant to the question but ...  >   > > Standard gridftp do not support multiple checksum algorithms without the installation of additional plugins  >   > This statement is incorrect. Adler32 support was added in 2016 (see [gridcf/gct@e0cd71a#diff-2c3c5b72076f8c6e8b30031f83bee8b2](https://github.com/gridcf/gct/commit/e0cd71abf9bc491f75933044fb4401c5ef06662f#diff-2c3c5b72076f8c6e8b30031f83bee8b2)). Older GridFTP servers were also quite annoying in that they would provide the md5 sum regardless of what algorithm you requested.  >   > (It's still good to handle multiple checksum algorithms better in Rucio for other reasons...)    Hi Brian,    Yeah, that's right. I have to rephrase that statement to something in the lines of \"some older gridftp versions do not [...]\". \ud83d\udc4d    Thanks for the heads up!", "Hi all,    For the records the branch I am working in to implement this functionality is here: https://github.com/gabrielefronze/rucio/tree/chksumsPR  Cheers,    Gabriele"], "2496": [], "2494": [], "2493": ["Removed from `tools/pip-requires-test`  It is still pulled in as a sub-dependency from Flask though, but with `2.10.1`"], "2488": [], "2485": [], "2482": [], "2480": ["@bari12  Maybe we should change the python3 test to check all changed files now instead of only files with the py3 header"], "2479": ["Fixed by #2725 "], "2469": [], "2468": ["@TWAtGH this will still need an extension in the downloadclient. list_replicas replies with `['']` in the json case (already now), and (after this patch) with `'<?xml version=\"1.0\" encoding=\"UTF-8\"?><metalink xmlns=\"urn:ietf:params:xml:ns:metalink\"></metalink>'` in the metalink case."], "2465": [], "2462": ["Just did some testing.  The database is initialized with did_meta having the columns `created_at` and `updated_at` but then the following migration is applied and the fields are missing.  https://github.com/rucio/rucio/blob/master/lib/rucio/db/sqla/migrate_repo/versions/688ef1840840_adding_did_meta_table.py    My suggestion would be to explicitly add the fields on the DIDMeta model and create a new migration.  ", "But in that case, it is enough to just add a new migration to \"fix\" it, since the columns are created from the models.py correctly (But dropped/added wrongly by the migration)", "So, I have fixed it in my own branch and also re-enabled the tests that were set to always pass.  I could open a pull request for review or do a combined one when I have https://github.com/rucio/rucio/issues/2840 ready."], "2460": ["I have marked a few dependencies in the issue text where I am not sure why we still need this. Does anyone remember?", "You can drop also :  - pygeoip==0.3.2 # GeoIP API"], "2456": [], "2446": [], "2445": [], "2444": [], "2443": [], "2440": [], "2436": [], "2434": ["The original point was to have a ChangeLog there, the copyright is just put to the header of each file. But since we are not using the ChangeLog, I agree, we should remove it.", "maybe keep it and mention in it that the changes are described in https://github.com/rucio/rucio/tree/master/doc/source/releasenotes instead.", "OK I can do this (@vingar's suggestion)", "Great, thanks! \ud83d\udc4d "], "2433": ["with defining a state:  [root@rucio-nagios-prod-02 grid_testing]# python update_state_bug.py  Traceback (most recent call last):   File \"update_state_bug.py\", line 7, in <module>     rucio_client.update_replicas_states(rsename, [{'scope':scope, 'name':filename, 'state':'A'}])   File \"/usr/lib/python2.7/site-packages/rucio/client/replicaclient.py\", line 238, in update_replicas_states     raise exc_cls(exc_msg)  rucio.common.exception.UnsupportedOperation: The resource doesn't support the requested operation.  Details: State AVAILABLE for replica user.ddmadin:test_2019-04-03.txt on UNI-FREIBURG_DATADISK cannot be updated  but right this gives different error message  ", "Is it possible that this replica was already in state `A`?", "hmmm, my bug, as usual. Scope is wrong. But still, the exception should be turned to something more understandable. Like, did doesn't exist or so.", "It's an optimisation of queries here. We don't check if the DID exists, we just execute an UPDATE query and check if it affected a row. If not, the error is raised. (Which could be: replica was already in that state, replica is in a state which should not be changed, replica does not exist etc.) It's difficult to give a specific error here without additional queries.  I am not sure if we should change that, since this is also not really a command a simple user is exposed to."], "2432": [], "2421": ["  had to revisit this, mysql5 support must not be lost ;-)"], "2413": [], "2412": ["Done with my part. I also addressed the Protection of sources too strict"], "2411": [], "2408": ["Isn't this a duplicate from #2168 ?", "Might be. Not sure yet. This is in traces, while the information can be still somewhere in the logs. It even might be, that this is actually more ticket to pilot than Rucio.", "But thx for pointing to the other ticket here. Can we link them together somehow?", "#2168 is about the logging messages while this one aims for more details in the traces.  We could maybe do a single issue for this. I think the complete error handling in the download and upload client should be overhauled.", "#2953   Can we close this? Log verbosity should actually be fine."], "2403": [], "2402": ["The conveyor should probably just stop starting, with an error, if no scheme is specified.", "I would have expected to have the conveyor to figure out automatically which schemes/protocols should be used for third party transfers out of the ones available for the replicas: here it was `gsiftp`, `davs`, `srm`.    Note also here the non expected behaviour of the `judge-repairer` after the configuration has been corrected to use `gsiftp`.", "It's also an option: If no scheme is given, just use all compatible ones.    For the `judge-repairer` I don't think anything has to be changed. The issue here is in the conveyor, as the request probably stays in it's state forever and never get's failed. Thus this is never reported back to the rule. For the judge it looks like that the request is still in submission.", "One observation after such error: the request stays in `then N`, `NO_SOURCES` state and then disappeared. The `judge-repairer` keeps it in the `S` state forever.", "Is it possible that there is no conveyor-finisher running? The finisher should process the `NO_SOURCES` ones and mark them as failed.    https://github.com/rucio/rucio/blob/39f66c08d429b09e18fe5b5500eca9ccc10b1869/lib/rucio/daemons/conveyor/finisher.py#L113", "The `finisher` runs too. ", "Can you track if the replicas affected by this bug are actually handled by the finisher?  But in any way, this has to be fixed in the submitter, than the workflow through the finisher/repairer is fine again.", "I'm seeing some messages in the finisher like requeing etc. Don't you have enough information to reproduce the problem and investigate more from your side ? ", "I only tested the `NO_SOURCES` one, and this works fine in my finisher. But I didn't test the full error yet. But yes, information is sufficient \ud83d\udc4d "], "2399": ["The PFN download doesn't query the Rucio server for PFN's so it doesn't have the checksums. Thats why `ignore_checksum=True` by default. And in my opinion it should stay like this.", "I would like to see chacksum after any download, no matter on download method. But I agree, that we don't want to add another query to Rucio DB. So, only option would be to make it optional, and pass the checksum from pilot, user etc. Pilot should have the src checksum stored in fspec object."], "2398": ["Hello,  could you also make sure that the message is clear in this case (copy on TAPE which cannot be accessed with 'rucio download'). In the previous case, it is just that the RSE does not exist.    lappsl6b.in2p3.fr> rucio -v download --rse RAL-LCG2_MCTAPE mc16_13TeV:AOD.15164692._000021.pool.root.1  2019-03-26 17:13:27,262 INFO    Processing 1 item(s) for input  2019-03-26 17:13:27,263 DEBUG   Processing item mc16_13TeV:AOD.15164692._000021.pool.root.1  2019-03-26 17:13:27,263 DEBUG   RSE-Expression: (RAL-LCG2_MCTAPE)&istape=False  2019-03-26 17:13:27,264 DEBUG   Splitted DID: mc16_13TeV:AOD.15164692._000021.pool.root.1  2019-03-26 17:13:27,361 DEBUG   1 DIDs after processing input  2019-03-26 17:13:27,361 DEBUG   Processing: {'resolve_archives': True, 'name': 'AOD.15164692._000021.pool.root.1', 'did': 'mc16_13TeV:AOD.15164692._000021.pool.root.1', 'rse': '(RAL-LCG2_MCTAPE)&istape=False', 'no_subdir': False, 'nrandom': None, 'transfer_timeout': 3600, 'scope': 'mc16_13TeV', 'type': u'FILE', 'force_scheme': None, 'base_dir': '.'}  2019-03-26 17:13:27,478 DEBUG   <?xml version=\"1.0\" encoding=\"UTF-8\"?>  <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\">    2019-03-26 17:13:27,484 DEBUG   Traceback (most recent call last):    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/bin/rucio\", line 159, in new_funct      return function(*args, **kwargs)    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/bin/rucio\", line 968, in download      result = download_client.download_dids(items, args.ndownloader, trace_pattern)    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 291, in download_dids      input_items = self._prepare_items_for_download(items)    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 914, in _prepare_items_for_download      files_with_pfns = self._parse_list_replica_metalink(metalink_str)    File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 996, in _parse_list_replica_metalink      raise error  ExpatError: no element found: line 3, column 0    2019-03-26 17:13:27,484 ERROR   no element found: line 3, column 0  2019-03-26 17:13:27,489 ERROR   Strange error: No section: 'policy'  lappsl6b.in2p3.fr> rucio list-file-replicas mc16_13TeV:AOD.15164692._000021.pool.root.1  +------------+----------------------------------+------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+  | SCOPE      | NAME                             | FILESIZE   | ADLER32   | RSE: REPLICA                                                                                                                                                                                                                                                                                                                        |  |------------+----------------------------------+------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  | mc16_13TeV | AOD.15164692._000021.pool.root.1 | 5.798 GB   | 197562c0  | BNL-OSG2_DATADISK: srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/BNLT0D1/rucio/mc16_13TeV/56/7e/AOD.15164692._000021.pool.root.1                                                                                                                                                                         |  | mc16_13TeV | AOD.15164692._000021.pool.root.1 | 5.798 GB   | 197562c0  | RAL-LCG2_MCTAPE: srm://srm-atlas.gridpp.rl.ac.uk:8443/srm/managerv2?SFN=/castor/ads.rl.ac.uk/prod/atlas/simRaw/atlasmctape/mc16_13TeV/AOD/e5814_e5984_s3126_r10724_r10726/mc16_13TeV.345120.PowhegPy8EG_NNLOPS_nnlo_30_ggH125_tautaul13l7.merge.AOD.e5814_e5984_s3126_r10724_r10726_tid15164692_00/AOD.15164692._000021.pool.root.1 |  | mc16_13TeV | AOD.15164692._000021.pool.root.1 | 5.798 GB   | 197562c0  | MWT2_DATADISK: srm://uct2-dc1.uchicago.edu:8443/srm/managerv2?SFN=/pnfs/uchicago.edu/atlasdatadisk/rucio/mc16_13TeV/56/7e/AOD.15164692._000021.pool.root.1                                                                                                                                                                          |  +------------+----------------------------------+------------+-----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+  ", "I think we could add an extra message, if the selected RSE is skipped because it is a tape RSE. That shouldn't be too difficult to add.  @TWAtGH Line 209 in the downloadclient should also be rewritten similar to line 1156. ", "Hey,  the error message for a metalink parsing error did change with 1.19.  The error here is that the metalink returned by list_replicas is incomplete (I guess an exception in the REST frontend).    If list_replicas doesnt't return any replicas for a file the message is something like 'No sources found'. This message should be enough @bari12 no?  In the case of this ticket we could print a more user friendly message like 'Error in server response'.    @bari12 we already discussed in a Rucio meeting that the code of line 209 can be removed #2021 \t", "BTW regarding 'this should raise RSENotFound'. I'm not sure about that. It's currently not really expected that list_replicas raises exceptions for reasons like this. Would need to check this.", "Problem confirmed in list_replicas REST frontend. Metalink starts streaming, then later the exception is raised, causing the client to think everything is in order."], "2390": [], "2379": [], "2378": ["Please let me know if I can begin working upon this, if the contributors feel this is a worthy change.", "Yes, please go ahead \ud83d\ude09 "], "2375": ["I would like to be assigned to this issue."], "2370": ["please look into this pr.  "], "2367": ["i have tweaked the dockerfile to remove any isssues during installation  "], "2363": [], "2362": ["```  (.venv) [13:49][twegner@twegner-dev rucio]$ rucio download \"238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\"  2019-03-25 13:50:19,406\tINFO\tProcessing 1 item(s) for input  2019-03-25 13:50:19,406\tINFO\tGetting sources of DIDs  2019-03-25 13:50:20,992\tINFO\tUsing main thread to download 0 file(s)  2019-03-25 13:50:20,993\tERROR\tNone of the requested files have been downloaded.  ```  could you please give debug output?", "```  # rucio -v  download  \"238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\"  --rse EISCAT   2019-03-25 12:56:10,672\tINFO\tProcessing 1 item(s) for input  2019-03-25 12:56:10,673\tDEBUG\tnum_unmerged_items=1; num_dids=1; num_merged_items=1  2019-03-25 12:56:10,673\tINFO\tGetting sources of DIDs  2019-03-25 12:56:10,673\tDEBUG\tschemes: ['davs', 'gsiftp', 'https', 'root', 'srm', 'file']  2019-03-25 12:56:10,673\tDEBUG\trse_expression: (EISCAT)\\istape=true  2019-03-25 12:56:10,673\tDEBUG\tnum DIDs for list_replicas call: 1  2019-03-25 12:56:10,942\tDEBUG\tnum resolved files: 1  2019-03-25 12:56:10,949\tDEBUG\t\"unzip -v\" returned with exitcode 127  2019-03-25 12:56:10,954\tDEBUG\t\"tar --version\" returned with exitcode 0  2019-03-25 12:56:10,954\tDEBUG\tnum list_replicas calls: 1  2019-03-25 12:56:10,954\tDEBUG\tQueueing file: 238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar  2019-03-25 12:56:10,955\tDEBUG\treal parents: set([])  2019-03-25 12:56:10,955\tDEBUG\toptions: {'238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar': {'ignore_checksum': False, 'transfer_timeout': 3600, 'destinations': set([('.', False)])}}  2019-03-25 12:56:10,955\tDEBUG\tPrepared sources: num_sources=0/0; num_non_cea_sources=0; num_cea_ids=0  2019-03-25 12:56:10,956\tERROR\t%d format: a number is required, not NoneType  2019-03-25 12:56:10,956\tDEBUG\tThis means the parameter you passed has a wrong type.  Completed in 0.2852 sec.  ```", "Thanks. The commit should fix it. Difficult to test though.", "It fixes the error message but it still doesn't download the file. It's a regression as a previous `rucio download `version can download the file, e.g., `1.17.8`.", "That's correct. It cannot be downloaded because there are no PFNs. I thought this is maybe expected.  Could you show me a `list-file-replicas --metalink` please?", "```  bash-4.2# rucio list-file-replicas --metalink  \"238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\"   <?xml version=\"1.0\" encoding=\"UTF-8\"?>  <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\">   <file name=\"manda4_cp1_3.00u_SW20140322.part1.tar\">    <identity>238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar</identity>    <hash type=\"adler32\">d9901472</hash>    <size>1954140160</size>    <glfn name=\"/eiscat3d/rucio/238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\"></glfn>    <url location=\"NDGF\" domain=\"wan\" priority=\"1\" client_extract=\"false\">gsiftp://preprod-srm.ndgf.org:2811/eiscat3d/rucio/238357;238356;238359/e6/19/manda4_cp1_3.00u_SW20140322.part1.tar</url>    <url location=\"NDGF\" domain=\"wan\" priority=\"2\" client_extract=\"false\">https://preprod-srm.ndgf.org:443/eiscat3d/rucio/238357;238356;238359/e6/19/manda4_cp1_3.00u_SW20140322.part1.tar</url>   </file>  </metalink>  ```", "Thanks. For the download you used `--rse EISCAT` but these rerplicas are on 'NDGF'.", "Nope, one replica is on EISCAT:  ```  # rucio list-file-replicas   \"238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\"   +----------------------+---------------------------------------+------------+-----------+--------------------------------------------------------------------------------------------------------------------------+  | SCOPE                | NAME                                  | FILESIZE   | ADLER32   | RSE: REPLICA                                                                                                             |  |----------------------+---------------------------------------+------------+-----------+--------------------------------------------------------------------------------------------------------------------------|  | 238357;238356;238359 | manda4_cp1_3.00u_SW20140322.part1.tar | 1.954 GB   | d9901472  | EISCAT: http://www.eiscat.se:37008/238357;238356;238359/manda4_cp1_3.00u_SW20140322.part1.tar                            |  | 238357;238356;238359 | manda4_cp1_3.00u_SW20140322.part1.tar | 1.954 GB   | d9901472  | NDGF: gsiftp://preprod-srm.ndgf.org:2811/eiscat3d/rucio/238357;238356;238359/e6/19/manda4_cp1_3.00u_SW20140322.part1.tar |  +----------------------+---------------------------------------+------------+-----------+--------------------------------------------------------------------------------------------------------------------------+  ```"], "2359": ["i would like this to be assigned to me.(Sorry uploaded a pr before hand)"], "2355": [], "2352": [], "2344": [], "2336": [], "2327": [], "2322": [], "2319": [], "2315": ["As discussed, the RSE selector in core/rse_selector.py needs to updated to first check local quotas and then global quotas. If a RSE does not have quota left from either local or globa quotal, it will get removed from the list of possible destination RSEs.    It still needs to be discussed how the free space should be calculated/displayed to the user. ", "How should I modify the CLI commands to display the limits and usage?  ```  rucio list-account-usage root  +-------+---------+---------+--------------+  | RSE   | USAGE   | LIMIT   | QUOTA LEFT   |  |-------+---------+---------+--------------|    rucio list-account-limits root  +-------+---------+  | RSE   | LIMIT   |  |-------+---------|  ```  Should I just leave it as it is and only show local quota because it might be confusing to mix it with global quotas? On the other side, it might be weird to see a rule failing but to also have enough (local) quota  ", "We could add the rse-expressions of the global quotas there as well (similar rows like the others) but this might be confusing. It might be better to separate this completely and have, essentially two tables.", "Yes, I was thinking the same. It is too messy in one table, also because the global quota is independent from one specific RSE"], "2314": [], "2313": [], "2306": [], "2305": [], "2304": ["I'll try to tackle this one."], "2299": [], "2298": [], "2291": [], "2290": [], "2288": [], "2285": [], "2277": [], "2266": [], "2265": ["@dchristidis are you also taking care of this one?", "Issue #1849 was actually fixed by @cserf."], "2262": ["Can be closed as duplicate of #2128"], "2261": [], "2257": [], "2253": ["Should I also add it to the account abacus ?", "Yes please, if we do it for one we might as well do it for both. \ud83d\ude04 ", "Should I add the history table to models.py or use `models.RSEUsage.__history_mapper__.class_` to access the history table? I think, adding it to models.py is better as we are not using the sqlalchemy versioned table?", "The plan was to change all of these tables with #2063 anyway.  However I think we should change them explicitly in this separate PR and not intermix them, just get's easier to track. So for now, keep using the history mapper."], "2249": [], "2248": [], "2245": [], "2240": [], "2238": [], "2237": [], "2233": [], "2227": [], "2226": ["This is no longer needed due to #2313 ?", "In the current implementation the download priories the root download (no explicit extraction) if:  1. the file is not in a zip that will be downloaded anyway (e.g., zip is prio 1 for another file)  2. the file is not in a zip with at least 10 other files that are needed    This decision is made there:  https://github.com/rucio/rucio/blob/master/lib/rucio/client/downloadclient.py#L1337"], "2221": [], "2220": ["(1) I wonder if we also need something in the submitter, since the decision should be on the tuple (src,dst,dataset). Right now, I don't see it, but we should keep it in mind.    (2) The \"deadline\" until datasets from tape are to be released should be configurable via core.config, with a default of 1 hour.", "Oh, the deadline can be different per RSE.", "How should the chosen strategy be configured? In the cfg file or config table or as daemon argument?", "Seems like MySQL has problems with the original version of core/request:release_waiting_requests as well my new query for grouped FIFO.     ### LIMIT and IN not supported together  - `This version of MySQL doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'`  - It seems like mysql never supported the query for core/request:release_waiting_requests  `UPDATE requests SET state=%s, updated_at=%s WHERE requests.id IN (SELECT requests.id AS requests_id \\nFROM requests \\nWHERE requests.dest_rse_id = %s AND requests.state = %s ORDER BY requests.requested_at ASC \\n LIMIT %s FOR UPDATE)`  - this means that the throttler is not working with mysql    ### Updating table while selecting from it  - `You can't specify target table 'requests' for update in FROM clause`"], "2219": [], "2217": ["Please also add a test case for this."], "2212": ["This still isn't quite right:    ```  dune_rucio_prod=> ALTER TYPE 'BAD_REPLICAS_STATE_CHK' RENAME TO 'BAD_REPLICAS_STATE_CHK_OLD';  ERROR:  syntax error at or near \"'BAD_REPLICAS_STATE_CHK'\"  LINE 1: ALTER TYPE 'BAD_REPLICAS_STATE_CHK' RENAME TO 'BAD_REPLICAS_...                     ^  ```    Those need to be double quotes, not single (or omitted entirely).", "It's worse than that. This won't work at all:    ```  ALTER TYPE \"BAD_REPLICAS_STATE_CHK\" RENAME TO \"BAD_REPLICAS_STATE_CHK_OLD\";    ALTER TABLE bad_replicas ADD CONSTRAINT \"BAD_REPLICAS_STATE_CHK\" CHECK (state in ('B', 'D', 'L', 'R', 'S', 'T'));    ALTER TABLE bad_replicas ALTER COLUMN state TYPE 'BAD_REPLICAS_STATE_CHK';    DROP TYPE 'BAD_REPLICAS_STATE_CHK_OLD';    ```    It renames the enum, tries to add a check constraint to the column, which fails because the column type is still the enum, then it would try to change the column type to a non-existent type, which would fail if it ever got that far.", "I think this works:    ```          if context.get_context().dialect.name == 'postgresql':               # For Postgres the ENUM Type needs to be changed to the new one and the old one needs to be dropped               op.execute('ALTER TYPE \"BAD_REPLICAS_STATE_CHK\" RENAME TO \"BAD_REPLICAS_STATE_CHK_OLD\"')  # pylint: disable=no-member               new_type = sa.Enum('B', 'D', 'L', 'R', 'S', 'T', name='BAD_REPLICAS_STATE_CHK')               new_type.create(op.get_bind())               op.execute('ALTER TABLE bad_replicas ALTER COLUMN state TYPE \"BAD_REPLICAS_STATE_CHK\" USING state::text::\"BAD_REPLICAS_STATE_CHK\"')  # pylint: disable=no-member               op.execute('DROP TYPE \"BAD_REPLICAS_STATE_CHK_OLD\"')  # pylint: disable=no-member           else:               drop_constraint('BAD_REPLICAS_STATE_CHK', 'bad_replicas', type_='check')               create_check_constraint(name='BAD_REPLICAS_STATE_CHK', source='bad_replicas', condition=\"state in ('B', 'D', 'L', 'R', 'S', 'T')\")   ```", "Thanks @illingwo! I'm currently rewriting all the alembic migrate scripts (now properly, via https://github.com/rucio/rucio/issues/2257) and was stumped by this as well."], "2207": [], "2203": [], "2200": [], "2199": [], "2194": [], "2186": [], "2185": [], "2181": [], "2180": [], "2177": [], "2174": [], "2173": [], "2168": ["The upload client part will be improved with #3084   The rsemgr/protocol part will be improved with the rsemgr overhaul.  So I close this one"], "2166": [], "2161": ["I wonder if we should make this configurable via the config table?", "> I wonder if we should make this configurable via the config table?    We should. IMHO all configuration that is not absolutely necessary to make a connection to the rucio server (client side), and to make the database connection (daemon, server side), should be moved into the config tables. New configs should not go into the .cfg", "The `--fts-source-strategy` makes sense as an override, but per default I think it just should come from the config table. But re-reading the ticket, I guess that's what @cserf meant anyway.", "I was thinking about completely getting rid of the `--fts-source-strategy` and just use the config table. If one has 2 ways to specify the source strategy, that's not obivious which one has the precedence on the other.", "It might make sense to have both ways, if we just want to test a strategy at one conveyor, without impacting all of them. (Thus command line has precedence) - But I am not really sure if it is needed for the source selection strategy."], "2160": ["I don't think this can be made generic, but I'm happy to be proven wrong ;-) The symlinking itself is simple enough, but for StoRM you'll need the additional webdav/etag lookup.    So, something like `storm://` should be perfectly reasonable (unless you can find a nice way to make it generic, then it would be something like `ln://`)"], "2156": [], "2155": [], "2154": [], "2149": ["Before you go through the effort, we should decide if we want to move all the bin/ scripts to console scripts, or everything back to bin/  Is there any major added benefit of the console_scripts? We had one issue where a dependency conflict was only affecting the console_scripts, not the bin/ scripts. So we should get this consistent in one way or the other."], "2144": [], "2142": ["@hahahannes can you please have a look on this.  In https://github.com/rucio/rucio/blob/ca3166ff5e63b7f24b61236646c2aa747f8aa40b/lib/rucio/core/account.py#L180  we probably need to join models.IdentityAccountAssociation with models.Identitiy to add the email there."], "2141": [], "2140": [], "2137": [], "2133": [], "2128": [], "2127": [], "2126": [], "2122": [], "2118": [], "2115": [], "2112": [], "2107": [], "2105": [], "2103": ["Duplicate of #2040 - It's fixed in 1.19.0"], "2102": ["This is a duplicate of #1749 - it's already merged. \ud83d\ude04 "], "2095": ["Fixed by #2461 "], "2090": [], "2089": [], "2080": [], "2076": ["Note - this seems to affect also other options stored in merged_options e.g. ignore_checksum"], "2069": [], "2065": [], "2054": [], "2051": [], "2048": ["This needs to be fixed in the downstream monitoring. Not a bug on our side, we are reporting correctly the site."], "2045": ["I had a quick look on this (without conclusion) and think this has something to do with the protocol (scheme) caching in the conveyor. The spacetoken is part of the extended attribute, it might be that (for some reason) it gets cached without one and recalled later (expecting one) - to be confirmed.", "Due to session reuse in FTS. Not a Rucio bug :-)"], "2040": [], "2039": [], "2038": ["No feedback = success...", "Yep, like rm, cp, or mv \ud83d\ude04 "], "2030": ["Could you maybe have a look on #1917 as well? It sounds quite related.", "Duplicate of #1729."], "2028": ["Notes:   # pip-requires  - numpy: There are some complications with numpy and wsgi, this needs to be handled separately in another issue. numpy stays at `1.14.2`    # pip-requires-client  - pylint: Upgrading pylint to `1.9.4` instead of `2.2.2`. 1.9.4 is the last version which is py2.7 compatible, everything else only py3.  - flake8 and pycodestyles: Leaving them at their respective version, as a new version generates some errors. This will be handled in a separate PR."], "2022": ["Thinking a bit more about my implementation it is probably not the best/correct one, because it doesn't really apply in case DownloadClient/UploadClient is called through API. I'll first have a look how it is used in pilot and than I could come back with different solution for use case described above.", "Yeah, I was about to comment that. I think it really has to be added to the DownloadClient and Uploadclient.", "Now, I'm even thinking if we should polute rucio code with this functionality or if it is job that should be done directly in pilot ... I'll discuss that with @TomasJavurek ", "I briefly discussed how to filter RSE protocols used by jobs on specific site and this functionality should go directly in pilot.    So drop this request."], "2021": ["Would be good to remove this and get it into the 1.20 release, as it creates some confusion.  There is some inconsistency there though, as we remove `rsemanager.download` (moved to downloadclient) but still use `rsemanager.upload` -> Should make a plan to remove it there too.", "I think, we can just remove it without touching any other part of the code. ", "Maybe but please check bin/rucio for something like `--archive-file` option", "right:  https://github.com/rucio/rucio/blob/master/bin/rucio#L988  ", "`download_file_from_archive` has been removed. `rsemanager.download` is still there, needs to be removed later."], "2020": ["the error coming from ```python3 bin/rucio-admin rse``` seems to be caused by a change how argparse handles errors in python2/3    ```python  import argparse    parser = argparse.ArgumentParser()  subparsers = parser.add_subparsers()  first_subparser = subparsers.add_parser('first', help='help first')  second_subparser = first_subparser.add_subparsers()  third_parser = second_subparser.add_parser('second', help='help second')  third_parser.add_argument('--version', help='version')    args = parser.parse_args()  print(args)  ```  ```shell  python file.py first #  error: too few arguments  python3 file.py first # Namespace()  ```"], "2015": [], "2010": [], "2007": [], "2006": ["One more, seems like the exception handler is borked (x509 is lowercase now, but cert does not exist)      ```  rucio-env) [rjoshi@rucio-bastion ~]$ rucio -v -a rjoshi -S x509 whoami    given client cert (/opt/rucio/etc/web/client.crt) doesn't exist    2019-01-21 09:53:37,149\tERROR\tCannot authenticate.    Details: x509 authentication failed    2019-01-21 09:53:37,150\tDEBUG\tTraceback (most recent call last):      File \"/opt/rucio-env/bin/rucio\", line 160, in new_funct        return function(*args, **kwargs)      File \"/opt/rucio-env/bin/rucio\", line 300, in whoami_account        client = get_client(args)      File \"/opt/rucio-env/bin/rucio\", line 273, in get_client        elif auth_type == 'x509_proxy':    UnboundLocalError: local variable 'auth_type' referenced before assignment        2019-01-21 09:53:37,151\tERROR\tlocal variable 'auth_type' referenced before assignment    2019-01-21 09:53:37,151\tERROR\t    Rucio exited with an unexpected/unknown error.    Please rerun the last command with the \"-v\" option to gather more information.    If it's a problem concerning your experiment or if you're unsure what to do, please followup at: alastair.dewhurst@cern.ch    If you're sure there is a problem with Rucio itself, please followup at: https://github.com/rucio/rucio/issues/    Completed in 0.0028 sec.  ```"], "2001": [], "2000": [], "1990": [], "1987": [], "1986": ["After discussion with @cserf: The blocking of these requests is essentially wanted. --allow-tape is reserved for admins. Regular users should not download from tape (uncontrolled staging requests to the robots) and rather transfer the data to a scratchdisk. So everything works as expected. Closing"], "1985": [], "1977": ["Reason was identified as a problematic query which changes the query plan. Plan stabilisation needed"], "1976": ["The check if a rule exists (uniqueness) is already done. The missing part is the uniqueness of list_child_datasets"], "1973": [], "1970": [], "1967": [], "1965": ["This adds a new RSE limit parameter 'MaxSpaceAvailable'. An alternative would be to use the free attribute of the rse counter entry for the relevant RSE, but there'd have to be some race-free way of setting it."], "1961": [], "1958": [], "1955": [], "1954": [], "1953": ["Now FTS issue a `409` when a double submission is done. Will modify the conveyor to handle this."], "1952": [], "1948": [], "1945": [], "1942": [], "1937": ["Not needed"], "1932": [], "1929": [], "1924": [], "1922": [], "1918": [], "1917": [], "1912": [], "1910": ["Kerberos is handled as an extra in setup.py so you would have to do a `pip install rucio[kerberos]` which installs `['kerberos>=1.2.5', 'pykerberos>=1.1.14', 'requests-kerberos>=0.11.0']`  I am not sure why it trys to load kerberos modules, in the baseclient it basically checks if `requests_kerberos` is available and if yes, loads HTTPKerberosAuth. Might be some issue with a very old requests_kerberos version?", "Ah-ha!    Looks like an old client has snuck into `PYTHONPATH`.  Line numbers above don't match with the current code."], "1909": ["Adding high priority flag cause this is causing major pain now"], "1901": ["Created PR #1902   Maybe we may want to test it on the CERN server before merging.", "Hi Andrea,  You created this PR against the next branch (Thus it is treated as a feature). This will then only go in the Rucio 1.19.0 release in February 2019. Is this fine? Otherwise please re-create as a patch.  See https://github.com/rucio/rucio/blob/master/CONTRIBUTING.rst", "Hi Martin,   ok sorry. I'm recreating the PR.", "Created #1904", "That's against the **hotfix** branch \ud83d\ude04  I'll create the two PRs for **master** and **next**. ", "Although I just saw you based your development off the **hotfix** branch (instead of master). It's not super clean to merge it like that. It would probably work fine, but we should stick to the normal workflow here.", "I'm really sorry.. I've read the doc too quickly and understood patches go in hotfix (I do not know how, since is clearly stated). I'll recreate 2 PR one for next and one for master (both from a branch based on master, if I understand correctly)", "Hi Andrea, no worries. The workflow we have is sadly not very simple for somebody new, since it is somewhat different than most projects do it, so I can understand the confusion. Just ping me on Slack if you have any question about the setup. \ud83d\ude04 "], "1898": [], "1888": ["You can ignore dq2client. It is deprecated and will be removed in less than 6 months.", "downloadclient can be ignored also, because the last python3 problem is the round() function that behaves different in python2/3 but we decided to ignore this because of the incompatibility of the fix  `from builtins import round` (see #1845 )"], "1885": ["I don't think we need the reaper-preparer. I mean the key difference here is that the main reaper does the partitioning different, on hash of dids and not RSEs. But it would work exactly the same if the selection happens on the expired replicas, instead of the ones marked as `BEING_DELETED` by the preparer.    The thing we have to get right here is not to overload an RSE. We probably would need some kind of cross-communication of the reapers here. Another thing where we have to be careful (It's probably fine, but just raising the point) is the whole \"remove the dataset once the last replica is deleted\" workflows, as there might be race conditions if different reapers delete on the same RSE.", "The problem if you hash on DIDs is that it can be you can get millions of files on the top of the queue on the same RSE. In that case, there will be no deletion on the other RSEs and one can quickly go into trouble. What I propose is to build a list of replicas to delete where each RSE gets a share (e.g. it can be `needed_free_space`). It would be something like :  ```  rep_to_delete = []  tot_share = sum of all needed_free_space on all rses  for rse in rses:      get a list of unlocked replicas on rse      take needed_free_space/tot_share replicas from the previous list and insert into rep_to_delete  return rep_to_delete  ```  Of course, all of this can be done in `list_unlocked_replicas`, but that makes this method a bit complicated, that's why I was thinking about a new daemon.    ", "Another proposal was chosen"], "1882": [], "1879": ["This is a duplicate of #1878 \ud83e\udd23 "], "1878": ["Actually I thought that this table gets automatically filled by sqlalchemy?", "Nope, apparently this was a procedure as well, which we deactivated in 2017. ", "The new procedure has been implemented."], "1874": [], "1871": ["Isn't this a duplicate of #403   I also don't think this should be a probe, but a service/daemon, since it is really a central functionality. Or do you suggest to do a probe first and the service later?", "Ah, right I forgot about the other ticket. I'm not sure it deserves to be a daemon.", "I think we should aim to have these functional parts, which are potentially useful for a wider audience, as daemons; We should try to get away from probes for these things."], "1870": [], "1864": [], "1861": [], "1853": [], "1852": ["Fixed by https://github.com/rucio/rucio/pull/1921"], "1849": [], "1848": [], "1845": ["Not sure why this was not caught by the Python 2.6 test though?", "I will remove the imports and add a basic test to the python2.6 client test to check at least imports. Also the pylint check for python3 has to be changed to ignore round errors."], "1842": [], "1841": [], "1836": ["destination or source ?", "Destination", "See also #53 ", "How would you change the behaviour? By a value in the config table ?", "I think this can be handled after #2220 because the grouped FIFO will also throttle over all activites per destination. So it will be easy add it to the current FIFO way.", "According to #2542, this is implemented. @hahahannes, can you confirm please ?", "This should be fixed by #2726 because there I add the new way of changing between all the different modes."], "1835": [], "1833": [], "1825": [], "1818": ["As discussed in #1820: If there is an API/CLI to access history information, we should add an index per default. If there is no API/CLI we still might want to add one, if this information is frequently queried by operations."], "1814": [], "1811": [], "1810": [], "1809": [], "1807": [], "1805": ["Issue to be solved with FTS people:  - the submission python api is missing the following file attributes:    - `selection_strategy`  - and the following job attribute:    - `s3alternate`    Opening a ticket to FTS support: [FTS Jira ticket](https://its.cern.ch/jira/browse/FTS-1339)", "Also, made a pull request to allow bulk status requests from the client (https://gitlab.cern.ch/fts/fts-rest/merge_requests/17)", "Requiring also to implement the following function in the FTS bindings:  - update priority  - query_latest  - set se status  - version    Needed before completing the migration", "This is superseded by #2518 Closing"], "1804": ["This is actually a change which is more relevant to #2444 since the algorithm needs to decide if to include greey RSEs or not.", "The new reaper relies on the RSE attribute `greedyDeletion`, so I propose to close this ticket."], "1803": ["In the medium term the plan was to get these VO specific modules (schema, policy, permissions, lfn2pfn algorithms) out of the core package. We discussed this some time ago and we want to offer two different ways to do that. It's summarized it in #533   If you can wait for that ticket to make progress, I would just leave it like this, otherwise I would suggest we move it to `rucio/lib/rucio/common/schema/` until we make progress with #533 ", "I think I'd like to get into schema while we wait for #533. This avoids us having to build our own containers and chase versions. It would allow us to just use rucio/containers. Maybe we can get a CMS person to look into #533."], "1792": ["Traceback (most recent call last):   File \"/opt/rucio/lib/rucio/daemons/bb8/t2_background_rebalance.py\", line 111, in <module>     rebalance_rse(source_rse['rse'], max_bytes=available_target_rebalance_volume, dry_run=False, comment='T2 Background rebalancing', force_expression=destination_rse['rse'])   File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 351, in new_funct     result = function(*args, **kwargs)   File \"/usr/lib/python2.7/site-packages/rucio/daemons/bb8/common.py\", line 339, in rebalance_rse     for scope, name, rule_id, rse_expression, subscription_id, bytes, length in list_rebalance_rule_candidates(rse=rse, mode=mode):   File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 360, in new_funct     raise DatabaseException(str(error))  rucio.common.exception.DatabaseException: Database exception.  Details: (cx_Oracle.DatabaseError) ORA-01476: divisor is equal to zero  [SQL: u\"SELECT dsl.scope as scope, dsl.name as name, rawtohex(r.id) as rule_id, r.rse_expression as rse_expression, r.subscription_id as subscription_id, d.bytes as bytes, d.length as length FROM atlas_rucio.dataset_locks dsl JOIN atlas_rucio.rules r ON (dsl.rule_id = r.id) JOIN atlas_rucio.dids d ON (dsl.scope = d.scope and dsl.name = d.name)\\nWHERE\\ndsl.rse_id = atlas_rucio.rse2id(:rse) and\\n(r.expires_at > sysdate+60 or r.expires_at is NULL) and\\nr.created_at < sysdate-60 and\\nr.account IN ('panda', 'root', 'ddmadmin') and\\nr.state = 'O' and\\nr.copies = 1 and\\nr.did_type = 'D' and\\nr.child_rule_id is NULL and\\nd.bytes is not NULL and\\nd.bytes/d.length > 500*1000*1000 and\\nd.is_open = 0 and\\nd.did_type = 'D' and\\nr.grouping IN ('D', 'A') and\\n1 = (SELECT count(*) FROM atlas_rucio.dataset_locks WHERE scope=dsl.scope and name=dsl.name and rse_id = dsl.rse_id)\\nORDER BY dsl.accessed_at ASC NULLS FIRST, d.bytes DESC\"] [parameters: {u'rse': u'OU_OSCER_ATLAS_DATADISK'}] (Background on this error at: http://sqlalche.me/e/4xp6)  ", "The original problem was, that I set manually the limit on fsize, calculated as bytes/length. This was done only localy at prod-02. I removed that and rather ordered the query by fsize. There is no division by 0 anymore. "], "1791": ["In https://github.com/rucio/rucio/blob/master/lib/rucio/web/rest/webpy/v1/did.py we did this with the SCOPE_NAME_REGEXP   There is an issue with this way though (see #1746) but in this case it should be fine.  I think just submitting these as params is not a bad idea, but due to backward compatibility I would go the regexp way.", "Ok, thanks, I missed the SCOPE_NAME_REGEXP thing. I'll go with that then."], "1788": [], "1787": ["One difference between the Google Cloud Storage signature plugin and S3/swift ones is the need to call an external service to generate the signed url. To no affect the performance on one of the most called APIs `list_replicas`, the signed urls operation has ben isolated in a dedicated endpoint. cf. \u00b4lib/rucio/web/rest/objectstore.py\u00b4. For example, with the degradation of this signed url external service, the responses time can increase together with the number of simultaneous db sessions  resulting in less slots available on http frontends (snowball effect). A solution might be between the two approaches.", "There's the possibility to sign S3 offline without having to contact an external service. This should work for both private S3 servers (because we control the private key) and for AWS (similar to the delegated credential loading we do for GCS).", "Interesting. It looks new. For which S3 version ? spec ?  ceph (the most popular s3 object stores for ATLAS) is usually behind for new features unfortunately. And for swift ?", "I'm not sure if it's mentioned in the documentation, but my experimentation shows that the signing operations in the boto3 API work fully offline (they still work if I disable the network on my VM) but the ones in the old boto API (currently used by Rucio) don't. So it might be necessary to use boto3 if we want offline url signing, unless it can somehow be made to work with boto 2."], "1786": ["@dciangot   Which access pattern would you see to retrieve this information ?  `requestId` is very internal to Rucio and exposed at few places. Drafting the corresponding REST APIs and workflow would help.    ", "@vingar  In fact the idea is that a system using Rucio clients (e.g. CRAB for CMS user data) wants to keep track of the FTS job ID/IDs for a certain DID against a user defined RSE, only for debugging purpose at the moment.    In this sense I think the access pattern could be as complicated as it needs to be, e.g. first retrieve request ID for a DID and then to a second call to retrieve the request details.  An optimal solution instead, but probably too specific for the use case, would be that the details of the last request will be shown providing a set of (did, source_rse, destination_rse).    Does it make any sense for you?    P.S. using the MQ would be a possibilty too for this problem, but I fill like it is a general-enough problem that one may want to pass it directly through the client.", "yes, it makes perfect sense.  I think the workflow will be from DID, RSE -> Rule -> RequestIds -> FTS jobs Ids.    Few things are already there at least in the rucio UI + rules CLIs for this purpose. It should be checked. ", "For what I have understood the only missing part there is RequestIds -> FTS jobs, at list from CLI point of view.    So a minimal solution here is essentially the porting of  RequestIds -> FTS jobs step into CLI/python bindings from where it is already available in Rucio core. At least this is my understanding/opinion at the moment.", "I had a look at the UI code showing the status of a rule (https://github.com/rucio/rucio/blob/master/lib/rucio/web/ui/static/rule.js) and there it calls `get_request_by_did` which corresponds to the REST call:         https://github.com/rucio/rucio/blob/master/lib/rucio/web/rest/webpy/v1/request.py    I have the feeling that's what you need. Maybe some convenient clients and wrappers missing.  ", "Yes, I think that what we are looking for is to integrate the info of `get_request_by_did` with the info coming from `query_request` (*) in some form.    (*) https://github.com/rucio/rucio/blob/next/lib/rucio/core/request.py#L333", "mmm... btw, probably `get_request_by_did` it is enough alone. If I understand well the query made there. So probably it is really something minor that is missing.   But I think that it is now more or less clear what I have to check.", "So, summarizing with a working plan:   1. add a call here (*), or in another client if you prefer, like:   ``` def get_request_by_did(self, name, rse, scope=None)```  this will use the rest call to: ```https://github.com/rucio/rucio/blob/master/lib/rucio/web/rest/webpy/v1/request.py```    2. Problem is that I have just checked that due to this (**), the request rest call is broken for CMS DIDs (containing `/`). So this is to be fixed before`1.` (that is pretty straight forward) can work.    How does it sound? Did I miss anything?    (*) https://github.com/rucio/rucio/blob/next/lib/rucio/client/didclient.py  (**) https://github.com/rucio/rucio/blob/master/lib/rucio/web/rest/webpy/v1/request.py#L38", "About 1.   With the current per resource logic, it should go in a new rucio/client/requestclient.py file.    2. looks fixable ;)", "> About 1.  > With the current per resource logic, it should go in a new rucio/client/requestclient.py file.  >     ok then, I'll put in `rucio/client/requestclient.py`    > 2. looks fixable ;)    yeah, indeed :) do you prefer a separate issue for that?    ", "Hi, yes, agree on a new `requestclient`. For 2 a new ticket would be helpful, thanks.", "I think a new requestclient would be good as well. The concern I see a bit, also what Vincent said, that previously the requestId was a rather internal concept, but since we are exposing it already via the REST (For the WebUI fts-id linking), I think it is good to expose it properly via a client. Also it's a proper usecase to be able to access fts-ids for external monitoring.", "from the described workflow, it looks not needed to interact with `requestId`", "Yes you are right, it's not interacting, but exposing (primarily for the fts-id)."], "1782": [], "1777": ["This is a duplicate of #1818 Closing"], "1774": [], "1772": ["Just on a side-note: The way we manage the PKs for history tables in the models.py is wrong.  For Oracle we do not create PKs, only in the models.py we create them, but as new experiments create their tables with the models.py install script, it should be handled differently.", "I think it's historical because sqlalchemy enforces the definition of PK on table, otherwise it fails. This explains why PKs are defined in models but not in Oracle.  ", "Yes, sqlalchemy expects a primary Key, otherwise the whole mapping from rows to addressable objects does not make sense.  What is a little bit better is to define a fake PK, like in the replication rule history table. With a history_id as PK (Which does not get created in Oracle) - but this is still kind of wrong for experiments which create their schema from models.py, because they will still end up with a PK. - But thats more of a long-term change.  In this specific case, the merge just doesnt work because merging on a table without a PK doesnt make much sense.", "yes in that case the merge is wrong.", "Fixing this with a PR now, more for the other tables in #1818 "], "1766": ["Hmmm... This one is a blocker for Kronos. I did a manual patch on the nodes, and I expected it would be in 1.18.5", "Actually this has been fixed by #1770 "], "1765": ["I guess the second sentence should read \"in the absence of messages in the queue\" ;-)"], "1758": ["In Rucio, what we call a block is going to be called a dataset. Beyond that, I'm having kind of a hard time understanding what you're looking for.     Rucio itself does not require that datasets (Rucio definition) live all at one site, they can be scattered file by file. We don't plan to do this in CMS, though.", "The dataset replicas API is `GET /replicas/<scope>/<name>/datasets`, that's correct. We don't have an API for dataset replicas for only a single RSE. Should be trivial to implement though.", "Eric, we use the following query in DAS: block dataset=/a/b/c site=XYZ where block is /a/b/c#123 patterns. So I must pass to API /a/b/c and get back /a/b/c#123 (whatever you call them). The site is part of response so we can filter, but it would be nice to add it as a parameter to the query. The later can be problematic due to REST logic. I can imaging this `/replicas/cms/<rse>/<name>` where we can supply both rse and dataset/block names or `/replicas/cms/<name>/<rse>` but it may be hard to implement since API path should be parsed properly. REST goes well with a single argument which is appended to the path of API, but handling multiple parameters in REST is harder, and solution may require fallback to URL parameters, e.g. /path?arg1=val1&arg2=val2, etc. Valentin.  On  0, Eric Vaandering <notifications@github.com> wrote: > In Rucio, what we call a block is going to be called a dataset. Beyond that, I'm having kind of a hard time understanding what you're looking for.  >  > Rucio itself does not require that datasets (Rucio definition) live all at one site, they can be scattered file by file. We don't plan to do this in CMS, though. >  > --  > You are receiving this because you authored the thread. > Reply to this email directly or view it on GitHub: > https://github.com/rucio/rucio/issues/1758#issuecomment-436312330 ", "The first should give you the mapping you want. I don't know what the client is doing internally in the second call, if its one REST API call or a dozen.    ```  [root@client-6c4466d746-76ldc /]# rucio list-content cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD  +------------------------------------------------------------------------------------+--------------+  | SCOPE:NAME                                                                         | [DID TYPE]   |  |------------------------------------------------------------------------------------+--------------|  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#642e11e4-375b-11e8-91bf-ac1f6b05ea26 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#8ca7b000-374f-11e8-a5e2-ac1f6b05e848 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#b171c1de-3748-11e8-a5e2-ac1f6b05e848 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#c6454d7a-3715-11e8-bf17-ac1f6b05ec6a | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#cd7def92-37b6-11e8-a513-02163e018570 | DATASET      |  | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#f8910b72-3704-11e8-bf17-ac1f6b05ec6a | DATASET      |  +------------------------------------------------------------------------------------+--------------+  [root@client-6c4466d746-76ldc /]# rucio list-dataset-replicas cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#b171c1de-3748-11e8-a5e2-ac1f6b05e848  +---------------------+---------+---------+  | RSE                 |   FOUND |   TOTAL |  |---------------------+---------+---------|  | T3_US_NERSC         |       0 |       0 |  | T2_US_Nebraska_Test |       0 |       0 |  +---------------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#cd7def92-37b6-11e8-a513-02163e018570  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#f8910b72-3704-11e8-bf17-ac1f6b05ec6a  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#8ca7b000-374f-11e8-a5e2-ac1f6b05e848  +---------------------+---------+---------+  | RSE                 |   FOUND |   TOTAL |  |---------------------+---------+---------|  | T2_US_Nebraska_Test |       0 |       0 |  +---------------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#c6454d7a-3715-11e8-bf17-ac1f6b05ec6a  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+    DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#642e11e4-375b-11e8-91bf-ac1f6b05ea26  +-------------+---------+---------+  | RSE         |   FOUND |   TOTAL |  |-------------+---------+---------|  | T3_US_NERSC |       0 |       0 |  +-------------+---------+---------+  ```", "Can we translate this into REST API calls?  My understanding that it is /replicas/cms/Charmonium/Run2017D-31Mar2018-v1/MINIAOD/datasets which produces nothing. It aligns with your output which shows 0 found results. Then the question is why we see files on these sites but not blocks (in CMS terminology) or DATASET (in Rucio one). Is it artefact or something else?  On  0, Eric Vaandering <notifications@github.com> wrote: > You can do it in two calls (or maybe just one, the second one) >  > ``` > [root@client-6c4466d746-76ldc /]# rucio list-content cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD > +------------------------------------------------------------------------------------+--------------+ > | SCOPE:NAME                                                                         | [DID TYPE]   | > |------------------------------------------------------------------------------------+--------------| > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#642e11e4-375b-11e8-91bf-ac1f6b05ea26 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#8ca7b000-374f-11e8-a5e2-ac1f6b05e848 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#b171c1de-3748-11e8-a5e2-ac1f6b05e848 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#c6454d7a-3715-11e8-bf17-ac1f6b05ec6a | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#cd7def92-37b6-11e8-a513-02163e018570 | DATASET      | > | cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#f8910b72-3704-11e8-bf17-ac1f6b05ec6a | DATASET      | > +------------------------------------------------------------------------------------+--------------+ > [root@client-6c4466d746-76ldc /]# rucio list-dataset-replicas cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#b171c1de-3748-11e8-a5e2-ac1f6b05e848 > +---------------------+---------+---------+ > | RSE                 |   FOUND |   TOTAL | > |---------------------+---------+---------| > | T3_US_NERSC         |       0 |       0 | > | T2_US_Nebraska_Test |       0 |       0 | > +---------------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#cd7def92-37b6-11e8-a513-02163e018570 > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#f8910b72-3704-11e8-bf17-ac1f6b05ec6a > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#8ca7b000-374f-11e8-a5e2-ac1f6b05e848 > +---------------------+---------+---------+ > | RSE                 |   FOUND |   TOTAL | > |---------------------+---------+---------| > | T2_US_Nebraska_Test |       0 |       0 | > +---------------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#c6454d7a-3715-11e8-bf17-ac1f6b05ec6a > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570 > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26 > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ >  > DATASET: cms:/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#642e11e4-375b-11e8-91bf-ac1f6b05ea26 > +-------------+---------+---------+ > | RSE         |   FOUND |   TOTAL | > |-------------+---------+---------| > | T3_US_NERSC |       0 |       0 | > +-------------+---------+---------+ > ``` >  > --  > You are receiving this because you authored the thread. > Reply to this email directly or view it on GitHub: > https://github.com/rucio/rucio/issues/1758#issuecomment-436391412 ", "This is still an outstanding issue. The dataset replicas are currently updated only by an PL/SQL procedure on the Oracle database. This PL/SQL I believe is not run on the CMS instance, hence the dataset replica counters not being updated...", "@vkuznet that's why I have suggested a couple of times that you install the rucio client and instrument it to see which REST calls are being made. It's a fool-proof way. Relying on me or others to read the code is time consuming and error prone.", "With #524 the daemon to update the collection replicas has been added. (It's in 1.18.9 release)  Thus with this one or the PL/SQL procedure it will update the collection_replicas to the correct stats."], "1755": [], "1749": ["This is potentially tricky. Changing the primary key is easily possible, however, this won't move the data already on the tablespace, it will just check that the new primary key constraint is still valid. What we actually want is for the data to be moved. So this will need an intermediate tmp_replicas table and thus some downtime as the DBA manually shovels data around. Changing the primary key can be done with the alembic upgrades, but shoveling the data can only be done manually if we don't want tmp_replicas to pollute our models.py", "We can do the primary key change anyway though. And we could provide a script for all databases to do it offline. Then it's the instance admins choice when/how they apply it (if they want to do it)", "It is fine to do it offline, but we need to document it somehow. I am surprised that this never showed up, but in smaller installations the queries will just be quick enough without going against the PK. And from a uniqueness point of view this PK is correct as well."], "1739": ["@briedel is this an icecube requirement? Because if not then it's almost certainly a mistake.", "Per @briedel in Slack, there's no need to allow single quotes.  Probably an error.    @illingwo - any chance you want to put in a PR?"], "1738": [], "1731": ["It's exactly one hour, but we can add this explicitly.    Proposal ISO8601 in UTC:  `X-Rucio-Auth-Token-Expiration: 2018-11-01T11:06:41Z`", "from here https://rucio.readthedocs.io/en/latest/rest.html:  ```  Date format    All dates returned are in UTC and are strings in the following format (RFC 1123, ex RFC 822):    Mon, 13 May 2013 10:23:03 UTC  In code format, which can be used in all programming languages that support strftime or strptime:    '%a, %d %b %Y %H:%M:%S UTC'  ```  ", "Hmmm, that might be the content of the docs, but in reality we are always returning ISO8601 in our JSONs :-)    e.g., `$ curl -H 'X-Rucio-Account: mlassnig' -H 'X-Rucio-Auth-Token: ..' https://rucio-lb-prod.cern.ch/accounts/mlassnig`  ```  {..... \"created_at\": \"2014-01-17T08:05:22\", ....}  ```  Maybe better to update the docs instead to reflect this.    For the HTTP headers that's a completely different story, there RFC1123 makes sense since it's also what the web servers return.", "Strange for `2014-01-17T08:05:22`    ```  # RFC 1123 (ex RFC 822)  DATE_FORMAT = '%a, %d %b %Y %H:%M:%S UTC'  ```  is the format used in dumps/loads json (cf. utils.py)  "], "1730": ["I guess it corresponds to answer with `Content-Type: application/x-json-stream`. To stream JSON, we used objects delimited by newlines. cf. https://rucio.readthedocs.io/en/latest/rest.html", "According to JSON specs:    ------- from [JSON](http://www.json.org/) specs -----  In JSON, they take on these forms:    An object is an unordered set of name/value pairs. An object begins with { (left brace) and ends with } (right brace). Each name is followed by : (colon) and the name/value pairs are separated by , (comma).    An array is an ordered collection of values. An array begins with [ (left bracket) and ends with ] (right bracket). Values are separated by , (comma).    -------- END OF QUOTE -----    Since your APIs provide an answer as a list of records (which are dicts), they need to be separated by comma and embraced with brackets.    The provided output of REST APIs, e.g.  ```  curl -H \"Content-Type: application/json\" -H \"X-Rucio-Auth-Token: $token\" http://cms-rucio-test.cern.ch/accounts/  {\"account\": \"das\", \"type\": \"USER\", \"email\": null}  {\"account\": \"dciangot\", \"type\": \"USER\", \"email\": null}  ```     does not satisfy the JSON specs. That's why the standard JSON parsers (in python or other languages) will fail to parse it. What you yield in APIs is a list of strings (even though they're represented dicts) but it is not a valid JSON document.    I provided an example of python parser flow which you can use to tests your documents  ```  curl -H \"Content-Type: application/json \"-H \"X-Rucio-Auth-Token: $token\" http://cms-rucio-test.cern.ch/accounts/ > account.json  # parse docs in python  import json  data = json.load(open(\"accounts.json\"))  ```  And, it fails to parse the document obtained from the REST API. I can easily substitute a file with file descriptor which is established in network call and `json.load(fileDescriptor)` should work.    In order to stream a data in JSON format someone needs to do the following:  ```  # obtain json serialized object  objectData = json.dumps(data)  # send objectData to the wire  ```", "@vkuznet - isn't that compatible with what Vincent said?    That is, the API returns `Content-Type: application/x-json-stream`.  A JSON stream is _not_ JSON (e.g., https://en.wikipedia.org/wiki/JSON_streaming)!", "@bbockelm , according to Rucio REST API page:  ```  The server answer can be one of the following content-type in the http Header:  Content-type: application/json  Content-Type: application/x-json-stream  In the last case, it corresponds to JSON objects delimited by newlines(streaming JSON for large answer),  { \"id\": 1, \"foo\": \"bar\" }  { \"id\": 2, \"foo\": \"baz\" }  ```    The MIME media type for JSON text is application/json and it is defined by [RFC4627](http://www.ietf.org/rfc/rfc4627.txt) The data with MIME type `application/x-json-stream` is not the same as data with MIME type `application/json`. The former can be defined as JSON objects delimited by newlines and later should be strict JSON document.    Vincent is correct about `application/x-json-stream` but that is not what I was looking for. I asked about `application/json` which right now returns data in `application/x-json-stream` format.  ", "Hi,  to explain, we stream the results from the database directly to the clients, that's why we do JSON streams (the server ignores client requests for different MIME types).    Now, if for a particular call you'd like to have a single JSON document, then we will have to add support for a client requesting MIME `application/json`. It's not really a problem, but the implications are of course that any wide-spread use of that will quickly exhaust the memory on our servers since the JSON has to be assembled there. For example, doing this for `list_replicas()` is a no go whereas for many other calls it would be ok.", "Mario,  I'm not sure I understand the technical issue here. Why do you need to assemble a JSON? If you have your stream from DB, the change I would expect is to add opening bracket `[` to the response, replace newline delimiter `\\n` to comma `,`, and end response with closing bracket `]`. It can be easily done by adding response wrapper (decorator) which will take whatever stream API you have and perform these three actions.    Here is a brief sketch of the program flow which does not assemble the JSON in memory but rather add additional wrapper around rucio data stream and yield proper JSON format:  ```  #!/usr/bin/env python  import json    def json_stream(func):      \"JSON streamer which yield correct JSON object\"      yield \"[\"      for idx, doc in enumerate(func()):          if idx:              sdoc += ','              yield sdoc          sdoc = json.dumps(doc)      yield sdoc      yield \"]\"    def rucio_data():      \"Provide JSON data from Rucio server\"      doc = {'srv':'rucio'}      for idx in range(3):          doc['counter'] = idx          yield doc    def main():      print(\"Rucio JSON stream\")      stream = ''      for doc in rucio_data():          print(doc)          stream += json.dumps(doc) + '\\n'      try:          data = json.loads(stream)      except ValueError:          print(\"ERROR: unparseable JSON stream\")      print(\"\\nRucio JSON object\")      stream = ''      for doc in json_stream(rucio_data):          print(doc)          stream += doc      data = json.loads(stream)      print(\"parsed json object: %s\" % data)    if __name__ == '__main__':      main()  ```  If you'll run it you'll get this output:  ```  Rucio JSON stream  {'counter': 0, 'srv': 'rucio'}  {'counter': 1, 'srv': 'rucio'}  {'counter': 2, 'srv': 'rucio'}  ERROR: unparseable JSON stream    Rucio JSON object  [  {\"counter\": 0, \"srv\": \"rucio\"},  {\"counter\": 1, \"srv\": \"rucio\"},  {\"counter\": 2, \"srv\": \"rucio\"}  ]  parsed json object: [{u'counter': 0, u'srv': u'rucio'}, {u'counter': 1, u'srv': u'rucio'}, {u'counter': 2, u'srv': u'rucio'}]  ```  ", "Hi Valentin,  sure. If we agree that all JSON streams can also be returned as a JSON list instead, then that is easy.", "In your example, you are not testing the serialisation between the server and the client through the network which was our problem at that time. That's what we went with newlines which was the way to stream json. Maybe now there is some better way. ", "@vingar, of course I don't know the details of your code, but example clearly demonstrate the idea. I would expect that you organize the pipe between your DB layer and streaming API. You consume data on your DB part and yield it back with proposed modifications to the streaming API.", "@mlassnig , Mario, you can leave JSON stream as is and they can be returned with `application/x-json-stream` type, but my request was to fix the data when client requests `application/json`.", "@vkuznet we started by what you proposed but got issue with the http client libraries(httplib, requests, etc) not being able to consume the streamed entries. ", "Valentin - you mentioned you used Content-Type. Shouldn\u2019t that be Accept?", "@vingar - indeed, it's probably easier for the client side to consume the JSON streaming format (not to mention things like handling HTTP chunk-encoding better!), but if the client explicitly asks for JSON instead, I guess we have to assume they know what they are doing!", "yes, good points. Feel free to post pull requests ;) ", "Ok, I guess I figured out why Valentin expected that he could request a JSON with Accept, since the statement in the doc is ambiguous:    \"...The server answer can be one of the following content-type in the http Header...\" However, this doesn't mean that the client has a choice ;-)    Anyway, adding an \"Accept\" parser to the REST frontend is easy.", "When the HTTP Header `Accept: application/json` is there, proper json should be returned.  One caveat is to check the rucio clients regarding this...    BTW, listing accounts is not the most memory usage critical operation and do no need streaming imo.    ", "Indeed, the proper request header should be `Accept`, sorry for confusion.", "Yet, another issue with ambiguity of documents returned by server. Imaging that one query will return a single record. Using x-json-stream we have (I took record as an example):  ```  {\"accessed_at\": null, \"name\": \"/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26\", \"rse\": \"T3_US_NERSC\", \"created_at\": \"Wed, 24 Oct 2018 12:45:12 UTC\", \"bytes\": 0, \"state\": \"UNAVAILABLE\", \"updated_at\": \"Wed, 24 Oct 2018 12:45:12 UTC\", \"available_length\": 0, \"length\": 0, \"scope\": \"cms\", \"available_bytes\": 0}  ```  By parsing this doc we'll get a Python dictionary data-type. While if server return multiple records, e.g.  ```  {\"accessed_at\": null, \"name\": \"/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#49a747ac-3736-11e8-91bf-ac1f6b05ea26\", \"rse\": \"T3_US_NERSC\", \"created_at\": \"Wed, 24 Oct 2018 12:45:12 UTC\", \"bytes\": 0, \"state\": \"UNAVAILABLE\", \"updated_at\": \"Wed, 24 Oct 2018 12:45:12 UTC\", \"available_length\": 0, \"length\": 0, \"scope\": \"cms\", \"available_bytes\": 0}  {\"accessed_at\": null, \"name\": \"/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#cd7def92-37b6-11e8-a513-02163e018570\", \"rse\": \"T3_US_NERSC\", \"created_at\": \"Tue, 30 Oct 2018 19:25:25 UTC\", \"bytes\": 0, \"state\": \"UNAVAILABLE\", \"updated_at\": \"Tue, 30 Oct 2018 19:25:25 UTC\", \"available_length\": 0, \"length\": 0, \"scope\": \"cms\", \"available_bytes\": 0}  ```  it will be converted (assuming parser can do it) into a list data-type. As you can see a single API can produce different data-type using this format. Without `application/json` we end-up with ambiguity of data-type from APIs. I hope you can take care of this ambiguity at a server level and return data such that they can be decoded into consistent data-type.", "For me the main issue here is that the documentation is a bit ambiguous and the server handling of the requested Content-Type is not strict enough.  If the application requests `Content-Type: application/json` (or any other content type that is) and the server cannot supply it, it should raise a `HTTP 406 Not Acceptable` (See [link](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.7)) instead of just replying with whatever it thinks is useful.    For me the handling is fine: If no specific type is requested we return `application/x-json-stream` - By keeping it like this we also ensure possible back-compatibility issues with (old) clients.  What we should change is, if a different type is requested (e.g. `application/xml`, `application/pdf`, ...) we should raise a 406  And we can also add the `application/json` handling, as asked by Valentin, it shouldn't be too hard to add.  Let's discuss this at the Dev Meeting on Thursday and then create the respective issues.", "This was discussed in the development meeting from [2018-11-08](https://indico.cern.ch/event/766990/)  Essentially we decided that if a content type is requested, which the server cannot reply, we will raise HTTP 406. However, we will try to add `application/json` type additionally to `application/x-json-stream`.", "How do we stand on this issue?    I found yet another problem with records using default `application/x-json-stream` mime type. It does not provide newline `\\n` for the last record, i.e. if we have multiple records in response they are separated by `\\n` except last one. Response with single record does not have `\\n` either. This requires to properly write a parser for x-json-stream which should separate record based on newline and collect last output till the end of the stream. According to https://www.wikiwand.com/en/JSON_streaming#/Line-delimited_JSON the line-delimited JSON should have `\\n` for every record, but it is unclear from the description if last record should have the delimiter or not.", "It's still in the plan, but as this is non-blocking, it doesn't have the highest priority.  @hahahannes Can you please look into the raising a `406` in case of non-supported content-type. We can discuss this on Monday. To also allow `application/json` for certain queries should be not difficult.    For the `\\n` for the last record. Just quickly looking into it I couldn't find a definitive answer on this. For [JSON Lines](http://jsonlines.org) (Which is  part of the Line delimited JSON) it says:  > The last character in the file **may** be a line separator, and it will be treated the same as if there was no line separator present.  "], "1729": ["It seems to me that the right logic here is to pick the most recent entry from rse_usage that isn't 'rucio' for each rse, and use that to sort the free space. The catch is that it isn't very easy to express this in SQL. You can do it with a window function like this:    ```  select rse, staging_area, id from rses left outer join (select rse_id, free from (select rse_id, free, row_number() over (partition by rse_id order by updated_at desc) as rownum from rse_usage where source != 'rucio') as ranked_free where rownum = 1) as recent_usage on (rses.id = recent_usage.rse_id) where deleted = false order by free;  ```    but I don't know how well MySQL supports this, and SQLite only added window function support this September."], "1726": [], "1725": [], "1718": [], "1710": [], "1703": ["we never delete RSE. It's on purpose a soft deletion with the deleted column instead...", "Ah alright, thanks.", "but \u00b4rse_exists(rse_name) # -> True\u00b4 should give back `False` instead if the flag is deleted."], "1702": ["Not a bug. This is CERN Megabus force overwriting the Rucio stomp.py dependency."], "1701": [], "1698": [], "1695": [], "1692": ["So the issue here is that the pip dependency resolver is not all that good https://github.com/pypa/pip/issues/988  We need to define stricter dependency ranges for this."], "1689": ["Related to RSE deletion, it might make sense to have a look at #26. If the RSE is not empty, a `RSEOperationNotSupported` should be raised."], "1686": [], "1685": ["Maybe `set_account_property`, `set_account_parameter` or something similar instead? So that it is aligned with the attribute functions `del_account_attribute`, `add_account_attribute`.   Then I would also replace the `get_account_status` with `get_account_property`. ", "Also I am not sure how I shall handle the permissions. For example [here](https://github.com/rucio/rucio/blob/master/lib/rucio/core/permission/atlas.py#L43).   I think the permission for set_account_status should apply for changes on all account properties ? Then I would also update the permission files."], "1682": [], "1677": ["This also links to #1427 "], "1672": [], "1669": ["How?", "Needs to be checked. ", "We can check again but I think the registration is working fine now"], "1668": [], "1664": [], "1663": [], "1656": ["Depends on #1685 "], "1651": [], "1647": ["Shall I also add 'ASN', 'availability' and 'usage' as there are also missing."], "1646": [], "1639": [], "1638": ["This was discussed in the [dev meeting on 2018-10-11](https://indico.cern.ch/event/760955/)", "New repository : https://github.com/rucio/probes  The probes will be migrated"], "1637": [], "1635": [], "1632": [], "1625": [], "1624": [], "1623": [], "1622": ["I think this is dependent on #1613 ", "#1613 is merged now. I think this bug should be fixed by that, but please check.", "It gets downloaded correctly using the current master and #1956 .  It's also done correctly in case the whole zip is downloaded."], "1619": ["It seems that the request to:  ```  https://rucio-ui.cern.ch/proxy/dids/data18_13TeV/data18_13TeV.00362204.physics_BphysLS.recon.AOD.k988_tid15553683_00/did_meta  ```   fails because of [a broken check for the oracle version](https://github.com/rucio/rucio/blob/master/lib/rucio/core/did.py#L1291) (should be session.connection() instead session.connection). I think it is not usable at the moment because of the missing oracle upgrade. So I would change the error message to a more meaningful like 'Could not retrieve DID meta informations' in case this request fails, but it would be still shown on top of the page. ", "get_did_meta should never be called at the moment (It needs the oracle version upgrade). I strongly suspect that we should call `../meta` here like in all other cases. I think this was changed as an error.    The connection() is a different bug and should be fixed, can you please create a ticket for this?", "Okay, linking the ticket for the connection bug [here](https://github.com/rucio/rucio/issues/1639)."], "1614": [], "1613": ["Not only filemeta is returned wrong but also the identity tags:  ```  (.venv) [15:19][twegner@twegner-dev rucio]$ rucio list-file-replicas --metalink --expression NIKHEF-ELPROD_DATADISK data16_13TeV:DRAW_RPVLL.11106701._000026.pool.root.1  <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\">   <file name=\"DRAW_RPVLL.14552406._000001.zip.1\">    <identity>data16_13TeV:DRAW_RPVLL.14552406._000001.zip.1</identity>  ...   </file>  </metalink>    (.venv) [15:19][twegner@twegner-dev rucio]$ rucio list-file-replicas --metalink  data16_13TeV:DRAW_RPVLL.11106701._000026.pool.root.1  <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\">   <file name=\"DRAW_RPVLL.11106701._000026.pool.root.1\">    <identity>data16_13TeV:DRAW_RPVLL.11106701._000026.pool.root.1</identity>  ...   </file>  </metalink>  ```  I'd prefer that the identity tag always contains the constituent DID"], "1610": [], "1606": [], "1601": [], "1600": [], "1599": ["Priority increased"], "1598": [], "1593": ["Link to #1623 as only one of them has to be implemented.", "#1623 has been implemented and merged. Therefore all downloads with eventType `get_sm` or `get_sm_a` shouldn't report any suspicious files"], "1590": [], "1589": [], "1584": [], "1582": ["one more error came:  ```  2018-09-28 08:42:34,153\t7986\tDEBUG\tTraceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/rucio/tests/daemons/Automatix.py\", line 115, in upload      client.add_dataset(scope=dsn['scope'], name=dsn['name'], rules=[{'account': account, 'copies': 1, 'rse_expression': rse, 'grouping': 'DATASET', 'activity': 'Functional Test'}], meta=meta, lifetime=dataset_lifetime)    File \"/usr/lib/python2.7/site-packages/rucio/client/didclient.py\", line 149, in add_dataset      lifetime=lifetime, dids=files, rse=rse)    File \"/usr/lib/python2.7/site-packages/rucio/client/didclient.py\", line 119, in add_did      raise exc_cls(exc_msg)  RucioException: An unknown exception occurred.  Details: [u'(cx_Oracle.DatabaseError) ORA-20101: Primary key constraint DELETED_DIDS_PK violated\\nORA-06512: at \"ATLAS_RUCIO.CHECK_DID_UNIQUENESS\", line 12\\nORA-04088: error during execution of trigger \\'ATLAS_RUCIO.CHECK_DID_UNIQUENESS\\'\\n']  ```", "I'm not sure what is the best thing to do. The patch that modifies the output of the upload method (  https://github.com/rucio/rucio/commit/c839a8ee4fb1399116853f532ef9e7d5a288af18 has comment \"rsemanager: dirty dirty fix of upload method return - #1487\"). So I would like to know if the dirty fix will stay, or will it be replaced by something cleaner.", "Need to check with @TomasJavurek and @TWAtGH "], "1579": ["@hahahannes can you please have a look on this. I expect it should be an easy fix.", "It is - I was just doing a pull request", "i just tried this in various ways and can't reproduce it.    `  $ rucio -v whoami  given client cert (/data/rucio/$X509_USER_PROXY) doesn't exist  2018-09-27 16:26:31,717 ERROR   Cannot authenticate.  Details: x509 authentication failed  `    respectively:  `Details: Cannot find a valid X509 proxy; not in /tmp/x509up_u23311, $X509_USER_PROXY not set, and 'x509_proxy' not set in the configuration file.`    or:  `2018-09-27 16:28:21,172 ERROR   [('PEM routines', 'PEM_read_bio', 'no start line'), ('SSL routines', 'SSL_CTX_use_certificate_file', 'PEM lib')]`      really interested in that pull-request ;-)", "Ahh, sorry \ud83d\ude04 Thanks a lot!"], "1578": ["Discussed in [Dev Meeting on 2018-10-11](https://indico.cern.ch/event/760955/): We should do it like planned."], "1569": [], "1568": ["I'll try to repeat it.", "Behaviour noticed from August.", "I was able to repeat the problem on lxplus:    ```  arcproxy -S atlas  while arcls -L rucio://rucio-lb-prod.cern.ch/replicas/mc16_13TeV/AOD.11189422._000449.pool.root.1; do sleep 1; done  ```  wait until it exits with  ```  ERROR: Failed to obtain information about file: No such file or directory: HTTP error when contacting server: Not Found  ```  To get more debug info you can add \"-d DEBUG\" to arcls.", "Ok, I had this running now for quite some time (several thousand requests both against server direct and through the loadbalance) and I could reproduce it with `arcls` but not with curl.    Just by chance, is arcls using the `GET /replicas` interface?    This worked for me thousands of times:    ```while true; do curl -L -si --capath /etc/grid-security/certificates -H 'X-Rucio-Account: mlassnig' -H 'X-Rucio-Auth-Token: ......' -d '{\"dids\":[{\"scope\":\"mc16_13TeV\",\"name\":\"AOD.11189422._000449.pool.root.1\"}]}' -X POST https://rucio-lb-prod.cern.ch/replicas/list | grep HTTP; done```", "Just to be precise, the GET interface works as well for me using curl.", "Both `/replicas/SCOPE/NAME `[GET] and `/replicas/list` [POST] use the same API in the back. That should be the same.", "do you mean `POST /replicas/` -> `POST /replicas/list` ?", "Yes; Will edit the comment. \ud83d\udc4d ", "the debug mode gives a bit more info:    ```  DEBUG: No security processing/check requested for 'incoming'  DEBUG: < HTTP/1.1 200 OK  DEBUG: < Date: Thu, 11 Oct 2018 11:30:13 GMT  DEBUG: < Server: Apache  DEBUG: < Access-Control-Allow-Origin: None  DEBUG: < Access-Control-Allow-Headers: None  DEBUG: < Access-Control-Allow-Methods: *  DEBUG: < Access-Control-Allow-Credentials: true  DEBUG: < Access-Control-Expose-Headers: X-Rucio-Auth-Token  DEBUG: < Cache-Control: no-cache, no-store, max-age=0, must-revalidate  DEBUG: < Cache-Control: post-check=0, pre-check=0  DEBUG: < Pragma: no-cache  DEBUG: < X-Rucio-Auth-Token:  DEBUG: < X-Rucio-Host: voatlasrucio-auth-prod.cern.ch:443  DEBUG: < Content-Length: 0  DEBUG: < Keep-Alive: timeout=3, max=1024  DEBUG: < Connection: Keep-Alive  DEBUG: < Content-Type: application/octet-stream  DEBUG: Acquired auth token for ...  DEBUG: Module Manager Init  DEBUG: Loaded /usr/lib64/arc/libmcchttp.so  DEBUG: Loaded /usr/lib64/arc/libmccsoap.so  DEBUG: Loaded /usr/lib64/arc/libmcctcp.so  DEBUG: Loaded /usr/lib64/arc/libmcctls.so  DEBUG: Loaded /usr/lib64/arc/libmccmsgvalidator.so  VERBOSE: Trying to connect rucio-lb-prod.cern.ch(IPv4):443  DEBUG: Loaded MCC tcp.client(tcp)  DEBUG: Loaded MCC tls.client(tls)  DEBUG: Loaded MCC http.client(http)  DEBUG: TCP client process called  DEBUG: No security processing/check requested for 'outgoing'  DEBUG: No security processing/check requested for 'incoming'  VERBOSE: Using cipher: ECDHE-RSA-AES256-SHA  DEBUG: Linking MCC tls.client(tls) to MCC (tcp) under (empty)  DEBUG: Linking MCC http.client(http) to MCC (tls) under (empty)  VERBOSE: Peer name: /DC=ch/DC=cern/OU=computers/CN=rucio-lb-prod-03.cern.ch  VERBOSE: Identity name: /DC=ch/DC=cern/OU=computers/CN=rucio-lb-prod-03.cern.ch  VERBOSE: CA name: /DC=ch/DC=cern/CN=CERN Grid Certification Authority  DEBUG: No security processing/check requested for 'outgoing'  DEBUG: > GET https://rucio-lb-prod.cern.ch:443/replicas/mc16_13TeV/AOD.11189422._000449.pool.root.1 HTTP/1.1  Host: rucio-lb-prod.cern.ch:443  Connection: keep-alive  user-agent: ARC  x-rucio-auth-token:       DEBUG: No security processing/check requested for 'incoming'  DEBUG: < HTTP/1.1 404 Not Found  DEBUG: < Content-Type: text/plain; charset=utf-8  DEBUG: < X-Content-Type-Options: nosniff  DEBUG: < Date: Thu, 11 Oct 2018 11:30:13 GMT  DEBUG: < Content-Length: 19  ERROR: Failed to obtain information about file: No such file or directory: HTTP error when contacting server: Not Found  ```    It corresponds to:    ```  while true; do curl -L -si --capath /etc/grid-security/certificates -H 'X-Rucio-Account: $RUCIO_ACCOUNT' -H 'X-Rucio-Auth-Token: ...'  -X GET https://rucio-lb-prod.cern.ch:443/replicas/mc16_13TeV/AOD.11189422._000449.pool.root.1 | grep HTTP; sleep 1; done  ```  which doesn't fail with a `404`. One difference is that a token is acquired for each operation.", "One issue is with the server instance on k8s supposed to get 1% of the load:    ```  # arcls rucio://rucio-k8s-int-xxxxx.cern.ch:80/replicas/mc15_13TeV/HITS.05608147._000165.pool.root.1  ERROR: Failed to obtain information about file: No such file or directory: HTTP error when contacting server: Not Found  ```  Maybe some caching are done in the arc-clients making that worst.", "Fixed now on kubernetes side."], "1561": [], "1560": ["I think we should just change the switch to --no-resolve-archives. If not set resolving is true by default.", "maybe shorter just `--no-archives` ?", "Hmm, it might be a bit confusing as one could assume that `--no-archives` means there are not archive replicas in the result. Although, to some extend `--no-resolve-archives` could be interpreted the same..."], "1559": [], "1555": [], "1554": ["Duplicate"], "1551": [], "1550": ["Not so simple. Imagine a file is marked as temporarily `UNAVAILABLE` and there is only one other copy of this file that is declared `BAD` to Rucio in the meantime. In that case, the necromancer won't see any `AVAILABLE` copy of the file and will set a Epoch tombstone on all the `UNAVAILABLE` replicas. ", "Yes, it's a quite heavy change touching all systems. That's why I think it would have to be a new `TEMPORARYUNAVAILABLE` state, so it is obvious to other daemons that this is not a normal `UNAVAILABLE` replica."], "1543": ["That should be fine. I think how JSON() is implemented in sqlalchemy is that if the database does not support it, it just creates a blob or big varchar, thus we can switch to this even before oracle12 is available, you just can't use the json functionality. I think it should be fine to have this in the next feature release."], "1535": ["Solved by #1535"], "1532": [], "1529": [], "1528": [], "1527": [], "1524": [], "1523": ["Reminder: For ATLAS we need to remove the puppet monkey patch were we update /usr/lib/python2.7/site-packages/urllib3/contrib/pyopenssl.py"], "1522": [], "1516": [], "1512": ["This dataset is only available at `CERN-PROD_DERIVED` which has the `istape` attribute set to `True`. Download is not allowed from tape endpoints by non-admin accounts.  Because of this no file is considered for download. Anyway this is probably already fixed with 1.17. Needs to be checked though.", "Thanks for the quick reply!     Yes, I should have said that I figured out this much myself as well. I think the output is still wrong and should be fixed.    Best,  Christian", "With 1.17 this case is checked and the `NoFilesDownloaded` exception is thrown. The CLI exit with a return code != 0. There is no error message though.", "@TWAtGH Can you add an error message for this case? ", "Yes I'm currently working on the downloadclient anyway"], "1506": ["After a closer look, I think it's exactly the same problem as #1487", "Ok, I will close this one as a Duplicate of #1487 then."], "1505": ["Here are two files which are not py3 compatible:  modified:   lib/rucio/rse/protocols/gfal.py:  ```  try:      import urlparse  except ImportError:      from urllib import parse as urlparse  ```    modified:   lib/rucio/rse/protocols/protocol.py  ```  try:      import urlparse  except ImportError:      from urllib import parse as urlparse    try:      from ConfigParser import NoOptionError, NoSectionError  except:      from configparser import NoOptionError, NoSectionError  ```    ", "Hi @hahahannes can you please prepare some kind of mini-proposal for the Dev meeting in 2 weeks (11.10.) Thanks \ud83d\ude04 ", "As discussed in the [meeting on 2010-10-11](https://indico.cern.ch/event/760955/): We should tag the files in the header if they are py3 compatible. This script should be added as a separate test in the travis test matrix. If a file, which is already tagged py3 compatible fails the pylint test, the test should fail.", "List of python3 compatible files/directories according to the command:  ```  pylint --py3k -d no-absolute-import [path]  ```  I removed warnings about [missing absolute imports](https://docs.pylint.org/en/1.6.0/features.html). Only the files in bin/ are compatible when it is activated. Not sure if this mandatory for python3 ?    bin  lib/rucio/client/accountclient.py  lib/rucio/client/accountlimitclient.py  lib/rucio/client/baseclient.py  lib/rucio/client/client.py  lib/rucio/client/configclient.py  lib/rucio/client/lifetimeclient.py  lib/rucio/client/lockclient.py  lib/rucio/client/metaclients.py  lib/rucio/client/objectstoreclient.py  lib/rucio/client/pingclient.py  lib/rucio/client/replicaclient.py  lib/rucio/client/rseclient.py  lib/rucio/client/ruleclient.py  lib/rucio/client/scopeclient.py  lib/rucio/client/subscriptionclient.py  lib/rucio/client/touchclient.py  lib/rucio/client/uploadclient.py  lib/rucio/api/account.py  lib/rucio/api/account_limit.py  lib/rucio/api/authentication.py  lib/rucio/api/config.py  lib/rucio/api/credential.py  lib/rucio/api/heartbeat.py  lib/rucio/api/identity.py  lib/rucio/api/lifetime_exception.py   lib/rucio/api/lock.py  lib/rucio/api/meta.py   lib/rucio/api/permission.py  lib/rucio/api/replica.py  lib/rucio/api/request.py   lib/rucio/api/rse.py  lib/rucio/api/rule.py  lib/rucio/api/scope.py  lib/rucio/api/subscription.py  lib/rucio/api/temporary_did.py  lib/rucio/common/config.py  lib/rucio/common/constants.py   lib/rucio/common/exception.py  lib/rucio/common/log.py  lib/rucio/common/rse_attributes.py  lib/rucio/common/schema/atlas.py  lib/rucio/common/schema/cms.py  lib/rucio/common/schema/icecube.py  lib/rucio/common/schema/generic.py  lib/rucio/common/dumper/consistency.py   lib/rucio/core/config.py  lib/rucio/core/distance.py  lib/rucio/core/identity.py  lib/rucio/core/lock.py  lib/rucio/core/message.py  lib/rucio/core/nongrid_trace.py  lib/rucio/core/quarantined_replica.py  lib/rucio/core/rse_selector.py  lib/rucio/core/scope.py  lib/rucio/core/staging.py  lib/rucio/core/temporary_did.py  lib/rucio/core/transfer_limits.py  lib/rucio/core/volatile_replica.py   lib/rucio/daemons/sonar_v3/distribution/distribution_daemon.py  lib/rucio/daemons/c3po/utils/dataset_cache.py  lib/rucio/daemons/c3po/utils/expiring_dataset_cache.py  lib/rucio/daemons/c3po/utils/expiring_list.py   lib/rucio/daemons/c3po/utils/popularity.py  lib/rucio/daemons/c3po/utils/timeseries.py   lib/rucio/daemons/c3po/collectors/agis.py  lib/rucio/daemons/c3po/collectors/free_space.py   lib/rucio/daemons/c3po/collectors/jedi_did.py   lib/rucio/daemons/c3po/collectors/mock_did.py   lib/rucio/daemons/auditor/hdfs.py  lib/rucio/db/sqla/constants.py  lib/rucio/db/sqla/sautils.py  lib/rucio/db/sqla/types.py   lib/rucio/db/sqla/migrate_repo/env.py  lib/rucio/rse/rsemanager.py  lib/rucio/rse/protocols/cache.py  lib/rucio/rse/protocols/dummy.py   lib/rucio/rse/protocols/gfal2.py   lib/rucio/rse/protocols/gsiftp.py   lib/rucio/rse/protocols/http_cache.py   lib/rucio/rse/protocols/mock.py   lib/rucio/rse/protocols/ngarc.py   lib/rucio/rse/protocols/posix.py   lib/rucio/rse/protocols/s3es.py  lib/rucio/rse/protocols/sftp.py   lib/rucio/rse/protocols/xrootd.py  lib/rucio/transfertool/fts3_myproxy.py  lib/rucio/web/ui/main.py  lib/rucio/web/rest/webpy/v1/config.py  lib/rucio/web/rest/webpy/v1/heartbeat.py  lib/rucio/web/rest/webpy/v1/lifetime_exception.py  lib/rucio/web/rest/webpy/v1/meta.py  lib/rucio/web/rest/webpy/v1/ping.py  lib/rucio/web/rest/webpy/v1/request.py  lib/rucio/web/rest/webpy/v1/scope.py  lib/rucio/web/rest/flaskapi/v1/config.py  lib/rucio/web/rest/flaskapi/v1/heartbeat.py  lib/rucio/web/rest/flaskapi/v1/lifetime_exception.py  lib/rucio/web/rest/flaskapi/v1/lock.py  lib/rucio/web/rest/flaskapi/v1/meta.py  lib/rucio/web/rest/flaskapi/v1/ping.py  lib/rucio/web/rest/flaskapi/v1/request.py  lib/rucio/web/rest/flaskapi/v1/scope.py", "I see, I think it is a bit too clustered to do this based on directories. It might be better if we add a \"tag\" such as `# PY3K COMPATIBLE` in the header of the file (below the authors for example) and the test essentially only tests if these tagged files stayed compatible. We just have to be consistent in the future to keep tagging the newly compatible files."], "1497": [], "1494": ["The PR for `rsemgr.upload` was merged #1509   `rsemgr.download` will be removed since it isn't used anymore.", "Ok, Closing with reference to #1378 "], "1490": ["We should check if there is a common implementation to do this. "], "1487": ["We have hole somewhere in rsemgr upload method. What is certainly wrong, that we don't have same structure on return, once list and once dictionary:  https://github.com/rucio/rucio/blob/master/lib/rucio/rse/rsemanager.py#L494    The case of upload to CERN-PROD_ES leads to the empty list, ret = [], so it returns [gs=True, []]  but this should actually never happen.   "], "1486": [], "1485": [], "1484": [], "1483": ["suggestion from xcache devs (@abh3, @wyang007):    ```  <glfn name=\"/atlas/rucio/mock:another.one.zip?xrdcl.unzip=zippedfile-0-5b7217bc734b41ed9547162d6693e5d2\"></glfn>  <url location=\"APERTURE_MOCK_LOSSXLHIQX\" domain=\"zip\" priority=\"1\"      client_extract=\"false\">root://root.aperture.com:1409//test/chamber/mock/d7/b0/another.one.zip?xrdcl.unzip=zippedfile-0-5b7217bc734b41ed9547162d6693e5d2</url>  ```", "Hi Mario,  Indeed, that was what I recommended today for Wei', conundrum. In general  we want to know the base file not the archive within it as we will be  cachin the whole zip file. That said, there are a lot of devious issues  involved here, especially when dealing with intermediate servers along the  way. So, I can't say we have addressed all the issues. However, as a first  take, what you say is the most reasonable option.  Andy   On Wed, 22 Aug 2018, Mario Lassnig wrote:  > suggestion from xcache devs (@abh3, @wyang007): > > ``` > <glfn name=\"/atlas/rucio/mock:another.one.zip?xrdcl.unzip=zippedfile-0-5b7217bc734b41ed9547162d6693e5d2\"></glfn> > <url location=\"APERTURE_MOCK_LOSSXLHIQX\" domain=\"zip\" priority=\"1\" >    client_extract=\"false\">root://root.aperture.com:1409//test/chamber/mock/d7/b0/another.one.zip?xrdcl.unzip=zippedfile-0-5b7217bc734b41ed9547162d6693e5d2</url> > ``` > > --  > You are receiving this because you were mentioned. > Reply to this email directly or view it on GitHub: > https://github.com/rucio/rucio/issues/1483#issuecomment-415303822 ", "This was fixed automatically through #1613 "], "1480": [], "1479": ["- add vhost support", "I would probably be able to provide a pull request for this (although my ability to test anything is limited since I'm still trying to get it working at all).", "We can test and confirm it. So if you want to provide the PR, please go ahead.", "I have this working (at least for the hermes and cache daemons, which are the only ones I'm currently using). But I have a question about IPv6 - the existing implementation doesn't use it since it only looked up the DNS A record. But getaddrinfo will return both unless instructed otherwise. For the time being I've forced IPv4 to keep the behaviour consistent, but that's not very future proof. Would it be better to use the v6 addresses if available? (I see atlas-mb.cern.ch does have them.)", "I think there's standard policy and procedure we can follow:  - Use V6 if that's available.  - If not, use V4  - (Extra credit) If both V6 and V4 are available, then, after waiting for 150ms for the V6 connection to finish, opportunistically try a V4 connection as well.  Take the first one to successfully connect.     - I think I've only seen this recommendation executed in big codebases, such as browsers.    I saw at the GDB today that 23% of LHC data is transported using IPv6; this suggests the simple case of \"use first record available\" is probably fine for most R&D networks.", "After having so much trouble with IPv6 on the CERN network I consider the extra credit option as mandatory unfortunately.", "One problem in this case is with the multiple responses to the name lookup I don't think it's possible to match the 6 and 4 addresses for the same host. So you could easily end up connecting twice to the same broker, which doesn't seem like a good idea.    How about a disable IPv6 option? - it'd be simpler to implement.", "That's perfectly viable. If we feel the need to support IPv6 properly down the line we can always do it.", "@illingwo did you manage to progress on this? if you already have some lines of code written I can integrate them during our coding-camp next week", "I never got around to doing the IPv6 part. I can turn what I do have - IPv4 only with the system resolver - into a pull request.", "Ok, cool! I'll take care of the ipv6 part then later on. Thanks!"], "1476": ["I have updated #819 and will close this ticket. Right now the clients are compatible with Python 3.5 but fail with 3.6+.", "Duplicate of #819 "], "1475": ["The attack vector is for Elgamal, which we are not using.    One option for pycrypto is to change to https://github.com/Legrandin/pycryptodome  To be investigated.", "We should be able to remove pycrypto. It's a secondary dependency, and oauth2client will automatically fallback to pyOpenSSL which we have anyway."], "1470": ["@TWAtGH during the creation of the PR we saw that UploadClient and DownloadClient are not inherited from BaseClient. Is there a special reason for this? ", "I don't really remember to be honest. But looking at it now the download and upload client don't really fit into the api like the other classes do. The two classes only need to use the client and don't do rest calls themselves.   I guess that were my thoughts when creating these classes. I probably wasn't aware of how the client object is meant to be used.  However changing this now will be a little bit challenging since the `__init__` calls are getting some custom parameters and should be downward compatible.", "I would suggest to close this one. As I've written above adding these classes to the main client is not necessary and could be quite problematic"], "1464": [], "1463": ["I think this is just a typo. I don't think there is a `===` verison operator in setuptools. ", "Which version of setuptools are you using?", "Setuptools 40.0.0, although the failure doesn't involve setuptools at all. There is actually a triple equals operator, but I'm not sure if it's expected to work in this context or not (see https://www.python.org/dev/peps/pep-0440/#arbitrary-equality)     ", "Yes, I just saw, the operator is not directly described in setuptools but apparently has been patched in in version 8.0. I cannot re-produce this on my machine, neither with 40.0 nor with 36.8.0 which we use on our machines.", "It works with pip 8.1.2 that comes with RH 7, but I'm not sure it's patched - more that the older version of pip is simpler.    You should be able to reproduce with a virtualenv:  ```  [illingwo@fermicloud014 ~]$ python -mvirtualenv testdir  New python executable in /cloud/login/illingwo/testdir/bin/python  [illingwo@fermicloud014 testdir]$ . bin/activate  (testdir) [illingwo@fermicloud014 testdir]$ pip install --upgrade pip  Collecting pip    Using cached https://files.pythonhosted.org/packages/5f/25/e52d3f31441505a5f3af41213346e5b6c221c9e086a166f3703d2ddaf940/pip-18.0-py2.py3-none-any.whl  Installing collected packages: pip    Found existing installation: pip 9.0.1      Uninstalling pip-9.0.1:        Successfully uninstalled pip-9.0.1  Successfully installed pip-18.0  (testdir) [illingwo@fermicloud014 testdir]$ pip install rucio  Collecting rucio  Exception:  Traceback (most recent call last):    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_internal/basecommand.py\", line 141, in main      status = self.run(options, args)    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_internal/commands/install.py\", line 299, in run      resolver.resolve(requirement_set)    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 102, in resolve      self._resolve_one(requirement_set, req)    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 306, in _resolve_one      set(req_to_install.extras) - set(dist.extras)    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2826, in extras      return [dep for dep in self._dep_map if dep]    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2871, in _dep_map      self.__dep_map = self._compute_dependencies()    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2881, in _compute_dependencies      reqs.extend(parse_requirements(req))    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2942, in parse_requirements      yield Requirement(line)    File \"/cloud/login/illingwo/testdir/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2951, in __init__      raise RequirementParseError(str(e))  RequirementParseError: Invalid requirement, parse error at \"'(===6.3.'\"  ```  ", "Confirmed. \u2705 related to pkg_ressources in the pip version.  I think we can change the === to a == that should be sufficient to fix this, since we do not need that strict comparison anyway."], "1457": [], "1455": [], "1454": [], "1446": ["I think I've found them all. rucio/core/account_counter.py also uses md5() on a uuid, but the concat function coerces its arguments to text anyway, so that one should be okay."], "1443": [], "1434": [], "1433": [], "1432": [], "1430": ["Proposing to close as duplicate of #42 "], "1427": ["Another problem with this is that the download API is currently designed to allow to specify parameters per DID (e.g. rse-expression, schemes, resolve_archives), while list_replicas takes those arguments per call.  Solutions are:  1. Merge the DIDs by the parameters to reduce the list_replicas calls to a minimum  2. Create a new method in the API that is more strict in specifying the parameters but fits to list_replicas  3. Drop the support of specifying parameters per DID in the current download API  4. Change list_replicas so that it is possible to specify parameters per DID", "Change for list_replicas():    Return (multiple) fields in metalink: `<parent>...did...</parent>`    Return single list field in json: `'parents': [did1, did2, ...]`    Convention is not to resolve all parents, only the ones the user is requesting."], "1423": [], "1422": ["Increased the priority to high. It can potentially generate high load on the servers.", "What was the reason for changing this to ALL again? I remember that it was on COLLECTION in the beginning?"], "1415": ["`rucio download --metalink=my_replicas.meta4 scope:name`"], "1412": ["Decided against. Spikes memory and network on server node.  Will provide @palnilsson a function to generate metalink pilot-side from the list_replica reply."], "1411": [], "1410": [], "1405": [], "1404": ["suggestion: \"--allow-tape\" :-)  "], "1394": [], "1390": ["Still does not work: https://readthedocs.org/projects/rucio/builds/7547086/", "So this probably needs to wait until pycurl is an extra in the fts3-rest-api package. For now, although only the requirements.readthedocs.txt should be used, it seems like rucio as a package is pulled in as well, which then requires pycurl via fts3-rest-api, crashing readthedocs build.  Needs a version > 3.7.1 https://pypi.org/project/fts3-rest-API/#history"], "1379": ["ok, confirmed the bug. this is not properly taking into account that there are two root doors (one internal, one external) and thus takes the higher priority one of the internal door (thus jumping in front of srm), when it should just ignore it and instead take the priority of the external door (which is left out completely at actual priority=2, since there already is a (bad) root priority assigned)"], "1378": ["Would be solved by:  https://github.com/PanDAWMS/pilot/pull/218"], "1377": ["@TWAtGH isn't this fixed with 1.17.6?", "No. #1395 will fix this."], "1376": [], "1375": [], "1370": [], "1367": [], "1366": [], "1361": ["This doesn't seem to be my fault:  with python 2.7:    CannotAuthenticate: Cannot authenticate.  ", "Although it can be, that certain activities can be processed only by certain accounts, and travis had not the rights to run this activity?  "], "1360": ["@TomasJavurek or @TWAtGH can one of you please have a look on this?", "Also adding @nikmagini as he had a look on the email already \ud83d\ude04 ", "We werent able to reproduce this and the problem is probably solved with 1.17 because the upload code was completely overhauled.  So I'm closing this issue.", "I got the problem with 1.17.7. In my case, It was due to the rsemgr sending back a different value then the expected one. I vaguely remember a dict with True instead of something else.  One side remark, The logging in the debug mode of the traceback error showing where the error is exactly happening in the upload function is missing and will be more than useful.  ", "Good call thanks. I've fixed it with #1533 There were also another bug."], "1359": [], "1355": [], "1354": ["I'm going to disable multi threaded download in case that multiple files should be downloaded from the same archive (guess it's not too unlikely for datasets?)    How shall we handle the archive file? I have different approaches in mind:  A  1. download archive  2. extract file  3. go to 1. for next file    B  1. download archive  2. extract file  3. delete archive  4. go to 1. for next file    C  1. download archive if it doesn't exist  2. extract file  3. go to 1. for next file  4. delete all archives    Or any of these approaches with downloading archives into /tmp/... ", "How about never downloading the archives into `/tmp`?    1.  Sort all files needed by archive name.  This way, you only download each archive once.  2.     *  If gfal2 module is present: For each archive needed, read the last 1MB of the archive, parse out the central directory of the zipfile, then read out only the bytes corresponding to files you need.     *  If gfal2 module is not present: download whole archive to the output directory, then open with python, unlink the file, and start extracting the files needed.    Should be relatively straightforward to avoid downloading the whole archive in the end...", "Do I have the archive name?  This ticket should only handle the case when the protocol cannot stream the single file from an archive. I was told to use an external program like unzip in this case. ( @bari12 )", "Even if I had the archive name sorting would not be that easy because this would probably break the PFN order", "Yes, only the actual local extract case should be handled here, streaming the content of the (uncompressed) zip is already handled in list_replicas.  The result of list_replicas (be it metalink or not) has the PFN of the archive.  Right now the loop is going logical file by file and then do the actual physical download. What you can do (Basically Brians suggestion) is to take the ones which only have replicas saying `client_extract=True` out, remove the duplicates from this list and only download these replicas. Afterwards you extract the files wanted (re-adding the logical files whose duplicate replicas you removed). I know this breaks the current simple \"one loop solves it all\" concept, but it shouldn't be all too complicated to add.", "Can't list_replicas return a file that has some PFNs with client_extract=True and some with False?  So if its always either True or False per file it should make things easier. But in this case client_extract should be a file attribute", "Otherwise I would be afraid of cases like:  ```  <metalink>    <file1>      <url client_extract=True prio=1>same_archive_pfn</url>      <url client_extract=False prio=2>pfn1</url>    </file>    <file2>      <url client_extract=False prio=1>pfn2</url>      <url client_extract=True prio=2>same_archive_pfn</url>    </file>  </metalink>  ```  You say in such cases I should skip the prio 1 pfn of file2 ?", "@TWAtGH right now, client_extract is set per file (current decision is based purely on if protocol==root)", "It should be set per replica. It's a valid case to have a file which has \"normal\" replicas but is also a constituent of an archive. Thus some replicas of this file would have client_extract=True some would not."], "1353": [], "1352": ["@mlassnig not sure if you have seen this ticket, would be great if you can have a look.", "ok, figured it out. zipped replicas which are not part of the rse_expression are not correctly removed in the response to the client."], "1346": [], "1338": [], "1335": [], "1334": ["The order of this configuration was changed some time in the past. So, what's currently in the demo rucio.cfg is wrong. Changing the config as file as you proposed will fix it. No need to fix the config reader."], "1329": [], "1324": [], "1323": [], "1318": [], "1317": [], "1310": [], "1307": [], "1306": [], "1305": ["The associated RSE for resolved archives is not correctly updated, it sets the RSE of the existing replica instead."], "1302": ["How far do we have to downgrade so it doesn't ask for the specific requests version?", "Just the previous versoin (which is from 2015), but it's not on PyPI anymore. This is starting to get tricky."], "1299": [], "1298": [], "1295": [], "1288": [], "1283": [], "1282": [], "1281": ["+ scope ?", "`Scope` might make sense, I'll have a look.", "Up", "Can we increase the priority of this ticket ? It's mainly for AMI"], "1276": [], "1273": ["We should really think about the syntax in `--filter`  In the meanwhile we could add a `--exclude-empty`", "Has there been progress on this?", "Hi Alex, no news on this. These days development time is thin as many people are on vacation and there were also some conferences and meetings in the last weeks. We will keep you posted in this ticket, but I don't have a specific ETA yet.", "Understandable. Thanks for the reply.", "Maybe it would be useful to implement the possibility to filter more generally. For example size.gt to filter out containers with a size greater than 0. And of course size.lt for less than.  ```shell  rucio list-dids user:bla --filter size.gt=0,type=CONTAINER  ```"], "1272": [], "1269": [], "1268": [], "1263": [], "1262": [], "1251": ["@cserf @mlassnig I can take the lead on this one if you want?", "This is more of a deployment issue, since from a code point of view a distributed memcached can be used. Moving this to ATLAS jira."], "1250": [], "1249": ["@mlassnig @nikmagini @TomasJavurek can one of you take over this ticket? It would actually be nice if we can get this into 1.17.0 (on Monday). If nobody has time I can take it as well."], "1241": ["Hmm, this is basically the same as #668 and maybe also partly #1222 - we should close some of them.", "Actually, #1222 can stay open, but I would close this one and leave #668."], "1240": [], "1228": [], "1227": [], "1226": ["Hi --     can you also add a chart / manifest to show how to configure the client using the rucio/rucio-client image. I tried and failed by     * bind mounting a `rucio.cfg`  * copying the ca.crt from ATLAS (`/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.16.3/etc/ca.crt`) into `RUCIO_HOME/etc/ca.crt`  * setting a providing a proxy file and setting X509_USER_PROXY, RUCIO_AUTH_TYPE and RUCIO_ACCOUNT env vars     ```  $> tree  .  \u251c\u2500\u2500 etc  \u2502\u00a0\u00a0 \u251c\u2500\u2500 ca.crt  \u2502\u00a0\u00a0 \u2514\u2500\u2500 rucio.cfg  \u2514\u2500\u2500 x509up_u36923  $> docker run --rm -it -v $PWD:/tmp -e X509_USER_PROXY=/tmp/x509up_u36923 -e RUCIO_AUTH_TYPE=x509_proxy -e RUCIO_ACCOUNT=lheinric rucio/rucio-clients:latest bash  ```    some files get downloaded but I also see some issues like    ```  2018-06-12 22:24:59,842\tINFO\tThread 1/2 : File group.phys-exotics:group.phys-exotics.14267504.XAMPP._000001.root trying from CA-WATERLOO-T2_SCRATCHDISK  2018-06-12 22:25:01,268\tWARNING\tThe requested service is not available at the moment.  Details: An unknown exception occurred.  Details: Could not open source: Failure (Neon): Server certificate verification failed: issuer is not trusted after 1 attempts  ```", "The last error is more a storage issue. If you get `rucio ping` working it's most likely fine.    Otherwise, there is the image rucio-clients-atlas:         https://hub.docker.com/r/rucio/rucio-clients-atlas/    If you are willing to test it, we can update it and update the instructions on dockerhub as well    ", "Hi @vingar,    yes, I'm very happy to test an ATLAS use-case. For the ATLAS image, would you be willing to also add the X509 auth (e.g. `voms-proxy-init` etc such that inside the container one could do     ```  $> voms-proxy init --voms atlas  $> rucio download ...  ```    In Kubernetes this could then be run as part of a `initContainer`", "yes, that's doable. But to do `voms-proxy init --voms atlas` one should mount the .globus directory as well or the variables X509_USER_CERT X509_USER_KEY.", "yes, my idea was to mount this via secrets. e.g.     ```  apiVersion: v1  kind: Secret  type: Opaque  data:    gridpw: ...    host.cert: ...    privkey.pem: ...  metadata:    name: atlas-auth    namespace: default  ```"], "1223": ["Pinging @hahahannes "], "1222": ["This is not a bug but a missing feature. Rucio download **does** respect the order but it's not able to determine if lan or wan should be used. Instead it always uses the default (=wan)."], "1219": [], "1218": ["So basically list-account-usage, but instead of per-user account per-rse?", "Yes", "Hi,  is it available in lxplus? how can I use it? Thanks,    Best,    Jose", "It is as TESTING, so you have to install the testing release with lsetup  There is a new option for list-rse-usage: --show-accounts", "> It is as TESTING, so you have to install the testing release with lsetup  > There is a new option for list-rse-usage: --show-accounts    Great!! Thanks! Just a detail, is there a way to get the response in a different format? Thanks again,  Jose", "I am sure we can change the formatting a bit. What would be your suggestion?", "> I am sure we can change the formatting a bit. What would be your suggestion?    I was just thinking about a line per user instead. I can parse the current output easily but just wondering if that was possible. Thanks again,", "Oh, I thought it is already 1 line per account, but I just tried it and saw that it is multiple lines. We will fix that and make it one line per account. Will be fixed in #1672"], "1216": [], "1215": [], "1214": ["The wrong traces were sent by download() method in downloadclient, that is going to be deprecated: https://github.com/rucio/rucio/issues/1378  the other method download_dids(), should send the traces correctly. Only missing piece is to add correct format to method download_pfns(), where the traces are not introduced at all. This is not problem for now, because pilot is sending traces alone (the switcher that distinguish between copytools is not active). The possibility of propagating the trace created by pilot to the rucio clients is being discussed.", "Pilot1 Done; Pilot2 still missing", "Any update?"], "1208": ["@TWAtGH FYI"], "1204": [], "1195": ["I'll be taking this up  ", "I'm currently still having troubles getting Oracle 12c reliably running on travis, i.e., you will need to add a switch in the code to enable and disable this feature based on the version of Oracle. So if the version is < 12 then disable json support otherwise enable it. This will be necessary anyway since our production DB is still 11g and we also would run tests both against 11g and 12c."], "1193": ["Duplicate of #1152 Closing"], "1190": [], "1185": ["@berghaus You recently did some changes in this code I think. Can you maybe have a look on this issue? \ud83d\ude03 "], "1184": [], "1179": [], "1177": [], "1176": ["This is fixed with Rucio 15.5.", "Can we confirm that this works correctly with the pilot? The test version of rucio on the ALRB is 1.16.1, so this should be possible."], "1173": ["PS I will not push it as far as having the email too, but that is yet another separate action if I don't know the account, I have to go to the CERN directory to find the user email. Sometimes the user doesn't exist because they have left for good.", "Hi Alessandra,    I think the Dump you actually want to use is the Datasetlocks dump, which should give you exactly this information. You might have to filter it a bit though, as in case where multiple rules exists for the dataset, you will have multiple entries. https://rucio-ui.cern.ch/dumps  I am not sure at the moment how to access this dump, maybe @tbeerman can comment?", "Hi,    do you mean the dataset consistency one?     All in all there is a \"Dataset per RSE\" (datasets_consistency) with the account name in it and a \"Dataset per RSE (Incomplete)\" (datasets_per_rse) without account name. Is there a reason why they cannot both have the account name? The dump with incomplete datasets contains everything that is on disk, I assume, so it is a better choice to check what is on disk at sites and who owns them. It also contains the creation and expiry date which I need to know what is the oldest stuff. While the datasets_consistency one contains other things that are not particularly useful to a system administrator.", "> other things that are not particularly useful to a system administrator     unless those number I see in datasets_consitency are unix epochs, i.e.     \\<create date> \\<guid> \\<?> \\<expiry date>     \\<?> could be either number of replicas but more likely 2=secondary, 1=primary?", "Ok so while consistency_datasets is actually useful and more easily scripted it does only account for a fraction of the TB in the RSE.     ```  aforti@vm7> awk '{sum+=$5/1000000000000} END {print sum}' UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK.txt|sort -nk2  274.266  aforti@vm7> awk '{sum+=$4/1000000000000} END {print sum}' UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK-1.txt|sort -nk2  383.021  ```  I need to have all the files, the account name and if 1 and 2 stand indeed for primary and secondary that would be useful too as is unix epochs. In short consistency_datasets format but for the datasets in datasets_per_rse    Sorry for all the replies to myself. I know it is annoying (trying to quit the habit). ", "The different dumps are coming from two different DB tables and only one has the account information (dataset_locks). For every rule on a dataset a dataset lock is created with the account that requested the rule. But the lock is only the storing the logical size and the dump only includes datasets which have an \"OK\" rule. This one cannot be used to check disk usage and it's more an overview of which user requested what to be replicated.  The other tables (collection_replicas) keeps track of the actual number of files and the size of a dataset replica at an RSE with or without a rule and rule. But there is no account here as there can also be replicas without a rule (secondaries). So this one will have the correct space usage number but not the accounts.  What I can do now is to join both dumps to get the account information from the dataset locks and the actual space usage from collection_replicas. But this will then also mean that there will be replicas with and account. But it should be minimal for LOCALGROUPDISK as replicas should be deleted directly after the rule is deleted.", "Hi Thomas,  thanks a lot, will you keep also the dates? (I think they are created,  expiration and last access) if possible it would be useful to keep them  all (also knowing which is which without guessing).   On 29/05/2018 14:07, Thomas Beermann wrote: > > The different dumps are coming from two different DB tables and only  > one has the account information (dataset_locks). For every rule on a  > dataset a dataset lock is created with the account that requested the  > rule. But the lock is only the storing the logical size and the dump  > only includes datasets which have an \"OK\" rule. This one cannot be  > used to check disk usage and it's more an overview of which user  > requested what to be replicated. > The other tables (collection_replicas) keeps track of the actual  > number of files and the size of a dataset replica at an RSE with or  > without a rule and rule. But there is no account here as there can  > also be replicas without a rule (secondaries). So this one will have  > the correct space usage number but not the accounts. > What I can do now is to join both dumps to get the account information  > from the dataset locks and the actual space usage from  > collection_replicas. But this will then also mean that there will be  > replicas with and account. But it should be minimal for LOCALGROUPDISK  > as replicas should be deleted directly after the rule is deleted. > > \u2014 > You are receiving this because you authored the thread. > Reply to this email directly, view it on GitHub  > <https://github.com/rucio/rucio/issues/1173#issuecomment-392768970>,  > or mute the thread  > <https://github.com/notifications/unsubscribe-auth/AE1IEDHlFM2NCx0mJy_sfjfZ07QdjMSHks5t3UgLgaJpZM4UClXo>. >  --  Respect is a rational process. \\\\// Fatti non foste a viver come bruti, ma per seguir virtute e canoscenza(Dante) For Ur-Fascism, disagreement is treason. (U. Eco) But but but her emails... covfefe!  ", "Yes, I'll keep the dates. But you don't really have to guess anything it is all described at https://rucio-ui.cern.ch/dumps", "Hi Alessandra,    the new dump is now available at https://rucio-hadoop.cern.ch/datasets_per_rse?rse=UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK    The new fields are:  RSE, scope, name, owners, size, created_at, updated_at, accessed_at, rule_ids and state    There can be multiple owner separated by ',' if there are multiple rules on the same dataset. Then there will also by multiple rule_ids. The rule_ids will be in the same order as the owners.  If there is no rule the owner will be 'None' and the rule_ids will be empty. But for LOCALGROUPDISK there should only be very few with this.    Please let me know if this is now ok for you.", "Hi Thomas,    looks better! thanks    Some of the entries have only two dates. Do you know which one might be missing every now and then?     For example     > UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK    archive mc15_13TeV.361107.PowhegPythia8EvtGen_AZNLOCTEQ6L1_Zmumu.merge.DAOD_EXOT0.e3601_s2576_s2132_r7725_r7676_p2669_tid08616109_  > 00    bhauruth        57643089909     2017-10-26T07:36:31.000Z        2017-10-26T07:38:11.000Z                7364D8CFB8AA4237B9663A1E596347B2        A  >     > UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK    data16_13TeV    data16_13TeV.00307354.physics_Main.merge.DAOD_STDM3.f739_m1667_p2950_tid10317429_00     jcrane,bhauruth 16657639758     2018-02-21T12:38:08.000Z        2018-05-25T14:37:45.000Z        2018-05-15T15:44:55.000Z        32933E0E07EB4D7ABB2AF5FF6B53E395,2A37F86828A54C8A9E4B7B660774469E       A", "Hi Alessandra,    created_at and updated_at will always be set but accessed_at can be empty if this dataset replicas was never accessed.", "ok, thanks. ", "Hi Thomas,    I was concentrating on the format and only now realised the data is not complete. There are ~ 390TB in the RSE, but if I sum the size of what is in the dump I only get a tiny fraction (~9GB). Was this just a test?   awk '{ sum=+$5 } END {print sum/1000000000}' UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK.txt   9.33449    thanks    cheers  alessandra", "You have a typo in you awk code. It should be `awk '{ sum+=$5 } END {print sum/1000000000}'`", "Thanks.", "For me we can close the ticket. I now have everything I need to know who's using the space, and how old are their data and if it was accessed or not plus their rules id if I need to take action.     thank you.", "Hi together,    the in-tree documentation:   https://github.com/rucio/rucio/blob/144c6fdff4ec80cc618abdad010157e9661b4174/lib/rucio/web/ui/templates/dumps.html#L43-L44  still lists the old format, without the newly available fields - should I open a new ticket about this?     Cheers,  Oliver", "Hi @olifre     No let's use this ticket. @tbeerman can you please update the template.    Cheers"], "1172": ["I discussed this a bit with Tobi already yesterday, can I have a look?", "Yes! Thanks a lot @TomasJavurek "], "1169": [], "1168": [], "1163": [], "1160": ["Can't reproduce?    ```  $ rucio add-rule data10_hi:NTUP_HI.289208._000500.root.1 1 NDGF-T1-MWTEST_DATADISK  2018-05-15 11:46:41,885 ERROR   RSE excluded due to write blacklisting.  Details: RSE excluded due to write blacklisting.  ```      and against 1.16.1:    ```  $ rucio -H https://rucio-lb-int.cern.ch add-rule data10_hi:NTUP_HI.289208._000500.root.1 1 NDGF-T1-MWTEST_DATADISK  2018-05-15 11:47:56,291 ERROR   RSE excluded due to write blacklisting.  Details: RSE excluded due to write blacklisting.  ```", "Identified.    when raising DuplicateRule the error is not passed to the exception"], "1159": [], "1158": ["I think for this it should be fine if `filename` gets added to the schema. (This is related to the change Brian did to be able to have filenames different than the `name`)   I am just wondering why this only shows up now or if basically no `rucio upload` works since this? @mlassnig @vingar Can one of you please fix this so we have it in the release next week?", "@TomasJavurek , @TWAtGH  isn't it fixed ?", "I tried to reproduce this but can't. Closing"], "1156": [], "1155": [], "1149": [], "1144": [], "1143": [], "1138": [], "1137": ["The code for bin/rucio @TWAtGH will add."], "1128": [], "1127": ["When is this going in production? UI is still on 1.16.0. I thought this would be quicker.", "The usual release cycle is 2 weeks. However, we did not deploy 1.16.1 due to a problem, thus we are still on 1.16.0  1.16.2 will be deployed tomorrow."], "1118": [], "1117": [], "1116": [], "1115": [], "1104": [], "1093": [], "1092": [], "1086": [], "1083": [], "1080": [], "1078": [], "1077": [], "1060": [], "1057": [], "1056": ["Hi @cserf     rucio list-file-replicas already has the --expression option to pass RSE expressions - does it do what you need?    Cheers  N.  ", "Yes, that's it. Closing the ticket."], "1055": ["See https://github.com/PanDAWMS/pilot/blob/master/TimerCommand.py for an example implementation of a timeout wrapper", "As discussed in the development meeting on [2019-01-10](https://indico.cern.ch/event/778899/) - Investigate if it is a possibility to kill e.g. gfal threads via a watcher thread.", "Maybe its worth to check if you can use the gfal context object in a watcher thread to abort the download.  The Gfal2Context object has a method called cancel that 'cancels running operations'.   ```  >>> import gfal2  >>> gfal2.Gfal2Context.cancel  <unbound method Gfal2Context.cancel>  ```", "Is there any update on this? It's quite high priority as that happens quite often on the grid.", ">   >   > Maybe its worth to check if you can use the gfal context object in a watcher thread to abort the download.  > The Gfal2Context object has a method called cancel that 'cancels running operations'.  >   > ```  > >>> import gfal2  > >>> gfal2.Gfal2Context.cancel  > <unbound method Gfal2Context.cancel>  > ```    Thanks for the tip - unfortunately, I Tried gfal2.Gfal2Context.cancel (on a root:// --> file:// transfer from EOS) but it blocks until the transfer is completed so it looks useless for cancelling a stuck transfer in general...    From what I see, async cancel is implemented only for gridftp  - I'll double-check with gfal devs", "> From what I see, async cancel is implemented only for gridftp - I'll double-check with gfal devs    On the other hand, gfal cancel worked fine on gridftp and http transfers in my tests. Checking with gfal devs what's wrong with xrootd cancel.", "Thanks Nikolo! Let's check with gfal devs about it, but unless we cannot cancel consistently all (or the at least the major) protocols, I think we have to look for another way. Let's discuss this in the dev meeting tomorrow.", "Andrea Manzi commented that it looks like a gfal issue with xrootd cancel. Anyway for the other protocols it seems to work, for now I've submitted a pull request to implement it     (note - currently it won't do anything due to a bug in propagation of transfer_timeout in the client- submitting a separate bug report)"], "1054": [], "1051": [], "1048": [], "1045": [], "1044": [], "1040": [], "1039": ["This is on hold until a new version of flake8 is released which supports pycodestyle==2.4.0  Only support for 2.3.1 at the moment."], "1038": [], "1036": [], "1032": [], "1029": [], "1025": [], "1024": [], "1023": [], "1022": ["I am not sure if we really need to add this in the days of containers. The argument for sqlite was always that it is easy as you do not need to install mysql etc for quick development. But now in the days of containers the point is not really valid anymore. If the code runs fine on mysql/postgres/oracle it is fine I think."], "1020": [], "1019": [], "1018": [], "1017": [], "1012": ["This is a duplicate of #195", "Perhaps we should create a second issue so we fix the E722 eventually and don't completely forget about them.", "yes +1"], "1009": [], "1007": [], "1005": [], "1004": ["This should probably be fixed by doing a MERGE INSERT into the history table. That should solve the issue. It's not high priority though \ud83d\ude04   Thanks for the report."], "1001": [], "999": ["After some investigation, it looks like this is actually a docker-compose issue on my dev environment.  See https://github.com/docker/compose/issues/1567#issuecomment-307149406", "After _further_ investigation, it seems that adding the correct docker-py version to the virtualenv solves the problem.", "Thanks Bruce. Should we expand this on in our documentation?", "Hi @mlassnig I'm working on a PR... just checking a few things :smile: ", "Cool, thanks ;-)"], "997": [], "992": ["it's easier to do it for the daemons first as there are no tests using the clis.    cf. http://python-packaging.readthedocs.io/en/latest/command-line-scripts.html"], "985": [], "976": ["It's in both?    https://github.com/rucio/rucio/blob/master/etc/rucio.cfg.atlas.client.template    https://github.com/rucio/rucio/blob/master/etc/rucio.cfg.template", "[...] tests"], "973": [], "965": [], "964": [], "963": [], "961": ["Additional one in the poller :  ```  2018-05-23 16:57:18,097 11839   ERROR   Failed to query FTS info: Traceback (most recent call last):    File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/poller.py\", line 231, in poll_transfers      resps = transfer_core.bulk_query_transfers(external_host, xfers, 'fts3', timeout)    File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 288, in bulk_query_transfers      fts_resps = FTS3Transfertool(external_host=request_host).bulk_query(transfer_ids=transfer_ids, timeout=timeout)    File \"/usr/lib/python2.7/site-packages/rucio/transfertool/fts3.py\", line 347, in bulk_query      raise Exception(\"Failed to parse the job response: %s, error: %s\" % (str(jobs), str(error)))  Exception: Failed to parse the job response: <Response [200]>, error: Expecting value: line 2 column 5 (char 14880)  ```", "For the first one, `RequestNotFound` should be raised instead of generic `RucioException` in transfer.py and the exception should be caught in common.py  The second one should catch `JSONDecodeError`  "], "954": [], "949": [], "943": [], "942": [], "941": [], "938": [], "935": [], "930": ["This works for me in Python 2.7?", "not for me on py3K. pip doesn't find the last version and that's why I relaxed the version requirement."], "929": ["The port 443 is taken by the docker-proxy application. I didn't see that before for me. This application might be the rucio one actually. If not, one naive comment should be to stop the application taking the port 443. Did you try to list the running docker containers `docker ps` ? Maybe the rucio one has been already started which explains this conflict. ", "You could try changing the exposed port. Just change line 15 of `etc/docker/demo/docker-compose.yml` to something like:  `- 6443:443`", "I faced the same problem yesterday. A simple `sudo service docker restart` worked for me.", "I  used ```docker ps``` and found that other application was using the port 443. Stopping that app and restarting docker service resolved the issue. I did not try the option given by @tbeerman, but I think that would work too.    Thank you all."], "928": ["Closing this; It's basically #1055"], "913": [], "912": ["@bari12 I would like to take this up if possible. Where exactly do the release notes have to be added?", "Part of the work is to replicate the changelog from here https://github.com/rucio/rucio/releases into individual rst files. I guess you can help on this but first @bari12 has to define the directory and file name conventions. ", "Hi, I have already started with this and prepared a structure and kind of template. (PR for that will come today) - If you would like to you could then fill all the missing release notes. I would be very thankful for that \ud83d\ude04 ", "Sure @bari12  I will wait for your PR then to get started. ", "The PR is added now. Let's wait a bit of some people have comments to the general structure etc. Once it is merged I think you can go through the github releases and generate the releases notes similar to the 3 I already created. (You can use the `tools/generate-release-notes <release> --rst` script for the rst output) - Thanks", "When I run the script, it asks for a Github token file. How am I supposed to get that? From what I understand it is needed for authentication to use the Github API.", "Go here and create a new one, and put it in the .githubtoken file ;-)    https://github.com/settings/tokens", "Thanks @mlassnig it's working now.  @everyone If I understand the task correctly, I am to create rst files for ALL releases on https://github.com/rucio/rucio/releases? If so I should also club clients & webui releases in the same file. And do I create files for each milestone number or separate files for hotfix releases as well?", "Yes essentially create one file for each release (including webui-only, client-only and hotfix releases). If you want you can create a subset of the files first and submit the PR so we can comment them; so you won't have to change all your files if there are some repeating issues. \ud83d\ude04 ", "@bari12 @mlassnig @vingar I have added a few files via PR #946. Is the formatting correct? I wrote a small script to create these files. Also I am unable to generate release notes for tags of type *.post1. How do I proceed? Also the release-notes tool is not returning anything for releases like 1.14.10. Do I skip these or enter them manually?", "Hi @tushar-97 The formatting is fine \ud83d\udc4d   Let me check what's broken with the release notes script.", "Ah I just realised. For the releases where there was a hotfix release afterwards there are two releases in the repository (two tags). A normal one such as 1.14.10 and a .post1. However, there is only one milestone (the 1.14.9) - that's why the generate-release-notes script is not returning anything. In this case just ignore the hotfix (.post1) ones and just create the notes for the normal ones.  For certain release (client releases) - such as 1.14.10 there is also no milestone, these you can also ignore.", "I added all the files but the travis-ci test has failed. I have no clue why that happened as I only added 12 rst files.", "This is a problem on the current master. Once solved, a rebase will solve the errors on your PR as well."], "910": ["Need confirmation for the mentioned link which contains the RSE expressions docs - https://gitlab.cern.ch/felopez/rucio/commit/8c5474fb119f0078afc39ab02a2e273695ca7bf8 , I would do the necessary change.", "This is the correct page it should reference: rse_expressions.html  (it's an upper/lowercase problem)    Thanks!", "All right.! I found similar link error in other places, created PR for this."], "906": [], "886": ["Is there anyway I can propose just one request with all changes across multiple files?", "Sure, just add multiple changed files to the same pull request.", "@mlassnig Done. Sorry for the mix up before"], "875": [], "870": [], "867": [], "862": [], "850": [], "849": [], "847": ["No comment? Alright I'll try out what I am suggesting ;-)    The other option I was able to think of is to create a separate function in the protocols that get the checksum.", "Hi Frank, sorry for the late reply. \ud83d\udc4d for the approach you are describing. Don't think we need the separate function in protocols.", "Thanks Martin, I'll get started with it :-)"], "819": ["There is a general ticket tracking the py3k migration.  https://github.com/rucio/rucio/issues/67  You can use this for general tracking.", "BTW: I found that the `six` python module makes these kinds of things easier during the transition period:  https://pythonhosted.org/six/#module-six.moves    It is also easier to find them later (searching for `six`) whenever the project is ready to go Python 3 only.", "I think all the client files are now py3k compatible. @hahahannes can you confirm ?", "The last ones missing are the didclient, fileclient, dq2client and the downloadclient. According to pylint there are just small changes needed. I can fix them."], "811": [], "808": [], "807": ["1. Should be indeed datetime.timedelta(hours=1)  2. Should be tomorrow = datetime.utcnow() + timedelta(1)"], "806": [], "805": ["First, we'll also need to make sure that all the transfer logic is indeed in the transfer tool: last I checked, some FTS code had snuck into the conveyor itself.", "Yes, there are some parts, especially related to activemq messages which are part of the conveyor directly. this needs to be properly disentangled and put into the transfertool.", "#857 is related to this.", "This has been implemented in #2905 "], "800": [], "799": [], "796": [], "793": [], "782": [], "769": [], "763": [], "762": ["this should go also in alembic and schema"], "761": [], "757": [], "754": [], "752": [], "749": [], "742": [], "738": ["I found same problem in core/request.py:            if (total_processes - 1) > 0:              if session.bind.dialect.name == 'oracle':                  bindparams = [bindparam('process_number', process), bindparam('total_processes', total_processes - 1)]                  query = query.filter(text('ORA_HASH(rule_id, :total_processes) = :process_number', bindparams=bindparams))              elif session.bind.dialect.name == 'mysql':                  query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_processes - 1, process)))              elif session.bind.dialect.name == 'postgresql':                  query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id::text))::bit(32)::int), %s) = %s' % (total_processes - 1, process)))            if (total_threads - 1) > 0:              if session.bind.dialect.name == 'oracle':                  bindparams = [bindparam('thread_number', thread), bindparam('total_threads', total_threads - 1)]                  query = query.filter(text('ORA_HASH(rule_id, :total_threads) = :thread_number', bindparams=bindparams))              elif session.bind.dialect.name == 'mysql':                  query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_threads - 1, thread)))              elif session.bind.dialect.name == 'postgresql':                  query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id::text))::bit(32)::int), %s) = %s' % (total_threads - 1, thread)))  ", "In fact, I found the same in many other places:  [rucio@ruciodev01 core]$ grep md5 *.py|grep filter | grep -v name  account_counter.py:            query = query.filter('mod(md5(concat(account, rse_id)), %s) = %s' % (total_workers + 1, worker_number))  account_counter.py:            query = query.filter('mod(abs((\\'x\\'||md5(concat(account, rse_id)))::bit(32)::int), %s) = %s' % (total_workers + 1, worker_number))  message.py:                subquery = subquery.filter('mod(md5(id), %s) = %s' % (total_threads - 1, thread))  message.py:                subquery = subquery.filter('mod(abs((\\'x\\'||md5(id))::bit(32)::int), %s) = %s' % (total_threads - 1, thread))  quarantined_replica.py:            query = query.filter('mod(md5(path), %s) = %s' % (total_workers - 1, worker_number - 1))  quarantined_replica.py:            query = query.filter('mod(abs((\\'x\\'||md5(path))::bit(32)::int), %s) = %s' % (total_workers - 1, worker_number - 1))  replica.py:                      models.DataIdentifier.md5).with_hint(models.DataIdentifier, \"INDEX(dids DIDS_PK)\", 'oracle').filter(condition)  request.py:                query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_processes - 1, process)))  request.py:                query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_processes - 1, process)))  request.py:                query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_threads - 1, thread)))  request.py:                query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_threads - 1, thread)))  request.py:            sub_requests = sub_requests.filter(text('mod(md5(rule_id), %s) = %s' % (total_processes - 1, process)))  request.py:            sub_requests = sub_requests.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_processes - 1, process)))  request.py:            sub_requests = sub_requests.filter(text('mod(md5(rule_id), %s) = %s' % (total_threads - 1, thread)))  request.py:            sub_requests = sub_requests.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_threads - 1, thread)))  rse_counter.py:            query = query.filter('mod(md5(rse_id), %s) = %s' % (total_workers + 1, worker_number))  rse_counter.py:            query = query.filter('mod(abs((\\'x\\'||md5(rse_id))::bit(32)::int), %s) = %s' % (total_workers + 1, worker_number))  rule.py:                    md5, bytes, adler32 = session.query(models.RSEFileAssociation.md5, models.RSEFileAssociation.bytes, models.RSEFileAssociation.adler32).filter(models.RSEFileAssociation.scope == lock.scope,  temporary_did.py:            query = query.filter(text('mod(abs((\\'x\\'||md5(path))::bit(32)::int), %s) = %s' % (total_workers - 1, worker_number - 1)))  transfer.py:                query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_processes - 1, process)))  transfer.py:                query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_processes - 1, process)))  transfer.py:                query = query.filter(text('mod(md5(rule_id), %s) = %s' % (total_threads - 1, thread)))  transfer.py:                query = query.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_threads - 1, thread)))  transfer.py:            sub_requests = sub_requests.filter(text('mod(md5(rule_id), %s) = %s' % (total_processes - 1, process)))  transfer.py:            sub_requests = sub_requests.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_processes - 1, process)))  transfer.py:            sub_requests = sub_requests.filter(text('mod(md5(rule_id), %s) = %s' % (total_threads - 1, thread)))  transfer.py:            sub_requests = sub_requests.filter(text('mod(abs((\\'x\\'||md5(rule_id))::bit(32)::int), %s) = %s' % (total_threads - 1, thread)))  ", "Hi,    we are usually testing against PostgreSQL 9.6+, this looks like a backward incompatibility.    Can you try to apply your fix and run the test-suite to see if it fixes your problem? The automatic checker in Travis will do the same for 9.6 so we can immediately see if there's a regression.", "I applied the change to the request.py and transfer.py and tested it, and it works fine with Postgres 9.5", "@imandr can you make a pull request please? thanks!    $ tools/create-patch-branch 738 \"database: postgres incompatibility\"  $ emacs  $ git add  $ git commit  $ tools/submit-pull-request", "Cf. https://github.com/rucio/rucio/blob/master/CONTRIBUTING.rst", "when I do tools/submit-pull-request, I get this:    [rucio@ruciodev01 rucio_cern]$ ./tools/submit-pull-request  Traceback (most recent call last):    File \"./tools/submit-pull-request\", line 18, in <module>      requests.packages.urllib3.disable_warnings()  AttributeError: 'module' object has no attribute 'urllib3'  ", "Maybe some incompatibilities in the environment.     You can try from a virtualenv with    ```  python tools/install_venv.py   source .venv/bin/activate  ```  ", "[rucio@ruciodev01 rucio_cern]$ python tools/install_venv.py   which: no virtualenv in (/usr/lib64/qt-3.3/bin:/opt/puppetlabs/bin:/usr/kerberos/sbin:/usr/kerberos/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/puppetlabs/bin:/home/rucio/.local/bin:/home/rucio/bin)  Failed to install virtualenv with curl.  Creating venv via curl...  [rucio@ruciodev01 rucio_cern]$ source .venv/bin/activate  -bash: .venv/bin/activate: No such file or directory  ", "Your hostmachine needs to have virtualenv installed (either via pip, apt-get or yum), that should solve it.", "virtualenv is not in your environment. `pip install virtualenv` or get it from https://raw.github.com/pypa/virtualenv/master/virtualenv.py  ", "(.venv) [rucio@ruciodev01 rucio_cern]$ ./tools/submit-pull-request  Loading github OAUTH token ... ERROR  No github token file found at /home/rucio/rucio_cern/.githubtoken  ", "(.venv) [rucio@ruciodev01 rucio_cern]$ ls -la  total 172  drwxrwxr-x. 10 rucio rucio  4096 Feb 23 15:32 .  drwxr-xr-x. 14 rucio rucio  4096 Feb 23 13:09 ..  -rw-rw-r--.  1 rucio rucio  2175 Feb 23 10:45 AUTHORS.rst  drwxrwxr-x.  3 rucio rucio  4096 Feb 23 10:45 bin  -rw-rw-r--.  1 rucio rucio   273 Feb  1 13:16 ChangeLog  -rw-rw-r--.  1 rucio rucio  5547 Feb 23 10:45 CONTRIBUTING.rst  drwxrwxr-x.  5 rucio rucio  4096 Feb  1 13:16 doc  drwxrwxr-x.  9 rucio rucio  4096 Feb 23 10:45 etc  drwxrwxr-x.  2 rucio rucio  4096 Feb  1 13:16 externals  -rw-rw-r--.  1 rucio rucio    41 Feb  1 13:16 .flake8  drwxrwxr-x.  8 rucio rucio  4096 Feb 23 10:54 .git  -rw-rw-r--.  1 rucio rucio  1037 Feb  1 13:16 .gitignore  -rw-rw-r--.  1 rucio rucio    49 Feb 23 10:45 ISSUE_TEMPLATE.md  drwxrwxr-x.  3 rucio rucio  4096 Feb  1 13:16 lib  -rw-rw-r--.  1 rucio rucio 11357 Feb  1 13:16 LICENSE  -rw-rw-r--.  1 rucio rucio   737 Feb  1 13:16 MANIFEST.in.client  -rw-rw-r--.  1 rucio rucio   919 Feb  1 13:16 MANIFEST.in.rucio  -rw-rw-r--.  1 rucio rucio   432 Feb  1 13:16 MANIFEST.in.webui  -rw-rw-r--.  1 rucio rucio    28 Feb  1 13:16 .pep8  -rw-rw-r--.  1 rucio rucio   500 Feb  1 13:16 PULL_REQUEST_TEMPLATE.md  -rw-rw-r--.  1 rucio rucio  7508 Feb  1 13:16 pylintrc  -rw-rw-r--.  1 rucio rucio  1250 Feb  1 13:16 README.client.rst  -rw-rw-r--.  1 rucio rucio  1544 Feb 23 10:45 README.rst  -rw-rw-r--.  1 rucio rucio  1235 Feb  1 13:16 README.rucio.rst  -rw-rw-r--.  1 rucio rucio  1245 Feb  1 13:16 README.webui.rst  -rw-rw-r--.  1 rucio rucio  5907 Feb 23 10:45 requirements.readthedocs.txt  lrwxrwxrwx.  1 rucio rucio     9 Feb 23 10:45 rucio -> lib/rucio  -rw-rw-r--.  1 rucio rucio  1014 Feb  1 13:16 setup.cfg  -rw-rw-r--.  1 rucio rucio  7841 Feb 23 10:45 setup.py  -rw-rw-r--.  1 rucio rucio  5697 Feb  1 13:16 setup_rucio_client.py  -rw-rw-r--.  1 rucio rucio  5845 Feb  1 13:16 setup_rucio.py  -rw-rw-r--.  1 rucio rucio  3011 Feb  1 13:16 setup_webui.py  drwxrwxr-x.  6 rucio rucio  4096 Feb 23 10:45 tools  -rw-rw-r--.  1 rucio rucio  2228 Feb  1 13:16 tox.ini  -rw-rw-r--.  1 rucio rucio  1086 Feb 23 10:45 .travis.yml  drwxrwxr-x.  7 rucio rucio  4096 Feb 23 15:33 .venv  ", "If you go here you can create a github access token:    https://github.com/settings/tokens    Generate a new token (just put all permissions for simplicity's sake) and write the token to a new file .githubtoken    That should solve it.    @bari12 we need to add this to the development guidelines ;-)", "I created the token:    (.venv) [rucio@ruciodev01 rucio_cern]$ ls -lad .git*  drwxrwxr-x. 8 rucio rucio 4096 Feb 23 16:43 .git  -rw-------. 1 rucio rucio   41 Feb 23 16:34 .githubtoken  -rw-rw-r--. 1 rucio rucio 1037 Feb  1 13:16 .gitignore    and now I am getting this error:    (.venv) [rucio@ruciodev01 rucio_cern]$ ./tools/submit-pull-request  Loading github OAUTH token ... OK  Checking if current branch is a patch/feature/hotfix branch ... OK  Username for 'https://github.com': imandr  Password for 'https://imandr@github.com':   Pushing the feature/patch/hotfix branch to origin ... ERROR  remote: Permission to rucio/rucio.git denied to imandr.  fatal: unable to access 'https://github.com/rucio/rucio.git/': The requested URL returned error: 403  ", "Did you fork the repository and clone your forked repository (imandr/rucio)?    This looks like you tried to clone the upstream repository (rucio/rucio). Submission to the upstream repository can only be done through merge requests from private forked repositories.", "No I did not initially, but now I did.    I went to GitHub and forked yours into mine.  Then I cloned it:   ```  git clone https://github.com/imandr/rucio.git my_forked  ```  Then I installed virualenv:   ```  cd my_forked  python tools/install_venv.py   ```  Then I activated it:  ```  source .venv/bin/activate  ```  Now I am trying to create the patch branch and I am getting new error:  ```  (.venv) [rucio@ruciodev01 my_forked]$ tools/create-patch-branch 738 \"database: postgres incompatibility\"  Switching to master  Already on 'master'  Updating master  Fetching origin  Already up-to-date.  Rebasing master  fatal: ambiguous argument 'upstream/master': unknown revision or path not in the working tree.  Use '--' to separate paths from revisions, like this:  'git <command> [<revision>...] -- [<file>...]'  Can't rebase to master. Unstaged changes?  ```", "Ok, never mind, I added \"upstream\" remote with same URL as \"origin\", which is the URL of my fork, and that error is gone now."], "731": [], "730": [], "725": [], "722": ["Ok it seems in certain versions of postgres only a date, instead of a datatime is returned when querying the db. With a date object this check becomes quite useless. I will disable it in case a date is returned."], "721": ["Wont't be done cause the grafana information does not provide what we want."], "718": [], "717": ["Files to re-add  :    ```(.venv) garvin:rucio garvin$ cat doc/source/api/upload.rst   --------------  Upload Methods  --------------    .. _upload:  .. automodule:: rucio.client.uploadclient      :members:      :undoc-members:      :show-inheritance:  (.venv) garvin:rucio garvin$ cat doc/source/api/download.rst   ----------------  Download Methods  ----------------    .. _download:  .. automodule:: rucio.client.downloadclient      :members:      :undoc-members:      :show-inheritance:  ````  just adding them in the repo generates errors/warning when doing `python setup.py build_sphinx`"], "716": ["JJ \"volunteered\", can you give him more details pls.", "Should this be a rucio-admin command? I was thinking about something like   ```  rucio-admin rse export RSE_NAME -o/--output rse.json (default to rse_name.json)  rucio-admin rse import rse_name.json  ```  Which attributes should be mandatory, in order to make the method fail proof?  For distances, maybe an update method that support json files will do the trick.  `rucio-admin update-distances --from-json distances.json`  May also work for protocols.  ", "The original idea of this came from Ale and what he had in mind was really a full importer. Thus essentially everything (rses, attributes, protocols, distances) can be represented as one big json and this can be imported by rucio to update/create/delete etc. I think we will need some further face2face discussion on this.", "I would propose to add the endpoints /import and /export endpoints to the API where the admin can upload the full json file. Instead of different rucio-admin commands, there would been one command  like `rucio-admin import data.json`.", "Other option, maybe as `/rse/configuration` with associated GET, POST, and DELETE?", "I think a more general endpoint like `/configuration` is better if we want to add more export functionality in the future. For example with users or scopes if this would be useful later.", "I think a separate endpoint might be better. It gives us more flexibility to widen the scope of the importer/exporter later on. ", "What endpoint name should we use in this case? Maybe `configuration` is a bit confusing as there already is an endpoint `config`.    Also I would like to reopen the PR [https://github.com/rucio/rucio/pull/1259](https://github.com/rucio/rucio/pull/1259) to use the export_distances() function."], "707": [], "705": ["Hi,  I am working on this bug and have setup an instance in docker. I can identify that the key_type is actually an enum in the database and 'F' (which is being passed to the database) is not among valid values. Valid values are ('ALL','CONTAINER','DERIVED','COLLECTION','DATASET','FILE').  I'm not sure how the tuple ('F', 'FILE') as in constants.py is being turned into string 'F' when evaluated.  ```  from rucio.db.sqla.constants import DIDType  DIDType.FILE.__dict__  >>> {'description': 'FILE', 'name': 'FILE', 'value': 'F', 'cls_': <class 'rucio.db.sqla.constants.DIDType'>}  ```  I could use some context here.  Thank you.", "This conversion is done by sqlalchemy I think. If you are preparing a patch, I am assigning the ticket to you \ud83d\ude04  Thanks a lot!", "Hi @bari12,  I decided to fire up a debugger and have identified the source of this conversion. It is being handled in DeclEnumType class in lib/rucio/db/sqla/enum.py  For some reason the migrations/create_all functions are creating this SQL statement  ```  CREATE TABLE `did_keys` (    `key` varchar(255) NOT NULL,    `is_enum` tinyint(1) DEFAULT '0',    `key_type` enum('ALL','CONTAINER','DERIVED','COLLECTION','DATASET','FILE') DEFAULT NULL,    `value_type` varchar(255) DEFAULT NULL,    `value_regexp` varchar(255) DEFAULT NULL,    `updated_at` datetime DEFAULT NULL,    `created_at` datetime DEFAULT NULL,    PRIMARY KEY (`key`)  )  ```  There is an inconsistency between the database and the application.  The database expected values: 'ALL','CONTAINER','DERIVED','COLLECTION','DATASET','FILE'  The application expected values: 'F', 'D', 'C', 'A', 'X', 'Y', 'Z'    In fact, manually executing below SQL statement brings the system in consistent state.  ```  alter table did_keys modify column key_type enum('F', 'D', 'C', 'A', 'X', 'Y', 'Z') default null;  ```  and then successfully allows running the following script.  ```  from rucio.core.meta import add_key  from rucio.db.sqla.constants import DIDType    add_key('datatype', DIDType.FILE)  ```    I think that either the SQLAlchemy's create_all function or Alembic's migrations are the issuing incorrect SQL. There is also possibility that DeclEnumType class is handling the conversions incorrectly but the code looks correct to me.  Also, I may be wrong as the codebase is big and I keep on finding surprises in it. Please let me know if that is the case.  Next, I'll try to find out what is issuing the incorrect SQL.  Thank you.", "I  think it's just a specific dialect conversion missing somewhere. We need to have a test for this in `lib/rucio/tests/test_meta.py` to reproduce this.     @shreyanshk Would it be fine for you to do a pull request with a test ? as such we can have a deeper look on this.", "@vingar, Please have a look at the opened pull requests and CI logs.  I added a test to test_meta.py and hopefully it is what was asked.  I was fully expecting tests on MySQL to fail but surprisingly all tests passed.  What is even more surprising is that tests on PostgreSQL failed (!).  I didn't touch application logic. So, I am having hard time following it.  What do you think?", "The test error should be unrelated as we have some instabilities in the test with dbs. I've relaunched the failed tests manually in travis. Thanks for the PR !"], "700": [], "697": [], "696": ["@cserf  Here's my feedback for the demo, all done on Debian:    - https://github.com/rucio/rucio/tree/master/etc/docker/demo needs to have at the beginning a link that one needs to git clone the repo ;-)    - dockerd was not running for my automatically, had to start manually    - compose ended with:    ERROR: for rucio  Cannot start service rucio: driver failed programming external connectivity on endpoint demo_rucio_1 (15c536c53720fcdcf7513b73d3764723223849ba4b56b50fe29c5ee7139234b4): Error starting userland proxy: listen tcp 0.0.0.0:443: bind: address already in use    changing it to 8443 made it work    (which was weird anyway, because i have nothing running on 443)    - other than that it worked", "The part describing the installation is nice ( a really good work guys!).    I've extended and rst-formated the README in #772 plus adding it in the main index of the documentation with a new Tutorial/demo section.    For me, the remaining part is to extend `doc/source/rucio_demo.rst` with more examples from rucio-ping, to how to configure rucio, rse, accounts etc, and then how to use rucio: find files, upload, download, etc.  Merge and integrate some examples from cli_admin_examples.rst and cli_examples.rst   "], "688": [], "684": [], "679": [], "678": [], "671": [], "670": ["This will also fix these issues:  #468   #567   #501   #29"], "669": ["This will also fix this issue  #567   #490   #1222"], "668": ["Actually, this is not done yet but got intermixed with the replica lan/wan issue. Reopening and moving to 1.15.4"], "667": [], "666": [], "665": [], "656": ["High ranking is actually ok as the probe sets it inversely to the agis distance. There are some inaccuracies and minor mistakes there which will be followed up in a different ticket."], "646": [], "641": [], "634": [], "633": ["Do we want this? I already had to turn off the slack github notifications, just too spammy :-)", "I like our #github channel ;-)", "I'm finding the github notifications quiet useful for me and having the report from travis will be also useful", "Ok, maybe @tbeerman can have a look and add it to travis \ud83d\ude04 ", "Sure", "it doesn't look to hard https://docs.travis-ci.com/user/notifications/#Configuring-Slack-notifications  ", "it looks quiet easy actually. just click click on slack.. I'm submitting a PR", "Done!"], "629": ["This will happen with the coming rse manager overhaul"], "626": [], "622": [], "619": [], "612": [], "604": [], "598": ["This is sufficiently covered by #1965   No additional development necessary."], "597": [], "596": [], "587": [], "586": [], "576": ["Fixed by #577 #578 .  Someday I'll get the GitHub linkage correct."], "575": ["Note - this is fixed in `next` but not in `master`.  So, I think this becomes a backport exercise."], "574": ["Currently Rucio unconditionally passes  spacetoken json parameter to FTS job.  No matter what value is passed, this causes fts error on CMS sites which do not support spacetokens.  The desired behavior is: if space_token is not in the 'extended_attributes' , do not add spacetoken to fts json.  ", "Lines that need fixing:    * https://github.com/rucio/rucio/blob/master/lib/rucio/transfertool/fts3.py#L171  * https://github.com/rucio/rucio/blob/master/lib/rucio/daemons/conveyor/common.py#L165", "Hi @bbockelm ,   how do I now define an RSE so that it would not pass the spacetoken  to FTS?   Adding 'srm' protocol still requires a non-empty spacetoken argument :   `# rucio-admin  rse add-protocol --hostname 'stormfe1.pi.infn.it' --scheme 'srm' --prefix '/cms/store/test/rucio'  --web-service-path '/srm/managerv2?SFN='  --port '8444' --impl 'rucio.rse.protocols.gfalv2.Default' --domain-json '{\"wan\": {\"read\": 1, \"write\": 1, \"delete\": 1, \"third_party_copy\": 1}}' --space-token ''  T2_IT_Pisa `  `Error: space-token and web-service-path must be provided for SRM endpoints.`"], "573": ["I think it was implemented by @nikmagini :  ```  d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  888)     # Group file list in separate sublists for different schemes  d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  889)     bad_files_pfns.sort()  d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  890)     bad_files_pfns_grouped = groupby(bad_files_pfns, lambda f: f[:f.find('://')])  d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  891)   d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  892)     for sublist in bad_files_pfns_grouped:  d8b79776 bin/rucio-admin  (Nicolo Magini    2018-02-19 17:39:47 +0100  893)         for chunk in chunks(list(sublist[1]), 500):  ```  Can you confirm ?", "Fixed"], "572": [], "571": [], "570": ["Fully agree with all Brian wrote \ud83d\ude04(This also hooks in a little bit with the way we import external policies. eventually we should have a better mechanism to import the policies, without the need of them being part of the rucio package - but lets have a separate issue for this)    Just one side note, as I looked at the code recently. For the site2site transfers a different method (`construct_surl`) is used to build the PFN for a site2site transferred replica. This has to be changed and the method should also inherit the algorithm from the lfn2pfn function of the respective policy.", "Ok, I have a work-in-progress branch for this -- will be testing this out for LIGO (where this is most urgent).", "Alright, I think this one is good to go!"], "567": ["As discussed today, this should probably be coordinated with @TomasJavurek as he is already working on the download function."], "566": [], "563": [], "560": [], "557": [], "554": [], "551": ["HI, there seems to be a typo  in the example for adding protocols: underscores are used instead of dashes in --web_service_path :    https://github.com/rucio/rucio/blame/3d9fe6112c64b155734cf9e7c8cb7d9bfedf5b69/bin/rucio-admin#L1442  ", "Hi Natalia,    Can you send a PR in for that change?    Thanks!    Brian", "not a big change... Just sent two PRs about it #814 #815    I'm sure @nataliaratnikova will find other inconsistencies and submit new PRs :)", "Hi Brian, Vincent already corrected this one, I believe. I noticed that Rucio client is not very consistent about using _ and -  in the options and names.\u00a0 It's not always clear to me what the intended  conventions are, and this particular code is under rapid development.  May be it is worth going over the whole client code, once it is stable  and the conventions are clarified. Thanks,  \u00a0\u00a0 Natalia.  On 3/5/18 11:34 PM, Brian Bockelman wrote: > > Hi Natalia, > > Can you send a PR in for that change? > > Thanks! > > Brian > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub  > <https://github.com/rucio/rucio/issues/551#issuecomment-370591800>, or  > mute the thread  > <https://github.com/notifications/unsubscribe-auth/AEexxPw2HKjEvolHfmS6va0wr3oi9ZIHks5tbb1agaJpZM4Ryges>. >  ", "Hi,  yes, I did that one as the example value was not appropriate as well.  I would be happy if you report more such inconsistency.    Vincent  ", "The convention is that \"-\" (minus) should be used in CLI arguments instead of \"_\" (underscore). Anything else is a typo/mistake.", "Submitted https://github.com/rucio/rucio/pull/818 with two more fixes,  hopefully all are caught now.  On 3/6/18 11:12 AM, Mario Lassnig wrote: > > The convention is that \"-\" (minus) should be used in CLI arguments  > instead of \"_\" (underscore). Anything else is a typo/mistake. > > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub  > <https://github.com/rucio/rucio/issues/551#issuecomment-370731687>, or  > mute the thread  > <https://github.com/notifications/unsubscribe-auth/AEexxJoPzQ35xrzelAR8TOL-K4nKFloyks5tbmEigaJpZM4Ryges>. >  "], "549": [], "548": [], "543": [], "535": [], "533": ["This was discussed in the [Dev Meeting on 2018-10-11](https://indico.cern.ch/event/760955/)  Two things should be added:  - Mentioning a module in the Rucio CFG which should be loaded. This module can be part of a config package (e.g. from ATLAS or CMS) which is installed via PyPi  - Alternatively also directly mentioning the path of a python file should be possible, which is loaded (More suitable for small experiments who do not want to package their configs)", "I've written some code to load the experiment-specific stuff from an external module: https://github.com/rucio/rucio/pull/2842. It will definitely need more work but I thought it might be a useful starting point for discussion.", "Thanks James, I'm going to have a look.", "I am not sure how best to take this forward. My main concern is how we would switch to the new policy packages without breaking existing installations. We would need packages to be available for all the VOs that need them - I could create the initial packages but I don't think I am the right person to maintain them since I have no connection with most of the VOs. We also need to make sure the right packages get installed so that everything continues to work, including for the tests.  There is also the multi-VO issue. I think we will want each VO to be able to have a different policy package but this would need quite significant changes to the code and I am not sure how best to do this.", "I think for the beginning we need some kid of dual stack solution. We would keep the current policies we have in the core package (cms, atlas, belle,...) and use them and notify these organisations that we will take the policies out of the core package within one or two feature releases. (Thus 4-8 month time) - We would also show them how to produce their own policy packages. (So we need good documentation for that)  During that time they can either provide a policy package in the config, or still use the files included in the repo. I don't think we need to prepare these for everyone, just show them how to create them and have some best practice recommendation.    The Multi-VO I would leave out of the picture for now. The current multi VO development does not support multiple policies, it should eventually, but I would do this step by step: Let's get the single VO packages working first and then also see with Eli, who will continue the Multi VO development in February. ", "That all sounds sensible. I'll work on making my current code into a dual stack version. The pull request for this already contains some documentation about building the policy packages."], "532": ["the openstack package pbr is nice for this and can be used (or at least as a source of inspiration)    cf. http://git.openstack.org/cgit/openstack-dev/pbr/tree/pbr/version.py      ", "Implemented the following:  ```  $ pip install git+git://github.com/vgaronne/rucio.git  $ rucio --version   rucio 1.13.2-52-ge63dfb28-dev1519211244  ```", "The ge63dfb28-dev1519211244 is just based on the commit id? ", "not completely. Have a look on the corresponding pull request.", "Ah I see. Did you solve the issue with the installation of dependencies?", "I didn't have a look on it. On next it is solved"], "531": ["This is a duplicate of #403"], "530": [], "529": ["Closed, this comes implicitly with Reaper 2.0"], "525": [], "524": ["@hahahannes this one might also be good for you. We can discuss the details one of these days."], "523": ["@bari12  I think this issue can be closed. We have #669 #670 and #717 for the next steps."], "522": [], "519": [], "514": ["Not needed anymore"], "513": [], "510": [], "509": [], "508": [], "505": ["I cannot reproduce this bug    ```  from rucio.client.ruleclient import RuleClient  r = RuleClient()  r.update_replication_rule('ee0ab3b8547749ac964aaa5849d69857',{'lifetime': 10000})  True  ```    I wonder if this is due to some version difference. What client/server version are you using?", "1.14.7 I'm re-building the docker to get up to date stuff, then I'll re-test  ", "well, after updating I ended up with the very same version.  But I still see the same error  >>> from rucio.client.ruleclient import RuleClient  >>> r=RuleClient()  >>> r.update_replication_rule('1b04a48135d0402894e64541dbec3f0c',{'lifetime': 10000})  Traceback (most recent call last):    File \"<stdin>\", line 1, in <module>    File \"/usr/lib/python2.7/site-packages/rucio/client/ruleclient.py\", line 122, in update_replication_rule      exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content)    File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 211, in _get_exception      data = parse_response(data)    File \"/usr/lib/python2.7/site-packages/rucio/common/utils.py\", line 213, in parse_response      return json.loads(data, object_hook=datetime_parser)    File \"/usr/lib64/python2.7/json/__init__.py\", line 351, in loads      return cls(encoding=encoding, **kw).decode(s)    File \"/usr/lib64/python2.7/json/decoder.py\", line 366, in decode      obj, end = self.raw_decode(s, idx=_w(s, 0).end())    File \"/usr/lib64/python2.7/json/decoder.py\", line 384, in raw_decode      raise ValueError(\"No JSON object could be decoded\")  ValueError: No JSON object could be decoded  >>>  ", "Ok, I need to do some more digging then \ud83d\ude04 ", "I still cannot reproduce this \ud83d\ude1e , I suspect this is a problem with the json decoder or apache. Can you add a `print options` before https://github.com/rucio/rucio/blob/master/lib/rucio/core/rule.py#L1112 and show the output from the log.  Alternatively you can also give me access to the machine.", "HI, I'm confused. I cannot find the rule.py file. Are you talking of client side or server side?", "Server side, depending on the location of the code it should be in the lib/rucio/core/ subdirectory.", "OK. I've no access to server. Maybe we should ask Brian? One point is not clear to me. This is a client side problem, right (or at least it is in the client the difference that generate the problem)?  Have you tried to reproduce it using Eric docker client environment? Maybe different versions of packages?", "I have only tested with the same version on my dev machine, but not the docker client yet.   I am not sure if it is a client or server problem, both is possible, on the server it could be some combination between httpd and the json decoder of the exact python version which leads to a different unpacking of this dictionary. On the client side it could also be the python version and json encoder which packs the data somehow differently so the server cannot unpack it. But it is just guessing basically.   What I see in the log is that on the server the dictionary gets unpacked in a way that lifetime is a NoneType, (so not None, but NoneType, which is a json thing). I have to try it with the docker images. ", "Hi @sartiran,  This ticket is quite old now but just to make a quick follow up on it: did you experience this bug in any of the newer version (1.16.0+)? ", "I'm quite sure it is fine now. I'll double-check asap and, if it is, I'll close the issue   ", "Hi @sartiran I am closing this now. If the problem shows up again, please open another ticket \ud83d\ude04 "], "502": [], "501": ["most likely due to no protocols registered at the destination RSE"], "496": [], "493": ["Actually this is a core issue. For some metadata, it checks that the metadata was updated :+1:   ```python          if not rowcount:              raise exception.UnsupportedOperation('%(key)s for %(scope)s:%(name)s cannot be updated' % locals())  ```    But for others no.", "I'm afraid that if a proper exception is raised now, It'll break certain third party programs...     The core method can return rowcount to the clients and the client can use this value to give the right status code. Or check the presence of the dataset..", "The core change will be intercepted by the web/rest layer, and eventually the client? It could be properly handled there and make the client not except out into userspace."], "490": [], "487": [], "486": [], "484": [], "483": [], "480": [], "469": [], "468": ["rucio download first tries on the same site with different protocols, then hops to another site and tries the protocols available there. for the upload, we only want the different protocols, we don't want to automatically upload to another site.", "Comment about the c/p suggestion: are you sure that the protocol failover in `rucio download` actually works? From a few tests and looking at the code, I don't think it's doing anything... AFAICT only the site failover works in `rucio download` but the protocol failover will always select the default (though hard to test for sure since I need to find a site where protocol 1 fails and then protocol 2 works...)", "pinging @TWAtGH , the download one should work with both protocol and rse failover. something for monday ;-)", "You mean because the protocol is assigned to the 'read_protocol' attribute and then 'read_protocol' is ignored afterwards? I think you're right but for any reason its working in my tests:    >rucio -v download --ndownloader 1 user.mlassnig:user.mlassnig.pilot.test.multi.hits  2018-01-22 13:30:06,722\tDEBUG\tThread 1/1 : 2 possible protocol(s) for read  2018-01-22 13:30:06,722\tDEBUG\tThread 1/1 : Trying protocol davs at IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:30:06,722\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:31:06,812\tWARNING\tThe requested service is not available at the moment.  Details: An unknown exception occurred.  Details: Could not open source: Connection timed out  2018-01-22 13:31:06,812\tDEBUG\tThread 1/1 : Failed attempt 1/2  2018-01-22 13:31:06,812\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:32:06,899\tWARNING\tThe requested service is not available at the moment.  Details: An unknown exception occurred.  Details: Could not open source: Connection timed out  2018-01-22 13:32:06,899\tDEBUG\tThread 1/1 : Failed attempt 2/2  2018-01-22 13:32:06,900\tDEBUG\tThread 1/1 : sending trace  2018-01-22 13:32:06,928\tDEBUG\tThread 1/1 : Trying protocol gsiftp at IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:32:06,928\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:34:14,225\tWARNING\tThe requested service is not available at the moment.  Details: An unknown exception occurred.  Details: Could not open source: globus_xio: Unable to connect to lapp-se99.in2p3.fr:2811 globus_xio: System error in connect: Connection timed out globus_xio: A system call failed: Connection timed out  2018-01-22 13:34:14,225\tDEBUG\tThread 1/1 : Failed attempt 1/2  2018-01-22 13:34:14,225\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  2018-01-22 13:36:21,457\tWARNING\tThe requested service is not available at the moment.  Details: An unknown exception occurred.  Details: Could not open source: globus_xio: Unable to connect to lapp-se99.in2p3.fr:2811 globus_xio: System error in connect: Connection timed out globus_xio: A system call failed: Connection timed out  2018-01-22 13:36:21,457\tDEBUG\tThread 1/1 : Failed attempt 2/2  2018-01-22 13:36:21,457\tDEBUG\tThread 1/1 : sending trace  2018-01-22 13:36:21,492\tDEBUG\tThread 1/1 : 3 possible protocol(s) for read  2018-01-22 13:36:21,493\tDEBUG\tThread 1/1 : Trying protocol srm at INFN-NAPOLI-ATLAS_DATADISK  2018-01-22 13:36:21,493\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from INFN-NAPOLI-ATLAS_DATADISK  2018-01-22 13:36:25,893\tINFO\tThread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 successfully downloaded from INFN-NAPOLI-ATLAS_DATADISK    Anyway this should be patched maybe.  ", "Hi @TWAtGH     which version of the rucio client are you using? For me, protocol fallback works until rucio client version 1.14.5 - see (1)    However it doesn't work anymore with the latest rucio client 1.14.7 - see (2) where davs is always used.    At first guess it may have been broken in this commit :    https://github.com/rucio/rucio/commit/0464c8b121150ee9c523cdf6bb9105b73c855094    I'll open a separate ticket for this    Cheers  N    (1)      > [nmagini@lxplus072 ruciotest]$ lsetup 'rucio 1.14.5'  >   > Requested:  rucio ...   >  Setting up emi 3.17.1-1_v3.sl6umd4v2 ...   >   Skipping: grid middleware already setup (from UI)  >  Setting up rucio 1.14.5 ...   > Info: Setting compatibility to slc6  > Information for user <<<<<<<<<<<<<<<<<<<<<<<<<  >  emi:  >    Your proxy has 11h:57m:2s remaining  >   > [nmagini@lxplus072 ruciotest]$ rucio --verbose download --rse IN2P3-LAPP-MWTEST_DATADISK mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:41:55,610 DEBUG   RSE-Expression: (IN2P3-LAPP-MWTEST_DATADISK)&istape=False  > 2018-01-22 14:41:55,648 DEBUG   Starting 1 download threads  > 2018-01-22 14:41:55,648 INFO    Thread 1/1 : Starting the download of mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:41:55,648 DEBUG   Waiting for threads to finish  > 2018-01-22 14:41:55,648 DEBUG   Thread 1/1 : Potential sources : [u'IN2P3-LAPP-MWTEST_DATADISK']  > 2018-01-22 14:41:55,725 DEBUG   Thread 1/1 : 2 possible protocol(s) for read  > 2018-01-22 14:41:55,725 DEBUG   Thread 1/1 : Trying protocol davs at IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:55,725 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:56,082 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:41:56,082 DEBUG   Thread 1/1 : Failed attempt 1/2  > 2018-01-22 14:41:56,082 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:56,261 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:41:56,261 DEBUG   Thread 1/1 : Failed attempt 2/2  > 2018-01-22 14:41:56,261 DEBUG   Thread 1/1 : sending trace  > 2018-01-22 14:41:56,296 DEBUG   Thread 1/1 : Trying protocol gsiftp at IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:56,296 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:57,135 WARNING Source file not found.  > Details: Source file not found.  > Details: globus_ftp_client: the server responded with an error 500 500-Command failed. : System error in name: No such file or directory  500-A system call failed: No such file or directory  500-  500 End.  > 2018-01-22 14:41:57,136 DEBUG   Thread 1/1 : Failed attempt 1/2  > 2018-01-22 14:41:57,136 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:41:58,079 WARNING Source file not found.  > Details: Source file not found.  > Details: globus_ftp_client: the server responded with an error 500 500-Command failed. : System error in name: No such file or directory  500-A system call failed: No such file or directory  500-  500 End.  > 2018-01-22 14:41:58,079 DEBUG   Thread 1/1 : Failed attempt 2/2  > 2018-01-22 14:41:58,079 DEBUG   Thread 1/1 : sending trace  > 2018-01-22 14:41:58,100 ERROR   Thread 1/1 : Cannot download file mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:41:58,100 DEBUG   All threads finished  > ----------------------------------  > Download summary  > ----------------------------------------  > DID mc15_14TeV:HITS.10075481._000455.pool.root.1  > Total files :                                 1  > Downloaded files :                            0  > Files already found locally :                 0  > Files that cannot be downloaded :             1  > Completed in 2.5720 sec.  >         (2)    > [nmagini@lxplus101 ~]$ lsetup 'rucio 1.14.7'  > ************************************************************************  > Requested:  rucio ...   >  Setting up emi 3.17.1-1_v3.sl6umd4v2 ...   >  Setting up rucio 1.14.7 ...   > Info: Setting compatibility to slc6  > Info: Set RUCIO_AUTH_TYPE to x509_proxy  > Information for user <<<<<<<<<<<<<<<<<<<<<<<<<  >  emi:  >    Your proxy has 11h:59m:24s remaining  > ************************************************************************  > [nmagini@lxplus101 ~]$ cd work/  > [nmagini@lxplus101 work]$ cd ruciotest/  > [nmagini@lxplus101 ruciotest]$ ls -ltra  > total 8  > drwxr-xr-x. 13 nmagini zp 2048 Jan 22 14:10 ..  > drwxr-xr-x.  9 nmagini zp 2048 Jan 22 14:10 rucio  > drwxr-xr-x.  4 nmagini zp 2048 Jan 22 14:11 .  > drwxr-xr-x.  2 nmagini zp 2048 Jan 22 14:26 mc15_14TeV  > [nmagini@lxplus101 ruciotest]$ rucio --verbose download --rse IN2P3-LAPP-MWTEST_DATADISK mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:30:53,925 DEBUG   RSE-Expression: (IN2P3-LAPP-MWTEST_DATADISK)&istape=False  > 2018-01-22 14:30:53,988 DEBUG   Starting 1 download threads  > 2018-01-22 14:30:53,988 INFO    Thread 1/1 : Starting the download of mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:30:53,989 DEBUG   Thread 1/1 : Potential sources : [u'IN2P3-LAPP-MWTEST_DATADISK']  > 2018-01-22 14:30:53,988 DEBUG   Waiting for threads to finish  > 2018-01-22 14:30:54,044 DEBUG   Thread 1/1 : 2 possible protocol(s) for read  > 2018-01-22 14:30:54,044 DEBUG   Thread 1/1 : Trying protocol davs at IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:54,044 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:54,695 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:30:54,696 DEBUG   Thread 1/1 : Failed attempt 1/2  > 2018-01-22 14:30:54,696 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:54,862 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:30:54,862 DEBUG   Thread 1/1 : Failed attempt 2/2  > 2018-01-22 14:30:54,862 DEBUG   Thread 1/1 : sending trace  > 2018-01-22 14:30:54,888 DEBUG   Thread 1/1 : Trying protocol gsiftp at IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:54,888 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:55,097 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:30:55,097 DEBUG   Thread 1/1 : Failed attempt 1/2  > 2018-01-22 14:30:55,098 INFO    Thread 1/1 : File mc15_14TeV:HITS.10075481._000455.pool.root.1 trying from IN2P3-LAPP-MWTEST_DATADISK  > 2018-01-22 14:30:55,239 WARNING Source file not found.  > Details: Source file not found.  > Details: Failure HTTP 404 : File not found , while  readding after 1 attempts  > 2018-01-22 14:30:55,239 DEBUG   Thread 1/1 : Failed attempt 2/2  > 2018-01-22 14:30:55,239 DEBUG   Thread 1/1 : sending trace  > 2018-01-22 14:30:55,279 ERROR   Thread 1/1 : Cannot download file mc15_14TeV:HITS.10075481._000455.pool.root.1  > 2018-01-22 14:30:55,280 DEBUG   All threads finished  > ----------------------------------  > Download summary  > ----------------------------------------  > DID mc15_14TeV:HITS.10075481._000455.pool.root.1  > Total files :                                 1  > Downloaded files :                            0  > Files already found locally :                 0  > Files that cannot be downloaded :             1  > Completed in 1.4746 sec.    ", "Hi,  I was using the current cvmfs version (1.13.2). With rucio testing (1.14.7) it doesnt work anymore. Seems like it is always retrying the first protocol (for me it was srm).  I'll do some more tests.  Cheers  Tobi", "Hi @TWAtGH     I opened a separate issue for the download https://github.com/rucio/rucio/issues/502 and provided a fix https://github.com/rucio/rucio/pull/503 - if it looks fine, I'll apply the same logic to upload.    Cheers  N", "Hey, how is the progress here? Because I already started refactoring the upload since it will be moved to the library.", "I got the failover in the upload working in 7883be7 but I need to fix the exit statuses in case of error; I'll have time to do it mid next week. If you refactor the upload I can restart from your new version, no big loss.", "I also started replacing `return FAILURE` statements with exceptions.   My plan is now to finish the upload part of #523 so that Tomas can push it in his repo.  I will also put in the failover during this and a couple of bugfixes and cleanups. "], "467": ["This is the statement that should be working (but doesn't):    self.__ctx.set_opt_string(\"XROOTD PLUGIN\", \"XRD.WANTPROT\", \"gsi\")    Might be necessary to follow this up with the gfal people.", "Hi Mario,    yes, it seems to be an issue with the gfal library. If I put xrd.wantprot explicitly in the root URL, I can force gfal to use GSI auth on xrootd(1) However, if I set XRD.WANTPROT as an env option, gfal falls back to krb5 auth (2) I'll follow up with gfal devs.    Cheers  Nicolo'    (1)  `  import gfal2  ctx=gfal2.creat_context()  ctx.filecopy(\"root://eosatlas.cern.ch//eos/atlas/atlasdatadisk/rucio/mc15_valid/ff/f4/EVNT.12362219._000320.pool.root.1?&xrd.wantprot=gsi\", \"file:///tmp/niccotest66\")  180112 18:11:10 7805 cryptossl_X509CreateProxy: Your identity: /C=IT/O=INFN/OU=Personal Certificate/L=Genova/CN=Nicolo Magini  Enter PEM pass phrase:  `    (2)  `  import gfal2  ctx=gfal2.creat_context()  ctx.set_opt_string(\"XROOTD PLUGIN\", \"XRD.WANTPROT\", \"gsi\")  0  ctx.get_opt_string(\"XROOTD PLUGIN\", \"XRD.WANTPROT\")  'gsi'  ctx.filecopy(\"root://eosatlas.cern.ch//eos/atlas/atlasdatadisk/rucio/mc15_valid/ff/f4/EVNT.12362219._000320.pool.root.1\", \"file:///tmp/niccotest66\")  0  `", "Great, thanks!", "Hi all,    summary of the investigation:    1) Indeed there's a bug in the current gfal2 version 2.14.2 with the 'XRD.WANTPROT' option which is not passed in the root TURL. The gfal2 team provided a fix which should be released in the next gfal2 version 2.15.0    2) However, even with this fix, adding xrd.wantprot to the root TURL doesn't seem to work on EOS - transfers fail with \"Network is unreachable\". Actually it's not gfal2 specific; I get this error even with the xrd client.    I'll continue to follow up with gfal and eos devs    Cheers  N.    `[nmagini@lxplus102 ~]$ xrdcopy \"root://eosatlas.cern.ch//eos/atlas/atlasdatadisk/rucio/mc15_valid/ff/f4/EVNT.12362219._000320.pool.root.1?xrd.wantprot=gsi\" /tmp/niccotest68`  `[0B/0B][100%][==================================================][0B/s]`  `Run: [ERROR] Server responded with an error: [3014] Unable to open file  /eos/atlas/atlasdatadisk/rucio/mc15_valid/ff/f4/EVNT.12362219._000320.pool.root.1; Network is unreachable`  ", "Note about the change: this is needed to make sure that 'xrd.wantprot' works on EOS. However it's not sufficient to disable the fallback to krb5 - for this we also need to wait for the release of gfal 2.15.0 as mentioned in the previous comment", "More details about the reason for the change, from gfal devs:    - On EOS we need to pass both gsi (for auth to the headnode) and unix (for auth between headnode and FSTs)  - On other xrootd storages gsi is sufficient; adding unix shouldn't do harm since as second option it won't be selected", "Thanks for the thorough investigation!"], "464": [], "460": ["Here's the protocol definition for the available destination RSE:  ```  Protocols:  ==========    gsiftp      extended_attributes: None      hostname: gftp.t2.ucsd.edu      prefix: /hadoop/oppo      domains: {u'wan': {u'read': 1, u'write': 1, u'third_party_copy': 0, u'delete': 1}, u'lan': {u'read': 0, u'write': 0, u'delete': 0}}      scheme: gsiftp      port: 0      impl: rucio.rse.protocols.gfalv2.Default  ```  and the source RSE:  ```  Protocols:  ==========    gsiftp      extended_attributes: None      hostname: red-gridftp.unl.edu      prefix: /dropfiles/cms/store      domains: {u'wan': {u'read': 1, u'write': 1, u'third_party_copy': 0, u'delete': 1}, u'lan': {u'read': 0, u'write': 0, u'delete': 0}}      scheme: gsiftp      port: 0      impl: rucio.rse.protocols.gfalv2.Default  ```", "This is the suspect line of code:    https://github.com/rucio/rucio/blob/master/lib/rucio/core/transfer.py#L566    When I remove the `domain=None` and just let it use the default keyword (`domain='wan'`), everything works.    However, that definitely wouldn't explain why transfers work for other folks though!", "This looks like a regression from the latest WAN/LAN domain support patch! We actually have a domain called \"third_party_copy\" which we would like to use in the future (because it might be different than WAN data access priority), but it's not used yet."], "457": [], "453": [], "450": [], "447": [], "444": [], "439": [], "436": [], "433": [], "430": [], "429": ["This is a duplicate of #367 ", "Actually that's a different problem than #367 . Reopening"], "426": [], "425": [], "422": ["Oops, I do the same on another server, the SSL error still exist...    > /opt/dicos-rucio/lib/rucio/client/baseclient.py(336)__get_token_x509()  -> result = self.session.get(url, headers=headers, cert=cert,  (Pdb)  > /opt/dicos-rucio/lib/rucio/client/baseclient.py(337)__get_token_x509()  -> verify=self.ca_cert)  (Pdb) p self.ca_cert  '/etc/grid-security/certificates/ASGCCA-2007.pem'  (Pdb) p url  'https://rucio-test03.twgrid.org:443/auth/x509_proxy'  (Pdb) self.session.get(url, headers=headers, cert=cert,verify=self.ca_cert)  *** SSLError: HTTPSConnectionPool(host='rucio-test03.twgrid.org', port=443): Max retries exceeded with url: /auth/x509_proxy (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert unknown ca')],)\",),))  (Pdb) p cert  '/opt/dicos-rucio/proxy'  (Pdb)  ", "Looks like the remote host isn't verifying the proxy?  You probably want to increase the debugging on the Apache-side and look for errors in the `error_log` -- there should be a lot of gridsite-based chatter.", "Dear All,      I change another way to test. There are two servers, one is old version (rucio-centos7) and another (rucio-test03) is new. I do curl on rucio-test03. I can success to authenticate rucio-centos7 but can not success on  rucio-test03 with same CA, proxy...  I use curl to test proxy: (.venv) [root@rucio-test03 rucio]# curl -i   --cert /tmp/x509up_u500_twgridpil --cacert /etc/grid-security/certificates/ASGCCA-2007.pem --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-test03.twgrid.org:4431/auth/x509 curl: (35) Peer does not recognize and trust the CA that issued your certificate. (.venv) [root@rucio-test03 rucio]# curl -i   --cert /tmp/x509up_u500_twgridpil --cacert /etc/grid-security/certificates/ASGCCA-2007.pem --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-centos7.twgrid.org:4431/auth/x509 HTTP/1.1 200 OK Date: Mon, 08 Jan 2018 05:52:15 GMT Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.1e-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.2 Access-Control-Allow-Origin: None Access-Control-Allow-Headers: None Access-Control-Allow-Methods: * Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: X-Rucio-Auth-Token Cache-Control: no-cache, no-store, max-age=0, must-revalidate Cache-Control: post-check=0, pre-check=0 Pragma: no-cache X-Rucio-Auth-Token: chwu-/C=TW/O=AS/OU=GRID/CN=Chi Hsun Wu 266135-unknown-5af549cd21834ae7bd03251752672e14 Content-Length: 0 Content-Type: application/octet-stream  Then I use certificate and key, both server is success:  (.venv) [root@rucio-test03 rucio]# curl -i --key /opt/rucio/.globus/userkey-nopass.pem  --cert /opt/rucio/.globus/usercert.pem --cacert /etc/grid-security/certificates/ASGCCA-2007.pem --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-centos7.twgrid.org:4431/auth/x509 HTTP/1.1 200 OK Date: Mon, 08 Jan 2018 05:57:07 GMT Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.1e-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.2 Set-Cookie: GRIDHTTP_PASSCODE=a21a838e9640b7d7TcKW5u; domain= rucio-centos7.twgrid.org; path=/; secure Access-Control-Allow-Origin: None Access-Control-Allow-Headers: None Access-Control-Allow-Methods: * Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: X-Rucio-Auth-Token Cache-Control: no-cache, no-store, max-age=0, must-revalidate Cache-Control: post-check=0, pre-check=0 Pragma: no-cache X-Rucio-Auth-Token: chwu-/C=TW/O=AS/OU=GRID/CN=Chi Hsun Wu 266135-unknown-74c51484731a4048a7e4466033282762 Content-Length: 0 Content-Type: application/octet-stream  (.venv) [root@rucio-test03 rucio]# curl -i --key /opt/rucio/.globus/userkey-nopass.pem  --cert /opt/rucio/.globus/usercert.pem --cacert /etc/grid-security/certificates/ASGCCA-2007.pem --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-test03.twgrid.org:4431/auth/x509 HTTP/1.1 200 OK Date: Mon, 08 Jan 2018 05:57:18 GMT Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.4 Set-Cookie: GRIDHTTP_PASSCODE=89b2a5f0940e1e75ofY3Ry; domain= rucio-test03.twgrid.org; path=/; secure Access-Control-Allow-Origin: None Access-Control-Allow-Headers: None Access-Control-Allow-Methods: * Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: X-Rucio-Auth-Token Cache-Control: no-cache, no-store, max-age=0, must-revalidate Cache-Control: post-check=0, pre-check=0 Pragma: no-cache X-Rucio-Auth-Token: chwu-/C=TW/O=AS/OU=GRID/CN=Chi Hsun Wu 266135-unknown-964d43b28cd44ebcb6a172c626feef0a Content-Length: 0 Content-Type: application/octet-stream  Recall that the error if I use python requests:  > /opt/dicos-rucio/lib/rucio/client/baseclient.py(337)__get_token_x509() -> verify=self.ca_cert) (Pdb) p url 'https://rucio-test03.twgrid.org:443/auth/x509_proxy <https://rucio-test03.twgrid.org/auth/x509_proxy>' (Pdb) p headers {'X-Rucio-Account': 'chwu'} (Pdb) p cert '/opt/rucio/proxy' (Pdb) p self.ca_cert '/etc/grid-security/certificates/ASGCCA-2007.pem' (Pdb) self.session.get(url, headers=headers, cert=cert,verify=self.ca_cert) *** SSLError: HTTPSConnectionPool(host='rucio-test03.twgrid.org', port=443): Max retries exceeded with url: /auth/x509_proxy (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert unknown ca')],)\",),))   I then check CA by echo | openssl s_client -showcerts -connect www.your.host.example:443 I use diff to found that two server only different on certificate or hostname. The CA part is the same.  In ssl log of rucio-centos7 (the server where proxy authentication is good):  [Mon Jan 08 06:16:44.790436 2018] [ssl:info] [pid 5069] [client 202.169.169.45:42712] AH01964: Connection to child 5 established (server rucio-centos7.twgrid.org:4431) [Mon Jan 08 06:16:44.791442 2018] [ssl:debug] [pid 5069] ssl_engine_kernel.c(1879): [client 202.169.169.45:42712] AH02043: SSL virtual host for servername rucio-centos7.twgrid.org found [Mon Jan 08 06:16:44.808863 2018] [:debug] [pid 5069] canl_mod_gridsite.c(2434): set GRST_save_ssl_creds [Mon Jan 08 06:16:44.810297 2018] [ssl:debug] [pid 5069] ssl_engine_kernel.c(1812): [client 202.169.169.45:42712] AH02041: Protocol: TLSv1.2, Cipher: ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits) [Mon Jan 08 06:16:44.811671 2018] [ssl:debug] [pid 5069] ssl_engine_kernel.c(224): [client 202.169.169.45:42712] AH02034: Initial (No.1) HTTPS request received for child 5 (server rucio-centos7.twgrid.org:4431) [Mon Jan 08 06:16:44.812294 2018] [authz_core:debug] [pid 5069] mod_authz_core.c(809): [client 202.169.169.45:42712] AH01626: authorization result of Require all granted: granted [Mon Jan 08 06:16:44.812318 2018] [authz_core:debug] [pid 5069] mod_authz_core.c(809): [client 202.169.169.45:42712] AH01626: authorization result of <RequireAny>: granted [Mon Jan 08 06:16:44.815207 2018] [:debug] [pid 5069] canl_mod_gridsite.c(3216): Examine ACL file /opt/rucio/etc/gacl (from ACL path /opt/rucio/etc/gacl) [Mon Jan 08 06:16:44.815928 2018] [:debug] [pid 5069] canl_mod_gridsite.c(3253): After GACL/Onetime evaluation, GRST_PERM=5 [Mon Jan 08 06:16:44.816361 2018] [authz_core:debug] [pid 5069] mod_authz_core.c(809): [client 202.169.169.45:42712] AH01626: authorization result of Require all granted: granted [Mon Jan 08 06:16:44.816381 2018] [authz_core:debug] [pid 5069] mod_authz_core.c(809): [client 202.169.169.45:42712] AH01626: authorization result of <RequireAny>: granted [Mon Jan 08 06:16:44.816701 2018] [:debug] [pid 5069] canl_mod_gridsite.c(3253): After GACL/Onetime evaluation, GRST_PERM=0 [Mon Jan 08 06:16:44.839398 2018] [:info] [pid 5069] [client 202.169.169.45:42712] mod_wsgi (pid=5069, process='', application='rucio-centos7.twgrid.org:4431|/auth'): Loading WSGI script '/opt/rucio/lib/rucio/web/rest/authentication.py'. [Mon Jan 08 06:16:45.865664 2018] [ssl:debug] [pid 5069] ssl_engine_io.c(992): [client 202.169.169.45:42712] AH02001: Connection closed to child 5 with standard shutdown (server rucio-centos7.twgrid.org:4431)  But in rucio-test03 where proxy authentication is failed:  [Mon Jan 08 06:18:56.323391 2018] [ssl:info] [pid 19315] [client 202.169.169.45:34350] AH01964: Connection to child 1 established (server rucio-test03.twgrid.org:4431) [Mon Jan 08 06:18:56.324621 2018] [ssl:debug] [pid 19315] ssl_engine_kernel.c(1890): [client 202.169.169.45:34350] AH02043: SSL virtual host for servername rucio-test03.twgrid.org found [Mon Jan 08 06:18:56.338921 2018] [ssl:info] [pid 19315] [client 202.169.169.45:34350] AH02008: SSL library error 1 in handshake (server rucio-test03.twgrid.org:4431) [Mon Jan 08 06:18:56.339189 2018] [ssl:info] [pid 19315] SSL Library Error: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed [Mon Jan 08 06:18:56.339209 2018] [ssl:info] [pid 19315] [client 202.169.169.45:34350] AH01998: Connection closed to child 1 with abortive shutdown (server rucio-test03.twgrid.org:4431)  I am wondering about if the version of Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.4 cause problems. Could I know what is the version on your server ? Thanks.    Here is the httpd configure for rucio-test03: LoadModule ssl_module /usr/lib64/httpd/modules/mod_ssl.so  LoadModule wsgi_module /usr/lib64/httpd/modules/mod_wsgi.so LoadModule gridsite_module /usr/lib64/httpd/modules/mod_gridsite.so  Listen 4431  WSGIPythonHome /opt/rucio/.venv WSGIPythonPath /opt/rucio/.venv/lib/python2.7/site-packages  <VirtualHost rucio-test03.twgrid.org:4431>   ServerName rucio-test03.twgrid.org:4431  ServerAdmin ph-adp-ddm-lab@cern.ch   SSLVerifyClient optional  SSLVerifyDepth 10  SSLOptions +StdEnvVars  SSLEngine on  SSLCertificateFile /etc/grid-security/hostcert.pem  SSLCertificateKeyFile /etc/grid-security/hostkey.pem  SSLCACertificatePath /etc/grid-security/certificates   LogLevel debug   ErrorLog /var/log/rucio/httpd_error_log  TransferLog /var/log/rucio/httpd_access_log   Include /opt/rucio/etc/web/aliases-py27.conf   <IfModule mod_ssl.c>     ErrorLog /var/log/apache2/ssl_engine.log     LogLevel debug  </IfModule>       <Directory /opt/rucio>      Require all granted   </Directory>    <LocationMatch /auth/x509_proxy>   GridSiteIndexes on   GridSiteAuth on   GridSiteDNlists /etc/grid-security/dn-lists/   GridSiteGSIProxyLimit 16   GridSiteEnvs on   GridSiteACLPath /opt/rucio/etc/gacl   </LocationMatch>  </VirtualHost>   And rucio-centos7:  LoadModule ssl_module /usr/lib64/httpd/modules/mod_ssl.so  LoadModule wsgi_module /usr/lib64/httpd/modules/mod_wsgi.so LoadModule gridsite_module /usr/lib64/httpd/modules/mod_gridsite.so    WSGIPythonHome /opt/rucio/.venv/ WSGIPythonPath /opt/rucio/.venv/lib/python2.7/site-packages Listen 4431 <VirtualHost rucio-centos7.twgrid.org:4431>   ServerName rucio-centos7.twgrid.org:4431  ServerAdmin ph-adp-ddm-lab@cern.ch   SSLEngine on  SSLCertificateFile /etc/grid-security/hostcert.pem  SSLCertificateKeyFile /etc/grid-security/hostkey.pem  SSLCACertificatePath /etc/grid-security/certificates  LogLevel debug  ErrorLog /var/log/rucio/httpd_error_log  TransferLog /var/log/rucio/httpd_access_log   Include /opt/rucio/etc/web/aliases-py27.conf   SSLVerifyClient optional  SSLVerifyDepth 20  SSLOptions +StdEnvVars   <Directory /opt/rucio>      Require all granted  </Directory>   <LocationMatch /auth/x509_proxy>   GridSiteIndexes on   GridSiteAuth on   GridSiteDNlists /etc/grid-security/dn-lists/   GridSiteGSIProxyLimit 16   GridSiteEnvs on   GridSiteACLPath /opt/rucio/etc/gacl  </LocationMatch>   </VirtualHost>    <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>  Best Regard Edward Wu ASGC  2018-01-05 23:09 GMT+08:00 Brian Bockelman <notifications@github.com>:  > Looks like the remote host isn't verifying the proxy? You probably want to > increase the debugging on the Apache-side and look for errors in the > error_log -- there should be a lot of gridsite-based chatter. > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-355577183>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4etSF8ebdlvzoTodDKXUSgu6DA01zeks5tHjsggaJpZM4RUIMT> > . > ", "Then I realized that the proxy create by voms-proxy-init is actually self-signed certificate. So the CA should be itself.  (.venv) [root@rucio-test03 rucio]# curl -i  --cert /tmp/x509up_u500_twgridpil --key /tmp/x509up_u500_twgridpil --cacert /tmp/x509up_u500_twgridpil --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-test03.twgrid.org:4431/auth/x509_proxy  HTTP/1.1 200 OK  Date: Mon, 08 Jan 2018 06:41:33 GMT  Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.4  Access-Control-Allow-Origin: None  Access-Control-Allow-Headers: None  Access-Control-Allow-Methods: *  Access-Control-Allow-Credentials: true  Access-Control-Expose-Headers: X-Rucio-Auth-Token  Cache-Control: no-cache, no-store, max-age=0, must-revalidate  Cache-Control: post-check=0, pre-check=0  Pragma: no-cache  X-Rucio-Auth-Token: chwu-/C=TW/O=AS/OU=GRID/CN=Chi Hsun Wu 266135-unknown-98933b34c7424b00b4b0d724953e2534  Content-Length: 0  Content-Type: application/octet-stream    But lxplus proxy authentication works well.  Could I know the necessarily setting for auth_proxy?  Here is what I have   [client]  rucio_host = https://rucio-test03.twgrid.org:4431  auth_host = https://rucio-test03.twgrid.org:4431  auth_type = x509_proxy  username = ddmlab  password = secret  ca_cert = /etc/grid-security/certificates/ASGCCA-2007.pem  client_cert = /root/.globus/usercert.pem  client_key = /root/.globus/userkey-nopass.pem  client_x509_proxy = $X509_USER_PROXY  account = chwu  request_retries = 3      ", "The logs on `rucio-test03` indicate that GridSite is not being loaded or used while GridSite is doing the authentication on `rucio-centos7`.  On `rucio-test03`, the `mod_ssl` module is passing the client chain directly to OpenSSL, which then fails.    This strongly suggests there's nothing wrong on the client side but `mod_gridsite` is not being loaded or used for some reason on `rucio-test03`.  I'd suggest looking through that configuration in more detail and/or earlier in the startup sequence to see why `mod_gridsite` isn't being used.", "Hi But after I use curl, I viewed log on rucio-test03. I can see mod_gridsite is working ?    (.venv) [root@rucio-test03 rucio]#  curl -i  --cert /tmp/x509up_u500_twgridpil --key /tmp/x509up_u500_twgridpil --cacert /tmp/x509up_u500_twgridpil --capath /etc/grid-security/certificates/ -H 'X-Rucio-Account: chwu'  -X GET https://rucio-test03.twgrid.org:4431/auth/x509_proxy HTTP/1.1 200 OK Date: Tue, 09 Jan 2018 00:50:47 GMT Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.4 Access-Control-Allow-Origin: None Access-Control-Allow-Headers: None Access-Control-Allow-Methods: * Access-Control-Allow-Credentials: true Access-Control-Expose-Headers: X-Rucio-Auth-Token Cache-Control: no-cache, no-store, max-age=0, must-revalidate Cache-Control: post-check=0, pre-check=0 Pragma: no-cache X-Rucio-Auth-Token: chwu-/C=TW/O=AS/OU=GRID/CN=ShoaTingTwgrid Cheng 496283-unknown-a02474a8829f4921898a17472e1f2008 Content-Length: 0 Content-Type: application/octet-stream  Log:  [Tue Jan 09 00:50:47.085681 2018] [ssl:info] [pid 19544] [client 202.169.169.45:37880] AH01964: Connection to child 0 established (server rucio-test03.twgrid.org:4431) [Tue Jan 09 00:50:47.086763 2018] [ssl:debug] [pid 19544] ssl_engine_kernel.c(1890): [client 202.169.169.45:37880] AH02043: SSL virtual host for servername rucio-test03.twgrid.org found [Tue Jan 09 00:50:47.102011 2018] [:debug] [pid 19544] canl_mod_gridsite.c(2434): set GRST_save_ssl_creds [Tue Jan 09 00:50:47.102047 2018] [:debug] [pid 19544] canl_mod_gridsite.c(2496): store GRST_CRED_AURI_0=dn:/C=TW/O=AS/OU=GRID/CN=ShoaTingTwgrid+Cheng+496283 [Tue Jan 09 00:50:47.102059 2018] [:debug] [pid 19544] canl_mod_gridsite.c(2496): store GRST_CRED_AURI_1=dn:/C=TW/O=AS/OU=GRID/CN=ShoaTingTwgrid+Cheng+496283/CN=2449328929 [Tue Jan 09 00:50:47.106517 2018] [ssl:debug] [pid 19544] ssl_engine_kernel.c(1823): [client 202.169.169.45:37880] AH02041: Protocol: TLSv1.2, Cipher: ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits) [Tue Jan 09 00:50:47.106928 2018] [ssl:debug] [pid 19544] ssl_engine_kernel.c(225): [client 202.169.169.45:37880] AH02034: Initial (No.1) HTTPS request received for child 0 (server rucio-test03.twgrid.org:4431) [Tue Jan 09 00:50:47.107112 2018] [authz_core:debug] [pid 19544] mod_authz_core.c(809): [client 202.169.169.45:37880] AH01626: authorization result of Require all granted: granted [Tue Jan 09 00:50:47.107141 2018] [authz_core:debug] [pid 19544] mod_authz_core.c(809): [client 202.169.169.45:37880] AH01626: authorization result of <RequireAny>: granted [Tue Jan 09 00:50:47.107388 2018] [:debug] [pid 19544] canl_mod_gridsite.c(2954): Using identity dn:/C=TW/O=AS/OU=GRID/CN=ShoaTingTwgrid+Cheng+496283 from SSL/TLS [Tue Jan 09 00:50:47.109162 2018] [:debug] [pid 19544] canl_mod_gridsite.c(3215): Examine ACL file /opt/rucio/etc/gacl (from ACL path /opt/rucio/etc/gacl) [Tue Jan 09 00:50:47.109311 2018] [:debug] [pid 19544] canl_mod_gridsite.c(3252): After GACL/Onetime evaluation, GRST_PERM=5 [Tue Jan 09 00:50:47.109459 2018] [authz_core:debug] [pid 19544] mod_authz_core.c(835): [client 202.169.169.45:37880] AH01628: authorization result: granted (no directives) [Tue Jan 09 00:50:47.109654 2018] [:debug] [pid 19544] canl_mod_gridsite.c(2954): Using identity dn:/C=TW/O=AS/OU=GRID/CN=ShoaTingTwgrid+Cheng+496283 from SSL/TLS [Tue Jan 09 00:50:47.109696 2018] [:debug] [pid 19544] canl_mod_gridsite.c(3252): After GACL/Onetime evaluation, GRST_PERM=0 [Tue Jan 09 00:50:47.153026 2018] [ssl:debug] [pid 19544] ssl_engine_io.c(993): [client 202.169.169.45:37880] AH02001: Connection closed to child 0 with standard shutdown (server rucio-test03.twgrid.org:4431)  I restart httpd and can see something in httpd/logs/error_log  [Tue Jan 09 01:07:35.909906 2018] [:info] [pid 31504] mod_wsgi (pid=31504): Python home /opt/rucio/.venv. [Tue Jan 09 01:07:35.909935 2018] [:info] [pid 31504] mod_wsgi (pid=31504): Initializing Python. [Tue Jan 09 01:07:35.924683 2018] [:info] [pid 31501] mod_wsgi (pid=31501): Attach interpreter ''. [Tue Jan 09 01:07:35.924683 2018] [:info] [pid 31502] mod_wsgi (pid=31502): Attach interpreter ''. [Tue Jan 09 01:07:35.924790 2018] [:info] [pid 31502] mod_wsgi (pid=31502): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path. [Tue Jan 09 01:07:35.924790 2018] [:info] [pid 31501] mod_wsgi (pid=31501): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path. [Tue Jan 09 01:07:35.926163 2018] [proxy:debug] [pid 31505] proxy_util.c(1843): AH00925: initializing worker proxy:reverse shared [Tue Jan 09 01:07:35.926201 2018] [proxy:debug] [pid 31505] proxy_util.c(1885): AH00927: initializing worker proxy:reverse local [Tue Jan 09 01:07:35.926244 2018] [proxy:debug] [pid 31505] proxy_util.c(1936): AH00931: initialized single connection worker in child 31505 for (*) [Tue Jan 09 01:07:35.926283 2018] [:info] [pid 31505] mod_wsgi (pid=31505): Python home /opt/rucio/.venv. [Tue Jan 09 01:07:35.926294 2018] [:info] [pid 31505] mod_wsgi (pid=31505): Initializing Python. [Tue Jan 09 01:07:35.933467 2018] [:debug] [pid 31502] canl_mod_gridsite.c(4041): Cutoff time for ssl creds cache: 1515459755 [Tue Jan 09 01:07:35.935788 2018] [:info] [pid 31503] mod_wsgi (pid=31503): Attach interpreter ''. [Tue Jan 09 01:07:35.935899 2018] [:info] [pid 31503] mod_wsgi (pid=31503): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path. [Tue Jan 09 01:07:35.937699 2018] [:info] [pid 31504] mod_wsgi (pid=31504): Attach interpreter ''. [Tue Jan 09 01:07:35.937778 2018] [:info] [pid 31504] mod_wsgi (pid=31504): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path. [Tue Jan 09 01:07:35.943947 2018] [:debug] [pid 31501] canl_mod_gridsite.c(4041): Cutoff time for ssl creds cache: 1515459755 [Tue Jan 09 01:07:35.944273 2018] [:debug] [pid 31503] canl_mod_gridsite.c(4041): Cutoff time for ssl creds cache: 1515459755 [Tue Jan 09 01:07:35.944788 2018] [:info] [pid 31505] mod_wsgi (pid=31505): Attach interpreter ''. [Tue Jan 09 01:07:35.944913 2018] [:info] [pid 31505] mod_wsgi (pid=31505): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path.  There are also something about mod_gridsite .          <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>  Best Regard Edward Wu ASGC  2018-01-08 23:50 GMT+08:00 Brian Bockelman <notifications@github.com>:  > The logs on rucio-test03 indicate that GridSite is not being loaded or > used while GridSite is doing the authentication on rucio-centos7. On > rucio-test03, the mod_ssl module is passing the client chain directly to > OpenSSL, which then fails. > > This strongly suggests there's nothing wrong on the client side but > mod_gridsite is not being loaded or used for some reason on rucio-test03. > I'd suggest looking through that configuration in more detail and/or > earlier in the startup sequence to see why mod_gridsite isn't being used. > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-356004940>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4etcHxMyIAqgBuItVZ0vfso2fdMhPwks5tIjlWgaJpZM4RUIMT> > . > ", "Hi Edward,    Would it be possible to post the `rucio-test03` logs for the `python-requests`-based interaction?    Brian", "Hi Brian:  Here is the log after http request sent:  [Tue Jan 09 15:31:07.210488 2018] [ssl:info] [pid 31626] [client 202.169.169.45:39020] AH01964: Connection to child 3 established (server rucio-test03.twgrid.org:4431) [Tue Jan 09 15:31:07.211781 2018] [ssl:debug] [pid 31626] ssl_engine_kernel.c(1890): [client 202.169.169.45:39020] AH02043: SSL virtual host for servername rucio-test03.twgrid.org found [Tue Jan 09 15:31:07.222153 2018] [ssl:info] [pid 31626] [client 202.169.169.45:39020] AH02008: SSL library error 1 in handshake (server rucio-test03.twgrid.org:4431) [Tue Jan 09 15:31:07.222266 2018] [ssl:info] [pid 31626] SSL Library Error: error:14089086:SSL routines:ssl3_get_client_certificate:certificate verify failed [Tue Jan 09 15:31:07.222282 2018] [ssl:info] [pid 31626] [client 202.169.169.45:39020] AH01998: Connection closed to child 3 with abortive shutdown (server rucio-test03.twgrid.org:4431)  Here is my request:  (.venv) [root@rucio-test03 rucio]# rucio whoami > /opt/rucio/lib/rucio/client/baseclient.py(337)__get_token_x509() -> result = self.session.get(url, headers=headers, cert=cert, (Pdb) p url 'https://rucio-test03.twgrid.org:4431/auth/x509_proxy' (Pdb) p headers {'X-Rucio-Account': 'chwu'} (Pdb) p cert '/tmp/x509up_u500_twgridpil' (Pdb) n > /opt/rucio/lib/rucio/client/baseclient.py(338)__get_token_x509() -> verify=self.ca_cert) (Pdb) p self.ca_cert '/etc/grid-security/certificates/ASGCCA-2007.pem' (Pdb) result = self.session.get(url, headers=headers, cert=cert,verify=self.ca_cert) *** SSLError: HTTPSConnectionPool(host='rucio-test03.twgrid.org', port=4431): Max retries exceeded with url: /auth/x509_proxy (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert unknown ca')],)\",),))   By the way, I also found some log under /etc/httpd for CURL, but nothing recorded when I use python request. [Tue Jan 09 15:36:50.214429 2018] [:debug] [pid 31626] grst_canl_x509.c(1032): GRSTx509ChainLoadCheck() starts [Tue Jan 09 15:36:50.214493 2018] [:debug] [pid 31626] grst_canl_x509.c(1061): Look for CA root file /etc/grid-security/certificates/b459ca48.0 [Tue Jan 09 15:36:50.214612 2018] [:debug] [pid 31626] grst_canl_x509.c(1072):  Loaded CA root cert from file [Tue Jan 09 15:36:50.214672 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 2 in chain [Tue Jan 09 15:36:50.214685 2018] [:debug] [pid 31626] grst_canl_x509.c(1096): Initialise chain [Tue Jan 09 15:36:50.214775 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 1 in chain [Tue Jan 09 15:36:50.214792 2018] [:debug] [pid 31626] grst_canl_x509.c(1150): Cert sig check 1 returns 0 [Tue Jan 09 15:36:50.214815 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 0 in chain [Tue Jan 09 15:36:50.214822 2018] [:debug] [pid 31626] grst_canl_x509.c(1165): Cert sig check 0 returns 0 [Tue Jan 09 15:36:50.215131 2018] [:debug] [pid 31626] grst_canl_x509.c(613): Found included VOMS cert in GRSTx509VerifyVomsSigCert() [Tue Jan 09 15:36:50.215178 2018] [:debug] [pid 31626] grst_canl_x509.c(656): Look for CA root file /etc/grid-security/certificates/b459ca48.0 [Tue Jan 09 15:36:50.215285 2018] [:debug] [pid 31626] grst_canl_x509.c(668):  Loaded CA root cert from file [Tue Jan 09 15:36:50.215386 2018] [:debug] [pid 31626] grst_canl_x509.c(698): X509_check_issued returns 0 [Tue Jan 09 15:36:50.220313 2018] [:debug] [pid 31626] grst_canl_x509.c(1032): GRSTx509ChainLoadCheck() starts [Tue Jan 09 15:36:50.220344 2018] [:debug] [pid 31626] grst_canl_x509.c(1061): Look for CA root file /etc/grid-security/certificates/b459ca48.0 [Tue Jan 09 15:36:50.220440 2018] [:debug] [pid 31626] grst_canl_x509.c(1072):  Loaded CA root cert from file [Tue Jan 09 15:36:50.220449 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 2 in chain [Tue Jan 09 15:36:50.220453 2018] [:debug] [pid 31626] grst_canl_x509.c(1096): Initialise chain [Tue Jan 09 15:36:50.220507 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 1 in chain [Tue Jan 09 15:36:50.220525 2018] [:debug] [pid 31626] grst_canl_x509.c(1150): Cert sig check 1 returns 0 [Tue Jan 09 15:36:50.220583 2018] [:debug] [pid 31626] grst_canl_x509.c(1086): Process cert at depth 0 in chain [Tue Jan 09 15:36:50.220598 2018] [:debug] [pid 31626] grst_canl_x509.c(1165): Cert sig check 0 returns 0 [Tue Jan 09 15:36:50.220747 2018] [:debug] [pid 31626] grst_canl_x509.c(613): Found included VOMS cert in GRSTx509VerifyVomsSigCert() [Tue Jan 09 15:36:50.220778 2018] [:debug] [pid 31626] grst_canl_x509.c(656): Look for CA root file /etc/grid-security/certificates/b459ca48.0 [Tue Jan 09 15:36:50.220920 2018] [:debug] [pid 31626] grst_canl_x509.c(668):  Loaded CA root cert from file [Tue Jan 09 15:36:50.221036 2018] [:debug] [pid 31626] grst_canl_x509.c(698): X509_check_issued returns 0 [Tue Jan 09 15:36:50.225229 2018] [:debug] [pid 31626] grst_gacl.c(764): GRSTgaclAclLoadFile() starting [Tue Jan 09 15:36:50.225735 2018] [:debug] [pid 31626] grst_gacl.c(794): GRSTgaclAclLoadFile parsing GACL [Tue Jan 09 15:36:50.242705 2018] [:info] [pid 31626] mod_wsgi (pid=31626): Create interpreter 'rucio-test03.twgrid.org:4431|/auth'. [Tue Jan 09 15:36:50.244134 2018] [:info] [pid 31626] mod_wsgi (pid=31626): Adding '/opt/rucio/.venv/lib/python2.7/site-packages' to path.   Best Regard Edward Wu ASGC  2018-01-09 22:23 GMT+08:00 Brian Bockelman <notifications@github.com>:  > Hi Edward, > > Would it be possible to post the rucio-test03 logs for the python-requests-based > interaction? > > Brian > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-356297543>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4eteFBDNMAdy3ufuLcFgulQScX4SoDks5tI3ZggaJpZM4RUIMT> > . > ", "Hi Edward, this is quite mysterious. I assume you have the same version of apache,mod_ssl and mod_gridsite on both centos7 and test03?    \"tlsv1 alert unknown ca\" means that the server is actively and purposefully rejecting the client.    Maybe there's a magic dependency between the configurations (they look similar to me, but not quite). Would it make sense to copy the working one over from centos7 to test03 (including the respective httpd.conf etc..)", "@mlassnig: `tlsv1 alert unknown ca` is often the error you receive when a grid proxy is utilized to authenticate with a service that doesn't understand grid proxies.  The proxy itself is an X509 certificate signed by a user acting as a CA -- OpenSSL is confused as the user isn't on its list of valid CAs.    Of course, it can also be a more mundane error - but that's why I was trying to figure out why mod_gridsite didn't appear to be processing his proxy cert.    It does seem somewhat client-dependent - I wonder if there is some configuration difference causing the `python-requests` client to map to a different virtual host?", "Hi     In fact the version of each server is different: rucio-test03 (.venv) [root@rucio-test03 rucio]# uname -a Linux rucio-test03.twgrid.org 3.10.0-693.11.1.el7.x86_64 #1 SMP Mon Dec 4 23:52:40 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux (.venv) [root@rucio-test03 rucio]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.2k-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.4  rucio-centos7 (.venv)[root@rucio-centos7 dicos-rucio]# uname -a Linux rucio-centos7.twgrid.org 3.10.0-229.14.1.el7.x86_64 #1 SMP Tue Sep 15 15:05:51 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux (.venv)[root@rucio-centos7 dicos-rucio]# cat /etc/redhat-release CentOS Linux release 7.1.1503 (Core) Server: Apache/2.4.6 (CentOS) OpenSSL/1.0.1e-fips mod_auth_kerb/5.4 mod_wsgi/3.4 Python/2.7.5 mod_gridsite/2.3.2  I have tried to downgrade gridsite/2.3.4 to gridsite/2.3.2 but it is not working.  I also remove httpd and delete all config then copy config from rucio-centos7 to rucio-test03 but it is still the same.  I setup another virtualhost on rucio-centos7 with same version of rucio-test03 and it is working.  Since there are so many packages that are different in version between centos7.1 and 7.4, I decide to make a new server in 7.1 and tried again.  Can I also know the OS version, openssl version, gridsite version and which repo do they came from (epel?umd-4?) in your server. And to install gfal2, I also put globus toolkit ( http://toolkit.globus.org/toolkit/downloads/latest-stable/). Do you install gfal2 by this ?  Best Regard Edward Wu ASGC  2018-01-10 22:20 GMT+08:00 Brian Bockelman <notifications@github.com>:  > @mlassnig <https://github.com/mlassnig>: tlsv1 alert unknown ca is often > the error you receive when a grid proxy is utilized to authenticate with a > service that doesn't understand grid proxies. The proxy itself is an X509 > certificate signed by a user acting as a CA -- OpenSSL is confused as the > user isn't on its list of valid CAs. > > Of course, it can also be a more mundane error - but that's why I was > trying to figure out why mod_gridsite didn't appear to be processing his > proxy cert. > > It does seem somewhat client-dependent - I wonder if there is some > configuration difference causing the python-requests client to map to a > different virtual host? > > \u2014 > You are receiving this because you commented. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-356615786>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4etR-0gDF7l97Y7ZdlJ7_MPGMz6bRoks5tJMdDgaJpZM4RUIMT> > . > ", "@edwardwufast - what version of python-requests are you using?    I got this exact same error on a newly configured hosts with python-requests at version 2.18.4.  I downgraded to an older version (2.11.1) and everything worked well.", "Actually - I did two things.  I downgraded the python-requests as noted above *and* switched python-urllib3 back to the one provided by RHEL7 (it had previously been installed via PyPI).    It may be possible that the python-urllib3 is the culprit here -- and that RedHat is adding some special patch to make it work?", "The rucio auth nodes in production have the following:    - CentOS7.4  - requests==2.18.4  - urllib3==1.22    All installed from pip.    The CentOS base python-urllib3.noarch 1.10.2-3.el7 and python-requests.noarch 2.6.0-1.el7_1 are not used.", "@edwardwufast - can you post the versions you are using compared to the ones Mario posted?", "Hi   For python package there are the same because I installed by pip-requirement under tools. For OS I have tried two version, but all the same: (.venv) [root@rucio-test03 rucio]# cat /etc/redhat-release CentOS Linux release 7.4.1708 (Core) [root@rucio ~]# cat /etc/redhat-release Derived from Red Hat Enterprise Linux 7.1 (Source) [root@rucio ~]# uname -a Linux rucio 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux  I also modify openssl version to 1.0.1e on 7.1 which is the same version as rucio-centos7, which is the working one. I really can not found the problems in configuration and CA. What if I provide the password and let you login to check?   <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> <#DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2>  Best Regard Edward Wu ASGC  2018-01-15 23:48 GMT+08:00 Brian Bockelman <notifications@github.com>:  > @edwardwufast <https://github.com/edwardwufast> - can you post the > versions you are using compared to the ones Mario posted? > > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-357719660>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4etel7tFnAkTzqugOvF6jiYXfjpTUMks5tK3NTgaJpZM4RUIMT> > . > ", "Hi      I then downgrade my rucio in rucio-test03 and rebuild virtualenv, then SSL problem disappear. I am wondering that maybe there is somethings to do with python packages. I then upgrade to 1.14.2 and tried again.  There is the pip-require in old version which is work for me : (.venv) [root@rucio-test03 rucio]# cat tools/pip-requires # All dependencies needed to run rucio should be defined here  SQLAlchemy==1.0.14 # db backend alembic==0.8.6 # Lightweight database migration tool for SQLAlchemy Mako==1.0.4 # Templating language python-editor==1.0.1 # Programmatically open an editor, capture the result flup==1.0.2 # Needed to deploy web.py in lighthttpd web.py==0.38 # Python web framework python-memcached==1.58 # Quick and small memcached client for Python ntplib>=0.3.3 # NTP library anyjson==0.3.3 # Wraps the best available JSON implementation available in a common interface jsonschema==2.5.1 # JSON Schema repoze.lru==0.6 # LRU (least recently used) cache implementation. python-dateutil==2.5.3 # Extensions to the standard datetime module ordereddict==1.1 # Ordered Dictionary meld3==1.0.2 # an HTML/XML templating system for Python 2.3+ pysftp==0.2.9 # forces installation of paramikoi and pycrypto paramiko==2.0.1 # SSH2 protocol library pycrypto==2.6.1 # Cryptographic modules s3cmd==1.6.1 # Package built from http://s3tools.org/download gearman==2.0.2 # Used only gor emulation framework stomp.py==4.1.11 # ActiveMQ Messaging Protocol dnspython==1.14.0 # To resolve ActiveMQ broker alias pystatsd==0.1.10 # Needed to log into graphite with more than 1 Hz pygeoip==0.3.2 # GeoIP API geoip2==2.4.0 # GeoIP2 API (for IPv6 support) maxminddb==1.2.1 # extension for reading the MaxMind DB format threadpool==1.3.2 # threadpool cffi==1.7.0 # Foreign Function Interface for Python calling C code cryptography==1.4 # Cryptographic recipes and primitives enum34==1.1.6 # Python 3.4 Enum backported gcloud==0.17.0 # API Client library for Google Cloud googleapis-common-protos==1.2.0 # Common protobufs used in Google APIs httplib2==0.9.2 # A comprehensive HTTP client library idna==2.1 # Internationalized Domain Names in Applications (IDNA) ipaddress==1.0.16 # IPv4/IPv6 manipulation library oauth2client==2.2.0 # OAuth 2.0 client library protobuf==3.0.0     # Protocol Buffers grpcio==1.0.0       # Package for gRPC Python. pyOpenSSL==16.0.0   # Python wrapper module around the OpenSSL library pyasn1==0.1.9 pyasn1-modules==0.0.8 # A collection of ASN.1-based protocols modules pycparser==2.14 # C parser in Python rsa==3.4.2 # Pure-Python RSA implementation setuptools==24.0.2 # Easily download, build, install, upgrade, and uninstall Python packages retrying==1.3.3 # general-purpose retrying library to simplify the task of adding retry behavior to just about anything six==1.10 # Python 2 and 3 compatibility utilities functools32==3.2.3.post2  # explicitly needed on CC7 (.venv) [root@rucio-test03 rucio]# cat tools/pip-requires-client # All dependencies needed to run rucio client should be defined here  argparse==1.4.0           # Command-line parsing library argcomplete==1.4.1        # Bash tab completion for argparse kerberos==1.2.2 pykerberos==1.1.13        # Kerberos high-level interface requests==2.10.0 requests-kerberos==0.10.0  # A Kerberos authentication handler for python-requests urllib3==1.16             # HTTP library with thread-safe connection pooling and file post support wsgiref==0.1.2            # WSGI (PEP 333) Reference Library dogpile==0.2.2            # Caching API dogpile.core==0.4.1       # Caching API plugins dogpile.cache==0.6.2      # Caching API plugins nose==1.3.7               # For rucio test-server boto==2.41.0              # S3 boto protocol python-swiftclient==3.1.0 # OpenStack Object Storage API Client Library tabulate==0.7.5           # Pretty-print tabular data progressbar==2.3          # Text progress bar bz2file==0.98             # Read and write bzip2-compressed files. python-magic==0.4.12      # File type identification using libmagic (.venv) [root@rucio-test03 rucio]# cat tools/pip-requires-test # All dependencies needed to develop/test rucio should be defined here  pinocchio==0.4.2                 # Extensions for the 'nose' unit testing framework Paste==2.0.3                     # Utilities for web development in pyton unittest2==1.1.0                 # backport of unittest lib in python 2.7 coverage==4.0.3                  # Nose module for test coverage Sphinx==1.3.6                    # required to build documentation Jinja2==2.8                      # template engine sphinxcontrib-httpdomain==1.4.0  # Provides a Sphinx domain for describing RESTful HTTP APIs stub==0.2.1                      # Temporarily modify callable behaviour and object attributes #PIL==1.1.7                      # !!! This library does not exist anymore Pygments==2.0                    # Python Syntax highlighter docutils==0.12                   # Needed for sphinx pep8==1.7.0                      # checks for PEP8 code style compliance pyflakes==1.0.0                  # Passive checker of Python programs flake8==2.5.4                    # Wrapper around PyFlakes&pep8 mccabe==0.4.0                    # McCabe checker, plugin for flake8 pylint==1.5.4                    # static code analysis. Last release compatible with python 2.6 virtualenv==15.0.0               # Virtual Python Environment builder tox==2.3.1                       # Automate and standardize testing in Python pytest==2.9.0 pytest-xdist==1.14               # py.test xdist plugin for distributed testing and loop-on-failing modes xmltodict==0.10.1                # Makes working with XML feel like you are working with JSON pytz==2016.7                     # World timezone definitions, modern and historical Babel==2.3.2                     # Internationalization utilities     Best Regard Edward Wu ASGC  2018-01-16 9:16 GMT+08:00 Wu Edward <edwardwufast@gmail.com>:  > Hi >   For python package there are the same because I installed by > pip-requirement under tools. > For OS I have tried two version, but all the same: > (.venv) [root@rucio-test03 rucio]# cat /etc/redhat-release > CentOS Linux release 7.4.1708 (Core) > [root@rucio ~]# cat /etc/redhat-release > Derived from Red Hat Enterprise Linux 7.1 (Source) > [root@rucio ~]# uname -a > Linux rucio 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 > x86_64 x86_64 x86_64 GNU/Linux > > I also modify openssl version to 1.0.1e on 7.1 which is the same version > as rucio-centos7, which is the working one. > I really can not found the problems in configuration and CA. What if I > provide the password and let you login to check? > > > > <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> > \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com > <https://www.avast.com/sig-email?utm_medium=email&utm_source=link&utm_campaign=sig-email&utm_content=webmail> > <#m_6041974507179988328_DAB4FAD8-2DD7-40BB-A1B8-4E2AA1F9FDF2> > > Best Regard > Edward Wu > ASGC > > 2018-01-15 23:48 GMT+08:00 Brian Bockelman <notifications@github.com>: > >> @edwardwufast <https://github.com/edwardwufast> - can you post the >> versions you are using compared to the ones Mario posted? >> >> \u2014 >> You are receiving this because you were mentioned. >> Reply to this email directly, view it on GitHub >> <https://github.com/rucio/rucio/issues/422#issuecomment-357719660>, or mute >> the thread >> <https://github.com/notifications/unsubscribe-auth/AQ4etel7tFnAkTzqugOvF6jiYXfjpTUMks5tK3NTgaJpZM4RUIMT> >> . >> > > ", "Was there any conclusion on this?", "I compare with three servers I setup in asgc and use wireshark to check ssl process. But still fail to get a conclusion.i will prepare what I have tested later.Now I just use one of the server that is working and put the problemic one aside.  2018\u5e743\u67089\u65e5 \u4e0b\u53485:44\uff0c\"Martin Barisits\" <notifications@github.com>\u5beb\u9053\uff1a  > Was there any conclusion on this? > > \u2014 > You are receiving this because you were mentioned. > Reply to this email directly, view it on GitHub > <https://github.com/rucio/rucio/issues/422#issuecomment-371764111>, or mute > the thread > <https://github.com/notifications/unsubscribe-auth/AQ4etffSDsc4nt9b8yaIObaiyMkoWT2Cks5tck8EgaJpZM4RUIMT> > . > "], "420": ["third_party_copy has been added to the schema in December 2017. This should be fine now."], "413": [], "412": [], "411": ["For example, I think we'll need a lot of patches along this line:  ```  --- a/lib/rucio/client/didclient.py  +++ b/lib/rucio/client/didclient.py  @@ -15,6 +15,7 @@     - Martin Barisits, <martin.barisits@cern.ch>, 2014-2015   '''     +from urllib import quote_plus   from json import dumps   from requests.status_codes import codes     @@ -375,7 +376,7 @@ class DIDClient(BaseClient):           :param scope: The scope name.           :param name: The data identifier name.           \"\"\"  -        path = '/'.join([self.DIDS_BASEURL, scope, name, 'meta'])  +        path = '/'.join([self.DIDS_BASEURL, quote_plus(scope), quote_plus(name), 'meta'])           url = build_url(choice(self.list_hosts), path=path)           r = self._send_request(url, type='GET')           if r.status_code == codes.ok:  ```  Notes:    - Not sure if we want to quote the scope or keep that relatively restricted.  - There's a corresponding Apache-side change to allow encoded slashes in the URL.", "Ok, some progress here.    Once the client properly escapes `/` to `%2F` and Apache doesn't change the URL, the next step is `mod_wsgi`: this again escapes and automatically \"cleans up\" `//` characters.  So,    ```  /dids/user.bbockelm//foo/bar/baz/meta  ```    will, with some regex hackery, invoke the right function with scope `user.bbockelm` and DID `foo/bar/baz` (note missing prefix `/`).    Since Rucio already has a pre-load hook, one can correct the path and make WSGI \"do the right thing\".  Example code for the hook:    ```      request_uri = ctx.env.get('REQUEST_URI', '')      script_name = ctx.env.get('SCRIPT_NAME', '')      if request_uri.startswith(script_name):          path_part = request_uri.split(\"?\")[0]          path_part = normpath(path_part[len(script_name):])          while path_part.startswith(\"//\"):              path_part = path_part[1:]          ctx.path = unquote(path_part)  ```    This causes the right handler to be invoked with scope `user.bbockelm` and DID `/foo/bar/baz`.    It works with the caveat that one has to tweak the regex for the WSGI application.  Instead of:    ```      '/(.*)/(.*)/meta', 'Meta',  ```    one needs:    ```      '/([^/]*)/(.*)/meta', 'Meta',  ```    (see `URLS` global in `lib/rucio/web/rest/did.py`)    So, that's a modest project to do minor fixups to all APIs that include a DID (with an ugly hack in the load hook).  Definitely an option, but not a particularly appealing one.    The second option is to completely rework the APIs to allow them to accept either query-style or path-style information.  That is, have:    ```  /dids/meta?scope=user.bbockelm&did=/foo/bar/baz  ```    equivalent to    ```  /dids/user.bbockelm//foo/bar/baz/meta  ```    One can keep backward compatibility with existing clients.  It's more work, but seems cleaner to me.    Thoughts?", "one third alternative is to have:  ```'/(.*)/meta', 'Meta',```    and a function splitting the obtained string in scope, name -- assuming that scope doesn't have a /. we can stay backward compatible with this change.    going for:  ```/dids/meta?scope=user.bbockelm&did=/foo/bar/baz```    will need to introduce an api version (api version variable in the http header or in the url /v2/dids/meta?..). ", "@vingar - no, unfortunately that won't work due to the substitution of the the `//` into `/`.  The resulting DID is `foo/bar/baz` instead of `/foo/bar/baz`.  You end up having to do the same loader hook trick I highlighted above (which again - it does indeed work without client changes, just I find it ugly).    Now, having an API version is a solid idea regardless - but it's not clear why `/dids/meta?scope=user.bbockelm&did=/foo/bar/baz` would require one?  That's not a valid request currently.    Or - do you mean that we'd want to introduce an API version so the client can more easily auto-detect the version of the server (and hence the API it wants to use)?", "hi, I need to go in the details of the code to know what is doable. Unfortunately, this discussion doesn't concern only the metadata part. The same logic has been applied to several resources (dids, replicas, rules, etc) and http operations (put/get/post/del).", "In the meantime I'm trying to figure out if we can configure mod_wsgi not do the slash escape.", "@bbockelm This looks like it: http://httpd.apache.org/docs/2.2/mod/core.html#allowencodedslashes", "Another option, which should keep backward compatibility with the clients:    with special-lookahead in regexp, we can extract the delimiter from the regexp. Example:  ```  >>> re.match('/([^/]*)(?=/)(.*)/meta', '/user.bbockelm/foo/bar/baz/meta').groups()  ('user.bbockelm', '/foo/bar/baz')  ```  one possibility then is to make certain regexp policy configurable. "], "410": ["It's a column which was introduced with the 1.14.0 feature release. It's in the #96 pull request.", "reseting the db should solve this.", "Gotcha, will close for now."], "407": [], "406": ["Will it be enough to just check in bin/rucio erase if there is a wildcard given and then print an error?  I'm wondering why set_metadata isn't rising an exception for the `(key='lifetime' and '*' in did_name)` case.", "Yes, the error should be raised on server side though. "], "403": ["Don't we have this behaviour already for files marked as BAD? So wouldn't it be enough to mark a suspicious file as bad?", "@gumond has volunteered for this task", "This was implemented as a probe in #1871, but it should probably be it's own daemon.", "I have not had a look at this task yet. I considered it closed. When I come back tomorrow, I will see with you & Cedric how rucio daemons are implemented and what could I do about creating this one. ", "bin/rucio-replica-recoverer:  - Contains, in an epilog variable, my current procedure for manual testing.    - Once I learn more about rucio, should I write a test for this daemon ?     lib/rucio/core/replica.py:   - Although there are some differences, the two functions 'get_suspicious_files' and the new 'get_available_suspicious_replicas' seem to be aiming for similar outputs. Could we merge the two into one ? "], "402": ["I already had a PR for this submitted, but reverted it as it ties in with the discussion about the whole policy/permissions thing.    The idea was to have something like this in rucio.cfg:    [common]  vo_support = <email_address>  rucio_support = https://github.com/rucio/rucio/issues      and depending on the error show the appropriate followup.", "pull request: #247   issue duplicated with: #243    Proposal:    ```  [policy]  organization_support = <>  rucio_support =<>  ```  in the pull request, the error messages are generated client side, which forces to have the right parameters with the clients (in addition to the hostnames, etc). Server side would have been more flexible.", "It is more flexible,  but some errors might be generated client side and then you would have to contact the server to know the addresses. Overall not a big issue I think, as the config has to be distributed anyway. I am closing this, let's follow up in #243 "], "399": [], "394": ["I don't think this patch worked. I try to create a dataset with the name ```SingleMuonLRun2017A-PromptReco-v2LMINIAOD#7ca0e53a-4c6a-11e7-a64a-001e67ac06a0``` and it returns OK as if it worked, however   ```  [root@16ed54928c0b /]# rucio list-dids user.ewv:SingleMuonLRun\\*  +----------------------------------------------------+--------------+  | SCOPE:NAME                                         | [DID TYPE]   |  |----------------------------------------------------+--------------|  | user.ewv:SingleMuonLRun2017A-PromptReco-v2LMINIAOD | DATASET      |  +----------------------------------------------------+--------------+  ```    So the dataset name was truncated at the # character.  ", "I suspect the patch worked but hit client escaping issues.  Can you try creating a different dataset with a CMS block name and note the timestamp\u00a0when you did it?  I can then poke around the Apache logs.", "From the log:  ```  [19/Dec/2017:16:14:28 -0600] \"POST /dids/user.ewv/SingleMuonLRun2017A-PromptReco-v2LMINIAOD HTTP/1.1\" 201 7  ```  the client generates the right url:  ```https://localhost:443/dids/mock/SingleMuonLRun2017A-PromptReco-v2LMINIAOD%237ca0e53a-4c6a-11e7-a64a-001e67ac06a0```    but then server side the url is changed and names with hash mark should be 'urllib.quoted' ?  ", "If its on the client, I can try to find and fix it. Please assign to me. I'll let you know if I can't.  ", "no sure, it's only on the clients. This might need some change in apache, wsgi and server code.  ", "Ah, \"hash\" is the HTML anchor. ", "@ericvaandering - if you want to tack a whack at this, you probably want to edit something like this:    https://github.com/rucio/rucio/blob/master/lib/rucio/client/didclient.py#L83    replacing    ```          path = '/'.join([self.DIDS_BASEURL, scope, name])  ```    with    ```          path = '/'.join([self.DIDS_BASEURL, scope, urllib.quote_plus(name)])  ```    However, I strongly suspect we should re-close this issue as this really appears to be a client problem, not the schema.", "Yes, this is not enough. Encoding the URL is not sufficient. It still comes back as dataset exists (dropping the # and everything after it seems). ", "Re-closing this issue as I can create these datasets on the CMS instance with the client patches in #437 applied."], "391": ["This has been done?"], "388": [], "383": [], "380": ["I appear twice in AUTHORS  Joaqu\u00edn Bogado <jbogado@linti.unlp.edu.ar>, 2014-2017   Joaquin Bogado <jbogadog@cern.ch>, 2017-2017  Only the first is valid.  Also, I think UNLP (Universidad Nacional de La Plata - Argentina) should appear as institution.  :)", "@jwackito it's because of the mail address associated to your commits and the difference of accent in the first name. Could you try to stick to one mail address ? ideally the one associated to you github account to get the activity history linked to it.", "@vingar I will do. Thanks!"], "377": ["I'm trying to wrap my head around the implications of this. This mapping of DID ( path+name on disk) <-> LFN(CMS namespace) needs to be stored somehow. If you just give a different LFN to the DID that you're storing, how will you be able to use/download it later again? (as the lookup is against the DID, and not against an LFN).", "If the LFN on disk differs, `rucio upload` can take a PFN attribute (LFN on disk). But this works for non deterministic endpoint.   For deterministic, the function LFN -> PFN is needed.  ", "@bbockelm can you maybe write down an example (or a few) how the data identifiers and paths on storage look right now and the different conventions in building the path (I remember you linked some XML file on slack); Just to give us an easier understanding of what we want to achieve.", "(sorry, was out sick on Friday...)    Let's assume for now that we call the CMS scope `cms` (currently, everything in the experiment is organized into a single namespace and socially, we're not likely to change this...).  Then, for the LFN we have now:    ```  /store/generator/Fall13pLHE/BprimeTToBZ_M1200GeV_Tune4C_13TeV-madgraph-tauola/GEN/START62_V1-v1/00000/5EE73686-1651-E411-B447-02163E00EF58.root  ```    we would use the following DID:    ```  cms:/store/generator/Fall13pLHE/BprimeTToBZ_M1200GeV_Tune4C_13TeV-madgraph-tauola/GEN/START62_V1-v1/00000/5EE73686-1651-E411-B447-02163E00EF58.root  ```    Currently, when `rucio upload` is invoked, the file name on the local disk is not necessarily the same as the filename in the LFN (`5EE73686-1651-E411-B447-02163E00EF58.root` in this case ... it's really a mistake in the case of CMS to attach too much value to the component after the last `/` in the LFN, as it's not really the file's name).    @mlassnig is right: this will need a corresponding patch for `rucio download` so the user can specify an output file name.  However, downloading a file is a nearly-unused case in CMS, which is why I wasn't worried about it at the outset here.  In almost all use cases, the files are accessed via the physics application, not the workflow layer (there's no concept of a \"mover\" for stagein as in ATLAS).    @bari12 - here's an example of what the PFN generation rules look like:    https://cmsweb.cern.ch/phedex/datasvc/xml/prod/tfc?node=T2_US_Nebraska    The discussion might really belong in #338, but we can get most of the use cases demonstrated by simply concatenating the LFN to the prefix.", "Oh - as an example, here's likely how CMS would want to be able to upload files:    ```  rucio --verbose upload --scope user.bbockelm --rse T2_US_NEBRASKA_USERDISK myGreatFile.root \\        --lfn /store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root  ```", "Ah I see more clearly what you mean now. I feel though that this mixes Rucio concepts a bit up, in your example above the did (Or in some sense the Rucio LFN) would be: `user.bbockelm: myGreatFile.root` but we additionally would have to store the CMS-LFN `/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root` plus the path on actual storage, as this cannot be derived just by the did `user.bbockelm:myGreatFile.root` itself. It is possible to expand the schema like this, but I wonder, would you ever address the file with the \"Rucio LFN/did\" (\"user.bbockelm:myGreatFile.root\")? Because if not, why not make the Rucio did in the first place: `user.bbockelm:/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root` We would have to extend the column and allow \"/\" in the name part of the did and tinker with the client a bit, but it seems like the more natural workflow to me? Please correct if I understand anything wrong \ud83d\ude04 ", "@bari12 - nope, I'm not sure that's correct.    In this example, `myGreatFile.root` is some random set of characters produced by the workflow management system which have no particular meaning once the job ends.  CMS refers to the file as:    ```  /store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root  ```    forever and ever after that.    Then, given that LFN, we construct the corresponding PFN -- usually via concatenation of a known prefix:    ```  gsiftp://red-gridftp.unl.edu/user/cms/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root  ```    Despite the `/` strongly suggesting a directory hierarchy, it is really treated as an opaque name.  For example, at RAL's Echo instance, I believe we have a pool group named `cms` and the object name is `/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root`.    So, for a Rucio DID, I think this file would be referred to as the concatenation of the scope (`cms`) and the LFN:    ```  cms:/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root  ```", "Ok, so the mapping will then be simply      DID:    cms:/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root    and the replica at RAL-ECHO would generate this PFN:      gsiftp://red-gridftp.unl.edu/user/cms/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root      That's certainly doable. I guess at a later stage you can explore the use of scopes in your workflow? But for now, like this your workflow management could continue as-is.", "@mlassnig - that's almost right:  `s|RAL-ECHO|T2_US_Nebraska|` and it'll be 100% correct.    Given how the workflows are architected and the CMS computing project is organized, I think it's likely we'll use a single scope for >95% of our data.  A few places where we might do additional scopes:    * Testing data for things like load-tests and scaling campaigns.  * Temporary outputs from the workflow management that are later combined into \"official\" files.  * Release validation is often run as quite separate from the rest of CMS.    But these are not a bit second-order for now.", "Great, that's what I thought too, that after the upload the original filename `myGreatFile.root` is basically meaningless to you. I would suggest (I think most changes are there already?) that we adapt rucio in a way to make the did `cms:/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root` If you want to use multiple scopes or not is really up to you, for Rucio there is no difference.    This should be much easier to manage, as we don't need to take care of the CMS-LFN separately, as it is already encoded in the did. For the determinism I think we could overload the 'default' handling with the hashes in the cms policy file with the lookup in the xml, such that the did `cms:/store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root` gets transformed to the PFN easily via the xml lookup.    Plus some changes in the client to deal with the input filename and the did with slashes correctly.", "@bari12 - for having alternate determinism policies, it seems the alternatives are:    1.  Have a RSE protocol attribute that enables this and have the appropriate LFN-to-PFN function look for the attribute and behave appropriately.  That's what I did in this case; you can imagine a full implementation would have a list of rules instead of a single boolean.  2.  Have a \"cms mode\" specified in the policy (a third policy?) where CMS can provide its own LFN2PFN function, which would internally lookup the associated RSE protocol and implement the mapping.  This could then be stored outside the Rucio source tree.    Did I capture that correctly?  Honestly, my preference would be to start with 1 as it is done (boolean toggle) here (instead of doing the full XML, simply do the concatenation without the hashing) and then do 2 later on in the evaluation.  What's done here will allow us to use probably 90% of sites -- I hope!", "Yes, in the long run I would really prefer the second method. My thinking was to \"overwrite\" the current default (with the two hashes) via the CMS policy file could have the advantage that you do not have to set the determinism_type for each RSE. It just works correctly out of the box. So Rucio won't do it wrong if you create an RSE and forget to set the setting. You could do this at first with this simple method you already implemented and later on expand in the full XML rule mode.    But if you feel more comfortable to do it first with option 1 and later on only change to option 2 this is completely fine.", "\ud83c\udf88 \ud83c\udf7e "], "376": ["duplicate with https://github.com/rucio/rucio/issues/375"], "375": [], "369": ["Would be very useful - GitHub metadata doesn't currently pick up the license.", "yes, I realized it when creating a new repo  "], "368": [], "367": ["Duplicate of #429 ?", "Yes, I'll close the other one."], "363": [], "359": [], "356": [], "351": [], "348": [], "345": [], "344": ["There is really no need to self-reference the issue number in the issue itself \ud83d\ude04 "], "343": [], "339": ["@vingar - is this fixed by #356 ?", "Yes it is. Closing \ud83d\ude04 "], "338": ["Fixed by #379."], "336": [], "331": [], "330": ["An example of where this is useful is a hosted Rucio instance where the VO may only interact via the REST API: the existing `rucio.core.distance` module wouldn't be usable in this case.", "Proposed specification for the REST API:    Resource: /rses/distances/{source}/{destination}   Operations: POST / DELETE / PUT / GET/", "That API looks good to me!    How hard is it to implement?  Is there a similar REST API I can look at?", "Yes,    The method path is always core <- api <- rest <- (network) <- client. So to make the rest api, you will have to implement the api as well. But all these are very simple and analogous. The api is basically the layer in between where permissions etc. are checked, in the core itself everything works without permissions.    I think a good stack to look at are the identities (Each account has different identities, such as username/pass, x509 certs, etc.)  So the API file is: https://github.com/rucio/rucio/blob/master/lib/rucio/api/identity.py  The REST interface is: https://github.com/rucio/rucio/blob/master/lib/rucio/web/rest/identity.py    You will also have to add the new apache endpoint to https://github.com/rucio/rucio/blob/master/etc/web/aliases-py26.conf and https://github.com/rucio/rucio/blob/master/etc/web/aliases-py27.conf so that apache/mod_wsgi can find it."], "329": [], "327": ["Fixed by #342 "], "325": [], "323": [], "322": ["Sounds like a good modification to me, but then, I don't use the docker dev env, so someone who does should confirm ;-) ", "Definitely a good idea.", "+1", "Great, I'll get to it :smile:     The docker build looks for a flake8 config `.flake8` which isn't in the repo. The `.pep8` has a `[flake8]` section though. I just took the liberty to move the flake8 section to its own file.", "Apologies, I had forgotten to format my commit messages, hence the rewrite.", "May I presume that we will build a container called `rucio-dev` that will be available from the `rucio` docker repository, i.e. so one could pull it like this:  ```  docker pull rucio/rucio-dev  ```", "yes, we should replace the current rucio container with rucio-dev      "], "321": [], "320": [], "317": [], "314": ["One option is to drop support for Python 2.6, it's not received any security updates for four years and is little used.    Here's the pip installs for rucio from PyPI for the last month (via `pypinfo --percent --pip rucio pyversion`)    | python_version | percent | download_count |  | -------------- | ------- | -------------- |  | 2.7            |   67.1% |            116 |  | 3.5            |   32.4% |             56 |  | 3.4            |    0.6% |              1 |  "], "313": [], "312": ["The solution is not streightforeward since we don't have dump of lock table in hadoop."], "307": ["Motivation  -----------    Badly merged conf.py file makes sphinx failing:    Running Sphinx v1.6.5  making output directory...    Traceback (most recent call last):    File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/sphinx/cmdline.py\", line 305, in main      opts.warningiserror, opts.tags, opts.verbosity, opts.jobs)    File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/sphinx/application.py\", line 168, in __init__      confoverrides or {}, self.tags)    File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/sphinx/config.py\", line 152, in __init__      raise ConfigError(CONFIG_SYNTAX_ERROR % err)  ConfigError: There is a syntax error in your configuration file: invalid syntax (conf.py, line 51)    Configuration error:  There is a syntax error in your configuration file: invalid syntax (conf.py, line 51)", "This is a duplicate of #255 ", "closed"], "298": [], "288": [], "285": [], "282": [], "281": [], "280": [], "279": [], "272": [], "265": [], "262": ["I suggest raising the priority on this."], "261": ["Can you give a concrete example how this would look like, especially how the two paths would look like for the mv command?", "We could do this with a very specific (mv/rsync) protocol, however, the workflow in Rucio does not support something like this as that protocol would only work at one specific host.    In ATLAS there is a similar workflow where workernodes copy their output themselves (with whatever way they think is correct) to the specific path on storage and then just register the already existing data in Rucio via the python clients (didclient.py / add_did with the RSE attribute). Would this workflow be an alternative for you?", "in the CLI it would be something like     `rucio upload <dataset> <files> --scope <scope> --local`    internally you can pass `gfal-copy file://original/path/to/file file://RSE/path/to/file`or `rsync /original/path/to/file /RSE/path/to/file`", "As discussed, I would add a --local argument like `rucio upload source_path --local destination_path` and perform a mv or rsync command in the uploadclient.     An own protocol is not feasible, if I understood correctly?", "There are some changes planned for this in regards to local moving protocols (storm)"], "258": [], "255": [], "246": [], "245": [], "244": ["What's the exact command you did?", "it's there but hidden in the code formatting:    rucio -v add-dataset user.briedel:test_test_test --lifetime 100  "], "243": ["I wonder if we should have something VO specific here, configurable in rucio.cfg. ", "Rucio support URL and Experiment support URL. Makes sense to me.", "maybe some rucio yaml/ini/etc file to provide some string substitution/variable based on the named policy", "Yes.  A lot of these errors could be VO specific, so it is not bad to have the first-line VO support \"filter\" before it goes to the Rucio github. And secondly some errors might include partially (?) confidential information where we might not want to encourage posting them on a public issue tracker."], "242": ["Thanks for the report @ericvaandering, we are currently rebuilding the entire documentation to get rid of errors like this.", "This is a duplicate now of #572   Closing this ticket as we use the other one to track the changes."], "237": [], "236": ["@tbeerman , I can see the access_cnt is not null for certain DIDs. So I guess it has been implemented. Can you confirm please ?", "Yes, it's implemented but I used the wrong issue number on the PR so it's not linked here. But the issue can be closed."], "235": ["Will be replaced by json import from CRIC. #716 "], "232": [], "227": [], "214": [], "213": [], "211": ["Proposal for consistency:    --trace-user  --trace-event-type"], "209": ["Having a simple test on all files to check that we don't have pylint errors would be good as well.   I quickly googled travis+pylint and few options exists apparently in travis but I didn't have time to investigate more. Something to look ?", "Running it over all files can be enabled easily, will just take a bit longer.", "There is two things: For the errors, testing everything is fine, because there really should not be any error in the code. Testing for score though you should do file by file. If you get a score for everything, the few files changed in the PR will not really move the general score, although the files themselves might have a very bad one."], "205": [], "204": [], "199": [], "196": [], "195": [], "192": [], "191": ["Cannot reproduce :   `  rucio list-account-usage  usage: rucio list-account-usage [-h] [--rse RSE] usage_account  rucio list-account-usage: error: too few arguments    rucio list-account-usage xyz  +------------------------------+------------+---------+--------------+  | RSE                          | USAGE      | LIMIT   | QUOTA LEFT   |  |------------------------------+------------+---------+--------------|  | xxx      | 25.254 GB  | 0.000 B | 0.000 B      |  | yyy         | 2.564 GB   | 0.000 B | 0.000 B      |  | zzz | 144.282 GB | 0.000 B | 0.000 B      |  +------------------------------+------------+---------+--------------+  `", "Weird, I don't know what I did then. Can be closed I guess."], "187": [], "185": [], "177": [], "174": [], "171": [], "168": [], "165": ["Git can handle symlinks alright, so no need to duplicate the file, right?", "yes, git can but the issue is more with the release procedure as this file is changed several times and how readthedocs accesses it", "True good point!", "For the release procedure it is fine, it is just overwriting the symlink, but never commiting back the code. If readthedocs can handle the symlink it is fine for the release procedure.", "try the symbolic link then "], "158": [], "155": [], "148": [], "145": ["alembic is fixed by another patch (thanks Martin ;-)"], "142": [], "141": [], "140": ["+1", "Hi Frank, just to confirm (as nobody is assigned to this ticket): You are planning to implement this, right? \ud83d\ude03 ", "There is no one assigned to the ticket because I can't assign anyone. I am planning to implement this. Sorry about the long delay, I got absorbed by something else for a moment. Back at it now - starting with dealing with the repo reset ;-)", "I just added you as collaborator for the repo :) You should be able to assign now.", "That worked, thanks!", "Going to work on #322 first to set up my dev environment (if you guys like the suggestions that is)."], "137": [], "136": [], "133": [], "123": [], "122": [], "121": ["Lets discuss in the next dev meeting. The interest part sounds a bit redundant to the lifetime exceptions to me. ", "Just to note what we discussed in the meeting. We decided to follow up with DAST what they think of the idea and then move from there. ", "Lets try to discuss with them in Hamburg", "Decided to keep it; Possibly a student project.", "Can I work on solving this issue?  ", "Hi @pujanm   Yes, this can be (part) of your gsoc project. Let's discuss this on Slack, so you can put together a proposal.", "Duplicate #2304 "], "119": [], "116": ["Raising UnsupportedOperation looks also a good option if the expiration date < now", "Right now, DataidentifierNotFound is raised in these cases, and I think we can keep it the same. This is really only for the case where a dataset is not deleted yet, but logically, it already should be."], "113": [], "109": [], "106": ["Hi,     I just tried the \"AUTHORS\" link from:   http://rucio.readthedocs.io/en/latest/contributing.html#    and it took me here:   http://rucio.readthedocs.io/en/latest/AUTHORS.rst    Saying that the page does not exist. Should:  _\\`AUTHORS \\<AUTHORS.rst \\>\\`_  become something like :   _\\`AUTHORS \\<https://github.com/rucio/rucio/blob/master/AUTHORS.rst \\> \\`_  or  _\\`AUTHORS \\<../AUTHORS.rst \\>\\`_    ?  Cheers,   Jaroslav", "It should point to `https://rucio.readthedocs.io/en/latest/authors.html`"], "104": [], "101": [], "98": [], "91": [], "86": [], "85": [], "84": [], "83": [], "82": ["- One way to identify them (dumps, query with index, etc.)  - One way to get rid of them (rucio admin replica tombstone/delete/etc.)", "@hahahannes the ticket text is a bit misleading. The idea of the ticket is a `rucio-admin` command which can put a tombstone on a specific replica (which has no locks, etc.)  We can discuss details in person."], "81": ["What is the status ?", "already implemented"], "80": [], "79": [], "78": ["I guess this is a duplicate for #80 and #81?", "pinging @tbeerman ", "well, in JIRA it was the main task and #80 and #81 where the sub-tasks", "Ah ok, what you can do in github which is a bit similar to epics in jira is create tasklist inside the ticket, which you could use as an epic.    - [ ] a  - [ ] b  - [ ] c", "Closing as duplicate"], "77": [], "76": ["@cserf I wonder what the correct behaviour is there. I cannot just catch it and ignore it, because then the rule will stay in the queue probably forever. On the other hand, if I just remove the rule than the user will never know that it was deleted, due to it's async nature. Maybe the right thing is to put it to SUSPENDED. Opinions?", "The behaviour should be that the rule is not replicated to the new RSE and kept at the current RSE", "But there is no current RSE?", "Ahhh... sorry. I thought it was BB8 :-) (didn't recover from my jet-lag fro Gex :p )  SUSPENDED would be the right thing to do"], "75": [], "74": [], "73": ["Right now also schemes compatible to the source schemes are used, which at the moment works out fine, but this should not be done as it could result in conflicts."], "72": [], "71": [], "70": ["Is it related to the INCOMPLETE message ? Do we have another issue related to it ?    Motivation:   When a dataset starts being deleted, If its state change to INCOMPLETE from a previous state != INCOMPLETE, then a message is produced:                add_message('INCOMPLETE', {'scope': scope, 'name': name})      ", "Yes, the epic issue for this is #69. I just noticed though, we added the INCOMPLETE state in the schema.sql, but not in models.py or constants.py yet.", "I checked on next and could not find the constants change ? models.py is not necessary.     ", "Yes, as I said, it's not there \ud83d\ude04 ", "but I did the merge request (100% sure)...", "It doesn't seem to be in there. Only schema.sql and the migrate_repo file.  https://github.com/rucio/rucio/commit/bf36f633622d469479074a15ecdaf28b91f86a9b", "But just coming back to the original question. I think one message is enough, no need to have one for DATASET and one for CONTAINER.", "did_type in the message ?", "Yes, I think so. To some extent it is redundant, as e.g. AMI would know it anyway, but maybe this is consumed by other services too which do not necessarily know. ", "issue #68 is about 'Protect storage from too many requests ' ?", "You mean about the epic issue link above? I accidentally linked #68 first, but then corrected to #69; Maybe in your eMail it still says the first version."], "69": ["I had a deep look in the code and it seems we overlooked the pros and cons:    When the last replica is removed, the file is also removed from the content.  Thus generating a INCOMPLETE state and message for a complete dataset from the system point of view is conceptually wrong.   E.g. A dataset has 10000 files, when the reaper deletes the last replica, it removes it from the content. When listing all replicas, the 9999 files defined in the content are there which contradict the INCOMPLETE state.  Flagging the file as unavailable (and then set the dataset to incomplete) would have been more consistent. The counter part is that we will keep accumulating datasets with unavailable files. When all files are unavailable in the dataset this one is deleted by the reaper ? This will require more changes.    One scenario is to communicate datasets which have no rules anymore against them to AMI. The 'unlocked' datasets can be used by the event index but with the risk that they disappear.  Possible changes in the judge-cleaner ?", "We discussed this in the meeting and I think conceptually this is fine. It really depends how you interpret the availability column of the did. For me this is not really a data-management related metadata but physics metadata, thus from a physics point of view the dataset is INCOMPLETE. As for data-management related things we do not need the availability column anyway.    The rule scenario would work, but it is really the same, as also in this case we need to store somewhere that the dataset is INCOMPLETE. From a workflow though it is better to do it in the reaper, as it is the last possible moment and the reaper needs to query anyway if this was the last replica, while for the judge this would be a completely new workflow.", "incomplete is maybe not the correct semantic. 'unlocked' (or similar) makes more sense. message and flag can be generated from the reaper now and later we can think about moving it elsewhere.", "Unlocked for me is a bit too confusing with lock/rule related things. Personally I feel \"incomplete\" is fine, but I agree there might be a better terminology for it. But as you said, we can move it later too.", "I'm still fighting with the concept.     availability: LOST/DELETED/AVAILABLE  A file is LOST if there are no known replicas of the file in Rucio, while at the same time at least one account requested a replica; a file is DELETED if no account requested a replica; otherwise the file is AVAILABLE. This is a derived attribute.    complete: True/False  A dataset/container where all files have replicas available is complete. Any dataset/container which contains files without replicas is incomplete. This is a derived attribute.", "A dataset is INCOMPLETE if there is at least one LOST or DELETED file in the dataset.    The fact that this file is not in the main table (only in the deleted one) anymore is fine in my opinion. Don't get me wrong, I fully understand what you mean that conceptually the DELETED/LOST file should be kept in the table (dataset) - but from this workflow we walked away a long time ago. We do remove lost/deleted files in the table (dataset) and move it to the deleted one."], "68": [], "67": ["Linking #1505 as it is relvant here too."], "66": ["Fix in an earlier patch"], "65": ["Should be dropped!"], "64": ["Just to link this to #62 where I layed out the plan for this. In the first iteration I will add empty signatures for domain to all the RSEManager methods. At that point, the list_replicas call can basically already use them.    What we said today is, that list_replicas for the client in general should use domain='wan'. If an RSE is found witch matches with the client location, it should query again with domain='lan' for the RSE.", "i will extend it to list_replicas(.. , domain='wan') once the signature for create_protocol is merged. then we can gradually start making the clients domain aware"], "63": ["Multiproces in Python 3"], "62": ["this is starting to affect sites with different LAN/WAN protocol settings"], "61": [], "60": ["@cserf Do you know when this actually showed up? I just checked the code and the cleaner actually does not read any rules which have an active `child_rule_id` set. The workflow is that once the child_rule is OK, the parents child_rule_id is removed which makes it eligible for deletion.", "Pinging @cserf \ud83d\ude04 ", "If a child_rule is expired (and replicating) it cannot be removed as the FK constraint of the parent rule is active. In this case, the child_rule_id is removed from the parent rule and the parent rule is set to infinite lifetime."], "59": [], "58": ["- [x] Poller  - [x] Submitter  - [x] Receiver  - [ ] Finisher"], "56": [], "55": ["Duplicate of #1367 "], "52": ["If no sources are available, maybe rule should inform user that this is permanent/terminal."], "51": ["Not needed/Too dangerous"], "50": ["I think this just needs to be activated in the menu.", "@hahahannes can you please have a look on this.", "It is indeed already [there](https://github.com/rucio/rucio/blob/8665bcca2b187e154db3c2dc393f6d1cf0db4805/lib/rucio/web/ui/static/base.js#L148), but it seems to be visible only for admins. ", "This page is a bit different. It shows the the usage of all the users on an RSE. I think the initial idea of this page was a page where a user sees how much quota he is using on all RSEs. ", "Okay, then I will create a new page, because I couldn't find one.", "I could find a page under the link /account_rse_usage which has no links directing to it, yet. Also it shows a search field. Should it be removed so that automatically the user account gets choosen or is this page relevant for admins?"], "49": [], "48": ["Conclusion of todays meeting was to talk to Gancho about the impact of switching to BLOB.", "Gancho just confirmed. We will get varchar(32k) with the next oracle release. sqlite and mysql are 32k already, psql is unlimited size.    Will add new column payload_clob for oracle, which we can drop again as soon as oracle gets 32k varchar support.", "it looks like a good use case for the json column type.", "i also thought about that, but we don't select on the message, it's just a payload holder. so the json would just be overhead.", "storing a json payload into a json column doesn't look too exotic even if we don't select on it. Something to add on the gsoc proposal as a first contribution ?", "This could have implications both on INSERT as well as on SELECT. For CLOB at least oracle has to make context switches into pl/sql to execute this. For JSON I imagine this could be the same, especially since oracle might try to create some structure of the stored json?  But I don't get the point of doing this, since we are never trying to search inside the payload, it's always the selection of the full row. It's an optimisation for something which we don't use, and I doubt that it makes the current workflow faster \ud83d\ude04 ", "The json type is quite new and it can be the opportunity to get more experience with it on a simple use case: support on different db, sqlachelmy, measure the overhead, save some json dumps/load operations etc. I would just put it as part of the gsoc project and part of the project", "Up", "Discussed in the [dev meeting on 2018-10-11](https://indico.cern.ch/event/760955/):  - Add new column CLOB  - Column should only be used if the normal column is NULL"], "47": ["this one is fixed"], "46": ["So more suggestion from Rod on the archive/workflow support:  'It might be instructive to put together a rucio client archive command. One might archive a dataset  - check for files or download  - zip to N GB archives  - register archives and contents  - upload archive  - mark individual replicas secondary'", "xrdcp has the option to just pull a file from a zip archive. It was a bit broken(for dpm at least) but fixed and in the next release I guess. It works for eos though  xrdcp -f --zip log.13685606._000001.job.log.tgz.1 root://eosatlas.cern.ch:1094//eos/atlas/atlasscratchdisk/rucio/user/walkerr/82/e4/user.walkerr.test2.zip /tmp/pants    Cheers,  Rod.", "I summarized issues we have with extracting the files here:  https://docs.google.com/document/d/10Lu4jkbGh8bJ9AmRZQnDFwB8KamcO5ZmEG2vEfw8cCs/edit#  "], "45": ["Non exhaustive list :  gearman==2.0.2  git-review==1.26.0          # Command-line tool for Git / Gerrit  idna==2.6                   # Internationalized Domain Names in Applications (IDNA)  ipaddress==1.0.18           # IPv4/IPv6 manipulation library  repoze.lru==0.7             # LRU (least recently used) cache implementation.  ...", "I ran pip-extra-reqs (https://pypi.python.org/pypi/pip_check_reqs/2.0.1). Here is the list returned :  anyjson in requirements.txt  argcomplete in requirements.txt  argparse in requirements.txt  babel in requirements.txt  cffi in requirements.txt  coverage in requirements.txt  cryptography in requirements.txt  docutils in requirements.txt  enum34 in requirements.txt  flup in requirements.txt  functools32 in requirements.txt  futures in requirements.txt  gcloud in requirements.txt  gearman in requirements.txt  git-review in requirements.txt  googleapis-common-protos in requirements.txt  grpcio in requirements.txt  httplib2 in requirements.txt  idna in requirements.txt  ipaddress in requirements.txt  jinja2 in requirements.txt  mako in requirements.txt  maxminddb in requirements.txt  meld3 in requirements.txt  ntplib in requirements.txt  numpy in requirements.txt  oauth2client in requirements.txt  ordereddict in requirements.txt  paste in requirements.txt  pinocchio in requirements.txt  protobuf in requirements.txt  pyasn1 in requirements.txt  pyasn1-modules in requirements.txt  pycparser in requirements.txt  pycrypto in requirements.txt  pygments in requirements.txt  pyopenssl in requirements.txt  pytest-xdist in requirements.txt  python-editor in requirements.txt  python-memcached in requirements.txt  python-swiftclient in requirements.txt  repoze-lru in requirements.txt  rsa in requirements.txt  setuptools in requirements.txt  six in requirements.txt  tox in requirements.txt    Some modules like argparse and argcomplete appear in this list because they are already in py2.7 by default"], "44": [], "42": ["Hi all,    IIUC the desired behavior is that all existing replicas of a file should be declared BAD when the LFN is passed as argument instead of the PFN, correct?    Should the file itself also be flagged in some way if all its replicas are BAD?    Thanks  N.  ", "Yes, exactly. On the file itself I don't think that this has to be flagged, I think the necromancer daemon will do that then. @cserf maybe you can comment?", "You just has to resolve the LFNs to the list of PFNs (list_replicas), then pass this list to declare_bad_file_replicas", "OK.    Additional point: I can make the client resolve automatically if the user has passed LFN vs PFN.  But please let me know if you prefer instead that the user should specify this explicitly through an option."], "41": [], "40": [], "39": ["How is this done with the rse_usage_history  --> Should be a daemon?", "Done by #1878 "], "37": ["Needs to be checked", "Needs to be done \ud83d\ude04 ", "Starting today, I will be marking each time DDM Ops is affected by this issue.  :fire:"], "34": ["Duplicate of #86 "], "30": ["There seems to be a limit of 1000 dids when registering files to dataset; One option would be to have the client split into chunks of 1000"], "29": ["Hi, if you don't mind I will take this too, since I need it for the new api upload.", "Hi,    sure, sorry for the delay. Maybe you can simply reuse my changes to rsemanager.py here:    https://github.com/rucio/rucio/commit/7883be7685d906b3edbb4dc9dfdb098980ad8968#diff-8777b68ebcb342123d7ce30a3ff039b5    Cheers  N.", "Ah great! Thanks for  the hint!", "Done in bin/rucio, but not in uploadclient", "It is  https://github.com/rucio/rucio/blob/master/lib/rucio/client/uploadclient.py#L218  https://github.com/rucio/rucio/blob/master/lib/rucio/client/uploadclient.py#L276  https://github.com/rucio/rucio/blob/master/lib/rucio/client/uploadclient.py#L296", "So this has to be prepared in the list of dicts given to the uploadclient?", "Exactly. Most options are file specific in the API.", "I see, thanks \ud83d\udc4d . Closing"], "28": [], "27": ["Proposed implementation:    1) For protocols/tools that natively support timeout (e.g. gfal), pass the timeout to the underlying command. Targeting this for the next client release.    2) Later, for all protocols implement a watchdog in rucio download/upload that kills the transfer after the timeout expiration. The plan is to do this only for the new downloadclient/uploadclient libraries.", "Checking the code, the gfal protocol already has a hardcoded 3600 s timeout"], "26": ["I am not sure how to efficiently check if the rse is empty. If I understood correctly, I could either check for rules like:  ```  [rule for rule in list_rules() for rse in parse_expression(rule['rse_expression']) if rse['rse'] == rse] == 0  ```  But there I would have to iterate through each rule.    Or to check if there are file or collection replicas like:  ```  session.query(models.CollectionReplicas).filter_by(rse_id=rse_id).count() == 0                                                                                                                                                                                                       session.query(models.RSEFileAssociation).filter_by(rse_id=rse_id).count() == 0  ```", "Hi,    The best way would be to check against the replicas table directly, thus:    `session.query(models. RSEFileAssociation).filter_by(rse_id=rse_id).count() == 0`    This query goes against the Primary Key directly, so it should be rather fast, especially if the rse is empty.", "Okay thanks, was not sure because of the big size of the RSEFileAssociation table.", "For this it should be fine, as deleting an RSE is a really rare thing to do. "], "23": [], "8": [], "5": []}