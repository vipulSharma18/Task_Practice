{"3389": "---------- A hint on REPLICAS_PK needs to be added on the query to get the number of replicas in `list_and_mark_unlocked_replicas` ", "3382": "---------- only one storm protocol is defined in AGIS:  two found in the attributes: rucio-admin rse info INFN-T1_DATADISK ` storm extended_attributes: None hostname: storm-fe.cr.cnaf.infn.it prefix: /storage/gpfs_atlas/atlas/atlasdatadisk/rucio/ domains: {u'wan': {u'read': 4, u'write': 0, u'third_party_copy': 0, u'delete': 0}, u'lan': {u'read': 1, u'write': 0, u'delete': 0}} scheme: storm port: 8443 impl: rucio.rse.protocols.storm.Default strom extended_attributes: None hostname: storm-fe.cr.cnaf.infn.it prefix: /webdav/atlas/atlasdatadisk/rucio/ domains: {u'wan': {u'read': 4, u'write': 0, u'third_party_copy': 0, u'delete': 0}, u'lan': {u'read': 1, u'write': 0, u'delete': 0}} scheme: strom port: 8443 impl: rucio.rse.protocols.storm.Default ` ------------ ", "3381": "---------- Debugging with Katy her attempt to multi hop, we noticed that while there is a check in non-multihopping code which ignores distances set to None, there is no check in the multi hop link finder. We're now running a version which ignores such links in its test. I also noticed the function is using [] as a default argument, which is a python no-no. I can make a PR that fixes both issues. Hopefully after we get MH working. ------------ ", "3378": "---------- Provide the possibility to specify push or pull mode for WebDAV transfers in conveyor. ", "3371": "---------- We encountered a rule where all but one file had been replicated. The missing file was in a strange state: it had a single replica in `COPYING` state. Trying to identify said file with the following did not work: ``` $ rucio list-file-replicas --missing --rse RSE DID ``` Granted, this is an exceptional case and requires database access to fix, but it should have been possible to identify the file with the client. ------------ 1. `--missing` should imply `--all-states` 2. Make a minor adjustment to the checking logic. ", "3368": "---------- ``` $ rucio list-file-replicas --rse foo user.mlassnig:user.mlassnig.pilot.test.single.hits +---------+--------+------------+-----------+----------------+ | SCOPE | NAME | FILESIZE | ADLER32 | RSE: REPLICA | |---------+--------+------------+-----------+----------------| +---------+--------+------------+-----------+----------------+ ``` ------------ Check the validity of the supplied argument, display an error message in case the RSE does not exist. ", "3364": "---------- Traceback ``` 2020-03-05 13:38:04,692 11088 CRITICAL Thread [33/220] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/reaper2.py\", line 518, in reaper delete_replicas(rse_id=rse_id, files=deleted_files) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 374, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (cx_Oracle.IntegrityError) ORA-02292: integrity constraint (ATLAS_RUCIO.DATASET_LOCKS_DID_FK) violated - child record found ``` ", "3361": "---------- For Kubernetes. ", "3355": "---------- The partitioning algorithm is not working correctly for `thread_id=0`. It then wrongly goes to a mode assuming there are no partitions. ", "3348": "---------- The templates are currently not automatically included in the packaging. ------------ Need to be included. ", "3337": "---------- Fixes to bugs discovered after first deployment. ------------ **rucio.core.oidc** - correct a behaviour for Rucio servers opting not to use the OIDC AuthN/Z (try/except around lines 79, 80) **rucio.client.baseclient** - fixing a bug in import of unavailable libraries (remove lines corresponding to core.monitor library) **bin/rucio** ``` File \"/usr/bin/rucio\", line 256, in get_client 'oidc_issuer': args.oidc_issuer.lower()} AttributeError: 'NoneType' object has no attribute 'lower' ``` - set default to empty string for confing_get() function when defining oidc_issuer in bin/rucio **investigate reason of internal server error on Rucio DOMA cluster for browser based OIDC logins** - CLI --oidc-auto works, CLI login with polling works and client fetches a token, but browser does not display confirmation message, CLI fetch code login method crashes on delivery of the fetchcode and browser displays internal error. ", "3334": "---------- Document how to use Globus Online as a Rucio transfer tool ------------ Please include rst file. ", "3324": "The rucio-admin script does not call the get_account_limits() with the correct local/global locality. Need to implement a wrapper function in order to select the correct option. ", "3320": "---------- Conveyor : strict_copy mode not properly working ", "3315": "---------- Enable the possibility to specify strict_copy (needed for object stores) ", "3314": "---------- Bug in the selection of compatible checksum in the conveyor : If no checksum type is specified as a RSE attribute, it assumes that all checksum are supported, and it override `verify_checksum` ", "3313": "---------- The current regex doesn't fit to Belle2 convention ", "3312": "---------- If the pilot uses the 'write_lan' activity to upload a file, it will send the corresponding pfn (usually an internal proxy) to panda which will then fail to add the replica in Rucio because it doesn't match the expected pfn. This is because the checks in add_replica are hard-coded to use the 'wan' domain:  ------------ The checks should be changed to allow adding replicas with 'lan' domain. ", "3308": "---------- New reaper not handling `ConfigNotFound` : ``` 2020-02-18 12:43:57,304 16767 INFO Thread [23/197] : Reaper started Exception in thread Thread-27: Traceback (most recent call last): File \"/usr/lib64/python2.7/threading.py\", line 812, in __bootstrap_inner self.run() File \"/usr/lib64/python2.7/threading.py\", line 765, in run self.__target(*self.__args, **self.__kwargs) File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/reaper2.py\", line 363, in reaper max_evaluator_backlog_count = get('reaper', 'max_evaluator_backlog_count') File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 284, in new_funct return function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/config.py\", line 165, in get raise ConfigNotFound ConfigNotFound: Configuration not found. Configuration not found. ``` ------------ ", "3292": "---------- Temporary unavailable replicas : CLI bug that declare the files lost ", "3289": "---------- It is no longer possible to upload files to Dynfed, because of an attempt to stat non-existing file ------------ This line doesn't seem to me correct in section related to `protocol.renaming == False`  ", "3286": "---------- ``` mlassnig@lxplus767:~ $ rucio upload --rse CERN-PROD_SCRATCHDISK --scope user.mlassnig --protocol gsiftp --name test.236 /etc/hosts 2020-02-06 08:57:55,048 INFO Preparing upload for file hosts 2020-02-06 08:57:55,138 INFO Successfully added replica in Rucio catalogue at CERN-PROD_SCRATCHDISK 2020-02-06 08:57:55,301 INFO Successfully added replication rule at CERN-PROD_SCRATCHDISK 2020-02-06 08:57:55,506 INFO Trying upload with gsiftp to CERN-PROD_SCRATCHDISK 2020-02-06 08:57:56,059 ERROR An unknown exception occurred. Details: Checksum not validated ``` This worked alright in 1.20.0, but not anymore since at least 1.20.4 Upload with other protocols is ok: ``` mlassnig@lxplus767:~ $ rucio upload --rse CERN-PROD_SCRATCHDISK --scope user.mlassnig --protocol root --name test.233 /etc/hosts 2020-02-06 08:57:44,756 INFO Preparing upload for file hosts 2020-02-06 08:57:44,904 INFO Successfully added replica in Rucio catalogue at CERN-PROD_SCRATCHDISK 2020-02-06 08:57:48,168 INFO Successfully added replication rule at CERN-PROD_SCRATCHDISK 2020-02-06 08:57:48,491 INFO Trying upload with root to CERN-PROD_SCRATCHDISK 2020-02-06 08:57:49,549 INFO Successfully uploaded file hosts ``` ", "3285": "---------- Follow up of issue:  ------------ - verification of reliability of the etag retrieval with rquests and davix. - implementation of more reliable solution - adding logging at the protocol level - propagating the command to the pilot log ", "3284": "---------- > reaper2, correctly, schedules the RSE for deletion, but then finds that it cannot delete these replicas because they are in COPYING but since it should delete the 1k (or whatever it is set) bulk size in the loop, it goes to other tombstoned replicas. Since the epoche ones in copying don\u2019t suddenly go away, it eats into the secondaries further ------------ > It should not schedule the RSE for deletion due to replicas in this state and secondly in the deletion loop it should still check the watermark levels and see if it needs to delete secondaries. ", "3282": "---------- We noticed in the data carousel that all datasets were going to a single site in each cloud, when they should have been distributed by the freespace weight. It looks like inject_rule does not use the weight properly. When `__create_locks_replicas_transfers` is called, `preferred_rse_ids` is empty  This eventually goes to `rule_grouping`  where if `preferred_rse_ids` is empty, `prioritize_order_over_weight=True` which means the first RSE in the list is chosen instead of taking the weight into account. ------------ The simple modification would be setting `preferred_rse_ids=rses` in `inject_rule` but there are probably other ways. ", "3275": "---------- The rowcount variable is not properly reinitialized in `delete_replicas` ", "3272": "---------- CMS RSEs will also need these approvals mechanisms ------------ ", "3262": "---------- 1) Lack of logging verbosity due to weak handling of return codes and errors:  2) Better timeout handling ------------ add 1) just catch it and propagate it into the log (throwing Exception with rcode) add 2) perhaps using a timer solution from @TWAtGH : ``` def run(cmd, timeout_sec): proc = Popen(shlex.split(cmd), stdout=PIPE, stderr=PIPE) timer = Timer(timeout_sec, proc.kill) try: timer.start() stdout, stderr = proc.communicate() finally: timer.cancel() ``` or proper but complicated solution from @vokac , see attached file. ", "3258": "---------- What can happen now is that list_dataset_replicas_vp can return virtual replica but when the job runs, it can not pull the data since the only copy is for example on TAPE. ------------ proper fix would be to check if regular replicas exist with protocol \"root\" and if not list_dataset_replicas_vp should return nothing. ", "3257": "---------- when VP replica is not created the list_dataset_replicas_vp should not return anything. Currently it returns regular replicas. ------------ It should not return anything if no VP replicas. ", "3253": "---------- As a result, BB8 might rebalance again a rule that has already been processed by a previous run. ------------ Skip rules that have `child_rule_id` set. ", "3236": "---------- The `source_rse_id` filling when queuing requests takes an unreasonable amount of time. ------------ Disable the functionality until a better way can be found to do that. ", "3228": "---------- Currently the filtering of blacklisted updated_dids is done on the daemon side. However, for datasets with lots of updated_dids in the queue this does not scale well. The blacklisted datasets should be filtered directly in the SQL query. ", "3225": "---------- The submitter crash when enabling multihop mode ", "3209": "PR # 2689 causes a regression crash in the server by requiring a [client] section in the config file for the server. Should use the no-exception parameter, right? ", "3204": "---------- I recall some recent discussion (CHEP or pre-GDB?) about possibly developing a Rucio CSI plugin for Kubernetes. Are there any plans to do this? If I understand correctly it seems like the right way to stage in data for grid jobs on kubernetes, because the staging would occur in the background before the compute job starts running and occupying resources. A user would request a Rucio volume and specify a secret (proxy) to use and the desired files to download. When the job starts running the files would be downloaded and available in the volume. ", "3201": "---------- Minos : Temporary unavailable files in bad_pfns not removed if processed after expiration date ", "3196": "", "3193": "---------- The stat command of gfal currently does a checksum call for each checksum in `GLOBALLY_SUPPORTED_CHECKSUMS` this currrently results in a very long processing time for SRM which finally times out after ~30 minutes. ------------ First try the `PREFERRED_CHCEKSUM` and if this validates successfully don't try the other checksums. ", "3188": "---------- Traceback : ``` 2019-12-12 10:00:30,709 20205 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/badreplicas/minos.py\", line 210, in minos update_replicas_states(chunk, nowait=False, session=session) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 381, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/replica.py\", line 1777, in update_replicas_states raise exception.ReplicaNotFound(\"No row found for scope: %s name: %s rse: %s\" % (replica['scope'], replica['name'], get_rse_name(replica['rse_id'], session=session))) ReplicaNotFound: Replica not found Details: No row found for scope: panda name: panda.um.user.kkrizka.data18_13TeV.physics_Main.19901029._003813.tree.root rse: NDGF-T1_SCRATCHDISK ``` ", "3187": "---------- Currently they are allowed to fail, should be required; ", "3182": "---------- Reaper1 generates some error messages with thread information included; ", "3181": "---------- Currently the download is aware of the client location, via the `list_replicas` call. A similar workflow must be added to the upload. - If the client_location sitename is equivalent to the sitename of the upload rse use the `lan` domain for the upload protocol creation. ", "3178": "---------- After discussion, it looks like many people prefer snake_case instead of CamelCase for RSE attributes for total and used space. That's obviously a wrong choice similar to choosing emacs instead of vi (my 2 satoshis), but since the majority decides, let it be... ", "3175": "---------- Currently the `core.replica.set_tombstone` method has a session leak, since it creates a new session with every method call. This breaks the session hand-through logic Rucio usually runs and needs to be fixed. ", "3165": "---------- The 1.21 release introduced local and global quotas and broke backwards compatibility to the old clients. ", "3161": "---------- On the request of a more detailed error message from the download client if the DID doesn't exist these lines were patched in:  ``` for input_did in item['dids']: input_did_str = '%s:%s' % (input_did['scope'], input_did['name']) did_exists = False for file_item in file_items: if input_did_str == file_item['did'] or input_did_str in file_item['parent_dids']: did_exists = True break if not did_exists: logger.error('No replicas found for DIDs: %s' % input_did_str) file_items.append({'did': input_did_str, 'adler32': None, 'md5': None, 'sources': []}) ``` If the download is called with at least two not existing DIDs it crashes because it adds a source-less entry to `file_items` without the `'parent_dids'` key set. ------------ Add by default `'parent_dids': set()` to the source-less did entry: `file_items.append({'did': input_did_str, 'adler32': None, 'md5': None, 'sources': []})`  ", "3160": "---------- This appears to be related to the introduction of global quota support. ``` > rucio-admin account set-limits dcameron NO-NORGRID-T2_LOCALGROUPDISK 150000000000 'Client' object has no attribute 'set_account_limit' Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers. Traceback (most recent call last): File \"/home/dcameron/dev/ddm/rucio/bin/rucio-admin\", line 129, in new_funct return function(*args, **kwargs) File \"/home/dcameron/dev/ddm/rucio/bin/rucio-admin\", line 386, in set_limits client.set_account_limit(account=args.account, rse=args.rse, bytes=byte_limit) AttributeError: 'Client' object has no attribute 'set_account_limit' ``` ------------ ~~Update `rucio-admin` to use the proper method.~~ Add necessary support to `rucio-admin` but keep `get-limits` and `set-limits` as aliases. ", "3153": "---------- In the locks overview, when requesting the URL to the FTS monitoring, the UI will show the message \u2018The request has not yet been submitted\u2019 even if the transfer is done and the Finisher has yet to process it. ------------ In addition to `SUBMITTED`, display the FTS URL when the request is either `DONE` or `FAILED`. ", "3138": "---------- If the custom checksum support RSE attribute `supported_checksums` is not set, the RSE is by default defined compatible with the standard checksum algorithms. Due to how `get_rse_attribute` works, when `supported_checksums` is not found, an empty string containing is returned instead of a `None` string. The following logic assumed to have an `None` string, instead of a string formatted like that. ------------ Update the logic to sense such behavior and return the standard `GLOBALLY_SUPPORTED_CHECKSUMS` list in that case. ", "3134": "---------- The idea is to only list files in list_replicas which are created after a certain timestamp. This will help workflows (such as T0) which continuously query for new replicas to quickly assess the files added to a dataset. ", "3129": "---------- When accessing the webui for the first time without any cookies set it loads correctly. Then, when reloading the page the token is not set in javascript anymore and no requests can be made to Rucio. ------------ ", "3128": "---------- The conveyor crashes when multihop is done through a non-deterministic endpoint ", "3127": "---------- Due to the change from simple limits/usage to local/global usage and limits, the backwards compatibility for old clients is currently broken. This is due to the fact that the REST endpoints where changed from ``` '/(.+)/limits', 'AccountLimits', '/(.+)/limits/(.+)', 'AccountLimits', '/(.+)/usage/', 'Usage1', '/(.+)/usage/(.+)', 'Usage2', ``` to ``` '/(.+)/limits/local', 'LocalAccountLimits', '/(.+)/limits/local/(.+)', 'LocalAccountLimits', '/(.+)/limits/global', 'GlobalAccountLimits', '/(.+)/limits/global/(.+)', 'GlobalAccountLimits', ``` thus the required endpoints are missing for old clients. ------------ The endpoints need to be re-added and pointing to the local version of the usage/limit. ", "3121": "---------- Lifetime Model : Limit the duration of the extension to one year in the UI ", "3118": "---------- Properly remove leftover stale files when running tests. ", "3111": "## ---------- Previous PR #2555 introduced this bug related to a missing parentheses at the formatting of a debug output string here:  ------------ Add the parentheses surrounding the formatting elements. ", "3108": "---------- ``` InvalidRequestError: This Session's transaction has been rolled back due to a previous exception during flush. To begin a new transaction with this Session, first issue Session.rollback(). Original exception was: (psycopg2.errors.UniqueViolation) duplicate key value violates unique cons traint \"ACCOUNT_ATTR_MAP_PK\" DETAIL: Key (account, key)=(root, jdoe-6803bb40932c423f99c0) already exists. [SQL: INSERT INTO dev.account_attr_map (account, key, value, updated_at, created_at) VALUES (%(account)s, %(key)s, %(value)s, %(updated_at)s, %(created_at)s)] [parameters: {'value': 'true', 'created_at': datetime.datetime(2019, 11, 22, 11, 4, 50, 279628), 'account': 'root', 'key': 'jdoe-6803bb40932c423f99c0', 'updated_at': datetime.datetime(2019, 11, 22, 11, 4, 50, 279621)}] (Background on this error at:  (Background on this error at:  ``` ------------ Add new error string handling. ", "3102": "Hi, Even with conveyor.retry_protocol_mismatches set to True, the finisher seems to fail to retry protocol mismatch errors at all (it just ignores them). This seems to be handled by these lines:   But it never gets any requests in the MISMATCH_SCHEME state, seemingly because this constant isn't included in the main database query filter here:  Would it be possible for this to be fixed? Regards, Simon ", "3093": "---------- The following error raised from further testing: ``` 2019-11-12 13:59:03,692 26328 CRITICAL Exception happened when trying to get transfer for request fbd324e85ddf4e6b863b9113ae98e5dd: Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 784, in get_transfer_requests_and_source_replicas source_rse_checksums = get_rse_supported_checksums(source_rse_id, session) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 284, in new_funct return function(*args, **kwargs) TypeError: get_rse_supported_checksums() got multiple values for keyword argument 'session' ``` ------------ Fix lines 783 and 784 in `rucio/core/transfer.py` replacing `session` bu `session=session`, in roder to use the wrapper defined DB session. ", "3089": "---------- In the `delete_replicas` method the archive_contents clause uses too many bind variables (700). ------------ Reduce the chunking from `100` to `30` ", "3086": "---------- Some minor improvements for multi-hop algorithm identified during testing. ", "3085": "---------- The list of RSEs to be processed by a reaper instance is evaluated once at the beginning. ------------ Reevaluate the list of RSEs regularly ", "3081": "---------- Tunable values for max_threads by host + prevent starting new threads in case there is not enough work ", "3078": "---------- ``` 2019-11-07 13:42:04,200 6560 CRITICAL Thread [1/1] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 165, in submitter retry_other_fts=retry_other_fts) File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 317, in __get_transfers failover_schemes=failover_schemes) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 1015, in get_transfer_requests_and_source_replicas new_req_id = new_req[0]['id'] IndexError: list index out of range ``` ", "3074": "---------- Currently to transfer files via Globus Online the globusLibrary.py makes explicit path references to config.yml to retrieve client_id and refresh_token. client_id = cfg['globus']['apps']['SDK Tutorial App']['client_id'] refresh_token = cfg['globus']['apps']['SDK Tutorial App']['refresh_token'] ------------ Retrieve globus_auth_app name in rucio.cfg conveyor property. client_id = cfg['globus']['apps'][GLOBUS_AUTH_APP]['client_id'] refresh_token = cfg['globus']['apps'][GLOBUS_AUTH_APP]['refresh_token'] ", "3071": "---------- In the core.replica.set_tombstone method a separate connection object is created. This has potential side effects, since it could operate in a separate transaction. If the engine object is needed, it would be better to resolve this from the session. ", "3070": "---------- check global quota not only when initializing the RSESelector but also when calling the select_rse method. ------------ update select_rse/__update_quota method in the RSESelector ", "3054": "---------- Dependency upgrade for 1.21 ", "3053": "---------- Now setting the quota is linked to the `country-xyz=admin` attribute. It would be good to have a quota_approver on the RSEs similar to rule_approver. ", "3050": "---------- When running in a container, the name and uid of the local user account are largely arbitrary and meaningless. In commonly used containers, the username is often left undefined by convention. However if the Rucio client is used , it tries to look up the name of the current user and fails if there is no name. This job is running with uid 1000.  ``` 2019-10-23 23:18:12| 3467|Mover.py | ERROR: Mover get data failed [stagein]: exception caught: 'getpwuid(): uid not found: 1000' 2019-10-23 23:18:12| 3467|Mover.py | Traceback (most recent call last): File \\\"/tmp/workdir/condorg_FgXUogoB/Panda_Pilot_3299_1571872670/PandaJob/Mover.py\\\", line 334, in get_data_new output = mover.stagein(files=files, analyjob=job.isAnalysisJob()) File \\\"/tmp/workdir/condorg_FgXUogoB/Panda_Pilot_3299_1571872670/PandaJob/movers/mover.py\\\", line 503, in stagein transferred_files, failed_transfers = self.stagein_real(files=normal_files, activity='pr', analyjob=analyjob, skip_transfer_failure=allowRemoteInputs) File \\\"/tmp/workdir/condorg_FgXUogoB/Panda_Pilot_3299_1571872670/PandaJob/movers/mover.py\\\", line 675, in stagein_real self.resolve_replicas(files) ## do populate fspec.replicas for each entry in files File \\\"/tmp/workdir/condorg_FgXUogoB/Panda_Pilot_3299_1571872670/PandaJob/movers/mover.py\\\", line 246, in resolve_replicas from rucio.client import Client File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/client/__init__.py\\\", line 19, in <module> from rucio.client.client import * # NOQA pylint: disable=wildcard-import File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/client/client.py\\\", line 29, in <module> from rucio.client.accountclient import AccountClient File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/client/accountclient.py\\\", line 32, in <module> from rucio.client.baseclient import BaseClient File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/client/baseclient.py\\\", line 103, in <module> class BaseClient(object): File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/client/baseclient.py\\\", line 108, in BaseClient TOKEN_PATH_PREFIX = get_tmp_dir() + '/.rucio_' File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/common/utils.py\\\", line 627, in get_tmp_dir user = getuser() File \\\"/usr/lib64/python2.7/getpass.py\\\", line 158, in getuser return pwd.getpwuid(os.getuid())[0] KeyError: 'getpwuid(): uid not found: 1000' ``` This causes jobs to fail when running on Kubernetes. ------------ It looks like the Rucio client is just trying to find some descriptive string to name a temporary directory: ``` File \\\"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.20.2.post1/lib/python2.7/site-packages/rucio/common/utils.py\\\", line 627, in get_tmp_dir user = getuser() ``` If a username is not defined, can it simply use the uid instead when choosing a name for the temporary directory? This would make the Rucio client more container-friendly. Thanks. ", "3039": "---------- Some `alembic` commands do not work with the context outside a callable. Moving them back into the callables. ", "3038": "---------- I had a few issues setting up the dev environment, mainly with the documentation at `etc/docker/dev/README.rst`. ------------ `docker exec -it dev_rucio_1 bin/bash` has a typo, it should be `/bin/bash` It should be mentioned that SELinux will block access to the directories mounted inside the container so it should be disabled. `logshow` doesn't work: ```[root@rucio rucio]# logshow --*- multitail 6.4.2 (C) 2003-2014 by folkert@vanheusden.com -*-- The following problem occured: ----------------------------- ttyname(0) failed. If this is a bug, please report the following information: The last system call returned: 2 which means \"No such file or directory\" Terminated ``` After this the terminal freezes up and the multitail processes have to be forcibly killed from outside the container. The tests create a lot of junk in the database (RSEs, accounts, etc), is there a way to reset the database back to a clean state? ", "3035": "---------- cf. Data carousel discussion for the full thread. In summary: It appears that the desideredLifetime / Copy_pin_lifetime values in FTS are set to -1 in FTS. FTS fallbacks to a different default values depending on the FTS settings. This pin lifetime is too small to retain files on the tape buffer which leads to have staged files deleted before being tranfered. ------------ Set the desideredLifetime / Copy_pin_lifetime to the same value. The rule lifetime is the most appropriate to express to the sites how much time the file should be kept around and get same value. ", "3032": "---------- Enlarge the clients Python 3.* tests ------------ - Move CLIENTS 3.6 test to *main* - Add CLIENTS 3.7 test to *allow failures* - Add CLIENTS 3.8 test to *allow failures* (Will probably fail terribly) ", "3028": "---------- minos-expiration doesn't handle properly ReplicaNotFound ", "3022": "---------- For the decommissioning tool, would be nice to have the ability to update the rse configuration directly from rucio-admin tool, i.e: to avoid new data be uploaded to the soon-to-be-decommissioned rse, an admin should be able to `rucio-admin rse update --rse RSE_NAME --parameter availability_write --value False` ------------- ", "3019": "---------- When `rucio-admin config get --section rse ` is used, the following error is shown: ``` Configuration not found. Details: No configuration found for section 'rse' Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers. Traceback (most recent call last): File \"/opt/rucio/bin/rucio-admin\", line 128, in new_funct return function(*args, **kwargs) File \"/opt/rucio/bin/rucio-admin\", line 714, in get_config res = client.get_config(section=args.section, option=args.option) File \"/usr/lib/python2.7/site-packages/rucio/client/configclient.py\", line 71, in get_config raise exc_cls(exc_msg) ConfigNotFound: Configuration not found. Details: No configuration found for section 'rse' ``` ------------ Include ConfigNotFound exception in the rucio-admin exception handler... ", "3013": "---------- There are a lot of different ways how dids and there scope- and name-components are passed and handled. E.g., ``` def attach_dids(self, scope, name, dids, rse=None): \"\"\" :param scope: The scope name. :param name: The data identifier name. :param dids: The content. \"\"\" def download_dids(self, items, num_threads=2, trace_custom_fields={}, traces_copy_out=None): \"\"\" :param items: List of dictionaries. Each dictionary describing an item to download. Keys: did - DID string of this file (e.g. 'scope:file.name') \"\"\" ``` In order to avoid confusion and simplify the code a unified type could be used that also provides utility functionality to work with DID types, e.g., `did_str = '%s:%s' % (scope, name)` --> `did_str = str(DIDType(scope, name))` or for an existing object `did_str = str(did)` `scope, name = split_did(did_str)` --> `scope = did.scope` `name = did.name` ------------ Implement a prototype for this type and start using it in non API breaking parts ", "3011": "Noticed this going through the requests documentation ", "3006": "---------- Some daemons doesn't have a description in argparse so documentation tool doesn't generate the description accordingly. ------------ Include the description of the daemon in the argparse description. ", "2996": "---------- Add some examples of rucio demons to be used in docker dev environment. ------------ ", "2993": "---------- For both the MySQL and PostgreSQL database types, the hashing algorithm used to partition worker thread responsibilities has the divisor of the modulo operation being incorrectly computed, erroneously using total_threads-1 instead of the raw value of total threads. This has the consequence of preventing some threads from ever being assigned work. ------------ I will provide a fix for this. I intend to create a function in db/sqla/util.py to do this, as this pattern for workload partitioning is repeated 21 times throughout the codebase. ", "2992": "---------- There's no difference between the output of `rucio list-dids empty:dataset ` and `rucio list-dids nonexisting:dataset ` Outputs should be different or at least drop a warning in the second case. ------------ Discussion needed about how to solve this without broke users. ", "2990": "---------- The current development docker-compose makes services listen to all incoming IP addresses, which is incompatible with security policies at some locations. ------------ Provide a separate docker-compose file that listens only to the loopback address. ", "2987": "---------- The current docker dev environment is missing any kind of monitoring. ------------ Extend the docker-compose file to also include an ELK stack and Grafana with some example dashboards. ", "2986": "---------- checksum after download #2399 rucio download should return a clear message if a DID doesn't exist #2661 Remove DownloadClient.download_file_from_archive and rsemanager.download #2021 ------------ ", "2980": "---------- At the moment the  need to download and upload files one at a time. The reason is that the pilot also needs to handle the traces. I would be usful if either the Rucio client can handle the traces - given the correct options - or return something that can easily be used to record the traces of a bulk upload. ------------ Either introduce code to optionally handle traces in the code of: * [ ] `rucio.client.downloadclient` * [ ] `rucio.client.uploadclient` or format the return data to allow the pilot to construct the trace information from it. ", "2970": "---------- ``` 2019-10-10 09:55:25,486 28542 CRITICAL Thread [38/50] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/reaper2.py\", line 425, in reaper delete_replicas(rse_id=rse_id, files=deleted_files) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (cx_Oracle.IntegrityError) ORA-02292: integrity constraint (ATLAS_RUCIO.SOURCES_REPLICAS_FK) violated - child record found [SQL: DELETE /*+ INDEX(REPLICAS REPLICAS_PK) */ FROM atlas_rucio.replicas WHERE atlas_rucio.replicas.rse_id = :rse_id_1 AND (atlas_rucio.replicas.scope = :scope_1 AND atlas_rucio.replicas.name = :name_1 OR atlas_rucio.replicas.scope = :scope_2 AND atlas_rucio.replicas.na me = :name_2 OR atlas_rucio.replicas.scope = :scope_3 AND atlas_rucio.replicas.name = :name_3 OR atlas_rucio.replicas.scope = :scope_4 AND atlas_rucio.replicas.name = :name_4 OR atlas_rucio.replicas.scope = :scope_5 AND atlas_rucio.replicas.name = :name_5 OR atlas_rucio. replicas.scope = :scope_6 AND atlas_rucio.replicas.name = :name_6 OR atlas_rucio.replicas.scope = :scope_7 AND atlas_rucio.replicas.name = :name_7 OR atlas_rucio.replicas.scope = :scope_8 AND atlas_rucio.replicas.name = :name_8 OR atlas_rucio.replicas.scope = :scope_9 AN D atlas_rucio.replicas.name = :name_9 OR atlas_rucio.replicas.scope = :scope_10 AND atlas_rucio.replicas.name = :name_10)] [parameters: {u'name_2': 'DAOD_STDM2.18683656._000507.pool.root.1', u'name_3': 'DAOD_EXOT8.18715924._000001.pool.root.1', u'name_1': 'DAOD_STDM2.18685082._000356.pool.root.1', u'name_6': 'DAOD_EXOT8.18716233._000001.pool.root.1', u'name_7': 'DAOD_EXOT8.18716233._000003.p ool.root.1', u'name_4': 'DAOD_EXOT8.18715924._000002.pool.root.1', u'name_5': 'DAOD_EXOT8.18715924._000003.pool.root.1', u'scope_10': 'mc16_13TeV', u'name_8': 'DAOD_EXOT8.18715667._000005.pool.root.1', u'name_9': 'DAOD_EXOT8.18716233._000002.pool.root.1', u'scope_6': 'mc 16_13TeV', u'scope_7': 'mc16_13TeV', u'scope_4': 'mc16_13TeV', u'scope_5': 'mc16_13TeV', u'scope_2': 'data16_13TeV', u'scope_3': 'mc16_13TeV', u'scope_1': 'data17_13TeV', u'name_10': 'DAOD_EXOT8.18715667._000012.pool.root.1', u'rse_id_1': <read-only buffer for 0x7fbe1d1d f618, size -1, offset 0 at 0x7fbe2c4c5570>, u'scope_8': 'mc16_13TeV', u'scope_9': 'mc16_13TeV'}] (Background on this error at:  ``` The problem is due to the relaxed constraint to keep files that are being transfered ", "2969": "---------- For example: ```Thread [17/20] : Deletion NOTFOUND of HOST.1 as IP|HOST:PORT/PATH on MWT2_DATADISK``` This is different from the previous Reaper implementation. Such traces reduce the deletion efficiency as we are currently calculating it in our monitoring. Is it intentional? ------------ Potentially do not submit traces for `SourceNotFound`. ", "2968": "---------- The traces, after the pattern matching and replacement from MONIT is applied, look like this: ```Thread [17/20] : Deletion NOTFOUND of HOST.1 as IP|HOST:PORT/PATH on MWT2_DATADISK``` The thread information is breaking the classification based on the current patterns, but it should probably not be included in the first place. ------------ Remove the thread information from the traces. ", "2960": "---------- When a replica has no replicas `AVAILABLE` and one declare one of these replicas as lost, all the requests for the other replicas are canceled but not the one of the file that was declared. ", "2957": "---------- From Diego: @dciangot Details: Account dciangot can not add file replicas on T2_IT_Legnaro_Temp ------------ Change the permission file for CMS ", "2954": "---------- In some of the permission checks we do `if _is_root(issuer) or issuer == 'ddmadmin':` which does not work anymore since we introduced the InternalAccount for the issuers. ------------ Change to `if _is_root(issuer) or issuer.external == 'ddmadmin':` or add ddmadmin to the `_is_root` check ", "2947": "---------- From Tim: > For the DOMA TPC work, we have added a workaround in the external gateways, so they strip leading /s in the path. I don't think we applied the same hack for access from the worker nodes. If necessary this might be possible, but would need careful testing with other VOs, so isn't something we can do trivially. > > Can we fix GFAL2 to not add the extra // ? For interactive use (eg. gfal-copy) we a fix was implemented some time ago, but it requires an option to enable. I'll check the details and follow up later. cf.  ", "2943": "---------- the python3 client tests should be activated on next branch ------------ ", "2931": "---------- File \"/usr/lib/python2.7/site-packages/rucio/transfertool/fts3.py\", line 221, in submit raise('No transfer id returned by %s' % self.external_host) TypeError: exceptions must be old-style classes or derived from BaseException, not unicode ------------ Needs to raise an actual exception class. ", "2925": "---------- Pilots using rucio mover do not report useful information on why a transfer failed. This is because on failure the download and upload client throw an exception with no information inside. The real reason for the error should be propagated better back to the caller of the client. ------------ The dowloadclient and uploadclient should return a dictionary of file status (containing appropriate error information) instead of raising an exception. ", "2924": "---------- The [docs at  describe using `RUCIO_ENABLE_LOGFILE` to enable logging in docker. The [starter  shows this should be `RUCIO_ENABLE_LOGS`. ------------ Update documentation ", "2917": "---------- For the add_files_to_datasets() call a list of attachments with pfns can be given to Rucio. In the case that these PFNs have different protocols, the PFN check fails, since the protocol is only initialised once (with the last protocol of the list) and assumes all PFNs have the same protocol. E.g.: `[{'scope': 'panda', 'name': 'panda.0908235529.354928.lib._19050738', 'dids': [{'adler32': '7dd69a5f', 'name': 'panda.0908235529.354928.lib._19050738.18598150846.lib.tgz', 'bytes': 72565480L, 'pfn': 'srm://f-dpm001.grid.sinica.edu.tw:8446/srm/managerv2?SFN=/dpm/grid.sinica.edu.tw/home/atlas/atlasscratchdisk/rucio/panda/23/40/panda.0908235529.354928.lib._19050738.18598150846.lib.tgz', 'meta': {'panda_id': 4478387679, 'guid': '81789F3E-572E-4DFA-BE8B-04667522031E'}, 'scope': 'panda'}, {'adler32': '048f10bf', 'name': 'panda.0908235529.354928.lib._19050738.18598150847.log.tgz', 'bytes': 72614448L, 'pfn': 'davs://f-dpm000.grid.sinica.edu.tw:443/dpm/grid.sinica.edu.tw/home/atlas/atlasscratchdisk/rucio/panda/80/c7/panda.0908235529.354928.lib._19050738.18598150847.log.tgz', 'meta': {'panda_id': 4478387679, 'guid': 'c71d6bf8-d372-437a-b0a8-efeb3dacbfa3'}, 'scope': 'panda'}], 'rse': 'TAIWAN-LCG2_SCRATCHDISK'}] ` ATLAS JIRA Link:  ------------ PFN check needs to initialise a protocol for each protocol-group of PFNs. ", "2916": "---------- Try to reactivate the test_bin_rucio.py tests in the SUITE=CLIENT ------------ remove core/db function from test_bin_rucio.py reactivate in .travis.yaml ", "2912": "---------- `declare_bad_file_replicas` is broken after the move to `InternalAccount` ", "2911": "---------- Right now there are methods to 'soft' delete an RSE and disable it but no methods to bring it back. Wouldn't it make sense to allow for an RSE to be enabled again without doing manual operations on the database? This might be useful for synchronization with CRIC  ------------ Add method that sets up deleted RSE as active again. Would a CLI command `rucio-admin rse recover` make sense? ", "2907": "---------- Once the database scheme changes for OIDC authentication are in place (see #2612 ), it will become possible to request a new (JWT) access token using a (JWT) refresh token and let Rucio continue acting on behalf of the user even after user session has expired. For this purpose each token will get a (boolean) refresh state in the DB (new column). If this state is set to True, e.g. by an anticipated long transfer operation requested by an authenticated user, a separate daemon must be watching this state change and get a new access token before the previous access token expires. Also, this daemon should be performing deletion of *all* (not only OIDC) expired Rucio tokens. ------------ - new daemon - DB scheme change (according to #2612) ", "2905": "---------- Expand Rucio support for Globus Online as a transfer tool to support sciences using Rucio as a data management tool but not using FTS to transfer files. ------------ Most of the changes are made in the conveyor daemons as well as new libraries for Globus. rucio.daemons.conveyor.submitter rucio.daemons.conveyor.common rucio.core.transfer rucio.daemons.conveyor.poller rucio.rse.protocols.globus.py rucio.transfertool.globus.py rucio.transfertool.globusLibrary.py rucio.transfertool.config.yml ", "2899": "---------- in the new request listing API from #2878 the use of sites does not work properly as the sitename gets resolved to only one RSE. ------------ resolve sitename to all RSEs with the site attribute ", "2896": "---------- I was experimenting with the Rucio-CRIC integration and while doing so, I was disabling and re enabling RSEs. What I noticed is that when the importer tries to configure an RSE that already exists on the database but is marked as 'Deleted', it will throw an error: `Details: RSE with id 'f9c7592cdc6344eabce00e8ea4d2641a' cannot be found` After further investigation I found out that when the importer sees an RSE on the json it will check if it already exists to avoid creating it again. It does so using the `rse_module.get_rse_id` method. This method though returns the id even if the RSE is already marked as deleted.  Further down on the code that the rse_id (of the deleted RSE) is referenced, of course it throws an error. ------------ Possible solution: Check if the RSE is marked as 'deleted' and enable it again before trying to modify it. ", "2895": "``` Traceback (most recent call last): File \"/opt/rucio/lib/rucio/daemons/bb8/nuclei_background_rebalance.py\", line 141, in <module> rebalance_rse(source_rse['rse'], max_bytes=available_target_rebalance_volume, dry_run=False, comment='Nuclei Background rebalancing', force_expression=destination_rse['rse']) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 356, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/daemons/bb8/common.py\", line 370, in rebalance_rse for scope, name, rule_id, rse_expression, subscription_id, bytes, length, fsize in list_rebalance_rule_candidates(rse=rse, mode=mode): File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 356, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/daemons/bb8/common.py\", line 280, in list_rebalance_rule_candidates models.DatasetLock.accessed_at).all() File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3161, in all return list(self) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3317, in __iter__ return self._execute_and_instances(context) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3342, in _execute_and_instances result = conn.execute(querycontext.statement, self._params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 988, in execute return meth(self, multiparams, params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 287, in _execute_on_connection return connection._execute_clauseelement(self, multiparams, params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1107, in _execute_clauseelement distilled_params, File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1182, in _execute_context e, util.text_type(statement), parameters, None, None File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1466, in _handle_dbapi_exception util.raise_from_cause(sqlalchemy_exception, exc_info) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py\", line 383, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1179, in _execute_context context = constructor(dialect, self, conn, *args) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py\", line 745, in _init_compiled for key in compiled_params File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py\", line 745, in <genexpr> for key in compiled_params File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/type_api.py\", line 1196, in process return impl_processor(process_param(value, dialect)) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/types.py\", line 160, in process_bind_param raise InvalidType('Cannot insert to db. Expected InternalAccount, got string type.') sqlalchemy.exc.StatementError: (rucio.common.exception.InvalidType) Provided type is considered invalid. Details: Cannot insert to db. Expected InternalAccount, got string type. [SQL: SELECT atlas_rucio.dataset_locks.scope AS atlas_rucio_dataset_lock_1, atlas_rucio.dataset_locks.name AS atlas_rucio_dataset_lock_2, atlas_rucio.rules.id AS atlas_rucio_rules_id, atlas_rucio.rules.rse_expression AS atlas_rucio_rules_rse_ex_3, atlas_rucio.rules.subscription_id AS atlas_rucio_rules_subscr_4, atlas_rucio.dids.bytes AS atlas_rucio_dids_bytes, atlas_rucio.dids.length AS atlas_rucio_dids_length, CASE WHEN (atlas_rucio.dataset_locks.length < :length_1 OR atlas_rucio.dataset_locks.length IS NULL) THEN :param_1 ELSE CAST(atlas_rucio.dataset_locks.bytes / atlas_rucio.dataset_locks.length AS INTEGER) END AS anon_1 FROM atlas_rucio.dataset_locks JOIN atlas_rucio.rules ON atlas_rucio.rules.id = atlas_rucio.dataset_locks.rule_id JOIN atlas_rucio.dids ON atlas_rucio.dataset_locks.scope = atlas_rucio.dids.scope AND atlas_rucio.dataset_locks.name = atlas_rucio.dids.name WHERE atlas_rucio.dataset_locks.rse_id = :rse_id_1 AND (atlas_rucio.rules.expires_at > :expires_at_1 OR atlas_rucio.rules.expires_at IS NULL) AND atlas_rucio.rules.created_at < :created_at_1 AND atlas_rucio.rules.account IN (:account_1, :account_2, :account_3) AND atlas_rucio.rules.state = :state_1 AND atlas_rucio.rules.did_type = :did_type_1 AND atlas_rucio.rules.copies = :copies_1 AND atlas_rucio.rules.child_rule_id IS NULL AND atlas_rucio.rules.grouping IN (:grouping_1, :grouping_2) AND atlas_rucio.dids.bytes IS NOT NULL AND atlas_rucio.dids.is_open = :is_open_1 AND atlas_rucio.dids.did_type = :did_type_2 AND CASE WHEN (atlas_rucio.dataset_locks.length < :length_2 OR atlas_rucio.dataset_locks.length IS NULL) THEN :param_2 ELSE CAST(atlas_rucio.dataset_locks.bytes / atlas_rucio.dataset_locks.length AS INTEGER) END > :param_3 AND (SELECT count(*) AS count_1 FROM atlas_rucio.dataset_locks dataset_locks_1 WHERE dataset_locks_1.scope = atlas_rucio.dataset_locks.scope AND dataset_locks_1.name = atlas_rucio.dataset_locks.name AND dataset_locks_1.rse_id = atlas_rucio.dataset_locks.rse_id) = :param_4 ORDER BY CASE WHEN (atlas_rucio.dataset_locks.length < :length_3 OR atlas_rucio.dataset_locks.length IS NULL) THEN :param_5 ELSE CAST(atlas_rucio.dataset_locks.bytes / atlas_rucio.dataset_locks.length AS INTEGER) END, atlas_rucio.dataset_locks.accessed_at] [parameters: [{}]] ``` ", "2886": "---------- The conversion of issuer into an object is not done (at least) for the delete_replication_rule : ``` def delete_replication_rule(rule_id, purge_replicas, issuer): \"\"\" Deletes a replication rule and all associated locks. :param rule_id: The id of the rule to be deleted :param issuer: The issuing account of this operation :raises: RuleNotFound, AccessDenied \"\"\" kwargs = {'rule_id': rule_id, 'purge_replicas': purge_replicas} if not has_permission(issuer=issuer, action='del_rule', kwargs=kwargs): raise AccessDenied('Account %s can not remove this replication rule.' % (issuer)) rule.delete_rule(rule_id=rule_id, purge_replicas=purge_replicas, soft=True) ``` It leads to a permission issue when trying to delete a rule. The other methods need to be checked too ", "2882": "---------- The fake storm:// protocol needs to be included in the list_replicas reply. ", "2878": "---------- We need a new API for the network people, such that they can query the queued and submitted requests for a given link. The complication comes that the network people basically only have the site name, and do not know about RSEs. ---------- Suggestion is to create a new endpoint in `web/request.py`: `/request/list` with the HTTP parameters: `request_state=[Q,S,....]` `src_rse`,`dst_rse` `src_site`,`dst_site` (where if the client uses sites instead of RSEs, then they have to be resolved to RSEs internally) ", "2877": "---------- The dark-reaper crashes with ``` 2019-08-29 13:56:57,631 18591 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/dark_reaper.py\", line 106, in reaper lfns=[{'scope': replica['scope'].external, 'name': replica['name'], 'path': replica['path']}], AttributeError: 'NoneType' object has no attribute 'external' ``` This is due to the fact that for quarantined replicas the scope (and name) can sometimes be NULL. But due to the InternalScope change where the external is forcefully read, the daemon crashes. This needs to be adapted to handle NULL scopes. ", "2874": "---------- As it is loaded as string instead of int, the method  crashes on line 738. ------------ config_get_int should be used here, however, this method needs to be expanded with the raise_Exception and default options similar to config_get and config_get_bool ", "2872": "---------- On next branch, the alembic CLI seems to not work anymore with the upgrade from version 0.9 to 0.11. ``` alembic -c etc/alembic.ini history File \"/usr/lib/python2.7/site-packages/alembic/util/langhelpers.py\", line 83, in _name_error % (name, cls.__name__) NameError: Can't invoke function 'get_context', as the proxy object has not yet been established for the Alembic 'EnvironmentContext' class. Try placing this code inside a callable. ``` ------------ Seems like all the schema declarations have to be moved to the upgrade/downgrade functions ", "2866": "---------- `[Wed Aug 28 13:22:08.394401 2019] [:error] [pid 5693:tid 139990440838912] Traceback (most recent call last): [Wed Aug 28 13:22:08.394428 2019] [:error] [pid 5693:tid 139990440838912] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/replica.py\", line 826, in GET [Wed Aug 28 13:22:08.394432 2019] [:error] [pid 5693:tid 139990440838912] for row in list_datasets_per_rse(rse=rse): [Wed Aug 28 13:22:08.394435 2019] [:error] [pid 5693:tid 139990440838912] File \"/usr/lib/python2.7/site-packages/rucio/api/replica.py\", line 323, in list_datasets_per_rse [Wed Aug 28 13:22:08.394437 2019] [:error] [pid 5693:tid 139990440838912] if 'scope' in filters: [Wed Aug 28 13:22:08.394440 2019] [:error] [pid 5693:tid 139990440838912] TypeError: argument of type 'NoneType' is not iterable` ------------ Change `filter = {}` ", "2865": "---------- We observed some cases where there a replication rules in state `REPLICATING` with no requests. Per definition of the state machine this should not be possible. The error was tracked down to the workflow of the `conveyor-finisher` which handles `FAILED` requests differently then `DONE` requests. For `DONE` requests the workflow is correct, for failed ones, in the 3rd failure attempt, the procedure is to archive the request first and at the end of the workflow update all failed replicas (and their rules). This update is done in a different transactions though. The issue is that if the update fails, e.g. due to row locks which is a valid case, there is no proper fallback to this, since the request has already been archived. ------------ The workflow needs to be changed similar to the `DONE` workflow, thus the request should only be archived AFTER the update was successful. ", "2861": "There is a problem with the way Hermes handles broker connections. In some situations at least (when used by CMS for instance) the connections are not reused and new ones are created. The old connections are then badly closed and generate errors on the messaging brokers: ``` 2019-08-22 07:29:34,575 [ActiveMQ NIO Worker 9053] WARN Transport - Transport Connection to: tcp://a.b.c.d:48452 failed: javax.net.ssl.SSLException: Inbound closed before receiving peer's close_notify: possible truncation attack? ``` Here is a log from Hermes showing that old connections are not reused: ``` 2019-08-16 13:29:17,733 8 DEBUG [broker] 0:1 - retrieved 11 messages 2019-08-16 13:29:17,733 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.1 2019-08-16 13:29:17,807 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.2 2019-08-16 13:29:17,886 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.3 2019-08-16 13:29:18,061 8 INFO [broker] 0:1 - submitted 11 messages 2019-08-16 13:29:18,061 8 DEBUG [broker] 0:1 - sleeping 1.53483104706 seconds 2019-08-16 13:29:19,607 8 DEBUG [broker] 0:1 - using: ['192.168.1.2', '192.168.1.1', '192.168.1.3'] 2019-08-16 13:29:19,743 8 DEBUG [broker] 0:1 - retrieved 13 messages 2019-08-16 13:29:19,764 8 INFO [broker] 0:1 - submitted 13 messages 2019-08-16 13:29:19,764 8 DEBUG [broker] 0:1 - sleeping 1.83455514908 seconds 2019-08-16 13:29:21,609 8 DEBUG [broker] 0:1 - using: ['192.168.1.2', '192.168.1.1', '192.168.1.3'] 2019-08-16 13:29:21,757 8 DEBUG [broker] 0:1 - retrieved 11 messages 2019-08-16 13:29:21,757 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.1 2019-08-16 13:29:21,833 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.2 2019-08-16 13:29:21,911 8 INFO [broker] 0:1 - connecting with SSL to 192.168.1.3 2019-08-16 13:29:22,266 8 INFO [broker] 0:1 - submitted 11 messages ``` ", "2858": "---------- In 1.20.4rc3, the conveyor reported a crash with the trace: ``` 2019-08-21 15:56:55,991 3229 ERROR Thread [1/31] : Failed to submit a job with error mc16_13TeV is not JSON serializable: Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/common.py\", line 125, in submit_transfer user_transfer_job=user_transfer_job) File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 90, in submit_bulk_transfers transfer_id = FTS3Transfertool(external_host=external_host).submit(files=job_files, job_params=job_params, timeout=timeout) File \"/usr/lib/python2.7/site-packages/rucio/transfertool/fts3.py\", line 186, in submit params_str = json.dumps(params_dict) File \"/usr/lib64/python2.7/json/__init__.py\", line 243, in dumps return _default_encoder.encode(obj) File \"/usr/lib64/python2.7/json/encoder.py\", line 207, in encode chunks = self.iterencode(o, _one_shot=True) File \"/usr/lib64/python2.7/json/encoder.py\", line 270, in iterencode return _iterencode(o, 0) File \"/usr/lib64/python2.7/json/encoder.py\", line 184, in default raise TypeError(repr(o) + \" is not JSON serializable\") TypeError: mc16_13TeV is not JSON serializableh ``` ------------ Utilise rucio.common.utils.APIEncoder in the fts transfertools files ", "2855": "It would be desirable to add an S3-compatible objectstore (based on  to the development `docker-compose`, in addition to the 3 Xrootd servers currently included. ", "2841": "---------- There is a bug in  (ValueError: too many values to unpack) which does not show up in the test. ------------ Fix the loop where the ValueError occurs and modify the test in tests/test_import_export.py to have distances to export ", "2840": "---------- Doing ``` $ rucio add-dataset testing:test_under2 $ rucio add-did-meta --did testing:test_under2 --key MYKEY --value MYVALUE $ rucio erase testing:test_under2 ``` Gives me the following error on undertaker 1.20.3 ``` (psycopg2.errors.ForeignKeyViolation) update or delete on table \"dids\" violates foreign key constraint \"DID_META_FK\" on table \"did_meta\" DETAIL: Key (scope, name)=(testing, test_under2) is still referenced from table \"did_meta\". [SQL: DELETE FROM dids WHERE dids.scope = %(scope_1)s AND dids.name = %(name_1)s AND (dids.did_type = %(did_type_1)s OR dids.did_type = %(did_type_2)s)] [parameters: {'name_1': 'test_under2', 'did_type_1': 'C', 'scope_1': 'testing', 'did_type_2': 'D'}] (Background on this error at:  2019-08-16 15:31:02,264 7 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/undertaker/undertaker.py\", line 109, in undertaker logging.error('Undertaker(%s): Got database error %s.', worker_number, str(error)) UnboundLocalError: local variable 'error' referenced before assignment 2019-08-16 15:31:03,276 7 INFO Undertaker(1/1): Live gives {'nr_threads': 1, 'assign_thread': 0} 2019-08-16 15:31:03,282 7 INFO Undertaker(1): Receive 1 dids to delete 2019-08-16 15:31:03,282 7 INFO Removing did testing:test_under2 (DATASET) (psycopg2.errors.ForeignKeyViolation) update or delete on table \"dids\" violates foreign key constraint \"DID_META_FK\" on table \"did_meta\" DETAIL: Key (scope, name)=(testing, test_under2) is still referenced from table \"did_meta\". [SQL: DELETE FROM dids WHERE dids.scope = %(scope_1)s AND dids.name = %(name_1)s AND (dids.did_type = %(did_type_1)s OR dids.did_type = %(did_type_2)s)] [parameters: {'name_1': 'test_under2', 'did_type_1': 'C', 'scope_1': 'testing', 'did_type_2': 'D'}] (Background on this error at:  2019-08-16 15:31:03,298 7 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/undertaker/undertaker.py\", line 109, in undertaker logging.error('Undertaker(%s): Got database error %s.', worker_number, str(error)) UnboundLocalError: local variable 'error' referenced before assignment ``` Having a look at the code I believe the UnboundLocalError has been already fixed for the next releases. The Foreign key constraint seems to be there. ------------ Delete metadata for did which is about to be deleted? I wouldn't mind having a look and doing a pr ", "2835": "---------- ------------ ", "2823": "---------- Some alembic scripts on next branch are not working in the new dev environment, because of a missing dot in the schema identifier ------------ In all the alembic files `schema = context.get_context().version_table_schema if context.get_context().version_table_schema else ''` should be `schema = context.get_context().version_table_schema + '.' if context.get_context().version_table_schema else ''` where schema is used in a string ", "2822": "---------- In the case where no backend cache is available either locally or remote for Dogpile to communicate with, Dogpile will return NO_VALUE, masking the true \"connection refused\" error due to no process actually listening at the given endpoint. ------------ Raise an exception or provide a warning when there is no caching service available at the configured endpoint provided to Dogpile. ", "2819": "---------- The current version of the Reaper 2.0 will not reap an RSE marked as greedy that does not have MinFreeSpace and MaxBeingDeletedFiles set. MinFreeSpace should not matter for a greedy RSE, and MaxBeingDeletedFiles is not used by the Reaper 2. ------------ Move the check for greedy mode in __check_rse_usage() to before the acquisition of RSE limits. ", "2812": "---------- The value specifying the address of the caching endpoint used by Dogpile should allow for values other than 127.0.0.1 so that remote caching services can be used. ", "2805": "---------- Now with new certs, FTS, and 3*xrd. ", "2802": "---------- Activate the tests in tests/test_throttler.py and tests/test_request.py when using MySQL. ------------ ", "2799": "---------- The Storm protocol in RSEManager returns the input lfn as the pfn in lfns2pfns. This causes a crash as an InternalScope is then used as a dictionary key in list_replicas. ------------ The lfns dictionary should be sanitised so that scope is returned as an external string. ", "2788": "---------- There are issues with the release candidate: - reaper2: messages fail to serialize - judge: hangs on some requests - REST list_replicas: TypeError, keys must be a string ------------ Reaper2 messages should be external representations. Judge is hanging as the string type new internaltypes use with the backend db is different. This causes the indexes to be ignored and a query can need to scan the full table. This type needs to be changed from unicode. ", "2787": "---------- Rucio 1.20.2 and earlier, when the attribute value is set to an integer (e.g. for key='tier'), the value='1' is displayed as 'True' in the client output. Similarly, '0' is shown as 'False'. ------------ According to FIXME comment in  this is a temporary work around legacy values in the ATLAS database, which should be removed after the database is fixed. ", "2777": "---------- Some usability enhancements for the development environment. ", "2772": "---------- Currently reaper is the only daemon without a specific binary, and is instead auto-generated. This causes a set of problems, especially w.r.t. dockers, and needs to be reverted. ", "2771": "---------- The following error occurs in the docker container for setting up rucio demo upon running `setup.py` (PS: The docker container was created as instructed  ``` INFO [alembic.runtime.migration] Context impl SQLiteImpl. INFO [alembic.runtime.migration] Will assume non-transactional DDL. INFO [alembic.runtime.migration] Running stamp_revision -> 2cbee484dcf9 RSE does not exist. Details: RSE 'SITE1_DISK' cannot be found (exceptions.ValueError) badly formed hexadecimal UUID string [SQL: SELECT rses.rse AS rses_rse FROM rses WHERE rses.id = ? AND rses.deleted = 0] [parameters: [{}]] Traceback (most recent call last): File \"./setup_data.py\", line 54, in <module> add_protocol('SITE1_DISK', params) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 356, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rse.py\", line 785, in add_protocol rse = get_rse_name(rse_id=rse_id, session=session, include_deleted=False) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 288, in new_funct return function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rse.py\", line 282, in get_rse_name result = query.one()[0] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3275, in one ret = self.one_or_none() File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3244, in one_or_none ret = list(self) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3317, in __iter__ return self._execute_and_instances(context) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3342, in _execute_and_instances result = conn.execute(querycontext.statement, self._params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 988, in execute return meth(self, multiparams, params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/elements.py\", line 287, in _execute_on_connection return connection._execute_clauseelement(self, multiparams, params) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1107, in _execute_clauseelement distilled_params, File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1182, in _execute_context e, util.text_type(statement), parameters, None, None File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1466, in _handle_dbapi_exception util.raise_from_cause(sqlalchemy_exception, exc_info) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/util/compat.py\", line 383, in raise_from_cause reraise(type(exception), exception, tb=exc_tb, cause=cause) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1179, in _execute_context context = constructor(dialect, self, conn, *args) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/engine/default.py\", line 719, in _init_compiled param.append(processors[key](compiled_params[key])) File \"/usr/lib64/python2.7/site-packages/sqlalchemy/sql/type_api.py\", line 1201, in process return process_param(value, dialect) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/types.py\", line 57, in process_bind_param return \"%.32x\" % uuid.UUID(value) File \"/usr/lib64/python2.7/uuid.py\", line 134, in __init__ raise ValueError('badly formed hexadecimal UUID string') sqlalchemy.exc.StatementError: (exceptions.ValueError) badly formed hexadecimal UUID string [SQL: SELECT rses.rse AS rses_rse FROM rses WHERE rses.id = ? AND rses.deleted = 0] [parameters: [{}]] ``` Apparently, when the `SITE1_DISK` folder is not created, then in `/usr/lib/python2.7/site-packages/rucio/db/sqla/types.py` ``` 56 if not isinstance(value, uuid.UUID): 57 return \"%.32x\" % uuid.UUID(value) ``` the variable `value` is the string \"SITE1_DISK\" which throws the error. However, if I run `setup.py` again, the string in `value` is changed to `db851d85729f495e84286e7a08fa5f23` but now it falls prey to `sqlalchemy.exc.IntegrityError`. ``` sqlalchemy.exc.IntegrityError: (sqlite3.IntegrityError) columns account, rse_id are not unique [SQL: INSERT INTO account_usage (account, rse_id, files, bytes, updated_at, created_at) VALUES (?, ?, ?, ?, ?, ?)] [parameters: ('root', 'db851d85729f495e84286e7a08fa5f23', 0, 0, '2019-08-01 06:15:22.608647', '2019-08-01 06:15:22.608657')] (Background on this error at:  ``` Any suggestions around this? ", "2769": "---------- From St\u00e9phane in  > Hello, > Currently, when a job tries to access a file through rucio mover (copy2scratch) or direct access through xcache, instead of using xcache (and maybe  the used protocol will depend on the prefer protocol of the remote site. Could this be corrected ? > Thanks > St\u00e9phane ", "2764": "---------- The instructions link for setting up docker goes to an [external  the documentation on setting up the environment has broken links. (link for docker compose file is missing) which becomes a bit frustrating. As an alternative, there's a setting up tutorial in the [docs  ------------ Replaced the external link  with internal reference:  in `doc/source/rucio.demo.rst` Expected Result ------------ The link for setting up Rucio demo environment should now give reference within the documentation. ", "2763": "---------- The **minos-temporary-expiration** daemon does `update_replica_states` queries with `nowait=False`, it would be better with `nowait=True`. An even better solution might be to change the core query in  to not call the `successful_transfer` lock update in case of transition from `TEMP_UNAVAILABLE` to `AVAILABLE`, since no transfers have to be marked. ", "2760": "---------- The dark reaper raises an error in 1.20.3 ------------ Fix the query in core/quarantined_replica.py -> list_rses() ", "2755": "---------- Looking at the monitor.py code, it's using pystatsd which seems to be abandoned. It appears that  is better supported and implements things like the context manager by default. In fact, this code looks almost like monitor.py Maybe best to refactor and make monitor a very thing layer over statsd. ------------ ", "2754": "---------- Hi @jamesp-epcc, @bjwhite-fnal reported an issue with the parent directory creation in gfal, related to your URL signing commit  > Looking at the code for __gfal2_copy() in rse/protocols/gfal.py I see that in Rucio 1.19.8 ctx.mkdir_rec(str(dir_name), 0o775) is still called in order to create the intermediate parent directories for every transfer, regardless of protocol. This comes with a comment in the source that # This function will be removed soon. gfal2 will create parent dir automatically. By Rucio version 1.20.1 this try block to create parent directories has been modified to only be executed for  transfers in accordance with this comment. However, as far as I can tell the most recent version of gfal2 (2.16.3) does not create the intermediate directories. This causes Rucio to instead raise a SourceNotFound exception if the parent directories of the destination do not already exist. Is this the expected behavior of Rucio 1.20.1? ``` $ rucio -v upload --rse DCACHE_BJWHITE_START --scope test $PWD/testfile2 2019-07-26 08:00:54,496 INFO Preparing upload for file testfile2 2019-07-26 08:00:54,497 DEBUG Registering file 2019-07-26 08:00:54,497 DEBUG Skipping dataset registration 2019-07-26 08:00:54,533 INFO File DID already exists 2019-07-26 08:00:54,534 DEBUG local checksum: 8dbd0963, remote checksum: 8dbd0963 2019-07-26 08:00:54,692 INFO Trying upload with gsiftp to DCACHE_BJWHITE_START 2019-07-26 08:00:54,759 ERROR Access to local destination denied. Details: Source file not found. Details: globus_ftp_client: the server responded with an error 550 File not found Completed in 0.3111 sec. ``` > Uploading a file to a currently existing directory works as expected (with gridFTP at least). You put the parent directory creation directly under the condition of `not self.renaming and dest[:5] ==  Is this because you tested and gfal2 automatically creates the parents? I was checking in the gfal2 changelog, but could not 100% confirm this. ", "2747": "---------- Conveyor submitter raised an error -> core.transfer.get_transfer_requests_and_source_replicas Light reaper raised an error -> Invalid UUID ------------ Reorder checks in get_transfer_requests_and_source_replicas Convert rse name to id in light reaper (and dark reaper) ", "2737": "---------- #2551 missed Kronos calls to touch_replica, touch_collection_replicas, and touch_dataset_locks ------------ Fix Kronos so that it converts to rse_id before making these calls. ", "2736": "---------- Currently it looks like the mode is functional, while it isn't. ------------ If used at the CLI, return NotImplemented error. ", "2732": "---------- Rucio was recently upgraded to use SQLAlchemy >=1.3.0 which changed the behavior of SQLAlchemy to raise an exception as opposed to a warning when passing strings as part of a query.  ------------ Declare text as text to keep SQLAlchemy happy. ", "2731": "---------- I'm trying to setup the dev container locally using the default docker-compose and run the tests. When I do that, it fails with the message: ``` Sync rse_repository cannot get auth_token Traceback (most recent call last): File \"tools/sync_rses.py\", line 34, in <module> c = Client() File \"/usr/lib/python2.7/site-packages/rucio/client/client.py\", line 64, in __init__ super(Client, self).__init__(rucio_host=rucio_host, auth_host=auth_host, account=account, ca_cert=ca_cert, auth_type=auth_type, creds=creds, timeout=timeout, user_agent=user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/accountclient.py\", line 44, in __init__ super(AccountClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/accountlimitclient.py\", line 40, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/metaclient.py\", line 45, in __init__ super(MetaClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/pingclient.py\", line 35, in __init__ super(PingClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/replicaclient.py\", line 45, in __init__ super(ReplicaClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/requestclient.py\", line 37, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/rseclient.py\", line 47, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/scopeclient.py\", line 46, in __init__ super(ScopeClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/didclient.py\", line 56, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/ruleclient.py\", line 39, in __init__ super(RuleClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, dq2_wrapper) File \"/usr/lib/python2.7/site-packages/rucio/client/subscriptionclient.py\", line 38, in __init__ super(SubscriptionClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/lockclient.py\", line 44, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/configclient.py\", line 44, in __init__ super(ConfigClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/touchclient.py\", line 41, in __init__ super(TouchClient, self).__init__(rucio_host, auth_host, account, ca_cert, auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/importclient.py\", line 36, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/exportclient.py\", line 35, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/credentialclient.py\", line 35, in __init__ auth_type, creds, timeout, user_agent) File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 227, in __init__ self.__authenticate() File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 642, in __authenticate self.__get_token() File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 550, in __get_token raise CannotAuthenticate('userpass authentication failed') rucio.common.exception.CannotAuthenticate: Cannot authenticate. Details: userpass authentication failed Failed to sync! ``` This may be a comparable issue to #1156, but that seems to have been solved months ago, so I am not really sure what to do here. Also, I think the tests will anyhow fail when one copies the instructions from the manual instructions on the documentation page  since that one actually sets the db, user and password to rucio while the tests assume mysql, root and secret respectively. ------------ Either fix the test or the setup so that the authentication works in the test of the dev container. Or tell me what I could be doing better. ", "2730": "---------- add import of user data from CRIC ------------ ", "2727": "---------- For the multi-VO work we will need to be able to store data on VOs ------------ Add table \"vos\" with columns: - vo (3 char tag for the vo, e.g SKA, ATL, CMS, ...) - description (full name/description of vo, e.g. \"Square Kilometre Array, ATLAS, ...) - email Add column \"vo\" to RSE table as FK into \"vos\". This will change the unique constraint on RSE table. (As 2 vos should be able to both have an RSE named the same thing) ", "2725": "---------- - catch exceptions (for example if distance gets created where the RSE got deleted before), rollback and return error message -> already done because session handling already rollbacks if there is an exception - sort protocols in order to compare the dictionaries in the tests - fix tests/test_import_export.py where destination RSE of distances are deleted before distance creation ------------ ", "2720": "---------- In each token request the authentication core removes all already expired tokens of the account. For accounts (e.g. panda, pilot) with many tokens, this currently leads to deadlocks. It is not clear if there is some query degradation, this is currently being investigated. This is the first time we are encountering this issue. To circumvent this issue, we currently disabled the token expiration.  The tokens lifetime is correctly validated, so this only leads to tokens being archived for longer. We should change this expiration to use `for_update(skip_locked=True)` or do the token cleanup entirely in a different process. ", "2717": "Currently, the S3 URL signing code uses the hostname plus bucket name to index its credentials. The RSE name should be used for this instead. This will mean adding a new RSE name parameter to the `get_signed_url` function. ", "2715": "For S3 URL signing to work with non-AWS hosts, it is necessary to pass an endpoint URL when initialising boto3. ", "2707": "---------- Reaper crashes when it tries to delete a DIDs that is a constituent of an archive ", "2702": "---------- This is a prerequisite for issue #2701. Having the deletions occur in a short amount of time might overload the Reaper nodes and the sites. ------------ Add a `--spread-period` command-line option that makes Atropos add a random factor to the lifetime of the rules. ", "2701": "---------- This was first requested during the [173rd CREM  (requires authentication). The justification is to avoid the partial existence of datasets when the Reaper decides to start deleting the replicas. ------------ Add a `--purge-replicas` command-line option that makes Atropos update the rules accordingly. ", "2691": "---------- Needed by Atropos to delete instead of secondarising when applying the Lifetime Model. ------------ Whitelist the option. ", "2681": "---------- The fts3 transfer tools in line 500 calls:  whereas the correct one would be: `policy = config_get('policy', 'permission')` ", "2678": "---------- Traceback : ``` Details: [u'(cx_Oracle.IntegrityError) ORA-00001: unique constraint (ATLAS_RUCIO.BAD_PFNS_PK) violated'] Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers. Traceback (most recent call last): File \"/usr/bin/rucio-admin\", line 129, in new_funct return function(*args, **kwargs) File \"/usr/bin/rucio-admin\", line 1034, in declare_temporary_unavailable_replicas client.add_bad_pfns(pfns=chunk, reason=args.reason, state='TEMPORARY_UNAVAILABLE', expires_at=expiration_date) File \"/usr/lib/python2.7/site-packages/rucio/client/replicaclient.py\", line 336, in add_bad_pfns raise exc_cls(exc_msg) RucioException: An unknown exception occurred. Details: [u'(cx_Oracle.IntegrityError) ORA-00001: unique constraint (ATLAS_RUCIO.BAD_PFNS_PK) violated'] ``` ", "2672": "---------- ``` 2019-06-19 14:17:43,558 32683 CRITICAL Thread [1/31] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 162, in submitter retry_other_fts=retry_other_fts) File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 286, in __get_transfers failover_schemes=failover_schemes) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 281, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (cx_Oracle.DatabaseError) ORA-00918: column ambiguously defined ``` ", "2671": "---------- CLI to declare replicas temporary unavailable ", "2664": "---------- By default, typ='colection'. This means that files are not returned. The example on this line:  is wrong thouhg. ------------ add: --filter 'type=all' ", "2661": "---------- `rucio download` should return a clear message if a DID doesn't exist ``` rucio download file.that.does.not.exist.txt 2019-06-18 10:26:46,596 INFO Processing 1 item(s) for input 2019-06-18 10:26:46,596 INFO Getting sources of DIDs 2019-06-18 10:26:49,885 INFO Using main thread to download 0 file(s) 2019-06-18 10:26:49,886 ERROR None of the requested files have been downloaded. ``` ", "2656": "---------- Wrong states on RSE avaiability state in __list_transfer_requests_and_source_replicas ", "2653": "---------- The `fts3-rest-api` package has workarounds in their setup.py which invalidates the dependency selection of M2Crypto package. Since there is now a new version of M2Crpypto this corrupts the installation of the dependency. Only possible workaround is to update the dependency version to have a consistent installation. In the medium term we should get rid of `fts3-rest-api` and `M2Crypto` dependencies. ", "2652": "---------- delete_messages crashes when a message longer than 4000 is provided. The problem is in `session.bulk_insert_mappings(MessageHistory, messages)`. The payload that is provided by Hermes comes from `retrieve_messages`, but in case the payload registered in the DB is `nolimit`, `retrieve_messages` replaces `payload with `payload_nolimit`, generating a Database exception : ``` DatabaseException: Database exception. Details: (cx_Oracle.DatabaseError) ORA-01461: can bind a LONG value only for insert into a LONG column [SQL: INSERT INTO atlas_rucio.messages_history (id, event_type, payload, updated_at, created_at) VALUES (:id, :event_type, :payload, :updated_at, :created_at)] [parameters: xxxx'}\"}] (Background on this error at:  ``` ", "2649": "---------- Use of the RUCIO_ACCOUNT environment variable should override the account specified in the Rucio configuration file. ", "2640": "---------- List of activities the conveyor-submitter should not submit. ", "2638": "---------- The xcache URL prefixes, stored in the config table, need to be populated from the agis probe. ", "2633": "---------- For the source throttling to work the src_rse_id of transfer requests needs to be filled when queuing the transfer. ", "2632": "---------- For CTA RSEs the conveyor needs to submit multihop transfers. ", "2625": "---------- Double requirement of `psyftp` blocks build of readthedocs. ", "2622": "---------- Only the final 100% notification is being sent; the intermediate ones are not; ", "2620": "---------- If the RSE is non-deterministic and a pfn is provided then the client prints `Upload with given pfn implies that no_register is True, except non-deterministic RSEs` and fails to register the replica. ", "2616": "---------- It turns out that there is a file count issue with transferring rules in Rucio. A transfer rule which is attached to a dataset does not fulfill the file transfer correctly, at least according to Rucio catalogue information itself. An example is given here: ``` (XENONnT_v1.0) Singularity osgvo-xenon:latest:~/Development> rucio list-rules x1t_SR001_170409_0531_tpc:raw ID ACCOUNT SCOPE:NAME STATE[OK/REPL/STUCK] RSE_EXPRESSION COPIES EXPIRES (UTC) CREATED (UTC) -------------------------------- ---------- ----------------------------- ---------------------- ------------------ -------- --------------- ------------------- f05c7d0c2f5340ee8bb334e9ff915d65 production x1t_SR001_170409_0531_tpc:raw REPLICATING[27/41/0] CNAF_TAPE_USERDISK 1 2018-04-12 21:39:44 64d09c9bce3d4cf993d26e37a7f8ba2e production x1t_SR001_170409_0531_tpc:raw OK[27/0/0] NIKHEF_USERDISK 1 2017-04-09 06:52:17 3d9182bfe7504a22ae71e5330abf56c9 production x1t_SR001_170409_0531_tpc:raw OK[27/0/0] UC_OSG_USERDISK 1 2017-04-09 06:52:26 ``` This example shows a wrong number of attached files which is in replication state to a certain transfer rule. The wrong file count is responsible that the data set is still in replication mode. A check of the files which are attached to the data set shows: ``` (XENONnT_v1.0) Singularity osgvo-xenon:latest:~/Development> rucio list-files x1t_SR001_170409_0531_tpc:raw +--------------------------------------------------------------------------+--------------------------------------+-------------+------------+----------+ | SCOPE:NAME | GUID | ADLER32 | FILESIZE | EVENTS | |--------------------------------------------------------------------------+--------------------------------------+-------------+------------+----------| | x1t_SR001_170409_0531_tpc:acquisition_monitor_data.pickles | 62D2D5A4-6908-4266-90D5-56F0F95AD412 | ad:f05fc324 | 53.446 MB | | | x1t_SR001_170409_0531_tpc:eventbuilder.log | 08954C29-9336-4903-BC49-E390784C150E | ad:76b8a72c | 66.256 MB | | | x1t_SR001_170409_0531_tpc:pax_info.json | 2247EB1B-1B81-47E2-B1AB-D9157AEF8EBF | ad:7121ef72 | 371.450 kB | | | x1t_SR001_170409_0531_tpc:trigger_monitor_data.zip | A40AF3EE-84A9-4F58-A9A2-4696BC045E65 | ad:2efd754a | 12.652 MB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000000000-000000999-000001000.zip | E49268B2-6A43-4D0B-B38F-46BCEDD1FAED | ad:82c7af1b | 1.694 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000001000-000001999-000001000.zip | 533612E3-0F52-421E-A55D-2484D73815C8 | ad:13501c4a | 1.717 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000002000-000002999-000001000.zip | D35606C4-D39E-42C2-A165-2CC9F1C67608 | ad:b00c665f | 1.664 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000003000-000003999-000001000.zip | DAD59BE3-9905-4621-9307-D203C22C72E0 | ad:0aa505f1 | 1.717 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000004000-000004999-000001000.zip | 5A1A14E9-8634-440D-850C-670D41170766 | ad:4378a532 | 1.682 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000005000-000005999-000001000.zip | 14221873-03AD-4828-A87D-184627C33C93 | ad:d8c0f817 | 1.613 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000006000-000006999-000001000.zip | DE9C1138-BCD5-4E69-B518-2C73F4B0EF5D | ad:b75eaa9f | 1.628 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000007000-000007999-000001000.zip | 81308FA9-454F-440D-99F6-B1DEA65912A9 | ad:354f0e0c | 1.722 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000008000-000008999-000001000.zip | 984FF39D-9771-42D9-B243-AF31480765C6 | ad:e0fd8d45 | 1.612 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000009000-000009999-000001000.zip | 7584BBDE-97E5-4B6A-86D3-0BA1E4C9EFC0 | ad:1242b0de | 1.670 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000010000-000010999-000001000.zip | 0B9AD66C-79CA-4543-8D82-DAA7EC619FBD | ad:409d237e | 1.857 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000011000-000011999-000001000.zip | 1C1965DA-00B5-404B-89F9-22E68239C7CD | ad:b94f5ef5 | 1.745 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000012000-000012999-000001000.zip | 8EBE71B0-52DE-43ED-8647-51DC6C971D75 | ad:0946c9a7 | 1.691 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000013000-000013999-000001000.zip | AABD4821-5F4F-4DD6-AE9A-7CCDBC9DD96B | ad:2f97b3d6 | 1.672 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000014000-000014999-000001000.zip | 8C855A81-B6CB-4E99-9BDA-C5AB6E436C8B | ad:af6f682a | 1.655 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000015000-000015999-000001000.zip | B3874FC3-4D01-4001-A1FA-ABEAEC93F7DA | ad:4136c0af | 1.552 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000016000-000016999-000001000.zip | 63769665-3901-49EE-A4E2-D1004D3AF820 | ad:9f09252b | 1.718 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000017000-000017999-000001000.zip | 8BC69C16-6F20-4E1A-9BB5-E597F598E95A | ad:8225a24c | 1.661 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000018000-000018999-000001000.zip | 0188C2BC-3EF5-4FB1-AE91-D05E852411D9 | ad:ff73cb94 | 1.705 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000019000-000019999-000001000.zip | 4E29DE70-924F-4958-AB42-6D226EF4BD95 | ad:e5a7ea54 | 1.776 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000020000-000020999-000001000.zip | 320BDAAF-023F-4F80-9B2A-4C4E41D7A3BD | ad:72375265 | 1.665 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000021000-000021999-000001000.zip | F38A41D8-844F-4A2D-8B86-17F070A14A73 | ad:e47a6c65 | 1.629 GB | | | x1t_SR001_170409_0531_tpc:XENON1T-8589-000022000-000022007-000000008.zip | CEFD695B-C3A5-4851-A631-0182DD5DCA9B | ad:185a2a0c | 5.582 MB | | +--------------------------------------------------------------------------+--------------------------------------+-------------+------------+----------+ ``` We have only 27 files attached to the data set. And to be 100% sure, the file replicas do also not look bad itself. Each file has three locations what means that the orginal file transfer was successful (27 files) but we have 41 \"ghost\" files which do not vanish. ``` (XENONnT_v1.0) Singularity osgvo-xenon:latest:~/Development> rucio list-file-replicas x1t_SR001_170409_0531_tpc:raw +---------------------------+------------------------------------------------+------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | SCOPE | NAME | FILESIZE | ADLER32 | RSE: REPLICA | |---------------------------+------------------------------------------------+------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | x1t_SR001_170409_0531_tpc | acquisition_monitor_data.pickles | 53.446 MB | f05fc324 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/5a/1e/acquisition_monitor_data.pickles | | x1t_SR001_170409_0531_tpc | acquisition_monitor_data.pickles | 53.446 MB | f05fc324 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/5a/1e/acquisition_monitor_data.pickles | | x1t_SR001_170409_0531_tpc | acquisition_monitor_data.pickles | 53.446 MB | f05fc324 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/5a/1e/acquisition_monitor_data.pickles | | x1t_SR001_170409_0531_tpc | eventbuilder.log | 66.256 MB | 76b8a72c | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/8c/cf/eventbuilder.log | | x1t_SR001_170409_0531_tpc | eventbuilder.log | 66.256 MB | 76b8a72c | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/8c/cf/eventbuilder.log | | x1t_SR001_170409_0531_tpc | eventbuilder.log | 66.256 MB | 76b8a72c | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/8c/cf/eventbuilder.log | | x1t_SR001_170409_0531_tpc | pax_info.json | 371.450 kB | 7121ef72 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/bc/75/pax_info.json | | x1t_SR001_170409_0531_tpc | pax_info.json | 371.450 kB | 7121ef72 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/bc/75/pax_info.json | | x1t_SR001_170409_0531_tpc | pax_info.json | 371.450 kB | 7121ef72 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/bc/75/pax_info.json | | x1t_SR001_170409_0531_tpc | trigger_monitor_data.zip | 12.652 MB | 2efd754a | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/5e/eb/trigger_monitor_data.zip | | x1t_SR001_170409_0531_tpc | trigger_monitor_data.zip | 12.652 MB | 2efd754a | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/5e/eb/trigger_monitor_data.zip | | x1t_SR001_170409_0531_tpc | trigger_monitor_data.zip | 12.652 MB | 2efd754a | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/5e/eb/trigger_monitor_data.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000000000-000000999-000001000.zip | 1.694 GB | 82c7af1b | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/84/e3/XENON1T-8589-000000000-000000999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000000000-000000999-000001000.zip | 1.694 GB | 82c7af1b | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/84/e3/XENON1T-8589-000000000-000000999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000000000-000000999-000001000.zip | 1.694 GB | 82c7af1b | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/84/e3/XENON1T-8589-000000000-000000999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000001000-000001999-000001000.zip | 1.717 GB | 13501c4a | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/e9/cc/XENON1T-8589-000001000-000001999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000001000-000001999-000001000.zip | 1.717 GB | 13501c4a | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/e9/cc/XENON1T-8589-000001000-000001999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000001000-000001999-000001000.zip | 1.717 GB | 13501c4a | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/e9/cc/XENON1T-8589-000001000-000001999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000002000-000002999-000001000.zip | 1.664 GB | b00c665f | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/60/e9/XENON1T-8589-000002000-000002999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000002000-000002999-000001000.zip | 1.664 GB | b00c665f | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/60/e9/XENON1T-8589-000002000-000002999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000002000-000002999-000001000.zip | 1.664 GB | b00c665f | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/60/e9/XENON1T-8589-000002000-000002999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000003000-000003999-000001000.zip | 1.717 GB | 0aa505f1 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/33/83/XENON1T-8589-000003000-000003999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000003000-000003999-000001000.zip | 1.717 GB | 0aa505f1 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/33/83/XENON1T-8589-000003000-000003999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000003000-000003999-000001000.zip | 1.717 GB | 0aa505f1 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/33/83/XENON1T-8589-000003000-000003999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000004000-000004999-000001000.zip | 1.682 GB | 4378a532 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/78/fd/XENON1T-8589-000004000-000004999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000004000-000004999-000001000.zip | 1.682 GB | 4378a532 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/78/fd/XENON1T-8589-000004000-000004999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000004000-000004999-000001000.zip | 1.682 GB | 4378a532 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/78/fd/XENON1T-8589-000004000-000004999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000005000-000005999-000001000.zip | 1.613 GB | d8c0f817 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/cc/78/XENON1T-8589-000005000-000005999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000005000-000005999-000001000.zip | 1.613 GB | d8c0f817 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/cc/78/XENON1T-8589-000005000-000005999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000005000-000005999-000001000.zip | 1.613 GB | d8c0f817 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/cc/78/XENON1T-8589-000005000-000005999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000006000-000006999-000001000.zip | 1.628 GB | b75eaa9f | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/ce/68/XENON1T-8589-000006000-000006999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000006000-000006999-000001000.zip | 1.628 GB | b75eaa9f | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/ce/68/XENON1T-8589-000006000-000006999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000006000-000006999-000001000.zip | 1.628 GB | b75eaa9f | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/ce/68/XENON1T-8589-000006000-000006999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000007000-000007999-000001000.zip | 1.722 GB | 354f0e0c | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/63/73/XENON1T-8589-000007000-000007999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000007000-000007999-000001000.zip | 1.722 GB | 354f0e0c | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/63/73/XENON1T-8589-000007000-000007999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000007000-000007999-000001000.zip | 1.722 GB | 354f0e0c | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/63/73/XENON1T-8589-000007000-000007999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000008000-000008999-000001000.zip | 1.612 GB | e0fd8d45 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/4c/e8/XENON1T-8589-000008000-000008999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000008000-000008999-000001000.zip | 1.612 GB | e0fd8d45 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/4c/e8/XENON1T-8589-000008000-000008999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000008000-000008999-000001000.zip | 1.612 GB | e0fd8d45 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/4c/e8/XENON1T-8589-000008000-000008999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000009000-000009999-000001000.zip | 1.670 GB | 1242b0de | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/02/e4/XENON1T-8589-000009000-000009999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000009000-000009999-000001000.zip | 1.670 GB | 1242b0de | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/02/e4/XENON1T-8589-000009000-000009999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000009000-000009999-000001000.zip | 1.670 GB | 1242b0de | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/02/e4/XENON1T-8589-000009000-000009999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000010000-000010999-000001000.zip | 1.857 GB | 409d237e | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/13/04/XENON1T-8589-000010000-000010999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000010000-000010999-000001000.zip | 1.857 GB | 409d237e | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/13/04/XENON1T-8589-000010000-000010999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000010000-000010999-000001000.zip | 1.857 GB | 409d237e | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/13/04/XENON1T-8589-000010000-000010999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000011000-000011999-000001000.zip | 1.745 GB | b94f5ef5 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/85/cf/XENON1T-8589-000011000-000011999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000011000-000011999-000001000.zip | 1.745 GB | b94f5ef5 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/85/cf/XENON1T-8589-000011000-000011999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000011000-000011999-000001000.zip | 1.745 GB | b94f5ef5 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/85/cf/XENON1T-8589-000011000-000011999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000012000-000012999-000001000.zip | 1.691 GB | 0946c9a7 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/11/7e/XENON1T-8589-000012000-000012999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000012000-000012999-000001000.zip | 1.691 GB | 0946c9a7 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/11/7e/XENON1T-8589-000012000-000012999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000012000-000012999-000001000.zip | 1.691 GB | 0946c9a7 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/11/7e/XENON1T-8589-000012000-000012999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000013000-000013999-000001000.zip | 1.672 GB | 2f97b3d6 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/fe/ad/XENON1T-8589-000013000-000013999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000013000-000013999-000001000.zip | 1.672 GB | 2f97b3d6 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/fe/ad/XENON1T-8589-000013000-000013999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000013000-000013999-000001000.zip | 1.672 GB | 2f97b3d6 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/fe/ad/XENON1T-8589-000013000-000013999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000014000-000014999-000001000.zip | 1.655 GB | af6f682a | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/cd/57/XENON1T-8589-000014000-000014999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000014000-000014999-000001000.zip | 1.655 GB | af6f682a | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/cd/57/XENON1T-8589-000014000-000014999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000014000-000014999-000001000.zip | 1.655 GB | af6f682a | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/cd/57/XENON1T-8589-000014000-000014999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000015000-000015999-000001000.zip | 1.552 GB | 4136c0af | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/55/ab/XENON1T-8589-000015000-000015999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000015000-000015999-000001000.zip | 1.552 GB | 4136c0af | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/55/ab/XENON1T-8589-000015000-000015999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000015000-000015999-000001000.zip | 1.552 GB | 4136c0af | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/55/ab/XENON1T-8589-000015000-000015999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000016000-000016999-000001000.zip | 1.718 GB | 9f09252b | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/60/2b/XENON1T-8589-000016000-000016999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000016000-000016999-000001000.zip | 1.718 GB | 9f09252b | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/60/2b/XENON1T-8589-000016000-000016999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000016000-000016999-000001000.zip | 1.718 GB | 9f09252b | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/60/2b/XENON1T-8589-000016000-000016999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000017000-000017999-000001000.zip | 1.661 GB | 8225a24c | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/0a/5a/XENON1T-8589-000017000-000017999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000017000-000017999-000001000.zip | 1.661 GB | 8225a24c | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/0a/5a/XENON1T-8589-000017000-000017999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000017000-000017999-000001000.zip | 1.661 GB | 8225a24c | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/0a/5a/XENON1T-8589-000017000-000017999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000018000-000018999-000001000.zip | 1.705 GB | ff73cb94 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/10/e3/XENON1T-8589-000018000-000018999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000018000-000018999-000001000.zip | 1.705 GB | ff73cb94 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/10/e3/XENON1T-8589-000018000-000018999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000018000-000018999-000001000.zip | 1.705 GB | ff73cb94 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/10/e3/XENON1T-8589-000018000-000018999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000019000-000019999-000001000.zip | 1.776 GB | e5a7ea54 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/c9/f0/XENON1T-8589-000019000-000019999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000019000-000019999-000001000.zip | 1.776 GB | e5a7ea54 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/c9/f0/XENON1T-8589-000019000-000019999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000019000-000019999-000001000.zip | 1.776 GB | e5a7ea54 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/c9/f0/XENON1T-8589-000019000-000019999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000020000-000020999-000001000.zip | 1.665 GB | 72375265 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/47/04/XENON1T-8589-000020000-000020999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000020000-000020999-000001000.zip | 1.665 GB | 72375265 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/47/04/XENON1T-8589-000020000-000020999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000020000-000020999-000001000.zip | 1.665 GB | 72375265 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/47/04/XENON1T-8589-000020000-000020999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000021000-000021999-000001000.zip | 1.629 GB | e47a6c65 | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/26/ce/XENON1T-8589-000021000-000021999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000021000-000021999-000001000.zip | 1.629 GB | e47a6c65 | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/26/ce/XENON1T-8589-000021000-000021999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000021000-000021999-000001000.zip | 1.629 GB | e47a6c65 | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/26/ce/XENON1T-8589-000021000-000021999-000001000.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000022000-000022007-000000008.zip | 5.582 MB | 185a2a0c | CNAF_TAPE_USERDISK: srm://storm-fe-archive.cr.cnaf.infn.it:8444/srm/managerv2?SFN=/xenonTape/rucio/x1t_SR001_170409_0531_tpc/3d/e8/XENON1T-8589-000022000-000022007-000000008.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000022000-000022007-000000008.zip | 5.582 MB | 185a2a0c | NIKHEF_USERDISK: srm://tbn18.nikhef.nl:8446/srm/managerv2?SFN=/dpm/nikhef.nl/home/xenon.biggrid.nl/rucio/x1t_SR001_170409_0531_tpc/3d/e8/XENON1T-8589-000022000-000022007-000000008.zip | | x1t_SR001_170409_0531_tpc | XENON1T-8589-000022000-000022007-000000008.zip | 5.582 MB | 185a2a0c | UC_OSG_USERDISK: gsiftp://xenon-gridftp.grid.uchicago.edu:2811/xenon/rucio/x1t_SR001_170409_0531_tpc/3d/e8/XENON1T-8589-000022000-000022007-000000008.zip | +---------------------------+------------------------------------------------+------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ``` This error is observed under Rucio server version 1.19.8. A simple solution would be to delete that particular transfer rule and create a new one after a while. But we see this error for ~13000 datasets with a certain amount of data which are already moved around. ", "2614": "---------- Rucio currently supports only x509 authentication. The need is to create a username/password authentication for the Web UI ------------ Files modified: `web/ui/common/utils.py`, `web/ui/main.py`, `web/ui/static/rucio.js`. New files: `web/ui/static/login.js`, `web/ui/templates/login.html` ", "2612": "---------- Main motivation to integrate OpenID Connect into Rucio can be found here:  As a first step, Rucio needs to be able to use OIDC protocol as authentication service. It means redirect to an Identity Provider (IdP), e.g. IAM XDC, to login there and (if for the first time) authorise Rucio to get user information, then Rucio checks if the user is known and ask the IdP for an access and refresh tokens. In order to do that, Rucio needs to: - implement a OIDC client - cache tokens for their period of validity - refresh tokens in case they expire using refresh tokens - eventually, on behalf of the user, perform the requested downstream actions using these tokens ------------ - implement a OIDC client in the authorisation scheme (running on the auth server) - implement the OpenID information to be recognised and mapped to the rucio accounts & identities - implement access token refresh mechanism - implement token validation checks (if not already part of the above) - implement token caching ", "2611": "---------- The throttler should be able to throttle based on source RSEs and not only destination RSEs. ------------ - this requires prefilling of source RSEs in the requests as the information is not available yet - and the possibility to release datasets after a defined waiting time - a way to configure the different modes to switch between source-destination and actvity-all_activities (see #2542) ", "2607": "---------- If a site is blacklisted but has a huge number of requests in state `Q` it can lead to starvation on other RSEs. ------------ Exclude RSEs blacklisted for write from the conveyor loop ", "2606": "The STOMP specification  indicates that connections should be closed using the `DISCONNECT` frame and a receipt. It seems that there are 4 places where STOMP connections are created but not closed explicitly using `stomp.py`'s `disconnect()`: `lib/rucio/core/trace.py`, `lib/rucio/core/nongrid_trace.py` and `lib/rucio/daemons/hermes/hermes.py`. FWIW, `disconnect()` is used elsewhere: `bin/rucio-cache-client`, `lib/rucio/daemons/cache/consumer.py`, `lib/rucio/daemons/conveyor/receiver.py` and `lib/rucio/daemons/tracer/kronos.py`. ", "2601": "---------- Currently the necromancer update up-to 10000000 bad replicas states in one transaction ------------ Update by chunks ", "2600": "---------- In WM, the traces are sent by pilot. We still need to populate error msgs on file level there. ------------ More possibilities, needs some discusssion with downloadclient developer. ", "2597": "---------- Now that the travis tests are using python3, there are still some problems while setting up the tests - psycopg2 throwing a type error for the binary type in the identity table while setting up the accounts - py3 incompatible helper scripts like `tools/sync_rses.py` ------------ ", "2592": "---------- Separate the `third_party_copy` activity into a `third_party_copy_read` and `third_party_copy_write` activity. Also possivle is to use Read WAN for the source. Needs discussion ", "2591": "---------- decomission mode failing after upgrading to sql alchemy, bcasue it was not returning file size properly. ------------ add file size here.  ", "2588": "---------- The tests fail because they take too much time. ------------ Try to improve the performance of the oracle container. ", "2579": "---------- `pip install -r tools/pip-requires-client -r tools/pip-requires -r tools/pip-requires-test` in the dev Dockerfile breaks on next branch with: `ERROR: Double requirement given: pysftp==0.2.9 (from -r tools/pip-requires (line 11)) (already in pysftp<0.3,>=0.2.9 (from -r tools/pip-requires-client (line 16)), name='pysftp')` ------------ ", "2576": "---------- Building and running the dev environment on master branch results in a ImportError for an oracle module cx_Oracle which should not be required as mysql is used. After installing it and running `tools/run_test_docker.sh`, I get the error message `Cannot locate a 64-bit Oracle Client library: \"libclntsh.so: cannot open shared object file: No such file or directory` ------------ ", "2560": "---------- unlike for the transfers the deletion event don't include the protocol which would be useful for monitoring. ------------ the protocol is available in a variable in the reaper loop where the messages are added. Just add it to the dictionary there. ", "2556": "---------- There are two `list_rses` requests. The RSE expressions use `1` and `0` instead of `True` and `False` for boolean values. The requests silently fail and consequently: 1. Every one hour, the elements on the DID page aren\u2019t properly populated and a refresh is required. 2. For new users, the Dataset Replicas panel will never load. ------------ Update the RSE expressions and refactor the code around them as necessary. ", "2553": "---------- For multi-vo the internal representation of scope and account will need to be different from the external representation. The translations for these should be done in a consistent way and this can be prepared beforehand. ------------ Create a new type for each of scope and account. Convert incoming account names and scopes to these types in the API directory so that multi-vo changes are more straight forward. If normal strings are used in core, raise an error. ", "2548": "---------- The docker image for the development instance is broken. The build fails at step: `RUN python setup.py develop` with `python: can't open file 'setup.py': [Errno 2] No such file or directory` ------------ ", "2545": "---------- Storm protocol has been implemented in protocols. Now, the rest of the code needs to be adapted. ------------ One known location is:  But there might be others. ", "2541": "---------- The travis tests for python3.5 uses python3 in the container but inside in the rucio container where python2 is still used. ------------ Use build arguments in the dockerfile to install rucio with python2 or python3 depending on the python version in the travis container. ", "2536": "---------- The docker demo container does not build due to a recently introduced bug in the Dockerfile. ------------ ", "2535": "---------- When other replica is not available, bb8 takes the tape one. This is not ideal since we need bb8 to be fast. ------------ 1) if rse attribute istape=True: bb8-enabled set to False 2) this is considered automatically by background modes 3) needs to be introduced in manual mode ", "2528": "---------- This is an R&D to test the virtual placement (VP) idea. Service (vpservice.cern.ch) that for a given dataset DID returns an ordered list of n sites where it should be processed (in case dataset is cache bound) or [\"other\"] if it should be processed in a regular panda way. It works responds queries in ~ 5ms. What we wanted to do is that: 1. when panda asks Rucio for list of sites having the dataset replica - Rucio in parallel to finding actual replicas also asks vpservice. If service returns \"other\", it returns to panda actual replicas. If service returns list of sites (eg. [\"AGLT2\", \"MWT2\"], it should return to panda this list and some flag eg. VP=true. 2. When panda receives reply with VP=true, it should send job to the hospital queue of the first site from the list and only if that site is in downtime send it to the second from the list. 3. Pilot will do correct thing since Rucio will give correct paths through caches. If this works out, the virtual placement can be integrated into Rucio directly. ", "2525": "---------- There is quite a lot of unused files in the repository. We should clean this up: - [x] `bin/` Should be fine - [x] `doc/` - `atlasnote/` remove - `design/` remove - [x] `etc/` - [x] Some cleanup in the folder... `netmodel`... ? - [x] `auditor` needed? - [x] `docker` - only keep demo and dev - [x] `schemas` json schemas - need to check if use - [x] `supervisord` remove or move to tools repo - [x] `web` - [x] ldap.cfg.template - This should be moved to a tools repo once we have it - [x] `lib/` ok - [x] `tools/` - [x] hadoop - remove - [x] patches/nose - remove - [x] space-usage ? - [x] travis - need - [x] `tox.ini` can go ", "2521": "---------- Files get assigned to previous replica's parent if the current lookup contains only a single replica. ", "2520": "---------- While running the up and downgrades on the newest mysql version, I found some constraints that are missing in the models.py - RULES_STATE_CHK - IDENTITIES_TYPE_CHK - REQUESTS_TYPE_CHK - REQUESTS_STATE_CHK - REPLICAS_STATE_CHK - BAD_REPLICAS_STATE_CHK ------------ ", "2519": "---------- LIGO frame files already follow an internal schema which determines their physical filename from the logical filename. We would like to be able to support this LFN2PFN algorithm in rucio. To support maximum flexibility, it seems advantageous to do so via an external module. We propose adding a new method to the `RSEDeterministicTranslation` which supports a plug-able lfn2pfn algorithm. [ I presume I don't have authority to assign this to anyone but @mlassnig said he'd take a look ] ------------ See [this  for a proposed starting point for this change. The implementation of the algorithm itself can be found  I (James) propose keeping the implementation separate from the upstream rucio repository to give flexibility for handling future exceptions and adaptation to other data types. This lfn2pfn algorithm is then used by setting the following in the server `rucio.cfg`: ``` [policy] permission = generic schema = generic lfn2pfn_algorithm_default = ligo ``` ", "2518": "---------- We currently only use this package for proxy delegation; For the other fts3 interaction we directly query the rest endpoints using python-requests. It would be good to replace the proxy delegation to work without this dependency, so we can remove it from our dependency file as well. ", "2514": "---------- If no policy section exists in the `rucio.cfg`, `rucio` exists with : ``` rucio -v list-rse-attributes TAIWAN-LCG2_TAPE-STAGING ... 2019-04-29 16:03:49,002 ERROR could not convert string to float: True 2019-04-29 16:03:49,002 ERROR Strange error: No section: 'policy' ``` ", "2511": "---------- Storageless sites still need to be integrated with Rucio and require special treatment. Right now the attributes required for many replica lookup operations are available on the RSE level only. For sites without RSEs this functionality is thus lost. ---------- Introduce a new SiteAttributes model, and extend the replica core to handle it properly. ", "2505": "---------- Today (2019/04/29) I didn't manage to build the demo anymore due to some problem with rucio builds and the current versions of libraries in pip. Specifically, it seems M2Crypto fails to builld ``` error: command 'gcc' failed with exit status 1 ---------------------------------------- \u001b[0m\u001b[91mERROR: Command \"/usr/bin/python2 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-EOjVou/M2Crypto/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-3foVk_/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-EOjVou/M2Crypto/ \u001b[0mService 'rucio' failed to build: The command '/bin/sh -c pip install rucio rucio-webui' returned a non-zero code: 1 ``` I attach the full output log of `sudo docker-compose --file etc/docker/demo/docker-compose.yml up -d` ``` Building rucio Step 1/39 : FROM rucio/rucio-systemd-cc7 ---> c221fab87755 Step 2/39 : ADD ca.repo /etc/yum.repos.d/ca.repo ---> Using cache ---> 34f6dd95d578 Step 3/39 : RUN pip install --upgrade pip ---> Using cache ---> f14e7dcc03e5 Step 4/39 : RUN pip install --upgrade setuptools ---> Using cache ---> d1aa1298bfce Step 5/39 : RUN pip install --ignore-installed ipaddress ---> Using cache ---> 843a4038c325 Step 6/39 : RUN pip install rucio rucio-webui ---> Running in 0f266cde29bb \u001b[91mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. \u001b[0mCollecting rucio Downloading  (5.7MB) Collecting rucio-webui Downloading  (460kB) Collecting SQLAlchemy==1.2.16 (from rucio) Downloading  (5.7MB) Collecting alembic==1.0.6 (from rucio) Downloading  (1.0MB) Collecting Mako==1.0.7 (from rucio) Downloading  (564kB) Collecting python-editor==1.0.3 (from rucio) Downloading  Collecting flup==1.0.3 (from rucio) Downloading  (52kB) Collecting web.py==0.39 (from rucio) Downloading  (93kB) Collecting python-memcached==1.59 (from rucio) Downloading  Collecting jsonschema==2.6.0 (from rucio) Downloading  Collecting python-dateutil==2.7.5 (from rucio) Downloading  (225kB) Collecting pysftp==0.2.9 (from rucio) Downloading  Collecting pycrypto==2.6.1 (from rucio) Downloading  (446kB) Collecting s3cmd==2.0.2 (from rucio) Downloading  (124kB) Collecting stomp.py==4.1.21 (from rucio) Downloading  (49kB) Collecting pygeoip==0.3.2 (from rucio) Downloading  Collecting geoip2==2.9.0 (from rucio) Downloading  Requirement already satisfied: ipaddress==1.0.22 in /usr/lib/python2.7/site-packages (from rucio) (1.0.22) Collecting maxminddb==1.4.1 (from rucio) Downloading  (264kB) Collecting cffi==1.11.5 (from rucio) Downloading  (407kB) Collecting pycparser==2.19 (from rucio) Downloading  (158kB) Collecting gcloud==0.18.3 (from rucio) Downloading  (454kB) Collecting googleapis-common-protos==1.5.6 (from rucio) Downloading  Collecting  (from rucio) Downloading  (218kB) Collecting pyOpenSSL==19.0.0 (from rucio) Downloading  (53kB) Collecting cryptography==2.4.2 (from rucio) Downloading  (2.1MB) Collecting oauth2client==4.1.3 (from rucio) Downloading  (98kB) Collecting protobuf==3.6.1 (from rucio) Downloading  (1.1MB) Collecting grpcio==1.18.0 (from rucio) Downloading  (10.4MB) Collecting enum34==1.1.6 (from rucio) Downloading  Collecting pyasn1==0.4.5 (from rucio) Downloading  (73kB) Collecting pyasn1-modules==0.2.3 (from rucio) Downloading  (65kB) Collecting rsa==4.0 (from rucio) Downloading  Collecting retrying==1.3.3 (from rucio) Downloading  Collecting redis==3.0.1 (from rucio) Downloading  (61kB) Collecting numpy==1.14.2 (from rucio) Downloading  (12.1MB) Collecting paramiko==2.4.2 (from rucio) Downloading  (193kB) Collecting Flask==1.0.2 (from rucio) Downloading  (91kB) Collecting idna==2.7 (from rucio) Downloading  (58kB) Collecting fts3-rest-API==3.7.1 (from rucio) Downloading  Collecting MyProxyClient==2.1.0 (from rucio) Downloading  Collecting setuptools<37.0.0,>=36.8.0 (from rucio) Downloading  (482kB) Collecting argcomplete<1.10.0,>=1.9.0 (from rucio) Downloading  Collecting requests<2.20.0,>=2.6.0 (from rucio) Downloading  (91kB) Collecting urllib3<1.24,>=1.23 (from rucio) Downloading  (133kB) Collecting dogpile.cache<0.7.0,>=0.6.5 (from rucio) Downloading  (323kB) Collecting nose>=1.3.7 (from rucio) Downloading  (154kB) Collecting tabulate<0.9.0,>=0.8.0 (from rucio) Downloading  (46kB) Collecting progressbar2<3.39.0,>=3.37.1 (from rucio) Downloading  Collecting bz2file<0.99,>=0.98 (from rucio) Downloading  Collecting python-magic<0.5.0,>=0.4.15 (from rucio) Downloading  Collecting six>=1.11.0 (from rucio) Downloading  Collecting pystatsd==0.1.10 (from rucio) Downloading  Collecting functools32==3.2.3.post2 (from rucio) Downloading  Collecting futures>=3.2.0 (from rucio) Downloading  Collecting boto>=2.48.0 (from rucio) Downloading  (1.4MB) Collecting MarkupSafe>=0.9.2 (from Mako==1.0.7->rucio) Downloading  Collecting docopt>=0.6.2 (from stomp.py==4.1.21->rucio) Downloading  Collecting google-gax<0.13dev,>=0.12.3 (from gcloud==0.18.3->rucio) Downloading  Collecting gax-google-pubsub-v1<0.9dev,>=0.8.0 (from gcloud==0.18.3->rucio) Downloading  Collecting grpc-google-pubsub-v1<0.9dev,>=0.8.0 (from gcloud==0.18.3->rucio) Downloading  Collecting gax-google-logging-v2<0.9dev,>=0.8.0 (from gcloud==0.18.3->rucio) Downloading  Collecting grpc-google-logging-v2<0.9dev,>=0.8.0 (from gcloud==0.18.3->rucio) Downloading  Collecting asn1crypto>=0.21.0 (from cryptography==2.4.2->rucio) Downloading  (101kB) Collecting pynacl>=1.0.1 (from paramiko==2.4.2->rucio) Downloading  (762kB) Collecting bcrypt>=3.1.3 (from paramiko==2.4.2->rucio) Downloading  (59kB) Collecting Werkzeug>=0.14 (from Flask==1.0.2->rucio) Downloading  (328kB) Collecting click>=5.1 (from Flask==1.0.2->rucio) Downloading  (81kB) Collecting itsdangerous>=0.24 (from Flask==1.0.2->rucio) Downloading  Collecting Jinja2>=2.10 (from Flask==1.0.2->rucio) Downloading  (124kB) Collecting M2Crypto>=0.16 (from fts3-rest-API==3.7.1->rucio) Using cached  Requirement already satisfied: pycurl>=7.19 in /usr/lib64/python2.7/site-packages (from fts3-rest-API==3.7.1->rucio) (7.19.0) Collecting chardet<3.1.0,>=3.0.2 (from requests<2.20.0,>=2.6.0->rucio) Downloading  (133kB) Collecting certifi>=2017.4.17 (from requests<2.20.0,>=2.6.0->rucio) Downloading  (158kB) Collecting python-utils>=2.3.0 (from progressbar2<3.39.0,>=3.37.1->rucio) Downloading  Collecting argparse>=1.2 (from pystatsd==0.1.10->rucio) Downloading  Collecting future>=0.15.2 (from google-gax<0.13dev,>=0.12.3->gcloud==0.18.3->rucio) Downloading  (829kB) Collecting ply==3.8 (from google-gax<0.13dev,>=0.12.3->gcloud==0.18.3->rucio) Downloading  (157kB) Requirement already satisfied: typing in /usr/lib/python2.7/site-packages (from M2Crypto>=0.16->fts3-rest-API==3.7.1->rucio) (3.6.6) Installing collected packages: SQLAlchemy, MarkupSafe, Mako, python-editor, six, python-dateutil, alembic, flup, web.py, python-memcached, functools32, jsonschema, idna, enum34, pycparser, cffi, asn1crypto, cryptography, pynacl, pyasn1, bcrypt, paramiko, pysftp, pycrypto, python-magic, s3cmd, docopt, stomp.py, pygeoip, maxminddb, chardet, urllib3, certifi, requests, geoip2,  setuptools, protobuf, googleapis-common-protos, rsa, pyasn1-modules, oauth2client, futures, grpcio, future, ply, google-gax, grpc-google-pubsub-v1, gax-google-pubsub-v1, grpc-google-logging-v2, gax-google-logging-v2, gcloud, pyOpenSSL, retrying, redis, numpy, Werkzeug, click, itsdangerous, Jinja2, Flask, M2Crypto, fts3-rest-API, MyProxyClient, argcomplete, dogpile.cache, nose, tabulate, python-utils, progressbar2, bz2file, argparse, pystatsd, boto, rucio, rucio-webui Running setup.py install for SQLAlchemy: started Running setup.py install for SQLAlchemy: finished with status 'done' Running setup.py install for Mako: started Running setup.py install for Mako: finished with status 'done' Running setup.py install for python-editor: started Running setup.py install for python-editor: finished with status 'done' Running setup.py install for alembic: started Running setup.py install for alembic: finished with status 'done' Running setup.py install for flup: started Running setup.py install for flup: finished with status 'done' Running setup.py install for web.py: started Running setup.py install for web.py: finished with status 'done' Running setup.py install for functools32: started Running setup.py install for functools32: finished with status 'done' Running setup.py install for pycparser: started Running setup.py install for pycparser: finished with status 'done' Running setup.py install for pysftp: started Running setup.py install for pysftp: finished with status 'done' Running setup.py install for pycrypto: started Running setup.py install for pycrypto: finished with status 'done' Running setup.py install for s3cmd: started Running setup.py install for s3cmd: finished with status 'done' Running setup.py install for docopt: started Running setup.py install for docopt: finished with status 'done' Running setup.py install for stomp.py: started Running setup.py install for stomp.py: finished with status 'done' Running setup.py install for maxminddb: started Running setup.py install for maxminddb: finished with status 'done' Found existing installation: chardet 2.2.1 Uninstalling chardet-2.2.1: Successfully uninstalled chardet-2.2.1 Running setup.py install for  started Running setup.py install for  finished with status 'done' Found existing installation: setuptools 41.0.1 Uninstalling setuptools-41.0.1: Successfully uninstalled setuptools-41.0.1 Running setup.py install for googleapis-common-protos: started Running setup.py install for googleapis-common-protos: finished with status 'done' Running setup.py install for future: started Running setup.py install for future: finished with status 'done' Running setup.py install for ply: started Running setup.py install for ply: finished with status 'done' Running setup.py install for google-gax: started Running setup.py install for google-gax: finished with status 'done' Running setup.py install for grpc-google-pubsub-v1: started Running setup.py install for grpc-google-pubsub-v1: finished with status 'done' Running setup.py install for gax-google-pubsub-v1: started Running setup.py install for gax-google-pubsub-v1: finished with status 'done' Running setup.py install for grpc-google-logging-v2: started Running setup.py install for grpc-google-logging-v2: finished with status 'done' Running setup.py install for gax-google-logging-v2: started Running setup.py install for gax-google-logging-v2: finished with status 'done' Running setup.py install for gcloud: started Running setup.py install for gcloud: finished with status 'done' Running setup.py install for retrying: started Running setup.py install for retrying: finished with status 'done' Running setup.py install for M2Crypto: started Running setup.py install for M2Crypto: finished with status 'error' \u001b[91m ERROR: Complete output from command /usr/bin/python2 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-EOjVou/M2Crypto/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-3foVk_/install-record.txt --single-version-externally-managed --compile: \u001b[0m\u001b[91m ERROR: running install running build running build_py copying  -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/EVP.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/threading.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/Rand.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/ASN1.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/ftpslib.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/Err.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/X509.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/DH.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/EC.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/DSA.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/util.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/BIO.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/Engine.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/m2crypto.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/m2urllib2.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/m2.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/six.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/BN.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/m2urllib.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/AuthCookie.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/__init__.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/m2xmlrpclib.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/callback.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/RC4.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/SMIME.py -> build/lib.linux-x86_64-2.7/M2Crypto copying M2Crypto/RSA.py -> build/lib.linux-x86_64-2.7/M2Crypto creating build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/Connection.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/Checker.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/Context.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/cb.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/timeout.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/ssl_dispatcher.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/__init__.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/SSLServer.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/Cipher.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/TwistedProtocolWrapper.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL copying M2Crypto/SSL/Session.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL running build_ext building 'M2Crypto._m2crypto' extension creating build/temp.linux-x86_64-2.7 creating build/temp.linux-x86_64-2.7/SWIG gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -I/usr/include/python2.7 -I/tmp/pip-install-EOjVou/M2Crypto/SWIG -c SWIG/_m2crypto_wrap.c -o build/temp.linux-x86_64-2.7/SWIG/_m2crypto_wrap.o -Wno-deprecated-declarations -DTHREADING SWIG/_m2crypto_wrap.c:4754:0: warning: \"PyUnicode_FromString\" redefined [enabled by default] #define PyUnicode_FromString(x) PyString_FromString(x) ^ In file included from /usr/include/python2.7/Python.h:85:0, from SWIG/_m2crypto_wrap.c:151: /usr/include/python2.7/unicodeobject.h:281:0: note: this is the location of the previous definition # define PyUnicode_FromString PyUnicodeUCS4_FromString ^ SWIG/_m2crypto_wrap.c:4755:0: warning: \"PyUnicode_Format\" redefined [enabled by default] #define PyUnicode_Format(x, y) PyString_Format(x, y) ^ In file included from /usr/include/python2.7/Python.h:85:0, from SWIG/_m2crypto_wrap.c:151: /usr/include/python2.7/unicodeobject.h:275:0: note: this is the location of the previous definition # define PyUnicode_Format PyUnicodeUCS4_Format ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_num': SWIG/_m2crypto_wrap.c:9882:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9882:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9882:26: note: each undeclared identifier is reported only once for each function it appears in SWIG/_m2crypto_wrap.c:9882:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9893:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:9894:3: warning: implicit declaration of function 'OPENSSL_sk_num' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_num((struct stack_st const *)arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_value': SWIG/_m2crypto_wrap.c:9904:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9904:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9904:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9919:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:9925:3: warning: implicit declaration of function 'OPENSSL_sk_value' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_value((struct stack_st const *)arg1,arg2); ^ SWIG/_m2crypto_wrap.c:9925:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_value((struct stack_st const *)arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_set': SWIG/_m2crypto_wrap.c:9935:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9935:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9935:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:9953:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:9963:3: warning: implicit declaration of function 'OPENSSL_sk_set' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_set(arg1,arg2,(void const *)arg3); ^ SWIG/_m2crypto_wrap.c:9963:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_set(arg1,arg2,(void const *)arg3); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_new': SWIG/_m2crypto_wrap.c:9973:3: error: unknown type name 'OPENSSL_sk_compfunc' OPENSSL_sk_compfunc arg1 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:9973:31: error: 'OPENSSL_sk_compfunc' undeclared (first use in this function) OPENSSL_sk_compfunc arg1 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:9973:52: error: expected ',' or ';' before numeric constant OPENSSL_sk_compfunc arg1 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:9975:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *result = 0 ; ^ SWIG/_m2crypto_wrap.c:9984:13: error: 'OPENSSL_STACK' undeclared (first use in this function) result = (OPENSSL_STACK *)OPENSSL_sk_new(arg1); ^ SWIG/_m2crypto_wrap.c:9984:28: error: expected expression before ')' token result = (OPENSSL_STACK *)OPENSSL_sk_new(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_new_null': SWIG/_m2crypto_wrap.c:9994:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *result = 0 ; ^ SWIG/_m2crypto_wrap.c:9997:13: error: 'OPENSSL_STACK' undeclared (first use in this function) result = (OPENSSL_STACK *)OPENSSL_sk_new_null(); ^ SWIG/_m2crypto_wrap.c:9997:28: error: expected expression before ')' token result = (OPENSSL_STACK *)OPENSSL_sk_new_null(); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_free': SWIG/_m2crypto_wrap.c:10007:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10007:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10007:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10017:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10018:3: warning: implicit declaration of function 'OPENSSL_sk_free' [-Wimplicit-function-declaration] OPENSSL_sk_free(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_pop_free': SWIG/_m2crypto_wrap.c:10028:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10028:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10028:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10040:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10047:3: warning: implicit declaration of function 'OPENSSL_sk_pop_free' [-Wimplicit-function-declaration] OPENSSL_sk_pop_free(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_deep_copy': SWIG/_m2crypto_wrap.c:10057:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10057:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10057:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10058:3: error: unknown type name 'OPENSSL_sk_copyfunc' OPENSSL_sk_copyfunc arg2 = (OPENSSL_sk_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10058:31: error: 'OPENSSL_sk_copyfunc' undeclared (first use in this function) OPENSSL_sk_copyfunc arg2 = (OPENSSL_sk_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10058:52: error: expected ',' or ';' before numeric constant OPENSSL_sk_copyfunc arg2 = (OPENSSL_sk_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10059:3: error: unknown type name 'OPENSSL_sk_freefunc' OPENSSL_sk_freefunc arg3 = (OPENSSL_sk_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10059:31: error: 'OPENSSL_sk_freefunc' undeclared (first use in this function) OPENSSL_sk_freefunc arg3 = (OPENSSL_sk_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10059:52: error: expected ',' or ';' before numeric constant OPENSSL_sk_freefunc arg3 = (OPENSSL_sk_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10065:18: error: 'result' undeclared (first use in this function) OPENSSL_STACK *result = 0 ; ^ SWIG/_m2crypto_wrap.c:10072:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10085:28: error: expected expression before ')' token result = (OPENSSL_STACK *)OPENSSL_sk_deep_copy((struct stack_st const *)arg1,arg2,arg3); ^ SWIG/_m2crypto_wrap.c:10057:18: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_insert': SWIG/_m2crypto_wrap.c:10095:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10095:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10095:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10113:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10123:3: warning: implicit declaration of function 'OPENSSL_sk_insert' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_insert(arg1,(void const *)arg2,arg3); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_delete': SWIG/_m2crypto_wrap.c:10133:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10133:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10133:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10148:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10154:3: warning: implicit declaration of function 'OPENSSL_sk_delete' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_delete(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:10154:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_delete(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_delete_ptr': SWIG/_m2crypto_wrap.c:10164:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10164:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10164:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10178:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10183:3: warning: implicit declaration of function 'OPENSSL_sk_delete_ptr' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_delete_ptr(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c:10183:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_delete_ptr(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_find': SWIG/_m2crypto_wrap.c:10193:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10193:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10193:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10207:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10212:3: warning: implicit declaration of function 'OPENSSL_sk_find' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_find(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_find_ex': SWIG/_m2crypto_wrap.c:10222:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10222:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10222:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10236:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10241:3: warning: implicit declaration of function 'OPENSSL_sk_find_ex' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_find_ex(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_push': SWIG/_m2crypto_wrap.c:10251:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10251:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10251:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10265:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10270:3: warning: implicit declaration of function 'OPENSSL_sk_push' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_push(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_unshift': SWIG/_m2crypto_wrap.c:10280:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10280:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10280:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10294:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10299:3: warning: implicit declaration of function 'OPENSSL_sk_unshift' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_unshift(arg1,(void const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_shift': SWIG/_m2crypto_wrap.c:10309:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10309:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10309:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10320:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10321:3: warning: implicit declaration of function 'OPENSSL_sk_shift' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_shift(arg1); ^ SWIG/_m2crypto_wrap.c:10321:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_shift(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_pop': SWIG/_m2crypto_wrap.c:10331:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10331:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10331:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10342:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10343:3: warning: implicit declaration of function 'OPENSSL_sk_pop' [-Wimplicit-function-declaration] result = (void *)OPENSSL_sk_pop(arg1); ^ SWIG/_m2crypto_wrap.c:10343:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (void *)OPENSSL_sk_pop(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_zero': SWIG/_m2crypto_wrap.c:10353:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10353:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10353:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10363:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10364:3: warning: implicit declaration of function 'OPENSSL_sk_zero' [-Wimplicit-function-declaration] OPENSSL_sk_zero(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_set_cmp_func': SWIG/_m2crypto_wrap.c:10374:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10374:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10374:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10375:3: error: unknown type name 'OPENSSL_sk_compfunc' OPENSSL_sk_compfunc arg2 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10375:31: error: 'OPENSSL_sk_compfunc' undeclared (first use in this function) OPENSSL_sk_compfunc arg2 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10375:52: error: expected ',' or ';' before numeric constant OPENSSL_sk_compfunc arg2 = (OPENSSL_sk_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10380:23: error: expected ';' before 'result' OPENSSL_sk_compfunc result; ^ SWIG/_m2crypto_wrap.c:10387:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10394:3: error: 'result' undeclared (first use in this function) result = (OPENSSL_sk_compfunc)OPENSSL_sk_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:10394:33: error: expected ';' before 'OPENSSL_sk_set_cmp_func' result = (OPENSSL_sk_compfunc)OPENSSL_sk_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:10374:18: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_dup': SWIG/_m2crypto_wrap.c:10404:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10404:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10404:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10408:18: error: 'result' undeclared (first use in this function) OPENSSL_STACK *result = 0 ; ^ SWIG/_m2crypto_wrap.c:10415:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10416:28: error: expected expression before ')' token result = (OPENSSL_STACK *)OPENSSL_sk_dup((struct stack_st const *)arg1); ^ SWIG/_m2crypto_wrap.c:10404:18: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_sort': SWIG/_m2crypto_wrap.c:10426:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10426:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10426:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10436:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10437:3: warning: implicit declaration of function 'OPENSSL_sk_sort' [-Wimplicit-function-declaration] OPENSSL_sk_sort(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_OPENSSL_sk_is_sorted': SWIG/_m2crypto_wrap.c:10447:3: error: unknown type name 'OPENSSL_STACK' OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10447:26: error: 'OPENSSL_STACK' undeclared (first use in this function) OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10447:41: error: expected expression before ')' token OPENSSL_STACK *arg1 = (OPENSSL_STACK *) 0 ; ^ SWIG/_m2crypto_wrap.c:10458:26: error: expected expression before ')' token arg1 = (OPENSSL_STACK *)(argp1); ^ SWIG/_m2crypto_wrap.c:10459:3: warning: implicit declaration of function 'OPENSSL_sk_is_sorted' [-Wimplicit-function-declaration] result = (int)OPENSSL_sk_is_sorted((struct stack_st const *)arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_STRING_new': SWIG/_m2crypto_wrap.c:10522:3: error: unknown type name 'sk_OPENSSL_STRING_compfunc' sk_OPENSSL_STRING_compfunc arg1 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10522:38: error: 'sk_OPENSSL_STRING_compfunc' undeclared (first use in this function) sk_OPENSSL_STRING_compfunc arg1 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10522:66: error: expected ',' or ';' before numeric constant sk_OPENSSL_STRING_compfunc arg1 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_STRING_pop_free': SWIG/_m2crypto_wrap.c:10776:3: error: unknown type name 'sk_OPENSSL_STRING_freefunc' sk_OPENSSL_STRING_freefunc arg2 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10776:38: error: 'sk_OPENSSL_STRING_freefunc' undeclared (first use in this function) sk_OPENSSL_STRING_freefunc arg2 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:10776:66: error: expected ',' or ';' before numeric constant sk_OPENSSL_STRING_freefunc arg2 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_STRING_find_ex': SWIG/_m2crypto_wrap.c:10946:3: warning: implicit declaration of function 'CHECKED_CONST_PTR_OF' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_STRING_find_ex(arg1,arg2); ^ In file included from /usr/include/openssl/crypto.h:129:0, from /usr/include/openssl/bio.h:69, from /usr/include/openssl/err.h:124, from SWIG/_m2crypto_wrap.c:3854: SWIG/_m2crypto_wrap.c:10946:17: error: expected expression before 'struct' result = (int)sk_OPENSSL_STRING_find_ex(arg1,arg2); ^ In file included from /usr/include/openssl/crypto.h:129:0, from /usr/include/openssl/bio.h:69, from /usr/include/openssl/err.h:124, from SWIG/_m2crypto_wrap.c:3854: SWIG/_m2crypto_wrap.c:10946:17: error: expected expression before 'char' result = (int)sk_OPENSSL_STRING_find_ex(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:10925:9: warning: variable 'arg2' set but not used [-Wunused-but-set-variable] char *arg2 = (char *) 0 ; ^ SWIG/_m2crypto_wrap.c:10924:35: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] struct stack_st_OPENSSL_STRING *arg1 = (struct stack_st_OPENSSL_STRING *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_STRING_deep_copy': SWIG/_m2crypto_wrap.c:11024:3: error: unknown type name 'sk_OPENSSL_STRING_copyfunc' sk_OPENSSL_STRING_copyfunc arg2 = (sk_OPENSSL_STRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11024:38: error: 'sk_OPENSSL_STRING_copyfunc' undeclared (first use in this function) sk_OPENSSL_STRING_copyfunc arg2 = (sk_OPENSSL_STRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11024:66: error: expected ',' or ';' before numeric constant sk_OPENSSL_STRING_copyfunc arg2 = (sk_OPENSSL_STRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11025:3: error: unknown type name 'sk_OPENSSL_STRING_freefunc' sk_OPENSSL_STRING_freefunc arg3 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11025:38: error: 'sk_OPENSSL_STRING_freefunc' undeclared (first use in this function) sk_OPENSSL_STRING_freefunc arg3 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11025:66: error: expected ',' or ';' before numeric constant sk_OPENSSL_STRING_freefunc arg3 = (sk_OPENSSL_STRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_STRING_set_cmp_func': SWIG/_m2crypto_wrap.c:11062:3: error\u001b[0m\u001b[91m: unknown type name 'sk_OPENSSL_STRING_compfunc' sk_OPENSSL_STRING_compfunc arg2 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11062:38: error: 'sk_OPENSSL_STRING_compfunc' undeclared (first use in this function) sk_OPENSSL_STRING_compfunc arg2 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11062:66: error: expected ',' or ';' before numeric constant sk_OPENSSL_STRING_compfunc arg2 = (sk_OPENSSL_STRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11067:30: error: expected ';' before 'result' sk_OPENSSL_STRING_compfunc result; ^ SWIG/_m2crypto_wrap.c:11081:3: error: 'result' undeclared (first use in this function) result = (sk_OPENSSL_STRING_compfunc)sk_OPENSSL_STRING_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_num': SWIG/_m2crypto_wrap.c:11103:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_num' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_num((struct stack_st_OPENSSL_CSTRING const *)arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_value': SWIG/_m2crypto_wrap.c:11134:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_value' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_value((struct stack_st_OPENSSL_CSTRING const *)arg1,arg2); ^ SWIG/_m2crypto_wrap.c:11134:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_value((struct stack_st_OPENSSL_CSTRING const *)arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_new': SWIG/_m2crypto_wrap.c:11144:3: error: unknown type name 'sk_OPENSSL_CSTRING_compfunc' sk_OPENSSL_CSTRING_compfunc arg1 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11144:39: error: 'sk_OPENSSL_CSTRING_compfunc' undeclared (first use in this function) sk_OPENSSL_CSTRING_compfunc arg1 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11144:68: error: expected ',' or ';' before numeric constant sk_OPENSSL_CSTRING_compfunc arg1 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11155:20: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_new' [-Wimplicit-function-declaration] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_new(arg1); ^ SWIG/_m2crypto_wrap.c:11155:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_new(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_new_null': SWIG/_m2crypto_wrap.c:11168:20: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_new_null' [-Wimplicit-function-declaration] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_new_null(); ^ SWIG/_m2crypto_wrap.c:11168:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_new_null(); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_free': SWIG/_m2crypto_wrap.c:11189:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_free' [-Wimplicit-function-declaration] sk_OPENSSL_CSTRING_free(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_zero': SWIG/_m2crypto_wrap.c:11210:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_zero' [-Wimplicit-function-declaration] sk_OPENSSL_CSTRING_zero(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_delete': SWIG/_m2crypto_wrap.c:11241:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_delete' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_delete(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:11241:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_delete(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_delete_ptr': SWIG/_m2crypto_wrap.c:11273:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_delete_ptr' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_delete_ptr(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c:11273:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_delete_ptr(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_push': SWIG/_m2crypto_wrap.c:11307:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_push' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_push(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_unshift': SWIG/_m2crypto_wrap.c:11341:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_unshift' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_unshift(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_pop': SWIG/_m2crypto_wrap.c:11365:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_pop' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_pop(arg1); ^ SWIG/_m2crypto_wrap.c:11365:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_pop(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_shift': SWIG/_m2crypto_wrap.c:11387:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_shift' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_shift(arg1); ^ SWIG/_m2crypto_wrap.c:11387:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_shift(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_pop_free': SWIG/_m2crypto_wrap.c:11398:3: error: unknown type name 'sk_OPENSSL_CSTRING_freefunc' sk_OPENSSL_CSTRING_freefunc arg2 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11398:39: error: 'sk_OPENSSL_CSTRING_freefunc' undeclared (first use in this function) sk_OPENSSL_CSTRING_freefunc arg2 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11398:68: error: expected ',' or ';' before numeric constant sk_OPENSSL_CSTRING_freefunc arg2 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11416:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_pop_free' [-Wimplicit-function-declaration] sk_OPENSSL_CSTRING_pop_free(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_insert': SWIG/_m2crypto_wrap.c:11457:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_insert' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_insert(arg1,(char const *)arg2,arg3); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_set': SWIG/_m2crypto_wrap.c:11500:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_set' [-Wimplicit-function-declaration] result = (char *)sk_OPENSSL_CSTRING_set(arg1,arg2,(char const *)arg3); ^ SWIG/_m2crypto_wrap.c:11500:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (char *)sk_OPENSSL_CSTRING_set(arg1,arg2,(char const *)arg3); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_find': SWIG/_m2crypto_wrap.c:11534:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_find' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_find(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_find_ex': SWIG/_m2crypto_wrap.c:11568:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_find_ex' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_find_ex(arg1,(char const *)arg2); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_sort': SWIG/_m2crypto_wrap.c:11591:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_sort' [-Wimplicit-function-declaration] sk_OPENSSL_CSTRING_sort(arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_is_sorted': SWIG/_m2crypto_wrap.c:11613:3: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_is_sorted' [-Wimplicit-function-declaration] result = (int)sk_OPENSSL_CSTRING_is_sorted((struct stack_st_OPENSSL_CSTRING const *)arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_dup': SWIG/_m2crypto_wrap.c:11635:20: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_dup' [-Wimplicit-function-declaration] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_dup((struct stack_st_OPENSSL_CSTRING const *)arg1); ^ SWIG/_m2crypto_wrap.c:11635:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_dup((struct stack_st_OPENSSL_CSTRING const *)arg1); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_deep_copy': SWIG/_m2crypto_wrap.c:11646:3: error: unknown type name 'sk_OPENSSL_CSTRING_copyfunc' sk_OPENSSL_CSTRING_copyfunc arg2 = (sk_OPENSSL_CSTRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11646:39: error: 'sk_OPENSSL_CSTRING_copyfunc' undeclared (first use in this function) sk_OPENSSL_CSTRING_copyfunc arg2 = (sk_OPENSSL_CSTRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11646:68: error: expected ',' or ';' before numeric constant sk_OPENSSL_CSTRING_copyfunc arg2 = (sk_OPENSSL_CSTRING_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11647:3: error: unknown type name 'sk_OPENSSL_CSTRING_freefunc' sk_OPENSSL_CSTRING_freefunc arg3 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11647:39: error: 'sk_OPENSSL_CSTRING_freefunc' undeclared (first use in this function) sk_OPENSSL_CSTRING_freefunc arg3 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11647:68: error: expected ',' or ';' before numeric constant sk_OPENSSL_CSTRING_freefunc arg3 = (sk_OPENSSL_CSTRING_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11673:20: warning: implicit declaration of function 'sk_OPENSSL_CSTRING_deep_copy' [-Wimplicit-function-declaration] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_deep_copy((struct stack_st_OPENSSL_CSTRING const *)arg1,arg2,arg3); ^ SWIG/_m2crypto_wrap.c:11673:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast] result = (struct stack_st_OPENSSL_CSTRING *)sk_OPENSSL_CSTRING_deep_copy((struct stack_st_OPENSSL_CSTRING const *)arg1,arg2,arg3); ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_CSTRING_set_cmp_func': SWIG/_m2crypto_wrap.c:11684:3: error: unknown type name 'sk_OPENSSL_CSTRING_compfunc' sk_OPENSSL_CSTRING_compfunc arg2 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11684:39: error: 'sk_OPENSSL_CSTRING_compfunc' undeclared (first use in this function) sk_OPENSSL_CSTRING_compfunc arg2 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11684:68: error: expected ',' or ';' before numeric constant sk_OPENSSL_CSTRING_compfunc arg2 = (sk_OPENSSL_CSTRING_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11689:31: error: expected ';' before 'result' sk_OPENSSL_CSTRING_compfunc result; ^ SWIG/_m2crypto_wrap.c:11703:3: error: 'result' undeclared (first use in this function) result = (sk_OPENSSL_CSTRING_compfunc)sk_OPENSSL_CSTRING_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:11703:41: error: expected ';' before 'sk_OPENSSL_CSTRING_set_cmp_func' result = (sk_OPENSSL_CSTRING_compfunc)sk_OPENSSL_CSTRING_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:11683:36: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] struct stack_st_OPENSSL_CSTRING *arg1 = (struct stack_st_OPENSSL_CSTRING *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_BLOCK_new': SWIG/_m2crypto_wrap.c:11766:3: error: unknown type name 'sk_OPENSSL_BLOCK_compfunc' sk_OPENSSL_BLOCK_compfunc arg1 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11766:37: error: 'sk_OPENSSL_BLOCK_compfunc' undeclared (first use in this function) sk_OPENSSL_BLOCK_compfunc arg1 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:11766:64: error: expected ',' or ';' before numeric constant sk_OPENSSL_BLOCK_compfunc arg1 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_BLOCK_pop_free': SWIG/_m2crypto_wrap.c:12005:3: error: unknown type name 'sk_OPENSSL_BLOCK_freefunc' sk_OPENSSL_BLOCK_freefunc arg2 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12005:37: error: 'sk_OPENSSL_BLOCK_freefunc' undeclared (first use in this function) sk_OPENSSL_BLOCK_freefunc arg2 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12005:64: error: expected ',' or ';' before numeric constant sk_OPENSSL_BLOCK_freefunc arg2 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ In file included from /usr/include/openssl/crypto.h:129:0, from /usr/include/openssl/bio.h:69, from /usr/include/openssl/err.h:124, from SWIG/_m2crypto_wrap.c:3854: SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_BLOCK_find_ex': SWIG/_m2crypto_wrap.c:12157:17: error: expected expression before 'struct' result = (int)sk_OPENSSL_BLOCK_find_ex(arg1,arg2); ^ In file included from /usr/include/openssl/crypto.h:129:0, from /usr/include/openssl/bio.h:69, from /usr/include/openssl/err.h:124, from SWIG/_m2crypto_wrap.c:3854: SWIG/_m2crypto_wrap.c:12157:17: error: expected expression before 'void' result = (int)sk_OPENSSL_BLOCK_find_ex(arg1,arg2); ^ SWIG/_m2crypto_wrap.c:12138:34: warning: variable 'arg1' set but not used [-Wunused-but-set-variable] struct stack_st_OPENSSL_BLOCK *arg1 = (struct stack_st_OPENSSL_BLOCK *) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_BLOCK_deep_copy': SWIG/_m2crypto_wrap.c:12233:3: error: unknown type name 'sk_OPENSSL_BLOCK_copyfunc' sk_OPENSSL_BLOCK_copyfunc arg2 = (sk_OPENSSL_BLOCK_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12233:37: error: 'sk_OPENSSL_BLOCK_copyfunc' undeclared (first use in this function) sk_OPENSSL_BLOCK_copyfunc arg2 = (sk_OPENSSL_BLOCK_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12233:64: error: expected ',' or ';' before numeric constant sk_OPENSSL_BLOCK_copyfunc arg2 = (sk_OPENSSL_BLOCK_copyfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12234:3: error: unknown type name 'sk_OPENSSL_BLOCK_freefunc' sk_OPENSSL_BLOCK_freefunc arg3 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12234:37: error: 'sk_OPENSSL_BLOCK_freefunc' undeclared (first use in this function) sk_OPENSSL_BLOCK_freefunc arg3 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12234:64: error: expected ',' or ';' before numeric constant sk_OPENSSL_BLOCK_freefunc arg3 = (sk_OPENSSL_BLOCK_freefunc) 0 ; ^ SWIG/_m2crypto_wrap.c: In function '_wrap_sk_OPENSSL_BLOCK_set_cmp_func': SWIG/_m2crypto_wrap.c:12271:3: error: unknown type name 'sk_OPENSSL_BLOCK_compfunc' sk_OPENSSL_BLOCK_compfunc arg2 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12271:37: error: 'sk_OPENSSL_BLOCK_compfunc' undeclared (first use in this function) sk_OPENSSL_BLOCK_compfunc arg2 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12271:64: error: expected ',' or ';' before numeric constant sk_OPENSSL_BLOCK_compfunc arg2 = (sk_OPENSSL_BLOCK_compfunc) 0 ; ^ SWIG/_m2crypto_wrap.c:12276:29: error: expected ';' before 'result' sk_OPENSSL_BLOCK_compfunc result; ^ SWIG/_m2crypto_wrap.c:12290:3: error: 'result' undeclared (first use in this function) result = (sk_OPENSSL_BLOCK_compfunc)sk_OPENSSL_BLOCK_set_cmp_func(arg1,arg2); ^ SWIG/_m2crypto_wrap.c: At top level: SWIG/_m2crypto_wrap.c:5671:1: warning: 'BIO_meth_free' defined but not used [-Wunused-function] BIO_meth_free( BIO_METHOD *meth ) ^ error: command 'gcc' failed with exit status 1 ---------------------------------------- \u001b[0m\u001b[91mERROR: Command \"/usr/bin/python2 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-install-EOjVou/M2Crypto/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-3foVk_/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-install-EOjVou/M2Crypto/ Service 'rucio' failed to build: The command '/bin/sh -c pip install rucio rucio-webui' returned a non-zero code: 1 ``` ------------ ", "2502": "---------- I tried the docker demo on Ubuntu 18.04 and it fails during the build process due to errors in the docker file, namely this line:  I think neither sudo nor the plus sign should be in that line. It worked after removing those ------------ remove of `sudo` and the `+` sign from  ", "2501": "---------- urllib3 is affected by  we need to upgrade it to `urllib3>=1.24.2` ", "2500": "---------- Currently name/id are interchangeable for RSEs, however for a multi-VO setup it will be important to only use the id in core methods for uniqueness. ------------ The API directory will be changed so that any rse passed into the core is done using the id. The methods in core will be edited to remove dependancy on rse name as inputs. ", "2498": "---------- Standard gridftp do not support multiple checksum algorithms without the installation of additional plugins. This caused some hassle when configuring Rucio to operate in a context in which: - vanilla gridftp offered only `md5` - StoRM gridftp offered only `adler32`, but required VOMS auth where provided. The problem was related to the fact that the Rucio client writes to the DB entry both checksums (`md5` and `adler32`), hence any following operation on the data tried to check both checksums, failing alternatively depending on which gridftp was used. Possible modification ------------ Several option have been partially discussed: - Modify the Rucio Client in order to avoid the upload in the DB of unsupported checksums (e.g. adding support for a runtime flag such `--checksums md5,adler32`). This solution is weak and fast to implement, but delegates the handling of the RSE-checksum compatibility to the uploader (being it human or automatic); - Insert in `rucio.cfg` a global list of supported checksums. This solution is more elegant and the setup is explicit, but making such selection global might be an issue in case of orthogonal compatibilities; - Add to the DB an RSE field able to record a per-RSE checksum compatibility list. Such information can then be used in the Rucio demons to avoid unsupported checks. Discussion ------------ This issue is aimed at triggering some discussion prior to the implementation of one of such solutions. ", "2496": "---------- It currently does not work due to missing `flake8`. ", "2494": "---------- SQLAlchemy versions `< 1.3.0` suffer a SQL Injection vulnerability.   ------------ Change dependency to a version `>=1.3.0` ", "2493": "---------- Sandbox escape vulnerability in **Jinja2**.  This affects versions before `2.10.1` and we are currently using `2.10` ------------ Verify that we still need **Jinja2** and upgrade dependency to at least `2.10.1` ", "2488": "---------- State transition `COPYING` to `UNAVAILABLE` - Replicas should get **epoche** tombstone. ", "2485": "---------- ------------ ", "2482": "---------- A daemon that automatically configures FTS storage settings, depending on specific transfer failures concerning a FTS storage, its settings will be set to reduce these transfer failures. ", "2480": "---------- - There are several compiling issues observed when installing Rucio in Python 3.6.8. These errors are coming mainly from Py2->Py3 translations so far. - An error in the encoding (utf8) is observed when calculating the hash for determination of the final file path. ------------ - s are necessary in storm.py and protocol.py are necessary. ", "2479": "---------- Tests in lib/rucio/tests/test_import_export.py sometimes fail because of a list equality check. ------------ Change to sets to prevent different item orders to cause a test fail. ", "2469": "---------- Cf. Reaper's proposal: - Global performance: All reaper daemon instances should have a steady deletion performance. A situation where some of them are busy and some are idle should not occur, if there is a deletion backlog to operate on. - Protection of storage: Measures need to be in place to protect the storage from deletion overload. It should be possible to specify the maximum number of threads concurrently interacting with a storage element. - Intelligent distribution of workload: The global deletion workload needs to be distributed intelligently. RSEs having a large deletion backlog need to be handled with a larger priority than RSEs with a very small backlog. At the same time, starvation of RSEs cannot happen either. ------------ The principle is to have a pool of threads available. The method is then to assign dynamically a list of RSEs to each thread with several constraints (max threads per RSE). This implementation is based on several hash functions and introduced feature flags to roll out gradually the reaper in the new auto mode in a backward compatible way and with reapers running without this `mode`. - add `--auto` mode for work automatically on RSE flagged with the reserved RSE attribute `tombstone`, e.g., `rucio-reaper --auto --total-workers 6 --threads-per-worker 10 ` - Exclude RSE flagged with the reserved RSE attribute tombstone in the non auto mode, - Add support for the `greedy` reserved RSE attribute in the `auto` mode - Add `dry-mode` option to check the logic without doing deletion, e.g., `rucio-reaper --auto --dry-mode --total-workers 6 --threads-per-worker 10` - add `--dynamic` mode to consider only the RSEs with something to delete - add support for several RSE-threads mapping function `--map-function`: `list_rses_for_thread_hash` and `list_rses_for_thread_sequential`. ", "2468": "---------- ``` >>> from rucio.client.client import Client >>> c = Client() >>> print(c.list_replicas([{'scope':'user.mlassnig', 'name':'afasf'}], metalink=True)) </metalink> ``` ------------ ", "2465": "---------- In .travis.yml commands like `then nosetests -v lib/rucio/tests/test_clients.py; nosetests -v lib/rucio/tests/test_bin_rucio.py ; fi` dont return 1 in case of a failing tests if the last command returns 0. Found by @bari12 ------------ Check return codes separately ", "2462": "---------- Somehow the columns `created_at` and `updated_at` dont get automatically created on the table `did_meta` while using the `models.py`. (Found in #2449) ------------ ", "2460": "---------- # pip-requires-client - `setuptools>=36.8.0,<37.0.0` --> `setuptools>=36.8.0,<41.0.0` - `argparse>=1.4.0; python_version == '2.6'` --> **REMOVE** - `requests>=2.6.0,<2.20.0` --> `requests>=2.20.0,<2.22.0` - `urllib3>=1.23,<1.24` --> `urllib3>=1.24.2,<1.25` - `dogpile.cache>=0.6.5,<0.7.0` --> `dogpile.cache >=0.6.5,<0.7.2` - `boto>=2.48.0; python_version >= '2.7'` --> `boto>=2.49.0,<2.50.0` - `progressbar2>=3.37.1,<3.39.0` --> `progressbar2>=3.39.0,<3.40.0` - `six>=1.11.0` --> `six>=1.12.0,<1.13.0` - `boto3>=1.9.86` --> `boto3>1.9.130,<1.10.0` # pip-requires - `SQLAlchemy==1.2.16` --> `SQLAlchemy==1.3.3` - `alembic==1.0.6` --> `alembic==1.0.9` - `Mako==1.0.7` --> Remove, it's a dependency from Alembic and should not be specified separately - `python-editor==1.0.3` --> \u2757\ufe0f Remove, I am not sure why we even need this - `flup==1.0.3` --> \u2757\ufe0f Do we still need this??? Comment says needed for deployment of web.py in  but we don't use this anywhere? - `jsonschema==2.6.0` --> `jsonschema==3.0.1` - `python-dateutil==2.7.5` --> `python-dateutil==2.8.0` - `pysftp==0.2.9` --> \u2757\ufe0f This is used by sftp protocol. But in that case, it should be part of `pip-requires-client`... Also # forces installation of paramikoi and pycrypto - `s3cmd==2.0.2` --> \u2757\ufe0f This only seems to be used to test s3 protocol. In that case it should be part of `pip-requires-test` - `stomp.py==4.1.21` --> `stomp.py==4.1.22` - `pygeoip==0.3.2` --> Remove - `ipaddress==1.0.22` --> Just a dependency of geoip2, should be removed? - `maxminddb==1.4.1` --> Dependency of geoip2, should be removed too - `cffi==1.11.5` --> `cffi==1.12.2` \u2757\ufe0f Do we still need this? - `pycparser==2.19` --> Dependency of cffi - remove - `gcloud==0.18.3` --> \u2757\ufe0f Where do we actually use this in the code? - `rsa==4.0` --> \u2757\ufe0f needed??? - `googleapis-common-protos==1.5.6` --> Dependency of gcloud, should be removed -  --> \u2757\ufe0f Where do we still use this in the code? - `pyOpenSSL==19.0.0` --> Dependency of MyProxyClient, should be removed - `cryptography==2.4.2` --> Dependency of PyOpenssl, should be removed - `protobuf==3.6.1` --> Dependency of gcloud, Remove - `grpcio==1.18.0` --> \u2757\ufe0f Do we use this anywhere??? - `enum34==1.1.6` --> Dependency of grpcio, Remove - `pyasn1==0.4.5` --> Dependency of oauth2client, remove - `pyasn1-modules==0.2.3` --> Dependency of oauth2client, remove - `redis==3.0.1` --> `redis==3.2.1` - `numpy==1.14.2` --> `numpy==1.16.2` - `idna==2.7` --> Dependency of requests, remove # pip-requires-test - `pinocchio==0.4.2` --> Not used, remove - `Paste==3.0.6` --> `Paste==3.0.8` - `unittest2==1.1.0` --> Not needed anymore since we drop py26 - `coverage==4.5.2` --> `coverage==4.5.3` - `Sphinx==1.8.3` --> `Sphinx==1.8.5` (Last py27 compatible version) - `sphinx-rtd-theme==0.4.2` --> `sphinx-rtd-theme==0.4.3` - `Jinja2==2.10` --> Remove - `Pygments==2.2.0` --> `Pygments==2.3.1` - `virtualenv==16.2.0` --> `virtualenv==16.4.3` - `tox==3.7.0` --> `tox==3.9.0` (Can possibly be dropped) - `pytest==4.1.1` --> Only needed by unused script; Remove - `xmltodict==0.11.0` --> `xmltodict==0.12.0` - `pytz==2018.9` --> `pytz==2019.1` `pyflakes`, `flake8`, `pycodestyle` and `pylint` will be upgraded in a separate PR! ", "2456": "The option is not propagated correctly. Thus the default `False` value is used. ", "2446": "---------- Yesterday a new release (2.8) of `psycopg2-binary` was published. With this release, the tests do not run through, as exceptions are handled differently with our current version of sqlalchemy.  ------------ For now we will fix `setup_rucio.py` to not install the 2.8 version thus: `psycopg2-binary>=2.4.2<2.8' Once we update sqlalchemy to a newer version, this can be relaxed. ", "2445": "---------- Actual storage deletion loop. ", "2444": "---------- Creating the list of RSEs based on thresholds, epoche replicas and greedy mode. `list_unlocked_replicas changes` needs to change too for SKIP LOCKED queries. ", "2443": "---------- ------------ ", "2440": "---------- The progress class is calculated like `int(float(replicating_locks) / float(total_locks) * 10) * 10` ------------ It should be calculated like `int(float(total_locks - replicating_locks) / float(total_locks) * 10) * 10` ", "2436": "---------- Currently sqlalchemy creates postgres ENUM columns instead of check-constraints (such as in Oracle). It might be good to change this to check-constraints too, to have the same behaviour as in oracle. It needs to be understood if there is any performance impact due to this. Do we need to change any of the alembic upgrade/downgrade scripts? ", "2434": "---------- The ChangeLog seems to be more of a COPYRIGHT file. ------------ Remove or rename ChangeLog. ", "2433": "---------- from rucio.client import Client rucio_client = Client() filename = 'test_2019-04-03.txt' scope = 'user.ddmadin' rsename = 'UNI-FREIBURG_DATADISK' rucio_client.update_replicas_states(rsename, [{'scope':scope, 'name':filename}]) returns: rucio.common.exception.RucioException: An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97')) but the site is ok and I can download the replica using gfal. ------------ ", "2432": "---------- ``` 2019-04-02 11:14:50,477 29788 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/atropos/atropos.py\", line 103, in atropos rses = parse_expression(rule.rse_expression) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 351, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rse_expression_parser.py\", line 86, in parse_expression raise InvalidRSEExpression('RSE Expression resulted in an empty set.') InvalidRSEExpression: Provided RSE expression is considered invalid. Details: RSE Expression resulted in an empty set. ``` Caused by trying to process rules that still use the old convention regarding boolean attributes. ------------ Add an exception handler to log and skip the rule. ", "2421": "---------- I am trying to use the latest mysql version 8 which results in multiple erros when trying to setup the schema via running the tests in the docker dev environment. This is needed for #2220. ------------ ### problem with loading the authentication plugin - `Unable to load authentication plugin 'caching_sha2_password'.` - solved by `--default-authentication-plugin=mysql_native_password` ### too long primary keys - `Specified key was too long; max key length is 3072 bytes` - solved by `--character-set-server=latin1` ### syntax error in setting up the rules table - `You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'IS NOT NULL` - solved by escaping ### missing index while applying upgrade 22d887e4ec0a - `Failed to add the foreign key constraint. Missing index for constraint 'SOURCES_REPLICA_FK' in the referenced table 'replicas'\") [SQL: u'ALTER TABLE sources ADD CONSTRAINT SOURCES_REPLICA_FK FOREIGN KEY(scope, name, rse_id) REFERENCES replicas (scope, name, rse_id)']` - solved by changing the primary key while upgrading the replicas table ", "2413": "---------- ------------ ", "2412": "---------- This issue is an overview ticket for the Reaper 2.0 re-design. All ideas/comments are collected in the [design  # Re-Design work packages - [x] Changes in heartbeat mode #2443 @bari12 - [x] Outer loop: Preparation of the list of RSEs to delete on #2444 @cserf - [x] Inner loop: Actual storage interaction and deletion #2445 @tbeerman - [x] Documentation - [x] Monitoring (Internal & External) # Besides these general architecture changes, the overhaul should also address the following issues: - [x] mode should be an RSE attribute #1804 - [x] Protection of sources too strict in the reaper #1637 ", "2411": "---------- From our original plan: \"For third-party-copy, FTS3 supports `s3://` as a protocol and can do the URL signing itself. Like for Google Cloud Storage, the credential will be stored in the FTS3 server additionally. The replica core and transfer daemon need to skip URL signatures if the request is for the transfer daemon, but correctly sign for the deletion daemon. There is a legacy  protocol implementation in Rucio which should be removed in favour of this common workflow\". ------------ Having looked at the code, it should be relatively straightforward to add a call to get the signed URL (if required for the RSE) to the reaper. The core credential code can already sign `DELETE` URLs for GCP, S3 and Swift. The legacy code could then be removed. I am not sure whether the planned rewrite of the reaper will affect this. ", "2408": "---------- We see in many cases in traces just following on the output: stageIn with API failed: None of the requested files have been downloaded. which is not explanatroy enough. ------------ Pilot propagates to traces an exception from Rucio. Fristly, we need to know whether this happens only for xrootd protocols and how to increase the verbosity. ", "2403": "---------- ```` bash-4.2# rucio get-did-meta manda:manda-5_hours.tar key3: 1.23 key2: 123 key1: value1 ```` but: ``` bash-4.2# rucio list-dids-by-meta --scope manda key3=1.23 bash-4.2# rucio list-dids-by-meta key3=1.23 bash-4.2# ``` ", "2402": "---------- When no scheme is specified in the configuration, the conveyor fails to find a common scheme for the source and destination files, here `gsiftp`: ``` 2019-03-27 08:51:47,386 3214 CRITICAL Exception happened when trying to get transfer for request 5d9c6d99ebf04465ad3bef4d8078219c: Traceback (most recent call last): File \"/usr/local/lib/python2.7/dist-packages/rucio/core/transfer.py\", line 624, in get_transfer_requests_and_source_replicas 'schemes': __add_compatible_schemes(schemes=[matching_scheme[0]], allowed_schemes=current_schemes), File \"/usr/local/lib/python2.7/dist-packages/rucio/core/transfer.py\", line 923, in __add_compatible_schemes if scheme in allowed_schemes: TypeError: argument of type 'NoneType' is not iterable ``` The respective rules stay in the STUCK state when asked for reevaluation: ``` 2019-03-27 08:49:23,677 3213 INFO rule_repairer[0/0]: Repairing rule 32d6775ead104b2c8be50cc946f01782 2019-03-27 08:49:23,749 3213 INFO Rule 32d6775ead104b2c8be50cc946f01782 [0/0/1] state=STUCK ``` ", "2399": "---------- the checksum is not yet checked after downloading a file. This is especially important for pilot. ------------ make it False by default:  ", "2398": "---------- From Stephane: > when I introduce a typo in the RSE name, the error message which I expect 'RSE does not exist' is very complicated : > > ``` > -bash-4.1$ rucio -v download --rse MWt2_DATADISK mc16_13TeV:HITS.17430965._017098.pool.root.1 > 2019-03-26 10:12:16,014 INFO Processing 1 item(s) for input > 2019-03-26 10:12:16,014 DEBUG Processing item mc16_13TeV:HITS.17430965._017098.pool.root.1 > 2019-03-26 10:12:16,015 DEBUG RSE-Expression: (MWt2_DATADISK)&istape=False > 2019-03-26 10:12:16,015 DEBUG Splitted DID: mc16_13TeV:HITS.17430965._017098.pool.root.1 > 2019-03-26 10:12:16,055 DEBUG 1 DIDs after processing input > 2019-03-26 10:12:16,056 DEBUG Processing: {'resolve_archives': True, 'name': 'HITS.17430965._017098.pool.root.1', 'did': 'mc16_13TeV:HITS.17430965._017098.pool.root.1', 'rse': '(MWt2_DATADISK)&istape=False', 'no_subdir': False, 'nrandom': None, 'transfer_timeout': 3600, 'scope': 'mc16_13TeV', 'type': u'FILE', 'force_scheme': None, 'base_dir': '.'} > 2019-03-26 10:12:16,080 DEBUG <?xml version=\"1.0\" encoding=\"UTF-8\"?> > <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\"> > > 2019-03-26 10:12:16,092 DEBUG Traceback (most recent call last): > File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/bin/rucio\", line 159, in new_funct > return function(*args, **kwargs) > File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/bin/rucio\", line 968, in download > result = download_client.download_dids(items, args.ndownloader, trace_pattern) > File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 291, in download_dids > input_items = self._prepare_items_for_download(items) > File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 914, in _prepare_items_for_download > files_with_pfns = self._parse_list_replica_metalink(metalink_str) > File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 996, in _parse_list_replica_metalink > raise error > ExpatError: no element found: line 3, column 0 > > 2019-03-26 10:12:16,092 ERROR no element found: line 3, column 0 > 2019-03-26 10:12:16,093 ERROR Strange error: No section: 'policy' ``` This should raise `RSENotFound`. ", "2390": "---------- ```python config.set('throttler', 'release_strategy', 'fifo') config.set('throttler', 'release_strategy', 'grouped_fifo') ``` will result in a database duplicate exception because `config.set()` checks if there is already a value which will be `False` if you try to set a new value under the expiration time because it got set to `False` with the first command. ------------ change the `has_option()` check in `config.set()` to `has_option(use_cache=False)` ", "2379": "---------- we shouldn't publish auth token even at debug level in clients ------------ remove or change following line:  ", "2378": "---------- I recently submitted a PR during which I ran into an error while executing ``` $ ./tools/submit-pull-request ``` as mentioned in the [Contribution  The error was ``` Traceback (most recent call last): File \"./tools/submit-pull-request\", line 14, in <module> import commands ModuleNotFoundError: No module named 'commands' ``` I use Arch linux, where the default python version is 3.x. A quick google search led me [to Arch's  according to which the `commands` module is unavailable in python3, thus for submitting a PR what actually worked was ``` python2 ./tools/submit-pull-request ``` Since Python2 will not be maintained post  I would suggest changing the default implementation for python3 compatibility, in the longer run that would be required anyway. ------------ Using `subprocess` module instead of `commands` in  ", "2375": "---------- Improve documentation, make it easier to follow ------------ Edit documentation files, apply formatting and grammatical edits ", "2370": "---------- while installing ipaddress with the demo docker build,pip tries to uninstall the previous version of ipaddress.Which,results in this error- \"Cannot uninstall 'ipaddress'. It is a distutils installed project and thus...\" ", "2367": "---------- the demo does not build in some unconfigured systems as by default pip version in docker is 8.1.0 which needs to be update gives rise to error \"you are using pip version xyz.please upgrade\" while installation. ------------ to the demo Dockerfile ", "2363": "---------- The 'RSE' field in the list_replicas reply is still used by the pilot. In the case of multiple different DIDs requested in a single list_replicas call, the sorting per RSE will only be correct for the last RSE if there are multiple potential RSEs. ", "2362": "---------- ``` # rucio --version rucio 1.19.4 # rucio download \"238357;238356;238359:manda4_cp1_3.00u_SW20140322.part1.tar\" 2019-03-21 13:58:45,998 INFO Processing 1 item(s) for input 2019-03-21 13:58:45,998 INFO Getting sources of DIDs 2019-03-21 13:58:46,434 ERROR %d format: a number is required, not NoneType ``` ", "2359": "---------- We need to adapt the docker-compose function so that it can built built under proxy given by system environment variables. We also need to supply these environment variables while executing the docker-compose function. ", "2355": "---------- In case the rule-deletion in the undertaker raises nowait DB error, the undertaker just retries the same did right now. Instead, the undertaker should pause these dids for 1-10min and only retry then. ", "2352": "---------- Conveyor Submitter continues to submit transfers jobs even for sites blacklisted for write ", "2344": "---------- The addition of the `check_accept_header_wrapper` decorator method in the flask rest api blocks the auto-documentation of the GET endpoints. These endpoints (mostly GET) are now removed from readthedocs. Needs to be fixed. ", "2336": "---------- To make the documentation free of grammatical errors ------------ Edit Documentation files and fix the grammatical errors. ", "2327": "---------- small things to change here and there ------------ - rse_expression 1/0 -> True/False ", "2322": "The documentation of `rucio-admin account add` does not specify the acceptable values for `--type`. ", "2319": "During yesterday\u2019s CREM meeting, it was agreed to create a new physics group `phys-hdbs`. The probe that automatically adds identities based on information from VOMS needs to be updated. ", "2315": "---------- Currently Rucio only supports local quotas. E.g. an account_limit for an account on a specific RSE. The idea of this feature would be to allow quotas on a set of RSEs. E.g. an aggregated limit for an account on all RSEs in the UK (`country=uk`) ------------ This requires changes in the replication rules quota checking and a whole system around calculating/managing these global quotas in an efficient way. ", "2314": "---------- Also see  Currently we force an `xrdcp` based download to stream the content of a zip file. In this case the command is build like this: `xrdcp -vf root://bohr3226.tier2.hep.manchester.ac.uk:1094//dpm/tier2.hep.manchester.ac.uk/home/atlas/atlasdatadisk/rucio/mc16_13TeV/46/d2/HITS.17117996._000005.zip.1?xrdcl.unzip=HITS.14859379._044566.pool.root.1 -z HITS.14859379._044566.pool.root.1 file:///tmp/walkerr/d1234567890/d1234567890/d1234567890/d1234567890/d1234567890/d1234567890/d1234567890/mc16_13TeV/HITS.14859379._044566.pool.root.1.part` The `file://` part seems to create problems for xrdcp and puts the file in a random location on disk. ------------ Remove the `file://` prefix for the local destination. ", "2313": "---------- Currently `list_replicas` orders replicas based on their protocol priorities. For zip replicas this is not efficient though, as root offers a more efficient way to access the constituent files in the zip. ------------ `list_replicas` should prioritise root protocol for zip replicas over all other protocols. ", "2306": "---------- Login to Rucio WebUI with SSO service. (E.g. cern SSO, social login, etc.) ", "2305": "---------- Add history plot on account_rse_usage ", "2304": "---------- Many operations can affect a collection (dataset/container) or its rules that can have effects on the end-users: - File loss : The files that are lost or corrupted are removed from the datasets which change the content of the dataset. Often these events has unnoticed but can have bad consequences on physics analyses (e.g. less statistics) - Dataset erased : The dataset is erased because it was buggy or superseded by a new version - Lifetime expiration : All the datasets have finite lifetime. In case the lifetime of a dataset expires, the users can ask for an extension of the lifetime. All these operations are run without notifying the users, or when a notification happens it is via the use of generic mailing list that might not reach the users really interested in these events. ------------ We propose here a new mechanism where users can \"follow\" the events affecting a dataset or container. Any event mentioned above affecting one the the dataset followed, will trigger a notification to the user (e.g. daily email summarizing all the events) ", "2299": "---------- One needs a view to monitor the evolution of used space by account for a given or a set of RSEs. ", "2298": "---------- Currently, the `space_method` key in AGIS has two possible values: `lcg-stmd` and `other`. The former essentially means using SRM. The latter signifies that the value of the related `space_usage_url` key is a URL to either an ATLAS or a WLCG JSON file. Recently AGIS introduced two new values: `storage` and `rucio`. The goal is to change the meaning of the `space_method` key. While previously it specified how to get the storage occupancy information, in the future it will mean which RSE Usage source to use for scheduling deletions (it will also affect the alarms in the ADC Livepage). ------------ Modify `check_rse_attributes` so that: 1. The `space_usage_method` RSE attribute is set solely based on the content of `space_usage_url`. 2. The `sourceForUsedSpace` RSE attribute is set based on the content of `space_method`. ", "2291": "---------- My first contribution towards rucio ------------ Fixed grammatical errors and typos in `database.rst` and `installing_daemons.rst` ", "2290": "---------- >>> from rucio.client.uploadclient import UploadClient >>> upload_client = UploadClient(logger=logger) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> NameError: name 'logger' is not defined >>> upload_client = UploadClient() >>> >>> f = {'no_register': True, 'path': '/afs/cern.ch/user/t/tjavurek/ADC/DDM/pilot-testing/pilot2/test.txt', 'did_scope': 'user.tjavurek', 'rse': 'UNI-FREIBURG_SCRATCHDISK'} >>> summary_file_path = './rucio_upload.json' >>> result = upload_client.upload([f], summary_file_path) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.18.5/lib/python2.7/site-packages/rucio/client/uploadclient.py\", line 234, in upload for file in summary: UnboundLocalError: local variable 'summary' referenced before assignment ------------ define summary before the file loop ", "2288": "----------  The first link \"setting up demo...\" is 404, should point to  ", "2285": "---------- Tests with postgres backend often fail because of too many connections to the database ------------ The max_connections value in the postgres configuration must be increased. ", "2277": "---------- There is now a new API to declare bad PFNs much more efficient that the one currently used for `rucio-admin replicas declare-bad`. Will move to the new API. ", "2266": "----------  This line needs chunking similar to the other queries. ------------ ", "2265": "This is similar to #1849. ", "2262": "---------- ``` >>> from rucio.core.config import set >>> set(section='throttler', option='Data Consolidation,xyz', value=2000, session=None) Control/space characters not allowed (key='has_option_throttler_Data Consolidation,xyz') Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 356, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/config.py\", line 203, in set if not has_option(section=section, option=option, session=session): File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 288, in new_funct return function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/config.py\", line 129, in has_option has_option = REGION.get(has_option_key, expiration_time=expiration_time) File \"/usr/lib/python2.7/site-packages/dogpile/cache/region.py\", line 665, in get value = self.backend.get(key) File \"/usr/lib/python2.7/site-packages/dogpile/cache/backends/memcached.py\", line 161, in get value = self.client.get(key) File \"/usr/lib/python2.7/site-packages/memcache.py\", line 1121, in get return self._get('get', key) File \"/usr/lib/python2.7/site-packages/memcache.py\", line 1065, in _get self.check_key(key) File \"/usr/lib/python2.7/site-packages/memcache.py\", line 1320, in check_key \"Control/space characters not allowed (key=%r)\" % key) memcache.MemcachedKeyCharacterError: Control/space characters not allowed (key='has_option_throttler_Data Consolidation,xyz') ``` ", "2261": "---------- rucio-admin config set silently fails, e.g. : ``` rucio-admin config set --section throttler --option \"Data Consolidation,xyz\" --value 2000 Set configuration: throttler.Data Consolidation,xyz=2000 ``` But on the server side, it returns a `500` ", "2257": "---------- No more mercy. ", "2253": "---------- For installations without Oracle we need a flag such that the RSE abacus reports the RSE usage every hour into the rse_usage_history table. (e.g., --enable-history) ", "2249": "---------- There should be a default value if no filter option is passed to the downloadclient. ------------ Change  to `item.get('filters', {})` ", "2248": "---------- The new mode has been added, but the schema validator was not updated. ", "2245": "---------- ``` Traceback (most recent call last): File \"./t\", line 5, in <module> s.connect((\"8.8.8.8\", 80)) File \"/usr/lib64/python2.7/socket.py\", line 224, in meth return getattr(self._sock,name)(*args) socket.error: [Errno 101] Network is unreachable Traceback (most recent call last): File \"./t\", line 5, in <module> s.connect((\"8.8.8.8\", 80)) File \"/usr/lib64/python2.7/socket.py\", line 224, in meth return getattr(self._sock,name)(*args) socket.error: [Errno 101] Network is unreachable ``` ", "2240": "---------- Function description seems to have been copy-pasted from somewhere without changes. ", "2238": "", "2237": "---------- update the json format of the RSE export in core/export.py ------------ - drop key 'ASN' - drop key 'ISP' - drop key 'continent' - drop key 'transfer_limits' - drop key 'limits' - remove outer protocol key - move key 'credentials' one level down to protocols - move key 'lfn2pfn' one level up ", "2233": "---------- Currently it considers TU replicas, which makes testing the feature impossible. ", "2227": "---------- Create config/se method in fts transfer tool ------------ ", "2226": "---------- If root protocol is available according to the list_replicas reply, skip all non-root protocols to minimise the effect of download+extract. ", "2221": "---------- Minos is not able to handle PFNs with different schemes ", "2220": "---------- Right now, the throttler releases transfers simply based on time. FIFO. For the Tape Carousel activity, it is necessary to release them in a smarter way: Grouped FIFO. This essentially means, if a transfer is being released, it should also release all the transfers of the same dataset, so they are submitted to FTS in the same time-window to allow subsequent grouping on FTS side, and thus on the tape system. ", "2219": "---------- This potentially creates all kind of contention problems. Thus the injector should not inject a rule which is about to expire soon (minutes). ", "2217": "---------- `core.list_replicas` does not recursively list the parents and thus does not resolve any replicas. This is blocking the deployment of new clients. ", "2212": "---------- (1) Example: ``` sqlalchemy.exc.DataError: (psycopg2.DataError) invalid input value for enum \"BAD_REPLICAS_STATE_CHK\": \"T\" [SQL: 'ALTER TABLE bad_replicas ADD CONSTRAINT \"BAD_REPLICAS_STATE_CHK\" CHECK (state in (\\'B\\', \\'D\\', \\'L\\', \\'R\\', \\'S\\', \\'T\\'))'] ``` In PSQL we cannot drop the constraint and create it anew, instead we have to alter the enum of the constraint: ``` ALTER TYPE \"BAD_REPLICAS_STATE_CHK\" ADD VALUE 'T' AFTER 'S'; ALTER TYPE \"RULES_NOTIFICATION_CHK\" ADD VALUE 'P' AFTER 'C'; ``` (2) Foreign keys are a hard dependency in PSQL, we cannot simply drop an index. This is now a problem with: ``` SOURCES_REPLICA_FK ``` To fix the replicas primary key migration: ```alter table sources drop constraint \"SOURCES_REPLICA_FK\";``` ```alter table replicas drop constraint \"REPLICAS_PK\";``` ```alter table replicas add primary key (scope,name,rse_id);``` ```alter table sources add constraint \"SOURCES_REPLICA_FK\" FOREIGN KEY (scope, name, rse_id) REFERENCES replicas(scope, name, rse_id);``` ", "2207": "---------- There are some leftovers of dict.iteritems that are not python3 compatible. ------------ Replace them with six.iteritems. ", "2203": "---------- tests/test_account.py is failing because of two overlapping PRs. ------------ The `password` argument needs to be added to the `add_account_identity` calls. ", "2200": "---------- Add a test to travis for SUITE=all with python3.5. It should be allowed to fail. ------------ ", "2199": "---------- There are some places in the code where FTS is hardcoded, e.g. FTS attribute on RSEs. Also changing the transfertool in the configuration is not implemented. ------------ ", "2194": "---------- First documentation patch fix to rucio. ------------ Fixed typos and grammatical errors in `rucio/doc/source/api.rst`, `rucio/doc/source/configuration.rst` and `rucio/doc/source/index.rst`. ", "2186": "---------- Submitting my first documentation patch fix in the organization ------------ Fixed some grammatical errors and typos in `overview_Replica_management.rst`, `clients.rst` and `cli_examples.rst` files. ", "2185": "---------- My first step towards contribution to Rucio. ------------ Fixed some typos in the documentation and, READMEs of demo and dev respectively. ", "2181": "---------- The `necromancer` always try to process replicas that are bad, even if the DID has availability `LOST`. Moreover it can sleep if it managed to process every files within `sleep-time` in the bulk even if there is still work to do. ", "2180": "---------- `etc/docker/dev/alembic_mysql.ini` is missing but needed in the Dockerfile. ------------ Copy from `etc/docker/travis/alembic_mysql.ini` ", "2177": "---------- ------------ ", "2174": "---------- When computing the freespace on a site, the probe doesn't take into account the min_freespace. It should be changed ", "2173": "---------- Add the possibility to declare bad files in the WebUI ", "2168": "Summary ------------- we observed following behaviour: 2019-02-08 06:21:34| 23631|FileHandling| Ski2019-02-08 05:22:59| 24559|rucio_sitemo| StageOut, attempt 1/2 2019-02-08 05:22:59| 24559|rucio_sitemo| _stageOutApi: {'no_register': True, 'path': 'panda.um.group.phys-exotics.16728385.XAMPP._000681.root', 'guid': '7dee6f62-e341-4ddd-9a7f-2952a32ebafb', 'did_scope': 'panda', 'rse': 'IN2P3-LAPP_SCRATCHDISK'} 2019-02-08 05:23:00| 24559|rucio_sitemo| Preparing upload for file panda.um.group.phys-exotics.16728385.XAMPP._000681.root 2019-02-08 05:53:00| 24559|rucio_sitemo| StageOut, attempt 2/2 where the logger message between the two attempts is not sufficient to understand the problem. As a consequence, pilot fails later with: PilotException: PilotException: 1220: stageOut with API failed: The requested service is not available at the moment. Details: An unknown exception occurred Code details ----------------- the message starting with \u2018Preparing upload \u2026\u2019 originates from Rucio\u2019s upload client:  it didn\u2019t reach the next logger message that is on line:  But on 60 lines between, we are calling rsemgr and protocols, which I suspect from failing somewhere without proper raise or logger message. The logger level is set to \u2018DEBUG\u2019. Note that srm protocol is the first priority one for the w_lan activity. Writing a job log into the same destination from the same job passed with no problems, thus most probably, there was a problem with the file itself. Solution ----------- we should expand logger messages in upload client on these lines at the first place. Secondly, we should investigate rsemgr and protocols for raising exceptions and let them provide more info to logger if possible. ", "2166": "---------- The upgrade of the replica state `T` is missing in the revision. ", "2161": "---------- The conveyor supports `--fts-source-strategy` option to specify which option should be chosen for the sources. This option is the same for all the transfers processed by a given conveyor instance. It would be better to be able to specify different options for different activities. ------------ Introduce a default `default-fts-source-strategy` in the configuration table as well a `activity-fts-source-strategy`, so that every activities can use a different strategy. ", "2160": "---------- StoRM sites have an effective way of getting a file from their shared FS. In such case, rucio shouldn't try to do direct copy for local reads. ------------ introduce a new protocol imitation, that will create a symlink in place of communicating and copying a file from a storage. ", "2156": "---------- There are new versions of flake8 and pycodestyles which need to be upgraded. This will raise additional errors though, which should be fixed beforehand. ", "2155": "---------- Some of the permission checks are not done correctly and need to be updated in all permission files. ", "2154": "---------- ```` ====================================================================== ERROR: Failure: ModuleNotFoundError (No module named 'retrying') ---------------------------------------------------------------------- Traceback (most recent call last): File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/nose/failure.py\", line 39, in runTest raise self.exc_val.with_traceback(self.tb) File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/nose/loader.py\", line 418, in loadTestsFromName addr.filename, addr.module) File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/nose/importer.py\", line 47, in importFromPath return self.importFromDir(dir_path, fqname) File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/site-packages/nose/importer.py\", line 94, in importFromDir mod = load_module(part_fqname, fh, filename, desc) File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/imp.py\", line 235, in load_module return load_source(name, filename, file) File \"/home/travis/virtualenv/python3.6.3/lib/python3.6/imp.py\", line 172, in load_source module = _load(spec) File \"<frozen importlib._bootstrap>\", line 684, in _load File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked File \"<frozen importlib._bootstrap_external>\", line 678, in exec_module File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed File \"/home/travis/build/rucio/rucio/lib/rucio/tests/test_bin_rucio.py\", line 38, in <module> from rucio.db.sqla import session, models File \"/home/travis/build/rucio/rucio/lib/rucio/db/sqla/session.py\", line 30, in <module> from retrying import retry ModuleNotFoundError: No module named 'retrying' ---------------------------------------------------------------------- ``` ", "2149": "---------- The reaper console script `rucio-reaper` is not tested in the testsuite. ------------ - Add test for the reaper console script. - Install the environnement with `python setup.py develop` in the docker env to have the generated console scripts available in the docker. - Extend the reaper argparse method and the reaper tests to validate the argparse main method and console script. ", "2144": "---------- Enable login to the Web-UI with a straightforward username/password form. (From discussions with AENEAS/SKA) ", "2142": "---------- Looking at the output from list_identities in the account client, it does not return the e-mail address for the identity. However, the set identity takes this parameter and the field exists in the database. Is this an oversight or is it intentional? The underlying motivation for this is that I'm trying to migrate the accounts and identities from one server to another. ------------ ", "2141": "---------- Adding generic metadata with add-did-meta does not work, and requesting with get-did-meta should not fail with \"No DID found\", when instead it should be \"No generic metadata found.\" ", "2140": "---------- userpass authentication requires a --password option, in addition to --id (which is for x509/ssh/kerberos). This is available through the API, but not bin/rucio ", "2137": "---------- The rucio/rucio-systemd-cc7 image is still used as a base for the dev and travis containers. There is no need for a systemd base image for those two and it can be replaced with the centos image instead. Then finally we can also remove the rucio-systemd-cc7 image from our docker hub repository. ------------ ", "2133": "---------- Change idna to 2.7 ", "2128": "---------- There seems to be a problem with memcached and core/config which was added by PR #1963. If I try to add a key with a space included, memcached throws an error. Not sure if 'User Subscriptions' is a valid key though. ```python from rucio.core.config import set set('throttler', 'User Subscriptions,MOCK', 1) # memcache.MemcachedKeyCharacterError: Control/space characters not allowed (key='has_option_throttler_User Subscriptions,MOCK2') ``` ------------ ", "2127": "---------- On readthedocs the documentation for setting up a demo environment is broken as there multiple entries in the sidebar for one page. ------------ Update styling and titles for headlines in /etc/docker/demo/README.rst ", "2126": "---------- Improve conveyor daemon documentation for submitter and throttler. ------------ ", "2122": "---------- Existing RSE attributes which use 0/1 as true/false are not found if they are the sole solution to an RSE expression. ", "2118": "---------- The upgrade script should be fixed in a hotfix release `1.19.0.post1` and the `1.19.0` release should be pulled from PyPi. This only impacts database upgrades from an older version. ", "2115": "---------- When RSE configurations are repaired then M-state requests are not picked up again automatically. Do this by default, but add switch for ATLAS to explicitly disable it. ", "2112": "---------- Allow python apis to list the DID status in a rule, in the context of a automatic script for monitoring the user file transfer for instance. AFAIU the rest call is already there, the only work needed is to expose that to the client ------------ Expose replicalocks call defined here:  ", "2107": "---------- The fts3 package is missing in the readthedocs environment. Therefore the build sometimes fails if the cache gets reset. ------------ It needs to be mocked because it can not be installed through the readthedocs.requirements.txt because of missing pycurl. ", "2105": "---------- The link on the rule page the DDM dashboard is still pointing to the old dashboard. ------------ Point to the new dashboard. ", "2103": "---------- `rucio list-rse-usage --show-accounts` sometimes crashes with `Internal Error`. The problem is due to division by zero on the server side : ``` [Mon Feb 04 13:31:23.641239 2019] [:error] [pid 18854:tid 139955217078016] float division by zero [Mon Feb 04 13:31:23.644559 2019] [:error] [pid 18854:tid 139955217078016] Traceback (most recent call last): [Mon Feb 04 13:31:23.644574 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/rse.py\", line 614, in GET [Mon Feb 04 13:31:23.644578 2019] [:error] [pid 18854:tid 139955217078016] usage = get_rse_usage(rse, issuer=ctx.env.get('issuer'), source=source, per_account=per_account) [Mon Feb 04 13:31:23.644580 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/rucio/api/rse.py\", line 265, in get_rse_usage [Mon Feb 04 13:31:23.644583 2019] [:error] [pid 18854:tid 139955217078016] return rse_module.get_rse_usage(rse=rse, source=source, per_account=per_account) [Mon Feb 04 13:31:23.644589 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f [Mon Feb 04 13:31:23.644592 2019] [:error] [pid 18854:tid 139955217078016] return Retrying(*dargs, **dkw).call(f, *args, **kw) [Mon Feb 04 13:31:23.644594 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call [Mon Feb 04 13:31:23.644596 2019] [:error] [pid 18854:tid 139955217078016] return attempt.get(self._wrap_exception) [Mon Feb 04 13:31:23.644599 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get [Mon Feb 04 13:31:23.644601 2019] [:error] [pid 18854:tid 139955217078016] six.reraise(self.value[0], self.value[1], self.value[2]) [Mon Feb 04 13:31:23.644603 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call [Mon Feb 04 13:31:23.644605 2019] [:error] [pid 18854:tid 139955217078016] attempt = Attempt(fn(*args, **kwargs), attempt_number, False) [Mon Feb 04 13:31:23.644607 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 270, in new_funct [Mon Feb 04 13:31:23.644609 2019] [:error] [pid 18854:tid 139955217078016] return function(*args, **kwargs) [Mon Feb 04 13:31:23.644612 2019] [:error] [pid 18854:tid 139955217078016] File \"/usr/lib/python2.7/site-packages/rucio/core/rse.py\", line 568, in get_rse_usage [Mon Feb 04 13:31:23.644614 2019] [:error] [pid 18854:tid 139955217078016] account_usages.append({'used': row.bytes, 'account': row.account, 'percentage': round(float(row.bytes) / float(total) * 100, 2)}) [Mon Feb 04 13:31:23.644616 2019] [:error] [pid 18854:tid 139955217078016] ZeroDivisionError: float division by zero ``` ", "2102": "---------- Cf.  `_table_args = (PrimaryKeyConstraint('rse_id', 'scope', 'name', name='REPLICAS_PK'),` ------------ `_table_args = (PrimaryKeyConstraint('scope', 'name', 'rse_id'; name='REPLICAS_PK'),` ", "2095": "---------- A few packages defined in the pip requirement files are not python3 compatible, like web.py, fts3, python-memcached. ------------ ", "2090": "---------- Test alembic up and downgrade on all databases, not only sqlite. ------------ Have separate alembic.ini per database to test. Remove template copy from top-level Dockerfile. ", "2089": "---------- the usercert.pem file is missing in the etc/docker/dev directory. Therefore the build fails. ------------ add usercert.pem from etc/docker/demo ", "2080": "---------- this ticket should remain open for longer time. Quite often there is a request to do a small change in the bb8 scriptis. All of them would be treated by this ticket. ", "2076": "---------- In the current HEAD of the download client, the transfer_timeout is not propagated to protocol.get (it is always None) I think this issue is caused by this refactoring:  The transfer_timeout is now stored in the merged_options dictionary, but _download_item is not updated accordingly. ------------ ", "2069": "---------- Staging area replicas should be available for use as transfer sources and appear in the replica list. This makes the staging area behave more like a \"virtual disk\" ------------ Drop the staging_area==False restrictions on the queries. ", "2065": "---------- There are broken links in the sidebar on  like `RUCIO_DAEMON` ------------ ", "2054": "---------- The sites/clouds squads need a view to list the suspicious files on their site. ", "2051": "---------- ``` Traceback (most recent call last): File \"check_voms\", line 220, in <module> print('ERROR getting info for %s' % (user._DN)) UnicodeEncodeError: 'ascii' codec can't encode character u'\\xed' in position 97: ordinal not in range(128) ``` ------------ As a temporary solution, use `repr()` until the issue better understood. ", "2048": "---------- wrong traces = wrong accounting ------------  should return another field called 'rse', where rse would be populated. Is that possilbe? ", "2045": "---------- Some sites reported that some 3rd party copy to their SRM are failing because no destination space token was specified despite the fact that the space token is properly defined in the extended attributes of the protocol. The problem is not systematic and only happens from time to time. It affects both single and multi-sources transfers. This needs to be fixed. ", "2040": "---------- `rucio list-rse-usage --show-accounts BNL-OSG2_LOCALGROUPDISK` fails with ``` 2019-01-25 09:07:28,376 ERROR An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o ', '\\xe2\\x9c\\x97')) ``` On the server the following exception is shown: ``` [Fri Jan 25 13:25:43.381208 2019] [:error] [pid 20836:tid 139736349660928] float division by zero [Fri Jan 25 13:25:43.383132 2019] [:error] [pid 20836:tid 139736349660928] Traceback (most recent call last): [Fri Jan 25 13:25:43.383155 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/rse.py\", line 614, in GET [Fri Jan 25 13:25:43.383159 2019] [:error] [pid 20836:tid 139736349660928] usage = get_rse_usage(rse, issuer=ctx.env.get('issuer'), source=source, per_account=per_account) [Fri Jan 25 13:25:43.383161 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/rucio/api/rse.py\", line 265, in get_rse_usage [Fri Jan 25 13:25:43.383164 2019] [:error] [pid 20836:tid 139736349660928] return rse_module.get_rse_usage(rse=rse, source=source, per_account=per_account) [Fri Jan 25 13:25:43.383166 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f [Fri Jan 25 13:25:43.383169 2019] [:error] [pid 20836:tid 139736349660928] return Retrying(*dargs, **dkw).call(f, *args, **kw) [Fri Jan 25 13:25:43.383171 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call [Fri Jan 25 13:25:43.383173 2019] [:error] [pid 20836:tid 139736349660928] return attempt.get(self._wrap_exception) [Fri Jan 25 13:25:43.383175 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get [Fri Jan 25 13:25:43.383178 2019] [:error] [pid 20836:tid 139736349660928] six.reraise(self.value[0], self.value[1], self.value[2]) [Fri Jan 25 13:25:43.383180 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call [Fri Jan 25 13:25:43.383182 2019] [:error] [pid 20836:tid 139736349660928] attempt = Attempt(fn(*args, **kwargs), attempt_number, False) [Fri Jan 25 13:25:43.383184 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 270, in new_funct [Fri Jan 25 13:25:43.383187 2019] [:error] [pid 20836:tid 139736349660928] return function(*args, **kwargs) [Fri Jan 25 13:25:43.383189 2019] [:error] [pid 20836:tid 139736349660928] File \"/usr/lib/python2.7/site-packages/rucio/core/rse.py\", line 568, in get_rse_usage [Fri Jan 25 13:25:43.383191 2019] [:error] [pid 20836:tid 139736349660928] account_usages.append({'used': row.bytes, 'account': row.account, 'percentage': round(float(row.bytes) / float(total) * 100, 2)}) [Fri Jan 25 13:25:43.383194 2019] [:error] [pid 20836:tid 139736349660928] ZeroDivisionError: float division by zero ``` ", "2039": "---------- LSM uses pcaches. In order to fulfil wishes of US admins, we need to implement them in rucio so, that rucio mover will use this feature. ------------ 1) stealing pcache script from lsm 2) implementing check, whether a file is listed in pcache, if so, make a hardlink to its location instead of downloading it again. 3) populating the pcache list. ", "2038": "---------- ``` Done > rucio-admin replicas declare-bad --reason \"dcache does not know anything about this file\" srm://lcg-se1.sfu.computecanada.ca:8443/srm/managerv2?SFN=/atlas/atlasdatadisk/rucio/data18_13TeV/19/db/DAOD_EXOT3.15351753._000211.pool.root.1 > echo $? 0 I did not get any feedback from the above command which is unnerving ! (DDM, would you consider a message printout as to what was done ?) Thanks ! regards, Asoka ``` ", "2030": "----------  will silently excludes RSEs that don\u2019t have either an `srm` or a `gsiftp` counter. The former might not exist in the future as sites move away from SRM and the latter was renamed to `json`. Additionally, the long-term plan is to drop the individual counters and solely use the new `storage` one. This was investigated and reported by @cserf. ------------ Modify the query so that it only filters against the `storage` counter. ", "2028": "------------ # pip-requires SQLAlchemy 1.2.7 --> 1.2.16 alembic 0.9.9 --> 1.0.6 python-dateutil 2.7.2 --> 2.7.5 s3cmd 2.0.1 --> 2.0.2 stomp.py 4.1.20 --> 4.1.21 dnspython 1.15.0 --> 1.16.0 geoip2 2.8.0 --> 2.9.0 maxminddb 1.3.0 --> 1.4.1 pycparser 2.18 --> 2.19 googleapis-common-protos 1.5.3 --> 1.5.6  0.11.3 --> 0.12.0 pyOpenSSL 18.0.0 --> 19.0.0 cryptography 2.3.1 --> 2.4.2 oauth2client 4.1.2 --> 4.1.3 protobuf 3.5.2.post1 --> 3.6.1 grpcio 1.11.0 --> 1.18.0 pyasn1 0.4.2 --> 0.4.5 pyasn1-modules 0.2.1 --> 0.2.3 rsa 3.4.2 --> 4.0 redis 2.10.6 --> 3.0.1 numpy 1.14.2 --> 1.16.0 paramiko 2.4.1 --> 2.4.2 Flask 0.12.4 --> 1.0.2 idna 2.6 --> 2.8 MyProxyClient 2.0.1 --> 2.1.0 # pip-requires-test Paste 2.0.3 --> 3.0.6 coverage 4.4.2 --> 4.5.2 Sphinx 1.6.5 --> 1.8.3 sphinx-rtd-theme 0.2.4 --> 0.4.2  1.6.0 --> 1.7.0 Pygments 2.2.0 --> 2.3.1 pyflakes 1.6.0 --> 2.0.0 flake8 3.5.0 --> 3.6.0 pylint 1.7.4 --> 2.2.2 virtualenv 15.1.0 --> 16.2.0 tox 2.9.1 --> 3.7.0 pytest 3.2.5 --> 4.1.1 pytest-xdist 1.20.1 --> 1.26.0 pytz 2017.3 --> 2018.9 Babel 2.5.1 --> 2.6.0 subprocess32 3.2.7 --> 3.5.3 pycodestyle 2.3.1 --> 2.4.0 ", "2022": "---------- Rucio download called directly from CLI can be forced to transfer data using specific protocol with \"--protocol\" command line option. I think it would be also useful to use environment variable (e.g. RUCIO_PROTOCOL) to achieve same behavior, because with this functionality it is pretty easy to pass this \"protocol filter\" to the jobs running in environment with limited network connectivity that allows only certain protocol to pass through. ------------ Just minor changes in rucio code to also look at environment variable RUCIO_PROTOCOL in addition to the \"--protocol\" command line option for download/upload ", "2021": "---------- ------------ ", "2020": "---------- There are a few python3 incompatibilities that were not found by ```pylint --py3k``` ------------ - change ```except Exception, e``` to ```except Exception as e``` -> use ```2to3 -f except``` - ```Namespace' object has no attribute 'which'``` when using ```python3 bin/rucio-admin rse``` - Import error for module ```exceptions``` - ```TypeError: Unicode-objects must be encoded before hashing``` in heartbeat core ", "2015": "---------- The FTS probe sometimes doesn't whitelist a SE after the end of downtime ", "2010": "---------- ------------ ", "2007": "---------- - BelleII schema - BelleII has also a specific construct_surl function for non-deterministic transfers. At the moment, these functions are part of the core repository, so it will be added there. In the medium term, this has to be changed similar to the lfn2pfn calls. ", "2006": "---------- Selection of the authentication type should be case-sensitive and not throw an error. ``` (rucio-env) [rjoshi@rucio-bastion ~]$ rucio -a rjoshi -S X509 -v whoami 2019-01-20 09:43:15,381 ERROR Cannot authenticate. Details: auth type 'X509' not supported 2019-01-20 09:43:15,382 DEBUG Traceback (most recent call last): File \"/opt/rucio-env/bin/rucio\", line 160, in new_funct return function(*args, **kwargs) File \"/opt/rucio-env/bin/rucio\", line 300, in whoami_account client = get_client(args) File \"/opt/rucio-env/bin/rucio\", line 273, in get_client elif auth_type == 'x509_proxy': UnboundLocalError: local variable 'auth_type' referenced before assignment 2019-01-20 09:43:15,383 ERROR local variable 'auth_type' referenced before assignment 2019-01-20 09:43:15,383 ERROR Rucio exited with an unexpected/unknown error. Please rerun the last command with the \"-v\" option to gather more information. If it's a problem concerning your experiment or if you're unsure what to do, please followup at: alastair.dewhurst@cern.ch If you're sure there is a problem with Rucio itself, please followup at:  Completed in 0.0026 sec. ``` ", "2001": "---------- ``` $ curl -k -s  <?xml version=\"1.0\" encoding=\"UTF-8\"?> <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\"> <file name=\"data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data\"> <identity>data17_13TeV:data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data</identity> <hash type=\"adler32\">386e681b</hash> <size>2621968192</size> <glfn name=\"/atlas/rucio/data17_13TeV:data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data\"></glfn> <url location=\"BNL-OSG2_DATADISK\" priority=\"1\">root://dcdoor11.usatlas.bnl.gov:1094//pnfs/usatlas.bnl.gov/BNLT0D1/rucio/data17_13TeV/e2/5b/data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data</url> </file> </metalink> ``` But when `sort=geoip` is added: ``` $ curl -k -s  <?xml version=\"1.0\" encoding=\"UTF-8\"?> <metalink xmlns=\"urn:ietf:params:xml:ns:metalink\"> <file name=\"data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data\"> <identity>data17_13TeV:data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data</identity> <hash type=\"adler32\">386e681b</hash> <size>2621968192</size> <glfn name=\"/atlas/rucio/data17_13TeV:data17_13TeV.00341534.physics_MinBias.daq.RAW._lb1254._SFO-2._0003.data\"></glfn> </file> </metalink> ``` ------------ Something seems to fail in the geoip_sorter. ", "2000": "---------- Currently the tests in  are failing if you use a cache in your tests because you can read a stale value that got set during running the test due to a quite long expiration time. ------------ - force a readout from DB to get the current value - add memcache to the travis test (because the PR that added caching went through because travis was not using any cache) ", "1990": "---------- Add missing alias for the import and export endpoints. ------------ Add ```/import``` and ```/export``` to etc/web/aliases-py27.conf and etc/web/aliases-py26.conf ", "1987": "---------- Chunksize of 50 seems to lead to weird ORA error. ", "1986": "---------- ``` mlassnig@lxplus119:~ $ rucio -v download --allow-tape group.phys-higgs:group.phys-higgs.mc15_13TeV.mc16a.aMCnloHwpp_tWH125_yt_plus1.MxAODDetailed.p3309.h016a_shsun1.totape_20180313.root 2019-01-16 15:09:47,418 INFO Processing 1 item(s) for input 2019-01-16 15:09:47,418 DEBUG Processing item group.phys-higgs:group.phys-higgs.mc15_13TeV.mc16a.aMCnloHwpp_tWH125_yt_plus1.MxAODDetailed.p3309.h016a_shsun1.totape_20180313.root 2019-01-16 15:09:47,418 DEBUG RSE-Expression: istape=False 2019-01-16 15:09:47,419 DEBUG Splitted DID: group.phys-higgs:group.phys-higgs.mc15_13TeV.mc16a.aMCnloHwpp_tWH125_yt_plus1.MxAODDetailed.p3309.h016a_shsun1.totape_20180313.root 2019-01-16 15:09:47,440 DEBUG 1 DIDs after processing input 2019-01-16 15:09:47,440 DEBUG Processing: {'resolve_archives': True, 'name': 'group.phys-higgs.mc15_13TeV.mc16a.aMCnloHwpp_tWH125_yt_plus1.MxAODDetailed.p3309.h016a_shsun1.totape_20180313.root', 'did': 'group.phys-higgs:group.phys-higgs.mc15_13TeV.mc16a.aMCnloHwpp_tWH125_yt_plus1.MxAODDetailed.p3309.h016a_shsun1.totape_20180313.root', 'rse': 'istape=False', 'no_subdir': False, 'nrandom': None, 'transfer_timeout': 3600, 'scope': 'group.phys-higgs', 'type': u'DATASET', 'force_scheme': None, 'base_dir': '.'} 2019-01-16 15:09:47,606 INFO Using main thread to download 0 file(s) 2019-01-16 15:09:47,606 DEBUG Start processing queued downloads Completed in 0.2389 sec. ``` `--allow-tape` does not modify the `istape=False`. If the only copy is on tape it won't find any replica. ------------ `--allow-tape` must change the tape part of the RSE expression into something like `istape=False|istape=True` ", "1985": "---------- Adapt the threat handling similar to abbacus collection replicas. Remove --process --total-process etc. ", "1977": "---------- ``` 2019-01-14 18:50:01,944 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 694.900 sec 2019-01-14 19:00:03,802 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 591.847 sec 2019-01-14 19:20:06,086 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 985.620 sec 2019-01-14 19:40:04,927 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 746.443 sec 2019-01-14 20:00:06,481 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 631.958 sec 2019-01-14 20:20:06,564 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 568.809 sec 2019-01-14 20:50:07,622 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 827.529 sec 2019-01-14 21:10:11,076 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 699.798 sec 2019-01-14 21:30:09,410 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 516.796 sec 2019-01-14 22:00:12,660 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 1058.377 sec 2019-01-14 22:20:14,131 panda.log.Adder: DEBUG 4212476111 registraion with backend=rucio for 2 files took 780.243 sec ``` ", "1976": "---------- The `split_container` option is enabled when requests on containers are done via R2D2. It creates rules on the constituent datasets of the container instead of a single rule on the container. - It can trigger `IntegrityError` in the case a rule for scope, name, account, rse_expression already exists. - Similarly the same error can be raised if 2 constituents contain the same datasets. ------------ To handle the first issue, the injector must check if a rule on scope, name, account, rse_expression already exists. To handle the second one, it should make a set from the list returned by `list_child_datasets`. ", "1973": "---------- Currently the quota for the groups is never updated on SCRATCHDISK endpoints ", "1970": "---------- ``` $ rucio add-rule protodune-sp:np04_raw_run_number_6300 1 FNAL_DCACHE_STAGING 2019-01-11 13:42:06,597 ERROR An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97')) ``` ", "1967": "---------- If the source RSE is non-deterministic then even if the deterministic flag is set to false on an associated staging_area the staged replicas don't set the path, making them unusable. ------------ Set the path when the RSE is non-deterministic. ", "1965": "---------- It should be possible to control the maximum space used for an RSE, in addition to account quotas. ------------ Add a configurable parameter for the maximum RSE usage and check against this in the RSE selector. ", "1961": "---------- Update last occurrences of python3 incompatible code in tests/common/... ------------ ", "1958": "---------- Currently the caching has to be handled by all modules using these methods, but it should be done natively. Option to force a readout from the DB should be there as well. ", "1955": "----------  only shows accounts which have a quota on a given RSE. It can happen that somebody has no quota on a RSE but uses some space on this RSE (typically somebody who requested a rule via R2D2). In that case, it would be good to have it in the table. ", "1954": "---------- This will assist the cloud squads. Pull request #1708 laid the groundwork to process LOST files identified during the consistency check. ------------ Convert the LFNs to PFNs, then report them as suspicious. The `reason` will be set to `Reported by Auditor` so that it is easily identifiable. ", "1953": "---------- The Deterministic External id (eid) sent to FTS is generated using as seeds `voname`, `base_id` (both static, but obtained from a `whoami` against the FTS server) and the `request_id` of the first file of the FTS job. This Determinitic eid was introduced to prevent submitting 2 jobs for the same file : if there is already one job submitted for a given request_id, FTS will reject another attempt because they will have the same eid. Problems can occur when a first submission is done to FTS and FTS doesn't respond even if the submission was successful. In that case a second attempt will be done with the same `request_id`. Here are the potential issues : - The `whoami` fails and in that case `voname` and `base_id` are set to None. In that case the external eid generated will be different even if the `request_id` is the same. - Even if the `whoami` succeeds, the fact that only the `request_id` of the first file is considered leads to the fact that the transfers of [file1, file2] and [file2, file1] will generate 2 different eid. So basically the eid is useless for jobs with multiple destination replicas (i.e. more than 90 % of our transfers) ", "1952": "---------- There is currently no tool to provide the account usage history on on RSE. ------------ Will implement : - Core+API+REST - CLI - WebUI view ", "1948": "---------- It would be nice to define a default value when reading a config from the config table.  ) ------------ ```python def get(section, option, session=None) ``` to ```python def get(section, option, default=None, session=None) ``` Also REST and API have to be updated. ", "1945": "---------- Based on scripts provided by Gancho. ", "1942": "---------- needed to use a different set of FTS activities for the DOMA TPC tests. ------------ ", "1937": "---------- In case of a long deletion queue for an RSE, reorder the files to put the largest ones first. ", "1932": "---------- Currently the undertaker expires rules bigger than 10k locks, instead of deleting them immediately. This value should be configurable via the config table. ", "1929": "---------- Currently, locked rules (i.e. on tape storages) are not automatically deleted, requiring an additional manual step. ------------ Disable the lock when applying the lifetime. ", "1924": "---------- Make the daemons python3 compatible. ------------ ", "1922": "---------- Currently Rucio has three notification modes: - `YES`: Notify all state changes - `NO`: Notify no state changes at all - `CLOSE`: Only notify when rule is `OK` and dataset is closed. For the TapeCarousel activity an additional mode would be beneficial, where a notification is sent once the rule reaches a certain percentage of completion. e.g. 75%. After that the mode can be changed to `CLOSE`. ", "1918": "---------- The quota for SCRATCHDISK endpoint is now a fraction of the total space. One should have the possibility to define a fixed value ", "1917": "---------- Just wanted to document this: ``` Traceback (most recent call last): File \"/usr/bin/rucio-reaper\", line 11, in <module> load_entry_point('rucio==1.18.6.post1', 'console_scripts', 'rucio-reaper')() File \"/usr/lib/python2.7/site-packages/rucio/clis/daemons/reaper/reaper.py\", line 55, in main exclude_rses=args.exclude_rses, include_rses=args.include_rses, delay_seconds=args.delay_seconds) File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/reaper.py\", line 399, in run while threads[0].is_alive(): IndexError: list index out of range ``` It goes away once I defined a single RSE ------------ ", "1912": "---------- Make the core module of Rucio python3 compatible. ------------ ", "1910": "---------- I'd expect Rucio to install on a fairly plain RHEL7 box with the following: ``` virtualenv-2 rucio-doma cd rucio-doma . bin/activate pip install rucio-clients ``` However, when I do this locally, I get: ``` $ rucio Traceback (most recent call last): File \"/home/cse496/bbockelm/software/rucio-doma/bin/rucio\", line 61, in <module> from rucio.client.client import Client File \"/home/cse496/bbockelm/projects/rucio/lib/rucio/client/__init__.py\", line 9, in <module> from rucio.client.client import * # NOQA pylint: disable=wildcard-import File \"/home/cse496/bbockelm/projects/rucio/lib/rucio/client/client.py\", line 17, in <module> from rucio.client.accountclient import AccountClient File \"/home/cse496/bbockelm/projects/rucio/lib/rucio/client/accountclient.py\", line 21, in <module> from rucio.client.baseclient import BaseClient File \"/home/cse496/bbockelm/projects/rucio/lib/rucio/client/baseclient.py\", line 39, in <module> from requests_kerberos import HTTPKerberosAuth ImportError: No module named requests_kerberos ``` That is, the `requests_kerberos` dep appears missing. Note this is with version 1.18.7. ------------ Either the base client should silently ignore the missing kerberos client (until someone tries to use it) or it should be added to `setup.py`. A simple `pip install requests_kerberos` seems to fix the problem. ", "1909": "---------- For very large rules, the undertaker is blocked a long time to delete them which leads to long backlogs. ------------ Instead, for rules > 10k files it could just flag the rule for expiration and set the timeout of the dataset 1 day in the future. Doing this, the judge-cleaner will take care of the rule deletion and thus free up the undertaker to do its job. ", "1901": "---------- in lib/rucio/common/schema/cms.py the RSE Name pattern is `^T[0-3]_[A-Z]{2}((_[A-Za-z0-9]+)+)$` while the pattern for the rse property of ATTACHMENT is `^([A-Z0-9]+([_-][A-Z0-9]+)*)$` as a result when we add replicas to a CMS RSE whose name is not all in capitals (e.g. T1_UK_RAL_Buffer): * if we use `add_replicas` and the `attach_dids` it works fine * if we use `attach_dids` directly it fails with `InvalidObject` ------------ Just put the same regexp for the two patterns (PR coming) ", "1898": "---------- Minor updates and pointing to the right URLs. ", "1888": "---------- Fix the last small changes to make the clients python3 compatible. This closes #819 - didclient - fileclient - dq2client - downloadclient ------------ ", "1885": "---------- The way the reaper is implemented now leads to the fact that we have different instances processing each a fixed partition. In the worst case scenario, one can have one reaper dedicated to one RSE that will have millions of files to process, whereas all the other instances stay idle because there is nothing to be deleted on the RSEs they serve. Here is a proposition to have a horizontally scalable reaper. ------------ The proposal is to split the reaper into 2 parts. - One first daemon `reaper-preparer` would run the `list_unlocked_replicas` on all the RSEs and identify all the replicas that need to be deleted based on the `needed_free_space`. All these replicas would get the state `BEING_DELETED`. - A second daemon (the true reaper) which will run the physical deletion gets the list of all `BEING_DELETED` replicas. The query can be partitioned on a hash of the name of the DIDs to have balanced partitions between different instances. Doing this way, we can ensure that all instances will get the same share of work. The true reaper physically deletes the files and issue the `delete_replicas`. There might be a few issues to address (e.g. it can be that all the reapers are running against the same RSE and can cause some denial of service). Comments welcome. ", "1882": "---------- Some traces are wrongly defined. Need a protection in kronos. ", "1879": "---------- The account_usage_history is not populated anymore. The Oracle procedure needs to be modified to populate it. ", "1878": "---------- The original procedure was dropped in 2017 and was forgotten \ud83d\ude22 We need a new procedure similar to the current ADD_RSE_USAGE procedure. Note: Check how this is done in #524 as well. ", "1874": "---------- It's defined on the DB, but not used anywhere. Should be removed. ", "1871": "---------- For the suspicious files that have multiple replicas, there is no need of human check to confirm that the replica is actually bad. One can have a probe to automatically recover them. ", "1870": "---------- The reporting of suspicious files in Kronos is broken ", "1864": "---------- The number of suspicious is way more bigger than the actually big one. One should have the possibility to filter out the suspicious files from the bad files summary plots ", "1861": "---------- In order to be generic in terms of DB, sql query in bb8 needs to be replaced by sql alchemy query. ------------ rewrite the query. ", "1853": "---------- Stumbled upon them while using Rucio. This issue collects them, though they aren\u2019t related. ------------ Small patches of little consequence. ", "1852": "---------- ------------ ", "1849": "This affects the `check_deletable_replicas` and `check_obsolete_replicas` probes. The SQL queries do not deal with RSEs where everything has already been deleted by the Reaper. ", "1848": "---------- Enhance deamon documentation ------------ ", "1845": "----------  is not compatible to Python 2.6 it seems, which is a problem for the deployment on the grid Py2.6: ``` [desilva@atlas-sl6x64 ~]$ rucio --version Traceback (most recent call last): File \"/ALRB/atlasadmin/atlas-sl6x64.triumf.ca/ATLASLocalRootBase/x86_64/rucio-clients/1.18.6/bin/rucio\", line 63, in <module> from rucio.client.downloadclient import DownloadClient File \"/ALRB/atlasadmin/atlas-sl6x64.triumf.ca/ATLASLocalRootBase/x86_64/rucio-clients/1.18.6/lib/python2.7/site-packages/rucio/client/downloadclient.py\", line 35, in <module> from builtins import round ImportError: No module named builtins ``` ", "1842": "---------- The output produced by the `get-metadata`, `list-rse-attributes` and `stat` commands is not easily readable. ------------ 1. Sort the keys alphabetically. 2. Align the values vertically (will use `tabulate` package). 3. Add a separator when running for multiple DIDs (does not apply to `list-rse-attributes`). ", "1841": "---------- Overhaul readmes, organise daemons, etc. ", "1836": "---------- Right now throtteling is possible per `(activity, destination_rse)` tuple. However, it would be good for the tape carousel to be able to throttle per destination over all activities. ", "1835": "---------- In  the error message says the fts host is missing for the source, while it should say it is missing for the destination. ", "1833": "---------- in some cases, the stat failed due to service unavailable, although the file was phsysically there. ------------ at line:  try, fail, wait 1 second, try, fail, wait 2 seconds, try, fail wait 4 seconds, try fail, wait 16 seconds, try fail, wait 32 seconds, try, fail finally ", "1825": "---------- Kronos crashes when updating eol_at ", "1818": "---------- There is an inconsistency with the PKs in history tables which has basically two reasons: - SQLAlchemy requires each table in models.py to have a PK (Otherwise it could not be a ORM) - In ATLAS on Oracle, we do not have PKs on History tables to save storage This creates an inconsistency how we (developers) expect these history tables to operate between instances which run the oracle schema (without PKs) and instances where the schema is generated from models.py (With PKs) For example, there was instances where we make MERGE INSERTS in the history tables. This clearly works in cases where there is a PK, but it doesnt work for ATLAS, as there is no PK on the table. In the other way around, multiple inserts work on the tables for ATLAS, but do not work on tables created by the models.py, due to the uniqueness of the PK. ------------ I would propose to change all models.py History table to define the PK as `mapper_args` e.g. ``` __mapper_args__ = { 'primary_key': [scope, name, child_scope, child_name] } ``` This basically tells SQLAlchmey that there is a PK, to satisfy this requirement, while in reality there is None. Secondly we would have to make an upgrade script which drops all PKs in these tables. Communities who query these history tables frequently, just need to create indices on these tables. ", "1814": "---------- ------------ ", "1811": "---------- The token generation in the auth core has changed. Now instead of just the token a dict with the token and expiration time is returned. ------------ Fix the code to extract the token from the dict. ", "1810": "---------- When a subscription is changed, we don't keep the history. Will implement subscriptions history ", "1809": "---------- First small integration project to familiarise myself with the rucio environment. ------------ Migrate the rest of the data from old ES instance to new es-atlas6.cern.ch. ", "1807": "---------- For some reason not understood yet, we can have some replicas having `lock_cnt!=0` but no rows in the locks table. The necromancer now crashes because of this. It must be able to handle it properly : ``` 2018-11-21 14:16:49,533 22982 CRITICAL Thread [1/1] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/necromancer.py\", line 85, in necromancer update_rules_for_lost_replica(scope=scope, name=name, rse_id=rse_id, nowait=True) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 351, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 1761, in update_rules_for_lost_replica raise RucioException('Problem with the locks') RucioException: An unknown exception occurred. ``` ", "1805": "---------- Fix the mixture of REST call and python bindings present in the FTS3 transfer tool. Particular attention on testing and consistency check between REST response and binding ones. ------------ - Mainly integrating what was done for [user  in the default tool - check for response equivalence w.r.t. REST response - mock_fts3 may be needed for a proper testing ", "1804": "---------- Right now the configuration of the reaper decides which mode of deletion is used. This causes us to run different types of reapers for no good reason. If the mode is an RSE attribute this is more flexible. ", "1803": "---------- Right now we have our python file which describes our LFN to PFN algorithm in our own repo:  This has the unfortunate side effect of requiring CMS to build it's own docker container. I'd like to move this into Rucio either at rucio/lib/rucio/common/schema/ where another policy file is or at rucio/lib/rucio/rse/protocols/ which is where the code that imports it lives. @sartiran, I think to import this we can just specify the full python path in the config file and all is good. ------------ Once we have agreement on whether to do this and where to put it, we just move the CMS file over. ", "1792": "---------- when calculating the ratio of secondaries and total:  the bb8 is sometimes attempting dividing by zero. ------------ exclude such a cases. ", "1791": "---------- As discussed in #1786 , the REST call of  get broken for DIDs containing `/` ------------ I think that rather than doing this (*) we can pass all the needed information (scope, name, rse) as params instead of including them in the url path. (*)  ", "1788": "---------- Traceback : ``` 2018-11-15 00:13:40,539 23840 ERROR Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/poller.py\", line 248, in poll_transfers for transfer_id in resps: UnboundLocalError: local variable 'resps' referenced before assignment ``` ", "1787": "---------- Rucio has support for S3-style objectstore download and upload, using shared secrets (with boto and 's3cmd'), as well as native support for S3 signed URLs. There are a few implications: * Shared secrets, as the name implies, require distribution of the secrets to clients which limits its usefulness. We cannot guarantee confidentiality as shared secrets can leak easily. * The code itself is still from early days of R&D and follows an old architecture. * Swift is an S3-style compatible protocol, targeted to OpenStack instead of Amazon, which exists as an untested prototype. In the meantime, URL signatures were implemented natively in Rucio, with a Google Cloud Storage signature plugin, which are similar to S3 signatures. The first objective would be to revisit the S3 and Swift signatures and bring it in line with the remainder of the code, such that it can be used transparently by the replica core. ------------ ", "1786": "---------- Clients should be able to retrieve request details (FTS jobID etc) for transfer monitoring purpose. ------------ Make results available in query_request (@api/request.py) into web/rest/request.py. If I have understood well the suggestion by @mlassnig . ", "1782": "---------- This was changed in a recent PR, however, the call on `urlparse.urlparse` is wrong, `urlparse` method should be called directly. ", "1777": "---------- The current Primary Key for `content_history` is based on `scope`, `name`, `child_scope`, `child_name`. Because of this, one cannot keep the history when a dataset is attached/detached multiple times. ------------ Change to another PK ", "1774": "---------- Currently the documentation builds fail with ``` Running Sphinx v1.6.5 making output directory... loading translations [en]... done Traceback (most recent call last): File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio/envs/latest/local/lib/python2.7/site-packages/sphinx/cmdline.py\", line 305, in main opts.warningiserror, opts.tags, opts.verbosity, opts.jobs) File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio/envs/latest/local/lib/python2.7/site-packages/sphinx/application.py\", line 196, in __init__ self.setup_extension(extension) File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio/envs/latest/local/lib/python2.7/site-packages/sphinx/application.py\", line 456, in setup_extension self.registry.load_extension(self, extname) File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio/envs/latest/local/lib/python2.7/site-packages/sphinx/registry.py\", line 199, in load_extension raise ExtensionError(__('Could not import extension %s') % extname, err) ExtensionError: Could not import extension rucio.common.doc.argparse.ext (exception: No module named rucio.common.doc.argparse.ext) Extension error: Could not import extension rucio.common.doc.argparse.ext (exception: No module named rucio.common.doc.argparse.ext) ``` I did not look too much into the issue yet, but I wonder why we added this `rucio.common.doc.argparse.ext` to the repository instead of using `sphinx-argparse`? @vingar do you maybe remember? ", "1772": "---------- The undertaker crash when a DID is attach/detached many times. The problem is due to the archival in the history table :  . ", "1766": "---------- Bad replicas declaration in kronos currently fails with an exception which slows down kronos a lot: ``` Exception in thread StompReceiverThread-114666: Traceback (most recent call last): File \"/usr/lib64/python2.7/threading.py\", line 812, in __bootstrap_inner self.run() File \"/usr/lib64/python2.7/threading.py\", line 765, in run self.__target(*self.__args, **self.__kwargs) File \"/usr/lib/python2.7/site-packages/stomp/transport.py\", line 342, in __receiver_loop self.process_frame(f, frame) File \"/usr/lib/python2.7/site-packages/stomp/transport.py\", line 191, in process_frame self.notify(frame_type, f.headers, f.body) File \"/usr/lib/python2.7/site-packages/stomp/transport.py\", line 245, in notify rtn = notify_func(headers, body) File \"/usr/lib/python2.7/site-packages/rucio/daemons/tracer/kronos.py\", line 126, in on_message self.__update_atime() File \"/usr/lib/python2.7/site-packages/rucio/daemons/tracer/kronos.py\", line 150, in __update_atime logging.error('Failed to declare suspicious file' + error) TypeError: cannot concatenate 'str' and 'exceptions.TypeError' objects ``` ------------ Convert correctly to str ", "1765": "---------- If transmogrifier is set up to run without specifying any subscriptions, the CPU load becomes excessive. Kronos shows similar heavy CPU loads in the absence of subscriptions. ------------ ", "1758": "---------- In CMS we get use to these concepts: a dataset is collection of blocks, a block is collection of files. I understand that in ATLAS a dataset is a container, etc. But apart from naming convention differences I need to find block replicas (in CMS terminology). If we call a dataset as /a/b/c pattern, a block as /a/b/c#123xys pattern and file is /file.root one. The `/replicas/scope/name` can take either dataset or block patterns and return list of files. How I can find list of block (/a/b/c#123xys pattern) replicas? Is it issue with /replicas/scope/name/datasets API in this  ticket or there is no such API? CC'ing Eric (@ericvaandering) to follow up with this issue. ", "1755": "---------- If the regular expression used to detect the suspicious files is bad, it can crash Kronos. Need to had a protection. ", "1749": "----------  The PK for the replicas table is in `order rse_id, scope, name` while it actually should be in order `scope, name, rse_id` This needs to be changed and we need a proper upgrade script so this can be adapted at all installations. ", "1739": "The scope validation regex for atlas/generic/icecube (but not cms) allows single quotes, which seems like a bad idea. `\"^[a-zA-Z'_'-.0-9]{1,%s}$\" % SCOPE_LENGTH` looks wrong, and I'm guessing it ought to be `\"^[a-zA-Z_\\\\-.0-9]{1,%s}$\" % SCOPE_LENGTH`. Unless someone really does have this requirement? ", "1738": "---------- Update REST API (webpy, flask) to be compatible with python3. Also needed for #1026 ------------ ", "1731": "---------- Once we perform authentication with rucio server it yield the valid token but does not provide lifetime of the token. Here is a typical example: ``` curl -vvv --key $X509_USER_PROXY --cert $X509_USER_PROXY -H \"X-Rucio-Account:das\" -A \"rucio-clients/1.16.1\"  * About to connect() to cms-rucio-authz.cern.ch port 443 (#0) * Trying 188.184.86.14... * Connected to cms-rucio-authz.cern.ch (188.184.86.14) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * NSS: client certificate from file * subject: CN=163734436,CN=Valentin Y Kuznetsov,CN=443502,CN=valya,OU=Users,OU=Organic Units,DC=cern,DC=ch * start date: Oct 31 21:49:00 2018 GMT * expire date: Nov 01 09:49:00 2018 GMT * common name: 163734436 * issuer: CN=Valentin Y Kuznetsov,CN=443502,CN=valya,OU=Users,OU=Organic Units,DC=cern,DC=ch * SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 * Server certificate: * subject: CN=cms-rucio-authz.cern.ch,OU=computers,DC=cern,DC=ch * start date: Oct 05 15:38:50 2018 GMT * expire date: Nov 09 15:38:50 2019 GMT * common name: cms-rucio-authz.cern.ch * issuer: CN=CERN Grid Certification Authority,DC=cern,DC=ch > GET /auth/x509 HTTP/1.1 > User-Agent: rucio-clients/1.16.1 > Host: cms-rucio-authz.cern.ch > Accept: */* > X-Rucio-Account:das > < HTTP/1.1 200 OK < Date: Wed, 31 Oct 2018 21:49:50 GMT < Server: Apache < Access-Control-Allow-Origin: None < Access-Control-Allow-Headers: None < Access-Control-Allow-Methods: * < Access-Control-Allow-Credentials: true < Access-Control-Expose-Headers: X-Rucio-Auth-Token < Cache-Control: no-cache, no-store, max-age=0, must-revalidate < Cache-Control: post-check=0, pre-check=0 < Pragma: no-cache < X-Rucio-Auth-Token: das-/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=valya/CN=443502/CN=Valentin Y Kuznetsov-unknown-5d16fe50fef3440694bbfcff899626e6 < X-Rucio-Host: cms-rucio-authz.cern.ch < Content-Length: 0 < Content-Type: application/octet-stream < * Connection #0 to host cms-rucio-authz.cern.ch left intact ``` Out of this request I don't know how long my X-Ruxio-Auth-Token is valid. I would expect that you'll supply corresponding lifetime of the token in separate HTTP header. ------------ ", "1730": "---------- It seems like the Rucio REST API does not actually return true JSON. This is an e-mail from @vkuznet to me about this. The context is that CMS uses something called DAS to aggregate info across our data management system, data catalog, and other systems. It relies on REST APIs to do this in a reasonable fashion (not a client library for each system). Thoughts? It seems Rucio APIs do not return proper JSON: ``` curl -H \"X-Rucio-Auth-Token: $token\"  {\"rse_type\": \"DISK\", \"id\": \"c7ed36b8906c4808b090b35976373f88\", \"region_code\": null, \"city\": null, \"updated_at\": \"Fri, 28 Sep 2018 19:11:56 UTC\", \"rse\": \"T2_US_Nebraska_Test\", \"created_at\": \"Fri, 28 Sep 2018 19:11:56 UTC\", \"ISP\": null, \"deleted\": false, \"time_zone\": null, \"longitude\": null, \"availability\": 7, \"deterministic\": true, \"volatile\": false, \"country_name\": null, \"latitude\": null, \"deleted_at\": null, \"continent\": null, \"ASN\": null, \"staging_area\": false} {\"rse_type\": \"DISK\", \"id\": \"1a8167d0f9c743749ecd3bf00d4fb04c\", \"region_code\": null, \"city\": null, \"updated_at\": \"Wed, 24 Oct 2018 12:06:07 UTC\", \"rse\": \"T3_US_NERSC\", \"created_at\": \"Wed, 24 Oct 2018 12:06:07 UTC\", \"ISP\": null, \"deleted\": false, \"time_zone\": null, \"longitude\": null, \"availability\": 7, \"deterministic\": true, \"volatile\": false, \"country_name\": null, \"latitude\": null, \"deleted_at\": null, \"continent\": null, \"ASN\": null, \"staging_area\": false} ``` and ``` curl -H \"X-Rucio-Auth-Token: $token\"  {\"account\": \"das\", \"type\": \"USER\", \"email\": null} {\"account\": \"dciangot\", \"type\": \"USER\", \"email\": null} ``` Yes, they return JSON dicts, but it is not single JSON document, and therefore it fails to be parsed. Here is example to prove it. ``` curl -H \"X-Rucio-Auth-Token: $token\"  > account.json python Python 2.7.5 (default, Jul 13 2018, 13:06:57) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. import json data = json.load(open(\"accounts.json\")) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib64/python2.7/json/__init__.py\", line 290, in load **kw) File \"/usr/lib64/python2.7/json/__init__.py\", line 338, in loads return _default_decoder.decode(s) File \"/usr/lib64/python2.7/json/decoder.py\", line 369, in decode raise ValueError(errmsg(\"Extra data\", s, end, len(s))) ValueError: Extra data: line 2 column 1 - line 9 column 1 (char 50 - 419) ``` I also tried to use HTTP header Content-Type: application/json and results are the same. I don't want to crawl over strings (this is what APIs return now) and convert them to JSON docs. Instead, I would [like] a JSON document. For the record, the proper JSON would be something like (based on accounts API output): ``` [ {\"account\": \"das\", \"type\": \"USER\", \"email\": null}, {\"account\": \"dciangot\", \"type\": \"USER\", \"email\": null} ] ``` It is also desired that all APIs will use the same data-type, a list of dicts, even if one item in a list is present. This will allow to abstract parsing code much easily and make it API independent. Best, Valentin. ------------ ", "1729": "The sort_rses() function at  excludes rses that have no usage source of 'gsiftp' or 'srm' - but only if the input list has more than one entry. This has the effect of making the reaper silently ignore rses that don't use legacy protocols unless there's only one rse per thread. Does the storage usage source test have any useful purpose? ", "1726": "---------- `core.did.set_metadata` only update the checksum and bytes in `dids` and `contents` table. But it doesn't do anything for the replicas. ", "1725": "---------- If `rucio-admin rse set-distance` is used with a source/destination rse combination that already has a distance the server replies with a Duplicate exception. This is not caught correctly but instead a traceback is shown for an unexpected error. ``` Set distance from UKI-MAN-DPM_X to UKI-GLASGOW-DPM_X to 1 with ranking 1 An object with the same identifier already exists. Details: Distance from 3937a5b36cbe4cdbbb7ba1c12617e936 to 3937a5b36cbe4cdbbb7ba1c12617e936 already exists! Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers. Traceback (most recent call last): File \"/opt/rucio/bin/rucio-admin\", line 121, in new_funct return function(*args, **kwargs) File \"/opt/rucio/bin/rucio-admin\", line 528, in add_distance_rses client.add_distance(args.source, args.destination, params) File \"/opt/rucio/.venv/lib/python2.7/site-packages/rucio/client/rseclient.py\", line 534, in add_distance raise exc_cls(exc_msg) Duplicate: An object with the same identifier already exists. Details: Distance from 3937a5b36cbe4cdbbb7ba1c12617e936 to 3937a5b36cbe4cdbbb7ba1c12617e936 already exists! ``` ------------ Provide a short and meaningful error message. ", "1718": "---------- The function to get the adler32 checksum of a given file:  doesn't close the file correctly if an exception is raised and also doesn't propagate the exception to the caller. ------------ ", "1710": "---------- Throttler must not release WAITING requests on blacklisted sites ", "1703": "---------- I am not sure if I found a bug in the RSE deletion in core.rse.del_rse() or if it is more a session problem. I tried this test but it did not work. ```python from rucio.core.rse import add_rse, del_rse, rse_exists rse_name = 'TEST' add_rse(rse_name) del_rse(rse_name) rse_exists(rse_name) # -> True ``` Same happens with this. I suspect it relates to the way sqlalchemy handles the deletion and relationships. ```python from rucio.db.sqla import session,models rse_name = 'TEST2' db_session = session.get_session() models.RSE(rse=rse_name).save(session=db_session) db_session.query(models.RSE).filter_by(rse=rse_name).count() # -> 1 rse = db_session.query(models.RSE).filter_by(rse=rse_name).one() # This deletion is used in core.rse.del_rse() rse.delete(session=db_session) db_session.commit() db_session.query(models.RSE).filter_by(rse=rse_name).count() # -> 1 ``` Strangely the deletion works when using the API and a RSENotFound error gets raised in a second deletion. But if I try to get the RSE usage, the request does not raise a RSENotFound which it should because it is querying the RSE to get the ID. ``` DELETE /rses/TEST -> 200 DELETE /rses/TEST -> 404 GET /rses/TEST/usage -> 200 ``` ------------ The deletion works as expected if I use the [sqlalchemy bulk  with the corresponding ON DELETE CASCADE added to the model definitions of RSEUsage, RSEAttrAssociation, RSEProtocols, etc. ```python from rucio.db.sqla import session,models rse_name = 'TEST3' db_session = session.get_session() models.RSE(rse=rse_name).save(session=db_session) db_session.query(models.RSE).filter_by(rse=rse_name).count() # -> 1 db_session.query(models.RSE).filter_by(rse=rse_name).delete(synchronize_session=False) db_session.query(models.RSE).filter_by(rse=rse_name).count() # -> 0 ``` ``` DELETE /rses/TEST -> 200 DELETE /rses/TEST -> 404 GET /rses/TEST/usage -> 404 ``` ", "1702": "---------- Traceback : ``` 2018-10-25 12:43:03,955 21738 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/hermes/hermes.py\", line 259, in deliver_messages conn.connect(wait=True) File \"/usr/lib/python2.7/site-packages/stomp/connect.py\", line 215, in connect Protocol12.connect(self, *args, **kwargs) File \"/usr/lib/python2.7/site-packages/stomp/protocol.py\", line 504, in connect headers[HDR_HOST] = self.transport.current_host_and_port[0] TypeError: 'NoneType' object has no attribute '__getitem__' ``` ", "1701": "---------- `--process`, `--total-processes` are not used anymore and should be removed ", "1698": "---------- Need to fix to 2.19.* and to urllib3 < 1.24 ", "1695": "---------- Some S3 storage report `204 No Content` if a file does not exist instead of `404` which creates error reports in Rucio. Expand the protocol to also interpret the `204` as a non existing file. ", "1692": "---------- There was an update of urllib3 to 1.24 which is incompatible with requests 2.19.1. However, requests specifies that it needs urllib<1.24, thus this should be considered by pip when installing the dependencies of Rucio. For some reason it is not: ``` > rucio /tmp/rucio_test_install/lib/python2.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24) or chardet (3.0.4) doesn't match a supported version! RequestsDependencyWarning) Traceback (most recent call last): File \"/tmp/rucio_test_install/bin/rucio\", line 57, in <module> from rucio.client.client import Client File \"/tmp/rucio_test_install/lib/python2.7/site-packages/rucio/client/__init__.py\", line 19, in <module> from rucio.client.client import * # NOQA pylint: disable=wildcard-import File \"/tmp/rucio_test_install/lib/python2.7/site-packages/rucio/client/client.py\", line 27, in <module> from rucio.client.accountclient import AccountClient File \"/tmp/rucio_test_install/lib/python2.7/site-packages/rucio/client/accountclient.py\", line 27, in <module> from requests.status_codes import codes File \"/tmp/rucio_test_install/lib/python2.7/site-packages/requests/__init__.py\", line 112, in <module> from . import utils File \"/tmp/rucio_test_install/lib/python2.7/site-packages/requests/utils.py\", line 26, in <module> from ._internal_utils import to_native_string File \"/tmp/rucio_test_install/lib/python2.7/site-packages/requests/_internal_utils.py\", line 11, in <module> from .compat import is_py2, builtin_str, str File \"/tmp/rucio_test_install/lib/python2.7/site-packages/requests/compat.py\", line 48, in <module> from urllib3.packages.ordered_dict import OrderedDict ImportError: No module named ordered_dict ``` ", "1689": "---------- If you delete the RSE attribute with the same name as the RSE, you can not delete the RSE afterwards. The request will fail with status code 500 and 'Internal server error' message. ``` DELETE /rses/MOCK/attr/MOCK DELETE /rses/MOCK ``` ------------ I would create a new exception class e.g. RSEAttributeNotFound and throw it  if a NoResultFound exception gets raised and then respond with the corresponding error message. Then there should be a check if the attribute to be deleted has the same name as the RSE to avoid this exception in the RSE deletion. ", "1686": "---------- The doc string for this  seems to be outdated. ------------ I would add the query parameters 'created_before', 'created_after', 'name', 'length', 'length.gt', 'length.lt', 'length.lte' and 'length.gte'. ", "1685": "---------- There is a `set_account_status` to update the status of an account, but there is no `update_account` method to update the account email for instance. Proposal is to get rid of `set_account_status` and replace it by `update_account(account, key, value)`. Change is needed at all levels (core, API, REST, CLI) ", "1682": "---------- New urllib3 version (1.24) was just released which is incompatible to requests 2.19.1. With the way we install the test environment, this dependency graph is not resolved, thus the tests fail. ", "1677": "---------- This needs to be in the normal and metalink return. See #1354 ", "1672": "---------- Currently the output of this command is ``` ... ------ account: xxx used: 562.593 MB percentage: 0.04 ------ ------ account: yyy used: 390.700 MB percentage: 0.03 ------ ------ account: zzz used: 369.034 MB percentage: 0.03 ... ``` while it should be: ``` account: xxx used: 562.593 MB percentage: 0.04 account: yyy used: 390.700 MB percentage: 0.03 account: zzz used: 369.034 MB percentage: 0.03 ``` With the `-----` only at the top and bottom of the entire list. Should be formatted so that the 3 columns are aligned. ", "1669": "---------- it can happen in uploadclient, that registration fails, but file proceeds with an upload. Such a data is most probably deleted by dark-reaper soon. The case when registration fails, should be mentioned in the trace['stateReason']. ------------ introduce return codes in _register_file method and populate the trace. ", "1668": "---------- While running the test of this  I encountered an exception only with PostgreSQL as database, that is not catched. It raised after tryting to attach a dataset to a container twice. ``` rucio add-container mock:container rucio add-dataset mock:dataset rucio attach mock:container mock:dataset rucio attach mock:container mock:dataset ERROR An unknown exception occurred. Details: [u'(psycopg2.IntegrityError) duplicate key value violates unique constraint \"CONTENTS_PK\"\\nDETAIL: Key (scope, name, child_scope, child_name)=(mock, co, mock, da) already exists.\\n'] ``` With other databases a DuplicateContent gets thrown. ------------ I would add it to the pattern matching of the IntegrityError  ", "1664": "---------- Discussion in DDM mailing list and suboptimal LDAP access in synchronization script. ------------ Improved check_voms synchronization script - use python ldap module instead of parsing ldapsearch command line output. Updated code use TLS for paged LDAP search to get info about all account (in less than 10s). ", "1663": "---------- Currently, if the constituent has a normal replica, even on tape, this one is preferred over the archive replica. However, if only a tap replica is available, the archive should be preferred. ", "1656": "---------- When a user change its primary account, the voms collector needs to update it. ", "1651": "---------- The Auditor should know the number of files on an RSE and compare it with the number of results produced by the consistency check. This will help automatically identify cases where the dump created by the site is incomplete or uses the wrong root directory. ------------ This information is already available. `rucio.core.rse:get_rse_usage()` will be modified to add it to the results, `bin/rucio` will be modified to only show it for the `rucio` source. ", "1647": "---------- It's not possible to set rse_type in add_rse. Same comment for latitude, longitude, etc. ", "1646": "----------   is only an issue in Paramiko server, which we do not use; Should update to 2.4.2 nevertheless. ", "1639": "---------- The check for the oracle version needed for the generic meta data has a typo in calling the connection function  ------------ ``` session.connection().connection ``` instead of ``` session.connection.connection ``` ", "1638": "---------- Right now the Nagios probes are part of the core repository, but they should not be. We should move them to a separate atlas-rucio-probes repository (or similar). Over time this can also converge to a common probe repository, as CMS most likely will also use a large fraction of these probes. ", "1637": "---------- Currently the reaper is over-protective with the protection of sources if a transfer is active. All possible sources of an active transfer are not eligible for deletion, which is too protective, if there are many possible sources. This should be changed to: - If only 1 source is available, protect it --> behaviour stays as it is - If 2 or more sources are available, all are eligible for deletion, except the one with the alphabetically first RSE (or the one which is oldest) - some shared knowledge so that multiple reapers would not accidentally delete the 2 remaining sources. This was discussed in the [dev meeting on  ", "1635": "---------- list_bad_replicas_status implicitely uses SRM. It can be a problem for the sites without SRM ", "1632": "---------- `bin/conveyor-submitter` and `bin/conveyor-transfer-submitter` point to the same code. Historically they were different, but now there is no point to have both. We should get rid of the conveyor-transfer-submitter in the 1.19.0 release. ", "1625": "---------- The list of SCRATCHDISK endpoints used for the second replication of users' data is currently static. One would need to have a dynamic list based on the `freespace`RSE attribute ", "1624": "---------- The `--group-policy` option is not propagated properly to the conveyor. ", "1623": "---------- Currently the suspicious files are reported by the conveyor, the pilot and `rucio download`. We can centralize everything so that Kronos checks the traces and reports as suspicious all the files with errors that match a certain pattern ", "1622": "---------- > Hi, > Download of data16_13TeV:DRAW_RPVLL.11106701._000026.pool.root.1 > > produces the file > > data16_valid/DRAW_RPVLL.11106701._000026.pool.root.1 > > This is the scope of the zip file containing it, but should be the scope of the constituent. > > This is not a problem until it is - I think in general the scopes will be the same. > > However, I don`t think there is a requirement that this be the case, so this should probably be fixed. > > Please also check that the file(s) end up in the correct directories in the case where the whole zip is downloaded. > > Cheers, > Rod. ", "1619": "---------- See  ", "1614": "---------- ------------ ", "1613": "---------- If the client requests a specific RSE expression then the returned file data (checksum, bytes, ...) is from the archive, not the constituent. ", "1610": "---------- The `bad_replicas` table was extended with a bytes column for monitoring purpose. This column needs to be populated when a bad file is declared ", "1606": "---------- In 1.17.* the wildcard download stopped working. (Related to re-implementation of downloader) ", "1601": "---------- CMS needs to use AllowEncodedSlashes as an HTTPD directive to allow DIDs with slashes in them. Document this in a couple of places. ------------ ", "1600": "---------- eg.  When clicking the add protocol button It links to the wrong URL. ", "1599": "---------- The current workflow when uploading a file is to register the replica as `UNAVAILABLE` first; Process the upload; Switch the replica to `AVAILABLE`. When the upload fails, the command can just be re-executed, however it is verified that a file with a similar checksum is uploaded. We recently were told about official workflows where the re-upload, after a failure, would be done with a different file; Thus same filename but different checksum, which does not work in this workflow. The proposed solution is to offer a switch `--register-after-upload` (or something similar) where the client uploads first and registers the replica only as `AVAILABLE` when successful. Thus the pre-registration as `UNAVAILABLE` is skipped. In this case however, if the upload is repeated, the destination file always has to be over-written, as it could always be a new file being uploaded, as long as it is not registered in the catalog yet. ", "1598": "---------- We implemented a workaround (#1596) since gfal doesnt correctly call xroot copy when an archive PFN is given. This can be removed again as soon as gfal is fixed. ------------ ", "1593": "---------- The download client only reports suspicious files if there is a checksum mismatch. In case the download fails because of missing files, it doesn't report anything. The client should parse the error reported by `protocol.get` and if it matches some pattern, report. ------------ Fix it ", "1590": "---------- Testcase runs too fast before cache is invalidated. ", "1589": "---------- Currently the travis tests do not use memcached for value caching. We recently had a test which returned positively without memcached running, but did not with memcached. To create a closer-to-production environment, we should also run memcahced in our travis tests. ", "1584": "---------- ``` 2018-09-28 06:02:51,797 DEBUG [Traceback (most recent call last): File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.17.8/bin/rucio\", line 154, in new_funct return function(*args, **kwargs) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.17.8/bin/rucio\", line 868, in upload upload_client.upload(items, summary_file_path) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.17.8/lib/python2.7/site-packages/rucio/client/uploadclient.py\", line 229, in upload final_summary[file_did_str] = {'scope': file['scope'], KeyError: 'scope' ``` ", "1582": "---------- `rucio/tests/daemons/Automatix.py` line 86 is expecting two values while only one dict is returned generating a too many values to unpack error. guilty line: `global_status, ret = rsemgr.upload(rse_info, lfns=lfns, source_dir=source_dir)` while it receives instead for example : `{0: True, 1: {u'tests:RAW.2b409202459145d2a02aad377438c897': True, u'tests:RAW.4cd16c00832b43fe8d9d85a96e13211e': True}, 'success': True, 'pfn': u'root://eosatlas.cern.ch:1094//eos/atlas/atlasdatadisk/ruciotest/rucio/tests/30/0a/RAW.4cd16c00832b43fe8d9d85a96e13211e'}` ------------ Fix the functional test! ", "1579": "The client fails if the authentication type is x509_proxy and the X509_USER_PROXY environment variable is not set with a very confusing error message: ``` $ rucio whoami 2018-09-27 09:22:57,749 ERROR coercing to Unicode: need string or buffer, module found ``` ", "1578": "---------- There is potential data loss if the judge-evaluator gets stuck and the reaper continues working. One possibility to prevent this, would be to stop the reaper once it is detected that the judge-evaluator has a significant backlog and restart it once the backlog is processed. ", "1569": "---------- ``` >>> from rucio.core.lifetime_exception import update_exception >>> from rucio.db.sqla.constants import LifetimeExceptionsState >>> update_exception('c7c01cb11b15411e95e68506155a02fb', LifetimeExceptionsState.APPROVED) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 351, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/lifetime_exception.py\", line 155, in update_exception query.one() File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 2887, in one \"Multiple rows were found for one()\") sqlalchemy.orm.exc.MultipleResultsFound: Multiple rows were found for one() ``` ------------ Replace call to `one()` with `first()`. ", "1568": "Recently (since ~mid-August) we have seen many errors in ARC CEs from looking up replicas in Rucio which look like the following: ``` DEBUG: > GET  HTTP/1.1 Host: rucio-lb-prod.cern.ch:443 Connection: keep-alive user-agent: ARC x-rucio-auth-token: dcameron-/DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=dcameron/CN=555105/CN=David Cameron-unknown-xxx DEBUG: No security processing/check requested for 'incoming' DEBUG: < HTTP/1.1 404 Not Found DEBUG: < Content-Type: text/plain; charset=utf-8 DEBUG: < X-Content-Type-Options: nosniff DEBUG: < Date: Wed, 26 Sep 2018 14:40:35 GMT DEBUG: < Content-Length: 19 ERROR: Failed to obtain information about file: No such file or directory: HTTP error when contacting server: Not Found ``` I can repeat the problem using arcls and it happens maybe one out of 20 times. ARC is using direct REST calls rather than the rucio client, and I cannot recreate the problem with the rucio client so maybe it has some internal retries. Transient errors with the Rucio server are not a big deal for ARC because they are retried but a 404 is interpreted as a permanent error so the job actually fails. Can you check what is causing it and if it is a temporary problem return something like 500 so the look up is transparently retried? ", "1561": "---------- ``` >>> from rucio.core.lifetime_exception import list_exceptions >>> next(list_exceptions('a7cfdb2f54b047cabc8f9386975dd420', None)) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 313, in new_funct for row in function(*args, **kwargs): File \"/usr/lib/python2.7/site-packages/rucio/core/lifetime_exception.py\", line 57, in list_exceptions query = query.filter(id=exception_id) TypeError: filter() got an unexpected keyword argument 'id' ``` ------------ Replace call to `filter()` with `filter_by()`. ", "1560": "---------- Should be default true everywhere and also work for datasets, not only files. ", "1559": "---------- While we are waiting for support for PFN-style `...?xrdcl.unzip=...` in `xrdcp` and `gfal-copy` to be deployed, we need a workaround. We can't globally switch everything from gfal as the default implementation to xrd yet (due to unknown side-effects, and we're still in data-taking), so we need a hardcoded override. If there's a download with a `xrdcl.unzip` extension, transparently convert it to `xrdcp --zip ... ... `. Once `xrdcp` and `gfal-copy` are there, we can take it out again. ", "1555": " ", "1554": "---------- It would be nice to have a WebUI page equivalent to the rucio-admin rse info command. ", "1551": "---------- Wrong handling of the replica restriction causes crashes in Judge repairer. ", "1550": "---------- > This comes up again for LRZ as we have a broken server. What is possible now, if we have a list of files to be marked temporarily unavailable? I am no longer pushing for rucio to know the pool info - a list produced by the site is fine, if it can be handled in bulk. > Files marked unavailable would > - not show up in list-replicas, also in --deep dataset replica lookup used by panda > - mark complete datasets as not complete (panda uses complete replicas, to avoid file replica lookup) > - not trigger recovery from other replicas The critical part here is what happens with the replication rules touching these replicas. The state-consistent workflow would be, to set them to STUCK, but this would issue an immediate recovery of these replicas. Leaving the rules in OK state breaks somewhat the consistency of the state-machine and could lead to problems if admins forget to set these replicas back to AVAILABLE again. ------------ - [ ] CLI command needed to operate over a list of replicas supplied in a txt file - [ ] Replicas are marked as UNAVAILABLE - [ ] Collection Replicas are updated respectively - [ ] Reverse CLI command needed to make replicas and collection replicas AVAILABLE again ", "1543": "---------- In order to use the CMS TFC lfn2pfn plugin we currently store, in json format, the TFC rules (a stack of regexp of the LFN to PFN translation) in the extended_attributes of the RSE protocol. Like in ``` Protocols: ========== srm extended_attributes: {u'space_token': u'CMS', u'tfc_proto': u'srmv2', u'web_service_path': u'/srm/managerv2?SFN=', u'tfc': [{u'protocol': u'direct', u'result': u'/dpm/in2p3.fr/home/cms/trivcat/store/PhEDEx_Debug/LoadTest07_GRIFLLR/LoadTest07_GRIFLLR_$1', u'element_name': u'lfn-to-pfn', u'path-match': u'/store/PhEDEx_Debug/LoadTest07Source/GRIF_LLR_(\\\\S+)'}, {u'protocol': u'direct', u'result': u'/dpm/in2p3.fr/home/cms/trivcat/store/PhEDEx_Debug/LoadTest07_GRIFLLR', u'element_name': u'lfn-to-pfn', u'path-match': u'/store/PhEDEx_Debug/LoadTest07Source'}, {u'protocol': u'direct', u'result': u'/dpm/in2p3.fr/home/cms/trivcat/store/PhEDEx_Debug/LoadTest07_GRIFLLR/LoadTest07_GRIFLLR_$1', u'element_name': u'lfn-to-pfn', u'path-match': u'.*/LoadTest07_GRIF_LLR_(.*)_.*_.*'}, {u'destination-match': u'.*', u'protocol': u'direct', u'result': u'/dpm/in2p3.fr/home/cms/trivcat/$1', u'element_name': u'lfn-to-pfn', u'path-match': u'/+(.*)'}, {u'protocol': u'srmv2', u'chain': u'direct', u'destination-match': u'.*', u'result': u'srm://polgrid4.in2p3.fr:8446/srm/managerv2?SFN=/$1', u'path-match': u'/+(.*)', u'element_name': u'lfn-to-pfn'}]} hostname: polgrid4.in2p3.fr prefix: /dpm/in2p3.fr/home/cms domains: {u'wan': {u'read': 1, u'write': 0, u'third_party_copy': 0, u'delete': 0}, u'lan': {u'read': 0, u'write': 0, u'delete': 0}} scheme: srm port: 8446 impl: rucio.rse.protocols.gfalv2.Default ... ``` The size of the field is currently VARCHAR2(1024). That's enough to contain all the relevant rules of the TFC tested so far (LLR and Nebraska) but bigger TFC may not fit into this. ------------ Talking with Mario Lassnig: it may be worth to migrate the extended_attribute field to an arbitrary size json encoded field (like for the DID metadata). This can be done over the timescale of a couple of releases. That will provide us with a definitive solution for entering the TFC rules in the RSE metadata. In the meanwhile most of the current TFC should fit in to the current size (I'll double-check this). ", "1535": "---------- Right now the glfn is hardcoded with the /atlas path in the metalink. Make it optional for non-ATLAS VOs. ", "1532": "---------- ------------ ", "1529": "---------- Right now, setting limits via the command line (`rucio-admin account set-limits`) is a bit of black magic. You have to specify in bytes (notoriously human-unfriendly) and there's an undocumented magic value of `-1` which is shorthand for \"infinity\". Proposed ------------ * Allow the CLI interface to accept units (`10GB` instead of `10000000000`) * Accept \"inf\" and \"infinity\" (case-insensitive). ", "1528": "---------- Currently list_replicas checks the `site` attribute of the RSE and matches it with the `client_location`. However, if the site attribute is not set the method crashes. This should be changed and just the wan replica should be returned. ", "1527": "---------- there is only few comments right now. Descriptiv comments would make the code much more understandable. ", "1524": "---------- It's not really an issue for us, as we do not use flask in production code, just for documentation generation, it should be upgraded nevertheless. ", "1523": "---------- There was a bug with the certificate chains in pyopenssl for urllib3  which has been fixed in urrlib3 version 1.23 We should change the version from `urllib3>=1.10.2` to `urllib3>1.23` We can probably also remove the direct dependency to pyopenssl in tools/pip-requires, but this has to be checked. ", "1522": "---------- Currently, when submitting a job to FTS where both RSEs have `\"verify_checksum\": False` it will be set to `\"none\"` in the metadata send to FTS but the checksum are still sent. This leads to certain problems with xrootd 3rd copy on DPM where checksum validation is not working. If a checksum is set in the metadata FTS will currently ignore the `\"verify_checksum\"` and will try to validate anyway. ------------ Don't send the checksum in the metadata when `\"verify_checksum\": \"none\"` ", "1516": "---------- It is not possible to access the REST API under  and to access the Web UI under  within the docker container build with the Dockerfile under /etc/docker/dev. ------------ A few modifications can be copied from the demo environment. ", "1512": "---------- I tried to download one file from a dataset, and the output from rucio doesn't make sense: ``` lxplus040:/tmp/cohm > rucio get --nrandom 1 data17_5TeV:data17_5TeV.00340634.calibration_PixelBeam.merge.DAOD_IDPIXLUMI.c1166_m1905 ---------------------------------- Download summary ---------------------------------------- DID data17_5TeV:data17_5TeV.00340634.calibration_PixelBeam.merge.DAOD_IDPIXLUMI.c1166_m1905 Total files : 0 Downloaded files : 0 Files already found locally : 0 Files that cannot be downloaded : 0 ``` Nothing was downloaded, so it should somehow say that there was a problem somewhere in the output. Could this be improved? Thanks a lot! Best, Christian ------------ ", "1506": "Trying to upload a file when the protocol doesn't support renaming fails with ```$ rucio upload --rse MANCHESTER --scope user.illingwo /tmp/rucio_test_file3.txt 2018-08-29 09:05:25,936 INFO Preparing upload for file rucio_test_file3.txt 2018-08-29 09:05:25,989 INFO Successfully added replica in Rucio catalogue at MANCHESTER 2018-08-29 09:05:26,054 INFO Successfully added replication rule at MANCHESTER 2018-08-29 09:05:27,261 INFO Trying upload with srm to MANCHESTER 2018-08-29 09:05:38,241 ERROR list indices must be integers, not str ``` The reason is that in the no-renaming branch starting here  the value of `ret` is updated if there's a problem, but it's not in the case of success. Since the return type depends on the length of `ret`, not setting it causes the index type error above. This looks very similar to #1487 ", "1505": "---------- Many files are now python3 compatible. It would be good to have a flag to identify them. The files found as python3 compatible must stay compatible. Any PR submitted that would break the compatibility should be automatically reject by Travis ", "1497": "---------- Stumbled upon them while using Rucio. This issue collects them, though they aren\u2019t related. ------------ Small patches of little consequence. ", "1494": "---------- E.g. the rsemanager.upload gives different returns if it is a bulk or a single upload, which crashes the uploadclient. See #1487 This needs to be reworked there should be a single return type independent of the method being called in bulk or single upload/download/delete etc. ", "1490": "---------- String responses cannot be decoded into UTF-8 in Python 3, since they're already UTF-8 by default. The function has been removed from the language. ------------ provided by @XeBoris ``` def parse_response(data): \"\"\" JSON render function \"\"\" ret_obj = None try: ret_obj = data.decode('utf-8') except AttributeError: ret_obj = data return json.loads(ret_obj) ``` ", "1487": "---------- The upload with --no-register --pfn does not work. ``` [wguan@lxplus078 wguan]$ /usr/bin/env rucio -v upload --rse CERN-PROD_ES --scope transient --no-register --pfn s3://cs3.cern.ch:443//atlas-eventservice/05cc36b8-383f-444f-a384-592fad3b918e123323 /bin/hostname 2018-08-22 17:21:13,312 INFO Preparing upload for file hostname 2018-08-22 17:21:13,312 WARNING Upload with given pfn implies that no_register is True, except non-deterministic RSEs 2018-08-22 17:21:13,530 INFO Trying upload with s3 to CERN-PROD_ES 2018-08-22 17:21:14,590 ERROR list indices must be integers, not str 2018-08-22 17:21:14,590 DEBUG This means the parameter you passed has a wrong type. Completed in 1.3510 sec. ``` ", "1486": "---------- Futures package should not be installed in Python 3 environments, in fact pip will not allow it, which crashes the installation under python 3.5. The package should be conditional to python < 3 versions. ", "1485": "---------- There are two `config_get('policy', ...)` calls in the bin/rucio code. However, if the policy section is not available in the config an error is raised. This should not happen. Default values should be used. (\"\") ", "1484": "---------- It is available in rucio download, but not in rucio get which makes the client crash. Needs to be added to the get parser too. ", "1483": "---------- Currently it points to the archive, should point to the file inside the zip instead. ", "1480": "---------- CVE-2018-10903 ", "1479": "---------- This touches all modules using stomp. - Replace dns.resolver with socket.getaddrinfo - Remove obsolete TLS session parameter ", "1476": "After today's TCB talk I tried to install rucio clients in a python3 virtualenv, but it fails to find the \"futures\" dependency: ``` $ python --version Python 3.6.5 $ pip install rucio-clients Collecting rucio-clients [...] Collecting futures>=3.2.0 (from rucio-clients) Cache entry deserialization failed, entry ignored Could not find a version that satisfies the requirement futures>=3.2.0 (from rucio-clients) (from versions: 0.2.python3, 0.1, 0.2, 1.0, 2.0, 2.1, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.1.5, 2.1.6, 2.2.0, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.0.5, 3.1.0, 3.1.1) No matching distribution found for futures>=3.2.0 (from rucio-clients) ``` From what I can see this package is a python2 backport of a python3 native package so shouldn't be installed in python3. ", "1475": "---------- pycrypto:  --> None yet --> Monitor ", "1470": "---------- Add the two clients to the main client object  ", "1464": "---------- if we use PFN, we should explain that also no_register is required  ------------ ", "1463": "Apparently newer versions of pip don't like the three equals in the cx_oracle dependency. (Somewhere along the line internally it sticks parentheses around the version description, and then the parser chokes on that combination). ``` (rucio-test) [illingwo@fermicloud014 rucio-test]$ pip --version pip 18.0 from /cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip (python 2.7) (rucio-test) [illingwo@fermicloud014 rucio-test]$ pip install rucio Collecting rucio Exception: Traceback (most recent call last): File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_internal/basecommand.py\", line 141, in main status = self.run(options, args) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_internal/commands/install.py\", line 299, in run resolver.resolve(requirement_set) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 102, in resolve self._resolve_one(requirement_set, req) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_internal/resolve.py\", line 306, in _resolve_one set(req_to_install.extras) - set(dist.extras) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2826, in extras return [dep for dep in self._dep_map if dep] File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2871, in _dep_map self.__dep_map = self._compute_dependencies() File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2881, in _compute_dependencies reqs.extend(parse_requirements(req)) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2942, in parse_requirements yield Requirement(line) File \"/cloud/login/illingwo/rucio-test/lib/python2.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2951, in __init__ raise RequirementParseError(str(e)) RequirementParseError: Invalid requirement, parse error at \"'(===6.3.'\" ``` ", "1457": "Thomas saved my day when I tried to run docker for the first time: etc/docker/dev/docker-compose.yml 'image: rucio-dev' --> 'image: rucio/rucio-dev' ", "1455": "---------- Currently, the clients only allow downloads from RSEs which are flagged with `istape=False`. There are two issues with this. For some reason, in ATLAS, the `istape` attribute has been stored with the value `\"False\"` instead of the value `0`, which means the rse_expression only works with the direct string check on `\"False\"`. Secondly, as this is hardcoded in the client, for communities which do not have their RSEs flagged with `\"istape=False\"` the download will not work. We need a better workflow for this. Some options are: - Extending the RSE Expression parameter with a system-default `*` parameter, which lists all RSEs. Currently, in ATLAS, the `ALL` flag is used for this, but again, this needs to be created by a script. A system internal parameter would be good. With this a new attribute could be used to flag tape rses: `tape=1`. The client can now do the download with the EXPRESSION `*\\tape=1`. Even for communities who do not have the tape flag, this query will work. ", "1454": "---------- helm is a tool to manage kubernetes charts (packages). It packages pre-configures K8s resources for easy installation. It furthermore provides tools to manage deployed installations on a cluster. Helping to keep track of current installations and performing easy rollbacks. ------------ Add the chart to the repository so that other users can download it and use it to install Rucio. ", "1446": "---------- It seems that some PSQL instances do not autoconvert uuid to text when calling md5(). We can force the conversion, e.g., this line: `query = query.filter(text('mod(abs((\\'x\\'||md5(id))::bit(32)::int), %s) = %s' % (total_workers + 1, worker_number)))` should actually be: `query = query.filter(text('mod(abs((\\'x\\'||md5(id::text))::bit(32)::int), %s) = %s' % (total_workers + 1, worker_number)))` ", "1443": "---------- list_replicas reply should now enforce protocol priority in rse contents: ``` u'rses': {u'CERN-PROD_DATADISK': [u'gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/atlasdatadisk/rucio/mc16_13TeV/22/b5/HITS.10701323._005432.pool.root.1',  u'root://eosatlas.cern.ch:1094//eos/atlas/atlasdatadisk/rucio/mc16_13TeV/22/b5/HITS.10701323._005432.pool.root.1', u'srm://srm-eosatlas.cern.ch:8443/srm/v2/server?SFN=/eos/atlas/atlasdatadisk/rucio/mc16_13TeV/22/b5/HITS.10701323._005432.pool.root.1']}, ``` Sort PFNs based on protocol priority per RSE, e.g., in this case, root, gsiftp, davs, srm. ", "1434": "---------- The find_matching_scheme function sometimes misbehaves returning non-compatible protocols, e.g. : ``` 2018-08-09 12:47:06,786 30323 INFO Thread [11/21] : PREPARING REQUEST a7d51858f91e4ff28246d83c4e0d9e68 DID data16_13TeV:DAOD_BPHY5.14797067._000213.pool.root.1 TO SUBMITTING STATE PREVIOUS None FROM [(u'FZK-LCG2_DATADISK', u'srm://atlassrm-fzk.gridka.de:8443/srm/managerv2?SFN=/pnfs/gridka.de/atlas/disk-only/atlasdatadisk/rucio/data16_13TeV/b3/8b/DAOD_BPHY5.14797067._000213.pool.root.1', 'a9dd4f46935d46ffa0962a9150235a67', 0)] TO [u'davs://lcg-se1.sfu.computecanada.ca:2880/atlas/atlasdatadisk/rucio/data16_13TeV/b3/8b/DAOD_BPHY5.14797067._000213.pool.root.1'] USING  2018-08-09 12:47:08,184 30323 INFO Thread [11/21] : PREPARING REQUEST 3e411761a4c747dfb7a667f77bdf0bd5 DID data17_13TeV:DAOD_BPHY1.14798758._000087.pool.root.1 TO SUBMITTING STATE PREVIOUS None FROM [(u'INFN-NAPOLI-ATLAS_DATADISK', u'srm://t2-dpm-01.na.infn.it:8446/srm/managerv2?SFN=/dpm/na.infn.it/home/atlas/atlasdatadisk/rucio/data17_13TeV/a0/a4/DAOD_BPHY1.14798758._000087.pool.root.1', 'fb6f5c24f1e84384a1841d0c7769f92f', 0)] TO [u'davs://lcg-se1.sfu.computecanada.ca:2880/atlas/atlasdatadisk/rucio/data17_13TeV/a0/a4/DAOD_BPHY1.14798758._000087.pool.root.1'] USING  2018-08-09 12:47:08,187 30323 INFO Thread [11/21] : PREPARING REQUEST 6eda530a7ed24dfdbc70db2c32df44e4 DID data17_13TeV:DAOD_BPHY1.14798758._000074.pool.root.1 TO SUBMITTING STATE PREVIOUS None FROM [(u'INFN-NAPOLI-ATLAS_DATADISK', u'srm://t2-dpm-01.na.infn.it:8446/srm/managerv2?SFN=/dpm/na.infn.it/home/atlas/atlasdatadisk/rucio/data17_13TeV/1d/03/DAOD_BPHY1.14798758._000074.pool.root.1', 'fb6f5c24f1e84384a1841d0c7769f92f', 0)] TO [u'davs://lcg-se1.sfu.computecanada.ca:2880/atlas/atlasdatadisk/rucio/data17_13TeV/1d/03/DAOD_BPHY1.14798758._000074.pool.root.1'] USING  2018-08-09 12:47:20,287 30323 INFO Thread [15/21] : PREPARING REQUEST 51c68d67c78b4b98bbeb8e12beee3d9a DID mc15_5TeV:log.09746993._000223.job.log.tgz.1 TO SUBMITTING STATE PREVIOUS None FROM [(u'INFN-COSENZA_DATADISK', u'davs://recas-se-01.cs.infn.it:443/dpm/cs.infn.it/home/atlas/atlasdatadisk/rucio/mc15_5TeV/b2/95/log.09746993._000223.job.log.tgz.1', '3f94daf42edb4b41bc7dc38217031d50', 0), (u'BOINC_DATADISK', u'davs://data-bridge.cern.ch:443/myfed/atlas/boinc/rucio/mc15_5TeV/b2/95/log.09746993._000223.job.log.tgz.1', 'f93f7c66c6ec40368e109f85ec4a9dff', 0)] TO [u'srm://se03.esc.qmul.ac.uk:8444/srm/managerv2?SFN=/atlas/atlasdatadisk/rucio/mc15_5TeV/b2/95/log.09746993._000223.job.log.tgz.1'] USING  ``` Suspecting some non-thread safe code. ", "1433": "---------- When deleting the last replica of a file, reaper deletes the file logically as well but does not delete the contents in the archive_contents table. This causes a foreign key constraint error and the reaper gets stuck. ", "1432": "---------- At the moment it is necessary in order to start up rucio-server and rucio-daemons containers to provide a full rucio.cfg file mounted into the container. ------------ Auto-generate a default rucio.cfg using j2 and make single variables configurable using ENV variables. It would make it easier especially for new instances as they would need come up with a complete rucio.cfg even though it's usually just necessary to set the DB connection string and keep the default values. It will also make it easier in Kubernetes to update single configuration values. ", "1430": "---------- If all the replicas of a file are bad, there should be an option in `rucio-admin replicas declare-bad` to declare all of them bad, ", "1427": "---------- Right now `list_replicas` is called in a loop for every did. In cases where the pilot runs over N=15 files this creates a lot of calls on the server. This should be changed to just one `list_replicas` bulk call. The reason why this is done is that the did could potentially be a dataset, and right now, if the call is made in bulk the association between resolved files and dataset gets lost. However, this association is needed for the traces. The idea would be to expand the list_replicas result with the used did. This requires changes in the client and the core. - [x] Adapt list_replicas to include the dataset/container name in the result - [x] Adapt the downloadclient to list replicas in bulk ", "1423": "---------- The heart_beat in the conveyor is done using the full executable + arguments. One should only keep the executable + the activity option. In addition a bug has been introduced by moving from `type(request_type) is not list` to `isinstance(request_type, list)` ", "1422": "---------- Currently all list-dids are done listing ALL types, this should be changed to COLLECTION. ", "1415": "---------- Instead of calling list_replicas use locally cached metalink file. Ref: @PalNilsson ", "1412": "---------- Pilot needs to cache locally the metalink of the replicas, without making a second replicas call. Extend list_replicas JSON reply with switch to embed metalink in there. ", "1411": "---------- ``` 2018-08-02 17:12:59,313 17566 WARNING Could not submit transfer to  - Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/transfertool/fts3.py\", line 175, in submit timeout=timeout) File \"/usr/lib/python2.7/site-packages/requests/api.py\", line 108, in post return request('post', url, data=data, json=json, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/api.py\", line 50, in request response = session.request(method=method, url=url, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/sessions.py\", line 464, in request resp = self.send(prep, **send_kwargs) File \"/usr/lib/python2.7/site-packages/requests/sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/adapters.py\", line 433, in send raise ReadTimeout(e, request=request) ReadTimeout: HTTPSConnectionPool(host='fts.usatlas.bnl.gov', port=8446): Read timed out. (read timeout=30.0) ``` ", "1410": "---------- The link to the DDM Dashboard in the rule page  are wrong in case the site as an underscore in it's name. ", "1405": "---------- Currently it's impossible to create an RSE with the nondet flag. ", "1404": "---------- `rucio download` does a check at the beginning to see if the account is an admin one in case the replica is on tape. It puts unneeded load on the servers. ------------ Propose to forbid any access to replicas on TAPE by defaut and add a `--alllowtape` option. If this option is set to True, then the admin check is done ", "1394": "---------- In bulk_group_transfer, grouping FTS jobs by `rule` is sometimes not optimal (plenty of jobs with only a few files). Grouping with policy `dest` is better, but it doesn't preserve the activity. A `activity_dest_rse` would be useful. ", "1390": "---------- Currently the build process on readthedocs fails. ``` Collecting pycurl>=7.19 (from fts3-rest-API==3.7.1->-r requirements.readthedocs.txt (line 44)) Using cached  Complete output from command python setup.py egg_info: Traceback (most recent call last): File \"<string>\", line 1, in <module> File \"/tmp/pip-build-jSh8ZH/pycurl/setup.py\", line 913, in <module> ext = get_extension(sys.argv, split_extension_source=split_extension_source) File \"/tmp/pip-build-jSh8ZH/pycurl/setup.py\", line 582, in get_extension ext_config = ExtensionConfiguration(argv) File \"/tmp/pip-build-jSh8ZH/pycurl/setup.py\", line 99, in __init__ self.configure() File \"/tmp/pip-build-jSh8ZH/pycurl/setup.py\", line 227, in configure_unix raise ConfigurationError(msg) __main__.ConfigurationError: Could not run curl-config: [Errno 2] No such file or directory ---------------------------------------- Command \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-jSh8ZH/pycurl/ You are using pip version 9.0.1, however version 18.0 is available. You should consider upgrading via the 'pip install --upgrade pip' command. ``` ", "1379": "---------- In this case srm is the top priority protocol. If no schemes are given, the top-protocol is returned. If schemes are given, including srm, the sorting is done all wrong. ``` >>> print(c.list_replicas([did], metalink=True, rse_expression=rse, client_location=l, schemes=None)) ... <url location=\"TRIUMF-LCG2_DATADISK\" domain=\"wan\" priority=\"1\">srm://srm.triumf.ca:8443/srm/managerv2?SFN=/atlas/atlasdatadisk/rucio/mc15_13TeV/8e/0c/EVNT.04972714._000024.pool.root.1</url> ... >>> print(c.list_replicas([did], metalink=True, rse_expression=rse, client_location=l, schemes=['davs', 'gsiftp',  'root', 'srm', 'file'])) ... <url location=\"TRIUMF-LCG2_DATADISK\" domain=\"wan\" priority=\"1\">root://xrootd.lcg.triumf.ca:1094//atlas/atlasdatadisk/rucio/mc15_13TeV/8e/0c/EVNT.04972714._000024.pool.root.1</url> <url location=\"TRIUMF-LCG2_DATADISK\" domain=\"wan\" priority=\"2\">srm://srm.triumf.ca:8443/srm/managerv2?SFN=/atlas/atlasdatadisk/rucio/mc15_13TeV/8e/0c/EVNT.04972714._000024.pool.root.1</url> <url location=\"TRIUMF-LCG2_DATADISK\" domain=\"wan\" priority=\"3\">davs://proxy.lcg.triumf.ca:2880/atlas/atlasdatadisk/rucio/mc15_13TeV/8e/0c/EVNT.04972714._000024.pool.root.1</url> ... ``` ", "1378": "---------- this functionality should be replaced by the other methods, also in rucio_sitemover in pilot. For now, schemes: [protocol] -> protocol, otherwise the list_replicas with metalink won't work. ------------ ", "1377": "---------- rucio upload --no-register --pfns fails while it works with 1.16.4. ``` [wguan@lxplus067 wguan]$ /usr/bin/env rucio -v upload --rse CERN-PROD_ES --scope transient --no-register --pfn s3://cs3.cern.ch:443//atlas-eventservice/05cc36b8-383f-444f-a384-592fad3b918e /bin/hostname 2018-07-25 14:54:14,552 WARNING Upload with given pfn implies that no_register is True 2018-07-25 14:54:14,658 INFO Preparing upload for file hostname 2018-07-25 14:54:16,608 ERROR An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97')) Completed in 2.1462 sec. ``` The problem is related to rsemanager.exists call, which trys to resolve the did on the server with lfns2pfns, which does not work as the replica does not exists. In 1.16.4 with bin/rucio it was first checked if the did exists, and if not, rsemanager.exists was not even called. ", "1376": "---------- Internal forward of the rule to the archive. How the callbacks are handled needs to be checked. ", "1375": "---------- Similar to list_replicas, in the deep listing of dataset replicas (Which resolves individual file replicas) the archives should be included. ", "1370": "---------- The bad_replicas table doesn't contain the bytes associated to lost or corrupted files, making it more complicated to provide summaries on lost files. ", "1367": "---------- WebUI page to list Liftetime Model exceptions ", "1366": " change: raise -> raise exception ", "1361": "---------- BB8 uses activity `Data Rebalancing` but the activity defined in ATLAS schema is `Data rebalancing`. We should use the same activity everywhere, and it should be consistent with the share defined in FTS ", "1360": "---------- When uploading files to objectstore with --pfn. It can work. But if adding --summary, the file will be uploaded successfully but it fails to print the summary. Maybe it has some problem to get summary from --pfn. [wguan@lxplus007 ~]$ which rucio /cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.16.4/bin/rucio [wguan@lxplus007 ~]$ export S3_ACCESS_KEY=KJE7GOEQJVSAWDKPK4MA; export S3_SECRET_KEY=*****; export S3_IS_SECURE=True; /usr/bin/env rucio -v upload --rse CERN-PROD_ES --scope transient --no-register --pfn s3://cs3.cern .ch:443//atlas-eventservice/05cc36b8-383f-444f-a384-592fad3b918e /bin/hostname 2018-07-18 16:02:03,903 DEBUG PFN option specified, extracting protocol from given PFN: s3 2018-07-18 16:02:03,905 DEBUG Extracting filesize (13880) and checksum (ed26537c) for file transient:hostname 2018-07-18 16:02:03,905 DEBUG Automatically setting new GUID 2018-07-18 16:02:04,019 DEBUG Using account wguan 2018-07-18 16:02:04,019 DEBUG Skipping dataset registration 2018-07-18 16:02:04,020 DEBUG Processing file transient:hostname for upload 2018-07-18 16:02:04,065 DEBUG Uploading source file to RSE 2018-07-18 16:02:07,079 INFO File transient:hostname successfully uploaded on the storage 2018-07-18 16:02:07,079 DEBUG sending trace 2018-07-18 16:02:07,115 DEBUG Finished uploading files to RSE. Completed in 3.4517 sec. [wguan@lxplus007 ~]$ export S3_ACCESS_KEY=KJE7GOEQJVSAWDKPK4MA; export S3_SECRET_KEY=******; export S3_IS_SECURE=True; /usr/bin/env rucio -v upload --rse CERN-PROD_ES --scope transient --no-register --summary --pfn s3://cs3.cern.ch:443//atlas-eventservice/05cc36b8-383f-444f-a384-592fad3b918e /bin/hostname 2018-07-18 16:02:21,171 DEBUG PFN option specified, extracting protocol from given PFN: s3 2018-07-18 16:02:21,172 DEBUG Extracting filesize (13880) and checksum (ed26537c) for file transient:hostname 2018-07-18 16:02:21,172 DEBUG Automatically setting new GUID 2018-07-18 16:02:21,262 DEBUG Using account wguan 2018-07-18 16:02:21,263 DEBUG Skipping dataset registration 2018-07-18 16:02:21,263 DEBUG Processing file transient:hostname for upload 2018-07-18 16:02:21,291 DEBUG Uploading source file to RSE 2018-07-18 16:02:22,766 INFO File transient:hostname successfully uploaded on the storage 2018-07-18 16:02:22,767 DEBUG sending trace 2018-07-18 16:02:22,805 DEBUG Finished uploading files to RSE. 2018-07-18 16:02:22,834 ERROR list indices must be integers, not str 2018-07-18 16:02:22,834 DEBUG This means the parameter you passed has a wrong type. Completed in 1.7402 sec. ------------ ", "1359": "---------- deletion left-overs of previous attempts of upload is silently failing.  consequently, upload of current attempt fails. ------------ Under investigation. ", "1355": "---------- Need to downgrade to 36.8.0 (Last py2.6 supported version) ", "1354": "---------- The client needs to check for replicas with client_extract=True and extract them. ", "1353": "---------- Currently only in the normal list_replicas ", "1352": "---------- ``` $ rucio -H  get --rse UAM-LCG2_DATADISK --resolve-archives data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 2018-07-17 09:39:52,928 INFO Processing 1 item(s) for input 2018-07-17 09:39:53,089 INFO Using 2 threads to download 2 files 2018-07-17 09:39:53,090 INFO Thread 1/2: Preparing download of data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 2018-07-17 09:39:53,093 INFO Thread 2/2: Preparing download of data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 2018-07-17 09:39:53,156 INFO Thread 1/2: Trying to download with srm from MPPMU_DATADISK: data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 2018-07-17 09:39:53,251 INFO Thread 2/2: Trying to download with srm from UAM-LCG2_DATADISK: data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 2018-07-17 09:39:54,580 INFO Thread 1/2: File data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 successfully downloaded. 2.419 MB in 1.22 seconds = 1.98 MBps 2018-07-17 09:39:57,384 INFO Thread 2/2: File data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 successfully downloaded. 137.346 MB in 3.68 seconds = 37.32 MBps ---------------------------------- Download summary ---------------------------------------- DID data15_13TeV:DRAW_RPVLL.11258124._000142.pool.root.1 Total files : 2 Downloaded files : 2 Files already found locally : 0 Files that cannot be downloaded : 0 -rwxr-xr-x. 1 walkerr zp 2418784 Jul 17 09:39 DRAW_RPVLL.14655879._000001.zip.1 -rwxr-xr-x. 1 walkerr zp 137346472 Jul 17 09:39 DRAW_RPVLL.14657167._000002.zip.1 ``` For some reason when --rse and --resolve-archives is used, out of the 1 file list_replicas makes two. ", "1346": "---------- upload() in rsemgr.py tries to delete left-overs of previous attempt. But if force-scheme is set, which is almost all the time to the write_wan protocol (case of upload), the upload function try to delete these left-overs with the force-scheme protocol. But this can be different from 1st priority deletion protocol in AGIS, e.g. webdav. That's also the case of NDGF-T1 ------------  -> protocol_delete = create_protocol(rse_settings, 'delete') ", "1338": "---------- The metalink frontend is still using the old RSE scheme instead of the new PFN scheme. ", "1335": "---------- ------------ ", "1334": "---------- If I try to access the demo web ui, running with docker-compose on  I get a 500 Internal Server error with the following error message from the error log: ```shell [Wed Jul 11 11:48:51.199437 2018] [:error] [pid 9] [client 172.18.0.1:58702] Traceback (most recent call last): [Wed Jul 11 11:48:51.199458 2018] [:error] [pid 9] [client 172.18.0.1:58702] File \"/usr/lib/python2.7/site-packages/rucio/web/ui/main.py\", line 69, in <module> [Wed Jul 11 11:48:51.199640 2018] [:error] [pid 9] [client 172.18.0.1:58702] POLICY = config_get('policy', 'permission') [Wed Jul 11 11:48:51.199657 2018] [:error] [pid 9] [client 172.18.0.1:58702] File \"/usr/lib/python2.7/site-packages/rucio/common/config.py\", line 57, in config_get [Wed Jul 11 11:48:51.199747 2018] [:error] [pid 9] [client 172.18.0.1:58702] raise err [Wed Jul 11 11:48:51.199782 2018] [:error] [pid 9] [client 172.18.0.1:58702] NoSectionError: No section: 'policy' ```` ------------ It could be fixed by changing the etc/docker/demo/rucio.cfg file from: ```shell [permission] policy=demo ``` to ```shell [policy] permission=demo ``` But I am not sure, if this configuration is needed somewhere else or if the config reader in lib/rucio/web/ui/main.py should be changed instead. ", "1329": "---------- I wanted to build the demo setup in /etc/docker/demo but the rucio image build fails at the step \"RUN pip install rucio rucio-webui\" with the error message \"Command python setup.py egg_info failed with error code 1 in /tmp/pip-build-UzlGRb/rucio/\". ------------ It is caused by a missing upgrade by setuptools (\"RUN pip install --upgrade setuptools\") as it is done in the Docker image in /etc/docker/server. ", "1324": "---------- Paramiko needs to be imported in a try/except block. ", "1323": "---------- ``` [desilva@atlas-sl6x64 t1]$ rucio --verbose get user.desilva:user.desilva.dataset.1d45168b-551a-451f-bb73-5c633641c03b 2018-07-09 11:09:03,073 INFO Processing 1 item(s) for input 2018-07-09 11:09:03,073 DEBUG Processing item user.desilva:user.desilva.dataset.1d45168b-551a-451f-bb73-5c633641c03b 2018-07-09 11:09:03,074 DEBUG RSE-Expression: istape=False 2018-07-09 11:09:03,074 DEBUG Splitted DID: user.desilva:user.desilva.dataset.1d45168b-551a-451f-bb73-5c633641c03b 2018-07-09 11:09:03,247 DEBUG 1 DIDs after processing input 2018-07-09 11:09:03,247 DEBUG Processing: {'name': 'user.desilva.dataset.1d45168b-551a-451f-bb73-5c633641c03b', 'did': 'user.desilva:user.desilva.dataset.1d45168b-551a-451f-bb73-5c633641c03b', 'rse': 'istape=False', 'no_subdir': False, 'nrandom': None, 'transfer_timeout': 3600, 'scope': 'user.desilva', 'type': u'DATASET', 'force_scheme': None, 'base_dir': '.'} 2018-07-09 11:09:03,492 DEBUG Queueing file: user.desilva:myFile.1d45168b-551a-451f-bb73-5c633641c03b.log 2018-07-09 11:09:03,493 INFO Using main thread to download 1 file(s) 2018-07-09 11:09:03,493 DEBUG Start processing queued downloads 2018-07-09 11:09:03,494 INFO Preparing download of user.desilva:myFile.1d45168b-551a-451f-bb73-5c633641c03b.log 2018-07-09 11:09:04,206 INFO Trying to download with root from TRIUMF-LCG2_SCRATCHDISK: user.desilva:myFile.1d45168b-551a-451f-bb73-5c633641c03b.log 2018-07-09 11:09:04,454 ERROR Failed to download item 2018-07-09 11:09:04,454 DEBUG Logger instance has no attribute 'getChild' 2018-07-09 11:09:04,454 ERROR An unknown exception occurred. Details: 1 items were in the input queue but only 0 are in the output queue Completed in 2.3565 sec. ``` ", "1318": "---------- Tests do not work with 6.4 ", "1317": "---------- It should actually fail the test ", "1310": "---------- As a new member, I am going through the documentation and steps to setup rucio development environment. ------------ Suggesting few typo corrections in the documentation and trying to clarify things which were not obvious for a rucio newbie. ", "1307": "---------- Currently travis is using the 11g version of Oracle XE which doesn't support json but a new 12c version of the docker image is available is already available on docker hub. ", "1306": "---------- Only resolve parent replicas if switch is True. ", "1305": "---------- ``` $ rucio -H  list-file-replicas data16_13TeV:DRAW_RPVLL.11106701._002800.pool.root.1 +--------------+-----------------------------------------+------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | SCOPE | NAME | FILESIZE | ADLER32 | RSE: REPLICA | |--------------+-----------------------------------------+------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | GOEGRID_DATADISK: srm://se-goegrid.gwdg.de:8443/srm/managerv2?SFN=/pnfs/gwdg.de/data/atlas/atlasdatadisk/rucio/data16_13TeV/b3/20/DRAW_RPVLL.11106701._002800.pool.root.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | GOEGRID_DATADISK: srm://lapp-se01.in2p3.fr:8446/srm/managerv2?SFN=/dpm/in2p3.fr/home/atlas/atlasdatadisk/rucio/data16_valid/b4/56/DRAW_RPVLL.14459284._000203.zip.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | BNL-OSG2_DATATAPE: srm://lapp-se01.in2p3.fr:8446/srm/managerv2?SFN=/dpm/in2p3.fr/home/atlas/atlasdatadisk/rucio/data16_valid/b4/56/DRAW_RPVLL.14459284._000203.zip.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | BNL-OSG2_DATATAPE: srm://dcsrm.usatlas.bnl.gov:8443/srm/managerv2?SFN=/pnfs/usatlas.bnl.gov/BNLT1D0/data16_13TeV/DRAW_RPVLL/r9264/data16_13TeV.00307619.physics_Main.recon.DRAW_RPVLL.r9264_tid11106701_00/DRAW_RPVLL.11106701._002800.pool.root.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | IN2P3-LAPP_DATADISK: srm://lapp-se01.in2p3.fr:8446/srm/managerv2?SFN=/dpm/in2p3.fr/home/atlas/atlasdatadisk/rucio/data16_13TeV/b3/20/DRAW_RPVLL.11106701._002800.pool.root.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | IN2P3-LAPP_DATADISK: srm://lapp-se01.in2p3.fr:8446/srm/managerv2?SFN=/dpm/in2p3.fr/home/atlas/atlasdatadisk/rucio/data16_valid/b4/56/DRAW_RPVLL.14459284._000203.zip.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | UKI-LT2-QMUL_DATADISK: srm://lapp-se01.in2p3.fr:8446/srm/managerv2?SFN=/dpm/in2p3.fr/home/atlas/atlasdatadisk/rucio/data16_valid/b4/56/DRAW_RPVLL.14459284._000203.zip.1 | | data16_13TeV | DRAW_RPVLL.11106701._002800.pool.root.1 | 176.903 MB | a187cd6e | UKI-LT2-QMUL_DATADISK: srm://se03.esc.qmul.ac.uk:8444/srm/managerv2?SFN=/atlas/atlasdatadisk/rucio/data16_13TeV/b3/20/DRAW_RPVLL.11106701._002800.pool.root.1 | +--------------+-----------------------------------------+------------+-----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ``` ", "1302": "---------- We have to default to requests>=2.6 because of the default yum-provided package. However, geoip2 forces requests>=2.9 breaking the install. We could either downgrade geoip2, temporarily disable it, or switch to a different type of installation (venv, ...) ", "1299": "---------- For #1149 we introduced the `RSEChecksumUnavailable` to make the rucio upload client respect the `verify_checksum` RSE flag. The same exception handling should be done for protocols that do not support renaming, i.e. when `protocol.renaming == False` ------------ - [x] `rse.rsemanager` handles `RSEChecksumUnavailable` when `protocol.renaming == False` ", "1298": "---------- Streamline and unify publication of our docker images on dockerhub. - [x] Transfer ownership of dockerhub rucio organisation to release manager - [x] Set up automated build via Dockerfile for server - [x] Set up automated build via Dockerfile for daemon - [x] Set up automated build via Dockerfile for client - [x] Highlight sanctioned images in main README.md ", "1295": "---------- Lifetime exception request interface doesn't properly show errors ", "1288": "---------- ``` cmd: rucio get user.desilva:user.desilva.dataset.1fe078e2-a369-42c4-9dd0-3b28d95032ac 2018-06-22 07:04:23,396 INFO Processing 1 item(s) for input 2018-06-22 07:04:23,867 ERROR findall() takes exactly 2 arguments (3 given) ``` It seems that the definition of findall on python 2.6 and python 2.7 uses 1 and 2 arguments respectively. As we need to be compatible to python 2.6 this needs to be fixed. ", "1283": "---------- The scripts for background rebalancing Uniformize the scripts for Nuclei and T2 diverged. Some options have been implemented in the T2 one, but not backported in the Nuclei one ", "1282": "---------- For consistency reasons. ", "1281": "---------- Idea is to be able to make filters on the broker based on the EVENT_TYPE. ", "1276": "---------- The resurrect method doesn't remove the expired_at when a DID is resurrected which triggers the immediate deletion of the resurrected DID ", "1273": "---------- Some users would be interested to filter out empty containers in list-dids, e.g. : `rucio list-dids user.blah:blahblah --filter type=CONTAINER,empty=False` ", "1272": "---------- ``` rucio set-metadata --did blahblah:qwerty --key lifetime --value 100000 2018-06-19 15:13:04,853 ERROR The resource doesn't support the requested operation. Details: lifetime for blahblah:qwerty cannot be updated ``` ", "1269": "---------- Currently the docker server and daemon builds fail while installing rucio because of the pycurl dependency needing openssl-devel installed. ------------ add yum install openssl-devel.x86_64 ", "1268": "---------- AGIS has moved its endpoint description to a new resource-based scheme. `rucio.common.dumper.ddmendpoint_url()` has not been adapted to this change. The URLs that it returns are invalid and, consequently, the Auditor cannot locate the dumps produced by the sites. ------------ Build the base URL using `arprotocols` or switch to the implementation that uses the Rucio client. Update tests and documentation. ", "1263": "---------- localsite is not filled in traces for downloadclient ", "1262": "---------- Currently the schemes have to be given explicitly. However, if no schemes are given all schemes should be replied, not only the top-priority one. ", "1251": "---------- Single memcached for each daemon/server brings up some inconsistency issues. We should investigate a single (distributed) memcached instance. ", "1250": "---------- STAGEIN FAILED: Get error: Staging input file failed: lfn=EVNT.04972714._000032.pool.root.1, error=stageIn with API faied: Logger instance has no attribute 'getChild' ------------ can be realted to:  ", "1249": "---------- At the moment it is used for ALL POST calls, but it should only be done for the list_replica one where it is needed. ", "1241": "---------- PFN priority needs to be encoded explicitly in data structure. Use that priority also for metalink reply. ", "1240": "---------- The eol_at is set on some rules on non-pledged RSEs. It shouldn't be the case. ", "1228": "---------- The update_replica_state in ATLAS doesn't return the proper permission if a user has a physgroup attribute but try to upload on a SCRATCHDISK area ", "1227": "---------- When trying to download a file from the browser it fails with : refreshed scratch storage did.js:878:21 UUID not found, no trace sent. ------------ Set UUID correctly for files on page load ", "1226": "---------- Make the current kubernetes config publicly available ------------ ", "1223": "---------- From @sartiran `[root@c79d3ace2f51 /]# rucio ping` `2018-06-03 21:02:40,634 ERROR Cannot authenticate.` `Details: HTTPSConnectionPool(host='rucio-cms.grid.uchicago.edu', port=443): Max retries exceeded with url: /auth/x509_proxy (Caused by SSLError(SSLError(1, u'[SSL: SSLV3_ALERT_CERTIFICATE_EXPIRED] sslv3 alert certificate expired (_ssl.c:579)'),))` `2018-06-03 21:02:40,635 ERROR Please verify that your proxy is still valid and renew it if needed.` `[root@c79d3ace2f51 /]#` ------------ This error has nothing to do with the client proxy, it's the server certificate that expired. ", "1222": "---------- The order of selected protocols is not correct in rucio download. (Using list_replicas directly produces the correct order, however) ", "1219": "---------- The verify_checksum option is nested too deep in the submission JSON to FTS. Needs to be elevated one level higher. ", "1218": "---------- rucio list-rse-usage returns the RSE usage from different sources (Rucio, storage, expired). It would be good to have an option to list the usage per account. ", "1216": "---------- Currently the manage quota page shows all accounts for an RSE even if they don't have quota or usage. ------------ Hide those accounts by default and add a new button to show them if wanted. ", "1215": "---------- Rucio clients are also very restrictive by pinning to specific versions of the dependencies. This is a problem when a different package manager overwrites e.g. urllib3 or requests which makes the rucio Entrypoints in the client fail. ------------ Relax some of the client dependency version requirements. ", "1214": "---------- The rucio mover does not correctly send traces by not reporting the actual usrdn, pandaid, etc. ------------ Use the CLI parameters provided by the rucio clients: `--trace_appid, --trace_dataset, --trace_datasetscope, --trace_eventtype, --trace_pq, --trace_taskid, --trace_usrdn` ", "1208": "---------- The old method for rucio download does not use the list_replicas PFNs, and instead constructs them manually. This workaround allows to use the list_replicas PFNs when necessary and will be removed again once we switch to download-api next month. ", "1204": "---------- Protection on Apache with a 10 minute timeout, thus it makes sense to put a 10 minute client timeout. ", "1195": "---------- Currently, we only have a fixed set of metadata that is principally exposed via the core/meta.py, api/meta.py, client/metaclient.py modules. These metadata are strictly enforced based on the schema of the DID and require a schema-change for every new metadata field. This task is to support the use of arbitrary key:value pairs per DID based on JSON. ------------ 1. [x] re-enable and redo test/test_meta.py to conform to a new generic metadata interface 2. [x] adapt client/metaclient.py and bin/rucio to use the new metadata interface 3. [x] then, either, create new metadata table with three principal columns \"scope\", \"name\", \"json metadata\", or extend _did_ table with \"json metadata\" column (to be decided) 4. [x] adapt api/meta.py and core/meta.py to use the new schema ", "1193": "---------- There is the suspcion that there are rare cases where server requests time out (DB session killed etc.) but the issuing client interprets these as succesful. This still needs to be confirmed and reproduced. ", "1190": "---------- Currently the automated build for rucio-server and rucio-daemons are failing because pip install rucio fails with ```[91mCannot uninstall 'ipaddress'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. ``` ------------ Upgrade pip and setuptools and manually remove the ipaddress package. ", "1185": "---------- rsemanager.download, downloads the file as <filename>.part if an adler32 checksum is given. If ignore_checksum is set to True the file will not be renamed thus the .part will not be removed from the file name. ------------ Make the os.rename call independent of ignore_checksum ", "1184": "---------- When the remoteSite in the trace is set to a not existing RSE the dataset thread in Kronos dies and is only restarted the next time kronos is restarted. The file thread is not affected by this. ------------ Catch the RSENotFound exception and skip the dataset update if the RSE is invalid. ", "1179": "---------- CMS data transfers for user analysis outputs need additional features for theirs integration: - user proxy retrieval from myproxy.cern.ch - user FTS proxy delegation - keep central processing conveyor separate from the user one ------------ - creation of a transfertool FTS3 subclass supporting: -- get user DN starting from scope name -- get proxy from myproxy: key = hash(username+some key from configuration) -- use fts3 python bindings for submitting transfers and delegating proxy only if needed - for certain activities and if specified in the Rucio configuration, allow conveyor to act in \"user mode\": -- split payload by scopes in order to separate connections to FTS server by user -- submit transfers using the FTS3 subclass created above ", "1177": "---------- Avoid using cronjobs for periodically delegating proxy to FTS3 and, more generally, make delegation function available within the daemons. ------------ Include fts3 proxy delegation function in fts3 transfertool using fts3 python bindings. ", "1176": "---------- excerpt from pilot: 2018-05-15 22:01:08|25261|Singularity.| Using command rucio upload --guid 96E8E8EF-9D7A-A44E-9CF1-E6FDD5A48A80 --no-register --rse UKI-NORTHGRID-MAN-HEP_DATADISK --scope mc16_13TeV DAOD_HIGG2D1.14122281._000002.pool.root.1 2018-05-15 22:01:08|25261|rucio_sitemo| stageOutCmd: rucio upload --guid 96E8E8EF-9D7A-A44E-9CF1-E6FDD5A48A80 --no-register --rse UKI-NORTHGRID-MAN-HEP_DATADISK --scope mc16_13TeV DAOD_HIGG2D1.14122281._000002.pool.root.1 2018-05-15 22:02:09|25087|Monitor.py | --- Main pilot monitoring loop (job id 3931554336, state:stageout (running), iteration 98) 2018-05-15 22:02:09|25087|ATLASExperim| Using path: /scratch/condor_pool/condor/dir_21395/condorg_94gA7pF0/Panda_Pilot_25087_1526415741/PandaJob/memory_monitor_summary.json 2018-05-15 22:02:09|25087|Monitor.py | Max memory (maxPSS) used by the payload is within the allowed limit: 7776994 B (2*maxRSS=32768000 B) 2018-05-15 22:02:09|25087|pUtil.py | Returning tail stdout dictionary with 1 entries 2018-05-15 22:02:09|25087|Monitor.py | onlyUpdateStateChangedJobs: True, lastState: stageout, currentState: stageout 2018-05-15 22:02:09|25087|Monitor.py | Debug: job prod state: running 2018-05-15 22:03:10|25087|Monitor.py | --- Main pilot monitoring loop (job id 3931554336, state:stageout (running), iteration 99) 2018-05-15 22:03:10|25087|ATLASExperim| Using path: /scratch/condor_pool/condor/dir_21395/condorg_94gA7pF0/Panda_Pilot_25087_1526415741/PandaJob/memory_monitor_summary.json 2018-05-15 22:03:10|25087|Monitor.py | Max memory (maxPSS) used by the payload is within the allowed limit: 7776994 B (2*maxRSS=32768000 B) 2018-05-15 22:03:10|25087|pUtil.py | Returning tail stdout dictionary with 1 entries 2018-05-15 22:03:10|25087|Monitor.py | onlyUpdateStateChangedJobs: True, lastState: stageout, currentState: stageout 2018-05-15 22:03:10|25087|Monitor.py | Debug: job prod state: running 2018-05-15 22:03:15|25261|rucio_sitemo| stageOutOutput: {0}2018-05-15 23:02:54,720 INFO [Manually set GUID: 96E8E8EF9D7AA44E9CF1E6FDD5A48A80] {0}2018-05-15 23:03:14,959 ERROR [An unknown exception occurred. Details: no error information passed  status code: 503 ('service_unavailable', 'unavailable'))] 2018-05-15 22:03:15|25261|rucio_sitemo| stageOut with CLI failed! Trying API. Error: {0}2018-05-15 23:02:54,720 INFO [Manually set GUID: 96E8E8EF9D7AA44E9CF1E6FDD5A48A80]{0}2018-05-15 23:03:14,959 ERROR [An unknown exception occurred.Details: no error information passed  status code: 503 ('service_unavailable', 'unavailable'))] ------------ The API must re-raise all exceptions in the same way the previous client did. ", "1173": "---------- Help putting together the information about users usage of localgroupdisk ------------ I download the dump `rse=\"UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK\" curl -s --capath $X509_CERT_DIR --cacert $X509_USER_PROXY --cert $X509_USER_PROXY  > ${rse}.txt` The format of the dump is the following (at least according to my guess since there are no headers). `<RSE> <scope> <dataset> <space> <creation date> <last access date>` while users have datasets with their name in it, for example `UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK user.marx user.marx.data12.periodE.physics_Muons.NTUP_SMWZ.DiLepSkim.051012v1.121005095942 44166341046 2014-11-13T12:03:34.000Z 2018-02-03T08:58:55.000Z` the bulk of the data is infact generic data, for example `UKI-NORTHGRID-MAN-HEP_LOCALGROUPDISK archive data16_13TeV.00299144.physics_Main.merge.DAOD_STDM3.f698_m1594_p2667_tid08562931_00 9512473489 2017-04-22T13:53:57.000Z 2017-04-22T14:00:12.000Z` to find the users and the rules associated with each dataset I have to loop on all the entries and do a rucio list-rules query, grep for the RSE again and extract the rule and the user for later use. If I wan't to do any basic algorythm like calculate the space, which datasets are older etc, I need to merge the information from the dump and from list-rules in one file and do other passes on that file and select the rules I can delete usually based on age of datasets, but because often the old datasets belong to someone who's gone away I have to chase people and ask other people fi they are still interested in those data. I obviously cannot ask the users to do anything similar. I tried to produce a simple bash script to do the basic for the user (name taken from the proxy), but it takes ages and I'm not surprised they don't use it. For a fixed user name I need to invert the logic and loop on the list-rules grepping the dump for the dataset, that should be easier. Easier still would be to replace the name of the RSE in the dump, which anyway is redundant since the dumps are organised by RSE, with the rule id and the user name. In this way I think everything is needed to identify, who,when, how much space and what to delete would be in the same file from the start. The format of the dump would be `<account> <rule> <scope> <dataset> <space> <creation date> <last access date>` ", "1172": "---------- The functionality we have for event-service in rucio upload --pfn requires --no-register. For non-det RSEs we can relax this restriction and allow users to directly rucio upload there. ", "1169": "---------- Travis fails to build containers because pip refuses to install ipaddress which is already installed by yum. ------------ - [ ] In travis Dockerfile: remove ipaddress site-package before pip calls ", "1168": "The `conveyor-transfer-submitter` throws errors like ``` ERROR Operation \"third_party_copy\" not supported by <RSE> with schemes <transfer schema> ``` in two instances:   This is misleading as this error gets thrown twice for different checks, and if any of the other settings in `wan` in `\"protocol\": \"domain\":{}` are not set properly. It also blames the destination when the source is the issue. ", "1163": "---------- The permission for update_replicas_states is too restrictive and should be extended to TEST endpoints ", "1160": "---------- With rucio 1.16.1. When adding a rule which already exists. Client side: ``` # rucio add-rule data10_hi:NTUP_HI.289208._000500.root.1 1 NDGF-T1-MWTEST_DATADISK 2018-05-15 09:13:27,815 ERROR An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97')) ``` Server side: ``` [Tue May 15 09:13:27.806133 2018] [:error] [pid 9] [client 127.0.0.1:45386] return self._delegate(fn, self.fvars, args) [Tue May 15 09:13:27.806139 2018] [:error] [pid 9] [client 127.0.0.1:45386] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 462, in _delegate [Tue May 15 09:13:27.806146 2018] [:error] [pid 9] [client 127.0.0.1:45386] return handle_class(cls) [Tue May 15 09:13:27.806153 2018] [:error] [pid 9] [client 127.0.0.1:45386] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 438, in handle_class [Tue May 15 09:13:27.806160 2018] [:error] [pid 9] [client 127.0.0.1:45386] return tocall(*args) [Tue May 15 09:13:27.806167 2018] [:error] [pid 9] [client 127.0.0.1:45386] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/rule.py\", line 281, in POST [Tue May 15 09:13:27.806174 2018] [:error] [pid 9] [client 127.0.0.1:45386] raise  'DuplicateRule', error.args[0]) [Tue May 15 09:13:27.806181 2018] [:error] [pid 9] [client 127.0.0.1:45386] IndexError: tuple index out of range ``` ------------ `error.args[0]` -> `error.args` ", "1159": "---------- ``` /usr/lib64/python2.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see:  ``` ", "1158": "---------- ``` [jww@lapa jww]$ rucio -H  --verbose upload --rse UKI-NORTHGRID-LANCS-HEP_SCRATCHDISK --scope user.jwalder data15and16_13TeV_r21_v01_p3180_somemissing_S2T1.root 2018-05-14 16:45:09,470 DEBUG Extracting filesize (9515202649) and checksum (2b39b7ef) for file user.jwalder:data15and16_13TeV_r21_v01_p3180_somemissing_S2T1.root 2018-05-14 16:45:09,470 DEBUG Automatically setting new GUID 2018-05-14 16:45:09,760 DEBUG Using account jwalder 2018-05-14 16:45:09,760 DEBUG Skipping dataset registration 2018-05-14 16:45:09,760 DEBUG Processing file user.jwalder:data15and16_13TeV_r21_v01_p3180_somemissing_S2T1.root for upload 2018-05-14 16:45:09,940 ERROR A Rucio exception occurred when registering a file replica in Rucio. 2018-05-14 16:45:09,940 ERROR A Rucio exception occurred when processing a file for upload. 2018-05-14 16:45:09,941 ERROR Provided object does not match schema. Details: Problem validating dids : Additional properties are not allowed (u'filename' was unexpected) Failed validating 'additionalProperties' in schema['items']: {'additionalProperties': False, 'description': 'Data Identifier(DID)', 'properties': {'adler32': {'description': 'adler32', 'pattern': '^[a-fA-F\\\\d]{8}$', 'type': 'string'}, 'bytes': {'description': 'Size in bytes', 'type': 'integer'}, 'md5': {'description': 'md5', 'pattern': '^[a-fA-F\\\\d]{32}$', 'type': 'string'}, 'meta': {'additionalProperties': True, 'description': 'Data Identifier(DID) metadata', 'properties': {'guid': {'description': 'Universally Unique Identifier (UUID)', 'pattern': '^(\\\\{){0,1}[0-9a-fA-F]{8}-?[0-9a-fA-F]{4}-?[0-9a-fA-F]{4}-?[0-9a-fA-F]{4}-?[0-9a-fA-F]{12}(\\\\}){0,1}$', 'type': 'string'}}, 'type': 'object'}, 'name': {'description': 'Data Identifier name', 'pattern': '^[A-Za-z0-9][A-Za-z0-9\\\\.\\\\-\\\\_]{1,250}$', 'type': 'string'}, 'pfn': {'description': 'Physical File Name', 'type': 'string'}, 'rules': {'description': 'Array of replication rules', 'items': {'additionalProperties': False, 'description': 'Replication rule', 'properties': {'account': {'description': 'Account name', 'pattern': '^[a-z0-9-_]{1,30}$', 'type': 'string'}, 'activity': {'description': 'Activity name', 'enum': ['Data Brokering', 'Data Consolidation', 'Data rebalancing', 'Debug', 'Express', 'Functional Test', 'Functional Test XrootD', 'Functional Test WebDAV', 'Group Subscriptions', 'Production Input', 'Production Output', 'Analysis Input', 'Analysis Output', 'Staging', 'T0 Export', 'T0 Tape', 'Upload/Download (Job)', 'Upload/Download (User)', 'User Subscriptions'], 'type': 'string'}, 'ask_approval': {'description': 'Rule approval request', 'type': ['boolean', 'null']}, 'asynchronous': {'description': 'Asynchronous rule creation', 'type': ['boolean', 'null']}, 'comment': {'description': 'Rule comment', 'maxLength': 250, 'type': ['string', 'null']}, 'copies': {'description': 'Number of replica copies', 'type': 'integer'}, 'dids': {'type': 'array'}, 'grouping': {'description': 'Rule grouping', 'enum': ['DATASET', 'NONE', 'ALL', None], 'type': ['string', 'null']}, 'ignore_availability': {'description': 'Rule ignore availability status', 'type': 'boolean'}, 'lifetime': {'description': 'Rule lifetime', 'type': ['number', 'null']}, 'locked': {'description': 'Rule locked status', 'type': ['boolean', 'null']}, 'meta': {'description': 'Rule wfms metadata', 'maxLength': 3999, 'type': ['string', 'null']}, 'notify': {'description': 'Rule notification setting', 'enum': ['Y', 'C', 'N', None], 'type': ['string', 'null']}, 'priority': {'description': 'Priority of the transfers', 'type': 'integer'}, 'purge_replicas': {'description': 'Rule purge replica status', 'type': 'boolean'}, 'rse_expression': {'description': 'RSE expression', 'type': 'string'}, 'source_replica_expression': {'description': 'RSE expression', 'type': ['string', 'null']}, 'split_container': {'description': 'Rule split container mode', 'type': ['boolean', 'null']}, 'subscription_id': {'description': 'Rule Subscription id', 'type': ['string', 'null']}, 'weight': {'description': 'Rule weight', 'type': ['string', 'null']}}, 'required': ['dids', 'copies', 'rse_expression'], 'type': 'object'}, 'maxItems': 1000, 'minItems': 1, 'type': 'array'}, 'scope': {'description': 'Scope name', 'pattern': \"^[a-zA-Z'_'-.0-9]{1,25}$\", 'type': 'string'}, 'state': {'description': 'Replica state', 'enum': ['AVAILABLE', 'UNAVAILABLE', 'COPYING', 'BEING_DELETED', 'BAD', 'SOURCE', 'A', 'U', 'C', 'B', 'D', 'S'], 'type': 'string'}, 'type': {'description': 'DID type', 'enum': ['DATASET', 'CONTAINER', 'FILE', 'F'], 'type': 'string'}}, 'required': ['scope', 'name'], 'type': 'object'} On instance[0]: {u'adler32': u'2b39b7ef', u'bytes': 9515202649, u'filename': u'data15and16_13TeV_r21_v01_p3180_somemissing_S2T1.root', u'meta': {u'guid': u'afd7beb380714e5fb2197c005faadc63'}, u'name': u'data15and16_13TeV_r21_v01_p3180_somemissing_S2T1.root', u'scope': u'user.jwalder', u'state': u'C'} Completed in 26.2589 sec. ``` ", "1156": "---------- When spooling up a new dev environment using the `rucio/rucio-dev` many tests fail. The first message is an exception that authentication with username and password fails. ------------ - [x] Update Dockerfile - [x] Update packages installed by yum - [x] Install all requirements with pip (such that `tools/run_tests_docker.sh` may be used) - [x] Remove python-ipaddress package to handle pip dependencies - [x] Update `rucio.cfg` to streamline with travis container - [x] Change mysql credentials in `docker-compose.yml` to rucio.cfg ", "1155": "---------- No exceptions are handled for list_rse_attributes in the REST API, in particular RSENotFound ", "1149": "---------- In issue #847 I handled an exception to respect the `verify_checksum` flag with the rucio cleint. Unfortunately the `protocol.stat` function raises a `ServiceUnavailable` when the endpoint does not return a checksum. In this case the client should raise a more specific exception that can be be handled in the rsemanager. ------------ * [x] In `rucio.common.exceptions` define `ChecksumUnavailable` exception. * [x] In `stat` function of protocols raise `ChecksumUnavailable` In place of `ServiceUnavailable` when the checksum query fails. * [x] In `rsemanager` catch `ChecksumUnavailable` and depending on `verify_checksum` flag: * set file as valid if verify_checksum is false * else raise `ServiceUnavailable` with same message ", "1144": "---------- Currently the [credentials] section in the config must be filled, otherwise Rucio does not work. However, as this is an optional feature the config should also be optional. ", "1143": "---------- When calling `lifetime = get_scratch_policy(account, rses, lifetime, session=session)` the `policy_filter` is applied to the method. However, if an organisation (such as generic) is set which is not in the mapping, the method is replaced by a null_funct, returning a NoneType. Thus in cases where the line above is called (with a lifetime of eg 100) it is replaced by `None`. ``` $ rucio add-rule user.mlassnig:my-test-dataset --lifetime 100 2 \"MANCHESTER|CAMBRIDGE\" f69edfedf5de494397550640877a0974 $ rucio rule-info f69edfedf5de494397550640877a0974 ... Expires at: None ... ``` When doing update_rule with lifetime, the method even crashes completely, as it cannot handle the Nonetype in that case. Expected behaviour -------------- In cases where no respective method is set, the lifetime argument should just be passed through. ", "1138": "---------- When querying for a did which is also content of an archive, list-replicas should return the did of the archive file in a way that the client can download the file. (e.g. use xroot to directly download the content of a zip) ", "1137": "---------- The CLI should be able to download a content file of an archive via xroot. At first the client should be able to do this by explicitly parameterizing both, the content file name as well as the archive name. In a second step, once the list-replicas issue #1138 is done the client should be able to do this transparently by just giving the content file name. ", "1128": "The regular expression for validating the did names in lib/rucio/common/schema/cms.py fails with [1] when validating datasets with a '-' in the name. It seems this is because \\ is doubled in the regex. I'll make a small PR with the fixed version Regards, Andrea [1] Details: Problem validating did : '/ZPrimeToTTJets_M500GeV_W5GeV_TuneZ2star_8TeV-madgraph-tauola/StoreResults-Summer12_DR53X-PU_S10_START53_V7A-v1_TLBSM_53x_v3_bugfix_v1/USER' does not match '^\\\\/[A-Za-z0-9][A-Za-z0-9\\\\\\\\.\\\\\\\\-\\\\\\\\_\\\\/\\\\#]{1,500}$' Failed validating 'pattern' in schema['properties']['name']: {'description': 'Data Identifier name', 'pattern': '^\\\\/[A-Za-z0-9][A-Za-z0-9\\\\\\\\.\\\\\\\\-\\\\\\\\_\\\\/\\\\#]{1,500}$', 'type': 'string'} On instance['name']: '/ZPrimeToTTJets_M500GeV_W5GeV_TuneZ2star_8TeV-madgraph-tauola/StoreResults-Summer12_DR53X-PU_S10_START53_V7A-v1_TLBSM_53x_v3_bugfix_v1/USER' >>> ---------- ------------ ", "1127": "---------- Right now the lifetime field default is empty, thus resulting in infinite rules (Except for SCRATCHDISK rules where it is 15 days). This should be changed to 180 days per default (except for SCRATCHDISK, where it should stay with 15) ", "1118": "---------- The conveyor crash if verify_checksum is a bool instead of a string ", "1117": "---------- To clearly separate functional tests for 3rd-party protocol commissioning. ", "1116": "---------- Returns only the first character of the error string, instead of the full string. ------------ [0][0] -> [0] ", "1115": "---------- For NONE grouped rules no DATASETLOCK_OK notifications are created, as they do not have datasetlocks. It would be helpful to have a new notification **RULE_OK** which is invoked for all rules when they reach the OK state. The notification mode (CLOSE) should be respected the same way as DATASETLOCK_OK notifications are handled. ", "1104": "---------- when md5 calculation fails, the key is missing in dictionary:  what causes failing the of the download, without declaring a file suspicious. ------------  'md5': None, proposed by @TWAtGH ", "1093": "---------- There are two private variables in the module which are not accessible to the class. ", "1092": "---------- Due to the new verify_checksum mapping. ``` 2018-04-24 14:36:42,467 2991 INFO 0:0 Got 1 transfers for Functional Test XrootD 2018-04-24 14:36:42,468 2991 INFO 0:0 Starting to group transfers for Functional Test XrootD 2018-04-24 14:36:42,468 2991 CRITICAL 0:0 Traceback (most recent call last): File \u201c/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\u201d, line 157, in submitter grouped_jobs = bulk_group_transfer(transfers, group_policy, group_bulk, fts_source_strategy, max_time_in_queue) File \u201c/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/common.py\u201d, line 156, in bulk_group_transfer \u2018verify_checksum\u2019: transfer[\u2018verify_checksum\u2019], KeyError: \u2018verify_checksum\u2019 ``` ", "1086": "---------- The current images for server and daemon rely on systemd to run  or supervisord. This complicates things if run directly on docker as privileged mode is needed and it doesn't work on Kubernetes. systemd is not really necessary when run in docker as only single services will run a container, i.e., apache can be started in FOREGROUND and one daemon will be started per container. This will make deployment in Kubernetes much easier and the images can still be used on a standalone docker installation. ------------ Remove the rucio-systemd-cc7 base image as it will not be needed anymore. The server and daemon images will be based directly on centos:7 and only necessary dependencies will be installed. This will then also reduce the size of images. ", "1083": "---------- lib/rucio/common/client.py is probably unused and can be removed ------------ ", "1080": "------------ SQLAlchemy package update to 1.2.7 alembic package update to 0.9.9 web.py package update to 0.39 python-dateutil package update to 2.7.2 geoip2 package update to 2.8.0 ipaddress package update to 1.0.22 cffi package update to 1.11.5  package update to 0.11.3 cryptography package update to 2.2.2 protobuf package update to 3.5.2.post1 grpcio package update to 1.11.0 numpy package update to 1.14.2 kerberos package update to 1.3.0 paramiko package update to 2.4.1 ", "1078": "------------ dogpile.cache package update to 0.6.5 progressbar2 package update to 3.37.1 futures package update to 3.2.0 ", "1077": "---------- If list-rules does not find a rule for a DID it automatically looks for rule on the content of a DID. The CLI should at least give a warning when it does this. ", "1060": "---------- The table mapping compatible mapping is erroneous ------------ Fix the mapping table ", "1057": "---------- Wrongly mangles third_party_copy into third_party_copy_wan ", "1056": "---------- rucio list-file-replicas --rse <RSE EXPRESSION> doesn't return anything whereas specifying explicitly the RSE vname works ------------ Fix list-file-replicas to support RSE expression ", "1055": "---------- rucio download/upload support the option --transfer-timeout (see issue #27 ) but this is not implemented in all protocols; and even for supported protocols (e.g. gfal) the underlying process can get stuck ------------ For all protocols implement a watcher process in the rucio downloadclient/uploadclient libraries that kills the transfer after the transfer_timeout expiration. ", "1054": "---------- Google Cloud Storage supports signed S3 URLs just like AWS (with a slightly different header). We need this support primarily for rucio clients to upload to and download from GCS, since we cannot distribute the GCS access keys to our users. ", "1051": "---------- ``` Traceback (most recent call last): File \"archive.py\", line 21, in <module> c.add_files_to_archive(name=zipfile,scope='user.walkerr',files=flist) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.14.11/lib/python2.7/site-packages/rucio/client/didclient.py\", line 281, in add_files_to_archive return self.attach_dids(scope=scope, name=name, dids=files) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.14.11/lib/python2.7/site-packages/rucio/client/didclient.py\", line 187, in attach_dids raise exc_cls(exc_msg) rucio.common.exception.RucioException: An unknown exception occurred. Details: [u'(cx_Oracle.IntegrityError) ORA-00001: unique constraint (ATLAS_RUCIO.DIDS_PK) violated\\n'] ``` ", "1048": "---------- ``` Hi Cedric, the checksum validation was improved to have all checksum possibilities. Before, when you where indicating verify checksum True was applying the checksum to both (source and destination) and if you were using False the verification was not applied. I just keep the True and False for compatibility with the API. But, we encouraged when this new feature was presented in the steering, to use both if you want a checksum verification in source and destination, source when you only wants the verification on the source, destination just to verify in destination and none for no verification. So, you can provide the verification that adapts better to your needs as it is documented. Cheers, Maria ``` ", "1045": "---------- Due to the join of the two tables, only accounts with usage and quota are listed, however, accounts without quota should be listed as well. ", "1044": "---------- Details of this issue are discussed in  The issue is that the current auto-approval of rules works the same for users with quota (which is full) and without. The idea here is to have a different auto-approval mode, which does not auto-approve accounts without quota, but only the ones with. (Due to group membership) ", "1040": "---------- The verify_checksum RSE attribute is passed as a string instead of a bool to FTS ------------ Cast the string to bool ", "1039": "---------- The pep8 package is deprecated and has been replaced by pycodestyle. This has to be changed in the packages to use the newest version of pycodestyle (2.4.0) ", "1038": "---------- rucio download and rucio upload now accept --transfer-timeout with a default currently hardcoded to 3600 seconds. The default should be configurable in rucio.cfg separately for download and upload; fallback to 3600 if not defined in rucio.cfg ------------ 1) Add new download and upload sections to rucio.cfg each with a configurable transfer-timeout parameter 2) bin/rucio should get transfer_timeout in order from: command line, rucio.cfg, default 3600 ", "1036": "------------ - Extend the client test to run ` nosetests -v lib/rucio/tests/test_bin_rucio.py` for python 3.6 (with allow faillures) - Make `lib/rucio/tests/test_bin_rucio.py` executable without any server dependencies ", "1032": "------------ SEs with type=proxyinternal need to be added/removed to each RSE from that particular site with the following rse_attribute: key = root-proxy-internal value = root://hostname:port (port mandatory (default=1094), no trailing slash) ", "1029": "---------- ------------ ", "1025": "---------- Add `sphinx-build -T doc/source/ doc/build/html` and check if the status is not different than 0. ", "1024": "---------- Fix setuptools package updaye to 39.0.1 to avoid issue with old setuptools version like: ```install fails with: `install_requires` must be a string or list of strings``` ", "1023": "---------- Add a complete list of all publications (master's, phd's, articles, papers, ...) related to DDM, DQ2, Rucio to the webpage. Link to PDF where possible. ", "1022": "---------- Add sqlite as a backend to test in travis ", "1020": "---------- Provide all dev. dependencies in extra-require for setup.py: `pip install .[dev]` ", "1019": "---------- Add flake8 and pylint python 3 syntax check for rucio (allow_failures mode) to start upgrading the code to be py3 syntax compatible. ", "1018": "---------- Test Rucio with different configuration: db testing: oracle, mysql, postgres, sqlite python: 2.6, 2,7, 3.4, 3.5, 3.6 packaging validation: rucio, client syntax validation: flake8, pylint with py2.7 and py3.6 test: server, client ", "1017": "---------- Test the packaging of `rucio-clients` with different python versions and execute the client test suite: `nosetests -v lib/rucio/tests/test_clients.py` against a Rucio server running within a container with sqlite. ", "1012": "---------- Update to the latest version of flake8(3.5.0) but with it, we have: ``` $ flake8 lib/|wc 181 1262 12472 ``` Mostly `E722 do not use bare except`. If we ignore this error, we are let with: ``` lib/rucio/tests/test_did.py:385:9: E741 ambiguous variable name 'l' lib/rucio/client/dq2client.py:1445:13: E741 ambiguous variable name 'l' lib/rucio/client/dq2client.py:1446:16: E741 ambiguous variable name 'l' ``` Modifcation ------------ - Ignore E722 in `.flake8` for now - Fix E741 ", "1009": "---------- Add conditional requirements to `./tools/pip-requires` to mark the non compatible python2.7 packages and support `pip install -r ./tools/pip-requires` for python 3. ", "1007": "---------- The CMS schema is too permissive on RSE names, scopes, and possibly LFNs ------------ Make these match actual CMS requirements and add testing to verify it ", "1005": "---------- CMS has run into two problems with permissions: 1) A user cannot remove their own rules (ATLAS allows this). 2) Root cannot delete replicas. As long as PhEDEx is moving data under the hood for us, we think we need to be able to do this. ------------ ", "1004": "---------- After detach, an entry will add into table contents_history. If one attach and detach it again, a same entry can not add into table because of primary key constrain. As a result, the DID can never be detached . ------------ ", "1001": "---------- When reading through the documentation ActiveMQ is presented as a main example for integration. While researching getting familiarized with the project it helps the reader to easily \"flow\" towards referenced projects (a.la. wikipedia). ------------ Add link to Apache's ActiveMQ project in the documentation areas where it is referenced. ", "999": "---------- After cloning the repo, I do a `pip install -r requirements.readthedocs.txt` which installs the dependencies :tada: However ``` docker-compose etc/docker/dev/docker-compose.yml Traceback (most recent call last): File \"/usr/bin/docker-compose\", line 11, in <module> load_entry_point('docker-compose==1.8.0', 'console_scripts', 'docker-compose')() File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 564, in load_entry_point return get_distribution(dist).load_entry_point(group, name) File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2662, in load_entry_point return ep.load() File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2316, in load return self.resolve() File \"/usr/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2322, in resolve module = __import__(self.module_name, fromlist=['__name__'], level=0) File \"/usr/lib/python2.7/dist-packages/compose/cli/main.py\", line 17, in <module> from ..bundle import get_image_digests File \"/usr/lib/python2.7/dist-packages/compose/bundle.py\", line 14, in <module> from .service import format_environment File \"/usr/lib/python2.7/dist-packages/compose/service.py\", line 13, in <module> from docker.utils import LogConfig ImportError: cannot import name LogConfig ``` I propose to add `logconfig>0.4` to the requirements. ------------ ", "997": "---------- ``` $ flake8 lib/rucio/client/clis/rucio.py lib/rucio/client/clis/rucio.py:2225:16: F821 undefined name 'result' ``` read `bin/rucio` instead of `lib/rucio/client/clis/rucio.py` ``` if args.estimate_ttc: if result[0] > 0: print(\"Expected transfer start in %0.2f minutes.\" % (rule['estimated_start_in'] / 60.)) print(\"Expected transfer end in %0.2f minutes.\" % (rule['estimated_end_in'] / 60.)) else: print('Couldn\\'t calculate the TTC for any of the transfers. No sources selected yet?') ``` but `result` doesn't exist. ", "992": "---------- Add entry points for the command rucio and rucio-admin to install them automatically onto the user\u2019s $PATH, so that they can be executed directly from a terminal session. ", "985": "---------- Fix typographical errors that are present in the Rucio documentation, acquire experience with the Rucio development pipeline. ------------ Fixing said typographical issues ", "976": "------------ ``` [policy]   ``` ? ", "973": "", "965": "---------- ``` # /usr/rucio/tools/probes/common/check_ping_rucio_servers rucio-server-int-01.cern.ch Traceback (most recent call last): File \"/usr/rucio/tools/probes/common/check_ping_rucio_servers\", line 18, in <module> print 'Rucio version installed : %s ' % c.ping()['version'] File \"/usr/lib/python2.7/site-packages/rucio/client/pingclient.py\", line 45, in ping if r.status_code == codes.ok: AttributeError: 'NoneType' object has no attribute 'status_code' ``` ", "964": "---------- ``` (rucio-cients) [root@rucio-server-int-01 virtualenv-15.1.0]# rucio-admin account list the JSON object must be str, not 'bytes' This means the parameter you passed has a wrong type. (rucio-cients) [root@rucio-server-int-01 virtualenv-15.1.0]# rucio-admin rse list the JSON object must be str, not 'bytes' This means the parameter you passed has a wrong type. (rucio-cients) [root@rucio-server-int-01 virtualenv-15.1.0]# rucio list-rses 2018-03-21 09:42:09,970 ERROR the JSON object must be str, not 'bytes' ``` ", "963": "---------- when there are multiple replicas, and client is at location where there is an outgoing proxy then the replica listing will only be correct if all replicas are at remote sites or when the local replica is the _last_ in the list to be evaluated ------------ Reset domain auto-selection request per iteration ", "961": "---------- Traceback : ``` 2018-03-20 15:36:45,244 10981 ERROR 0:1 Failed to prepare requests ['37b310899cba4e7caaa263ccd3515403', '62ade923b2734244b7edc335d4495a78', '6e4fd6f2bea64f4c9808d2ec75e77b67'] state to SUBMITTING(Will not submit jobs but return directly) with error: Traceback (most r ecent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/common.py\", line 64, in submit_transfer transfer_core.prepare_sources_for_transfers(xfers_ret) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 347, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 104, in prepare_sources_for_transfers raise RucioException(\"Failed to prepare transfer: request %s does not exist or is not in queued state\" % (request_id)) RucioException: An unknown exception occurred. Details: Failed to prepare transfer: request 37b310899cba4e7caaa263ccd3515403 does not exist or is not in queued state ``` ", "954": "---------- Currently there is no interface definition for the transfertool; The fts3transfertool is the only implementation and de-facto the interface definition. However, there are many proprietary methods and especially not necessary methods in the fts3tranfertool. Thus it is beneficial to specify a proper interface and to also cleanup the fts3tranfertool. ", "949": "---------- Classifiers were changed to tuple, which does not work, needs to be a list. ", "943": "---------- (cx_Oracle.NotSupportedError) Variable_TypeByValue(): unhandled data type result [SQL: u'UPDATE atlas_rucio.dids SET events=:events, updated_at=:updated_at WHERE atlas_rucio.dids.scope = :scope_1 AND atlas_rucio.dids.name = :name_1'] [parameters: {'events': (1000,), u'name_1': 'mc16_13TeV.364186.Sherpa_221_NNPDF30NNLO_Wtaunu_MAXHTPTV0_70_BFilter.simul.HITS.e5340_e5984_s3126_tid13424328_00', 'updated_at': datetime.datetime(2018, 3, 16, 11, 13, 6, 247121), u'scope_1': 'mc16_13TeV'}] (Background on this error at:  ------------ ...one() --> .one()[0] ", "942": "---------- Exception from server is not properly returned to client. [Fri Mar 16 12:12:52.483227 2018] [:error] [pid 32457:tid 140443203675904] Traceback (most recent call last): [Fri Mar 16 12:12:52.483275 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/did.py\", line 595, in POST [Fri Mar 16 12:12:52.483300 2018] [:error] [pid 32457:tid 140443203675904] issuer=ctx.env.get('issuer'), recursive=recursive) [Fri Mar 16 12:12:52.483304 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib/python2.7/site-packages/rucio/api/did.py\", line 243, in set_metadata [Fri Mar 16 12:12:52.483306 2018] [:error] [pid 32457:tid 140443203675904] return did.set_metadata(scope=scope, name=name, key=key, value=value, recursive=recursive) [Fri Mar 16 12:12:52.483309 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 347, in new_funct [Fri Mar 16 12:12:52.483311 2018] [:error] [pid 32457:tid 140443203675904] result = function(*args, **kwargs) [Fri Mar 16 12:12:52.483313 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 1214, in set_metadata [Fri Mar 16 12:12:52.483316 2018] [:error] [pid 32457:tid 140443203675904] update({key: value}, synchronize_session='fetch') [Fri Mar 16 12:12:52.483318 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/query.py\", line 3369, in update [Fri Mar 16 12:12:52.483329 2018] [:error] [pid 32457:tid 140443203675904] update_op.exec_() [Fri Mar 16 12:12:52.483332 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 1326, in exec_ [Fri Mar 16 12:12:52.483334 2018] [:error] [pid 32457:tid 140443203675904] self._do_exec() [Fri Mar 16 12:12:52.483336 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 1484, in _do_exec [Fri Mar 16 12:12:52.483362 2018] [:error] [pid 32457:tid 140443203675904] values = self._resolved_values [Fri Mar 16 12:12:52.483378 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/persistence.py\", line 1451, in _resolved_values [Fri Mar 16 12:12:52.483380 2018] [:error] [pid 32457:tid 140443203675904] desc = _entity_descriptor(self.mapper, k) [Fri Mar 16 12:12:52.483382 2018] [:error] [pid 32457:tid 140443203675904] File \"/usr/lib64/python2.7/site-packages/sqlalchemy/orm/base.py\", line 383, in _entity_descriptor [Fri Mar 16 12:12:52.483384 2018] [:error] [pid 32457:tid 140443203675904] (description, key) [Fri Mar 16 12:12:52.483387 2018] [:error] [pid 32457:tid 140443203675904] InvalidRequestError: Entity '<class 'rucio.db.sqla.models.DataIdentifier'>' has no property 'EVENTS' [Fri Mar 16 12:12:52.483393 2018] [:error] [pid 32457:tid 140443203675904] ", "941": "``` Setting up rucio 1.14.11 ... > rucio list-datasets-rse UCSC_LOCALGROUPDISK SCOPE:NAME ---------- >>> from rucio.client import Client >>> c = Client() >>> c.get_dataset_locks_by_rse('UCSC_LOCALGROUPDISK') Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.14.11/lib/python2.7/site-packages/rucio/client/lockclient.py\", line 70, in get_dataset_locks_by_rse status_code=result.status_code) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.14.11/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 212, in _get_exception data = parse_response(data) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.14.11/lib/python2.7/site-packages/rucio/common/utils.py\", line 215, in parse_response return json.loads(data, object_hook=datetime_parser) File \"/usr/lib64/python2.6/json/__init__.py\", line 318, in loads return cls(encoding=encoding, **kw).decode(s) File \"/usr/lib64/python2.6/json/decoder.py\", line 319, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) TypeError: expected string or buffer ``` ", "938": "---------- Extend setup_rucio_client.py to support more python version. ", "935": "---------- Grammatical error in the client API reference documentation. In the file api.rst ------------ ", "930": "---------- ``` Collecting futures>=3.2.0 (from -r tools/pip-requires-client (line 14)) Could not find a version that satisfies the requirement futures>=3.2.0 (from -r tools/pip-requires-client (line 14)) (from versions: 0.2.python3, 0.1, 0.2, 1.0, 2.0, 2.1, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.1.5, 2.1.6, 2.2.0, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.0.5, 3.1.0, 3.1.1) No matching distribution found for futures>=3.2.0 (from -r tools/pip-requires-client (line 14)) ``` ", "929": "---------- **Setting up Rucio demo environment** I am trying to set up Rucio demo environment and following -  I have docker compose tool and running the command - sudo docker-compose --file etc/docker/demo/docker-compose.yml up -d gives the following error: demo_mysql_1 is up-to-date Starting demo_rucio_1 ... error ERROR: for demo_rucio_1 Cannot start service rucio: driver failed programming external connectivity on endpoint demo_rucio_1 (55028af47bd18fa8197d2a0a45913b9cd661237bf85fb06ed2e6c96504db5dfa): Bind for 0.0.0.0:443 failed: port is already allocated ERROR: for rucio Cannot start service rucio: driver failed programming external connectivity on endpoint demo_rucio_1 (55028af47bd18fa8197d2a0a45913b9cd661237bf85fb06ed2e6c96504db5dfa): Bind for 0.0.0.0:443 failed: port is already allocated ERROR: Encountered errors while bringing up the project. I have docker installed : following is the 'docker version' output  I tried 'sudo netstat -ntpl | grep 443' , here is the output : tcp6 0 0 :::443 :::* LISTEN 3806/docker-proxy I also tried 'lsof -i:443' , output :  Any plausible way to work this out.? ", "928": "---------- Probably due to propagation of SIG* in the code? ", "913": "---------- filter and replications_rules are hard to read in subscriptions table ------------ Pretty print json in subscriptions table ", "912": "---------- Add the release notes to the repository, so that they are available on the documentation. ", "910": "---------- Broken link in the documentation for RSE Expressions. Link on the page -  ------------ Two possible ways - 1. Replace broken link with this one -  (need confirmation.!) 2. Create a new Github page with the content on the link -  and add some modification if required ", "906": "---------- ``` File \"/usr/lib/python2.7/site-packages/rucio/web/rest/rule.py\", line 271, in POST raise  'DuplicateRule', error.args[0]) IndexError: tuple index out of range ``` [0] needs to be removed. ", "886": "---------- To go through the documentation and submit my first patch. ------------ Fixed some grammatical errors in the documentation. ", "875": "---------- For the dynamic lfn2pfn function the module `importlib` is used. However, importlib is not part of python 2.6, thus it breaks the clients used on the ATLAS local root base. One option would be to use the backport of importlib for python 2.6 (on pypi) - needs to be investigated.  * Adding importlib to the dependencies works for python 2.6 and 2.7, but a install on python 3.x fails like this. For now we move the import to a conditional part of the code, as no module gets dynamically loaded at the moment anyway; This needs to be addressed later on with the dynamic loading of permissions and policies, as this should be handled in a similar way. ", "870": "---------- Causes component:message templates to be prepended even when CLI message is given. ------------ bash magic ", "867": "---------- To submit first patch. ------------ Fixing some typos in documentation. ", "862": "---------- Script needs to scroll pages. ", "850": "------------ setup.py and setup_rucio.py should be the same file ", "849": "---------- Sometimes transfers end-up in protocol mismatch error despite there are matching schemes ------------ Fix it ", "847": "---------- When uploading to `CERN-EXTENSION_GRIDDISK` the Rucio client fails because it attempts to verify the checksum. At the moment the `verify_checksum` flag is set for the RSE and not ask the storage for the checksum. The suggested modification will then also skip the verification of the file size on storage. The stat call checking the file size should be OK, and in this scenario is even more important to be verified (in  I loathe to put this modification at the protocol level hence the suggested modification, please let me know if you have better ideas. ------------ - [x] copy `verify_checksum` RSE attribute to RSE info - [x] get `verify_checksum` from RSE info in `rsemgr.upload` - [x] always declare uploaded file as valid if `ignore_checksum` is True. **Some notes** On line  of the rsemgr add ```python valid = True if ignore_checksum is True else False ``` After discussion with Martin we decided that `verify_checksum` should conceptually be RSE info and to copy it analagously to the `lfn2pfn_algorithm` attribute/info. ", "819": "---------- We need to make Rucio clients Python3 compatible ------------ Proposed incremental scenario (each step should be one or several pull requests): - [x] Enable 2to3 in travis to report py3k incompatibility - [x] change print by print() - [x] change xrange -> range - [x] Fix dict.keys()/dict.values() to list() - [x] Fixed exception to be py3K compatible - [x] compatible imports - [x] float, Long, etc - [x] Unicode vs byte string - [x] having a py3K packaging +ping test in the test suite for the clients once the migration is complete (e.g. tox) - [x] Check compatibility with different Python 3 versions; Currently works with 3.5 but not with 3.6 others maybe, python3 interpreter will tell... ", "811": "---------- Mario suggested a patch when I added the MD5 sum calculation to the clients in #472 ------------ - [x] add myself as author to the headers (and it's 2018, not 2017) :-) - [x] Sanitize variables (eg., adler32 checksum vs md5_digest) - rename: `checksum => a32_checksum` - rename: `md5_digest => md5_checksum` - _note_: in the DB the keys will stay `adler32` and `md5` - [x] test that the \"hello test\\n\" md5sum is correctly reported ", "808": "---------- Following the [RSE  on  leads to a non-existing page. ------------ Either the link in  needs to be made lower-case or the file  needs to be renamed to `RSE_Expressions`. ", "807": "## `datetime.timedelta(1)` is not as readable as `datetime.timedelta(days=1)`. Though there are only four such cases in the Rucio source code, two of those require additional discussion as they might reveal subtle bugs. What is implied and what is actually done differs. 1. In `tools/probes/common/check_used_space`, the current time and the timestamp in the database are compared. If the range between the two is longer than `datetime.timedelta(1)`, the entry is considered stale. However, the message is \u2018Used space has not been refreshed for more than 1 *hour*.\u2019 2. In `lib/rucio/tests/test_replica.py`, there is the following assignment: `tomorrow = datetime.utcnow() + timedelta(2)`. It feels like either the argument is wrong or the variable name is misleading. ## I intend to explicitly add the `days` keyword to all instances and, depending on the feedback, provide fixes for the two cases described above. ", "806": "## Storage Resource Reporting (SRR) is a specification of\u2014among other things\u2014how storage services should report usage information. For that purpose, it is intended to become an alternative to (or replacement of) SRM. Two of the WLCG storage providers, EOS and DPM, are actively working on it and have provided prototype implementations. Other providers have expressed their intention to support it. Recently, EOS started reporting usage information on three instances (among them `EOSATLAS`) at CERN based on this specification. The goal is to have support for this file format in Rucio so that it may be used as more WLCG sites start to provide it. ## My intention is to work on a patch for `probes/common/check_storage_space` so that it can transparently consume both JSON formats. The discussion on whether SRR can replace the Rucio JSON and, if so, the task of updating the documentation and the schema may be left for a later date. ## Resources 1.  2. [Extended  3. [GDB  Finally, here\u2019s how the reporting of the first entry from `tools/space-usage/space-usage-sample.json` would look with SRR: ```JSON { \"storageservice\": { \"storageshares\": [ { \"name\": \"ATLASLOCALGROUPDISK\", \"servingstate\": \"open\", \"totalsize\": 50000000000000, \"path\": [ \"/castor/ads.rl.ac.uk/prod/atlas/stripInput/atlasdatadisk/\" ], \"usedsize\": 1270000000000, \"numberoffiles\": 123456, \"timestamp\": 1447936989, \"message\": \"The report can not be created because ...\" } ] } } ``` ", "805": "---------- To be able to access storage at HPC centres (e.g. US leadership-class facilities) we will need to add Globus as an option to transfer files to/from the site ------------ Add `globus://` as protocol and supporting Globus as a `transfertool` ", "800": "---------- Flask is being imported in lib.rucio.common.utils file, but it is not part of the pip-requires-clients file. This should be resolved (It should not be part of it) ", "799": "Fix typos in  ", "796": "There is a typo in `doc/source/overview_File_Dataset_Container.rst`, where \u2018smalles\u2019 should be corrected to \u2018smallest\u2019. ", "793": "---------- ------------ ", "782": "---------- rucio list-dids allows to get a list of DIDs matching a pattern. It would be good to have an option to download by giving a metadata filter, e.g.: rucio download --scope mc15_13TeV --filter guid=2e2232aafac8324db452070304f8d745 ------------ Implement a --filter option in rucio download ", "769": "---------- The output of rucio-admin rse have some rst titles in it which breaks the rst ``` $ rucio-admin rse info JDOES-TEST_DATADISK Settings: ========= ``` ------------ add proper example formatting which should be propagated to all usage examples. ", "763": "", "762": "`CREATE UNIQUE INDEX ATLAS_RUCIO.QUARANTINED_REPLICAS_PATH_IDX on ATLAS_RUCIO.QUARANTINED_REPLICAS(PATH,RSE_ID) tablespace ATLAS_RUCIO_FACT_DATA01; ` ", "761": "---------- When developing the WebUI or running the demo it is necessary to run the webui with a modified path, e.g.,  That introduces problems with all the links then pointing to invalid addresses. ------------ Automatically modify the links to add the necessary prefix. ", "757": "", "754": "", "752": "---------- My conveyor transfer submitter is getting this error: 2018-02-22 21:57:10,386 121 WARNING Could not submit transfer to  - Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/transfertool/fts3.py\", line 289, in submit_bulk_transfers timeout=timeout) File \"/usr/lib/python2.7/site-packages/requests/api.py\", line 112, in post return request('post', url, data=data, json=json, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/api.py\", line 58, in request return session.request(method=method, url=url, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/sessions.py\", line 508, in request resp = self.send(prep, **send_kwargs) File \"/usr/lib/python2.7/site-packages/requests/sessions.py\", line 618, in send r = adapter.send(request, **kwargs) File \"/usr/lib/python2.7/site-packages/requests/adapters.py\", line 506, in send raise SSLError(e, request=request) SSLError: HTTPSConnectionPool(host='cmsfts3.fnal.gov', port=8446): Max retries exceeded with url: /jobs (Caused by SSLError(SSLError(\"bad handshake: Error([('SSL routines', 'ssl3_read_bytes', 'tlsv1 alert unknown ca')],)\",),)) I looked at the code and I see that in fts3 module, it says \"do not try to verify the certificate\": r = requests.post('%s/jobs' % external_host, verify=False, cert=(__USERCERT, __USERCERT), data=params_str, headers={'Content-Type': 'application/json'}, timeout=timeout) So I am puzzled why this is happening ? ------------ ", "749": "---------- ------------ ", "742": "---------- In some cases the the DN reported by Apache to the authentication module of the webui misses the leading slash. This doesn't affect us on our production system because the DNs is reported correctly there. But in the docker demo with the self signed certificates this becomes a problem. ------------ Manually add the leading slash if it is missing. The same is already done in the authentication module in the Rucio REST interface. ", "738": "---------- I am using rucio with Postgres 9.5 backend. I am getting the following error in the covneyor transfer submitter: 2018-02-21 20:33:54,140 120 CRITICAL 0:0 Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 135, in submitter retry_other_fts=retry_other_fts) File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/submitter.py\", line 268, in __get_transfers failover_schemes=failover_schemes) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 274, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (psycopg2.ProgrammingError) function md5(uuid) does not exist LINE 4: ...activity = 'User Subscriptions' AND mod(abs(('x'||md5(rule_i... the SQL is: SELECT anon_1.id AS anon_1_id, anon_1.rule_id AS anon_1_rule_id, anon_1.scope AS anon_1_scope, anon_1.name AS anon_1_name, anon_1.md5 AS anon_1_md5, anon_1.adler32 AS anon_1_adler32, anon_1.bytes AS anon_1_bytes, anon_1.activity AS anon_1_activity, anon_1.attributes AS anon_1_attributes, anon_1.previous_attempt_id AS anon_1_previous_attempt_id, anon_1.dest_rse_id AS anon_1_dest_rse_id, replicas.rse_id AS replicas_rse_id, rses.rse AS rses_rse, rses.deterministic AS rses_deterministic, rses.rse_type AS rses_rse_type, replicas.path AS replicas_path, anon_1.retry_count AS anon_1_retry_count, sources.url AS sources_url, sources.ranking AS sources_ranking, distances.ranking AS distances_ranking \\nFROM (SELECT requests.id AS id, requests.rule_id AS rule_id, requests.scope AS scope, requests.name AS name, requests.md5 AS md5, requests.adler32 AS adler32, requests.bytes AS bytes, requests.activity AS activity, requests.attributes AS attributes, requests.previous_attempt_id AS previous_attempt_id, requests.dest_rse_id AS dest_rse_id, requests.retry_count AS retry_count \\nFROM requests \\nWHERE requests.state = %(state_1)s AND requests.request_type = %(request_type_1)s AND requests.activity = %(activity_1)s AND mod(abs(('x'||md5(rule_id))::bit(32)::int), 1) = 0 \\n LIMIT %(param_1)s) AS anon_1 LEFT OUTER JOIN replicas ON anon_1.scope = replicas.scope AND anon_1.name = replicas.name AND replicas.state = %(state_2)s AND anon_1.dest_rse_id != replicas.rse_id LEFT OUTER JOIN rses ON rses.id = replicas.rse_id AND rses.staging_area = false AND rses.deleted = false LEFT OUTER JOIN sources ON anon_1.id = sources.request_id AND rses.id = sources.rse_id LEFT OUTER JOIN distances ON anon_1.dest_rse_id = distances.dest_rse_id AND replicas.rse_id = distances.src_rse_id ------------ I think it can be fixed by changing the SQL fragment mod(abs(('x'||md5(rule_id))::bit(32)::int), 1) to mod(abs(('x'||md5(rule_id::text))::bit(32)::int), 1) ", "731": "---------- ``` File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/rucio-1.14.10-py2.7.egg/rucio/common/doc/argparse/ext.py\", line 415, in run f = open(os.path.abspath(self.options['filename'])) IOError: [Errno 2] No such file or directory: u'/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/checkouts/latest/doc/source/bin/rucio' ``` ", "730": "---------- Traceback : ``` [Tue Feb 20 23:32:25.701947 2018] [:error] [pid 5431:tid 139990096934656] Traceback (most recent call last): [Tue Feb 20 23:32:25.701980 2018] [:error] [pid 5431:tid 139990096934656] File \"/usr/lib/python2.7/site-packages/rucio/core/replica.py\", line 752, in _list_replicas [Tue Feb 20 23:32:25.701984 2018] [:error] [pid 5431:tid 139990096934656] domain=domain)['scheme']) [Tue Feb 20 23:32:25.701986 2018] [:error] [pid 5431:tid 139990096934656] File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 126, in select_protocol [Tue Feb 20 23:32:25.701989 2018] [:error] [pid 5431:tid 139990096934656] candidates = _get_possible_protocols(rse_settings, operation, scheme, domain) [Tue Feb 20 23:32:25.701991 2018] [:error] [pid 5431:tid 139990096934656] File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 102, in _get_possible_protocols [Tue Feb 20 23:32:25.701994 2018] [:error] [pid 5431:tid 139990096934656] ' found : %s.' % str(rse_settings)) [Tue Feb 20 23:32:25.701996 2018] [:error] [pid 5431:tid 139990096934656] RSEProtocolNotSupported: RSE does not support requested protocol. ``` ------------ Need to catch RSEProtocolNotSupported exception ", "725": "---------- documentation for Installing dev environment ", "722": "---------- ```# /bin/rucio-judge-cleaner Traceback (most recent call last): File \"/bin/rucio-judge-cleaner\", line 30, in <module> run(once=args.run_once, threads=args.threads) File \"/usr/lib/python2.7/site-packages/rucio/daemons/judge/cleaner.py\", line 143, in run if db_time - client_time > max_offset or client_time - db_time > max_offset: TypeError: unsupported operand type(s) for -: 'datetime.date' and 'datetime.datetime' ``` ", "721": "---------- The FTS probe currently query the FTS monit page and generate quite some load on the DB. ------------ Move to Grafana json ", "718": "---------- Add auto-generated CLIs documentation for the daemons ------------ -- Function to get parser -- consistent headers ", "717": "---------- The downloadclient and uploadclient have been already merged to the repository due to prioritised testing. The code still needs to be sanitised and documented. ", "716": "---------- To have a structured way of importing RSEs, their protocols and even distances a json based importer would be beneficial. ", "707": "---------- 'domain' variable is used without being declared first ------------ add variable ;-) ", "705": "---------- >>> from rucio.core.meta import add_key >>> from rucio.db.sqla.constants import DIDType >>> add_key('datatype', DIDType.FILE) (_mysql_exceptions.DataError) (1265, \"Data truncated for column 'key_type' at row 1\") [SQL: u'INSERT INTO did_keys (`key`, key_type, value_type, value_regexp, updated_at, created_at) VALUES (%s, %s, %s, %s, %s, %s)'] [parameters: ('datatype', 'F', None, None, datetime.datetime(2018, 2, 19, 13, 41, 49, 985745), datetime.datetime(2018, 2, 19, 13, 41, 49, 985773))] Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 358, in new_funct raise DatabaseException(str(error)) rucio.common.exception.DatabaseException: Database exception. Details: (_mysql_exceptions.DataError) (1265, \"Data truncated for column 'key_type' at row 1\") [SQL: u'INSERT INTO did_keys (`key`, key_type, value_type, value_regexp, updated_at, created_at) VALUES (%s, %s, %s, %s, %s, %s)'] [parameters: ('datatype', 'F', None, None, datetime.datetime(2018, 2, 19, 13, 41, 49, 985745), datetime.datetime(2018, 2, 19, 13, 41, 49, 985773))] Probably related to str vs unicode ------------ Fit it ", "700": "---------- All the datasets produced by Automatix follow the following naming convention dsn = 'tests:%s.%s.%s.%s.%s.%s' % (metadata['project'], metadata['run_number'], metadata['stream_name'], metadata['prod_step'], metadata['datatype'], metadata['version']) ------------ Provide the possibility to define different naming convention in the automatix configuration ", "697": "If there are multiple equal-weight and otherwise-identical protocols, we should shuffle them when doing the selection. This would prevent the same protocol from being selected over and over. A use case we encountered for this is at NERSC, where they offer 9 identical GridFTP servers (all with different endpoints) to utilize as part of their RSE -- they consider load-balancing between the hosts to be part of the file transfer service's responsibility. ", "696": "---------- Demo is empty ------------ Create RSEs, DIDs... ", "688": "---------- The travis test script wrongly generates the list of changed files also including files containing 'py' in the name, e.g., \"aliases-py27.conf\" ", "684": "---------- The fix to #574 didn't seem to make FTS happy. Instead, if space tokens are omitted from the RSE definition or are none, just don't include them in the JSON sent to FTS. ", "679": "---------- It looks like all custom rucio exceptions are calling super().__init__(args, kwargs) with the wrong arguments. ------------ The syntax should be super().__init__(*args, **kwargs) Expected behavior ----------------- ", "678": "---------- Try to improve the finisher speed ------------ Add more debug statements + remove unneeded sleep ", "671": "---------- Implement upload capabilities in uploadclient ------------ Expected behavior ----------------- ", "670": "---------- As the uploadclient is implemented now, the upload code in bin/rucio should use the uploadclient. ", "669": "---------- As the downloadclient is implemented now, the download code in bin/rucio should use the downloadclient. ", "668": "---------- The `list_replicas` method orders the replicas, based on the client location, on the server side, but then puts them into a dictionary, hence the order of the replicas is lost. For each replica the dictionary should be expanded with an additional value (e.g. order) which represents the sorting order. This can be used by the client to order the replicas. ", "667": "---------- The replica selection in the conveyor is based on two things: ranking in the sources table and ranking in the distances table. Two things need to be changed: - [x] In certain branches of the code sources with distance `ranking` of 0 are excluded. This is wrong. Only ranking of `None` should be excluded. - [x] For the tape selection the distance ranking with the highest value is used. This is wrong. The lowest value should be used. ", "666": "---------- Enable file and dataset downloads from non-WebDAV enabled RSEs. ------------ Automatically create transfer requests to move requested file or (partial) dataset to a browser enabled RSE. ", "665": "---------- We want to able to monitor also downloads done directly from the WebUI. ------------ Send trace when user clicks on download link. ", "656": "---------- We have two different ranking columns. - One in the `sources` table where a **HIGH** ranking value is best and the more often a replica is unsuccessfully tried the lower the ranking gets. The source with the highest ranking is used first. The initial ranking value for a source is 0. - But we also have a ranking column in the `distances` table. Here the value is similar to the agis distance. 0 is LAN, the higher the value goes the worse the link is. Thus a **LOW** ranking value is best. However, the sorting algorithm here also prefers a high value, which is wrong. ", "646": "---------- Currently, FTS3 requires a compatibility option to authenticate with S3 storage. This can be removed once Davix is modified to do this automatically. ------------ Set \"s3alternate\":True in the job_param ", "641": "---------- File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/rucio-1.14.8-py2.7.egg/rucio/db/sqla/session.py\", line 51, in <module> DEFAULT_SCHEMA_NAME = config_get(DATABASE_SECTION, 'schema') File \"/home/docs/checkouts/readthedocs.org/user_builds/rucio-blueprint/envs/latest/local/lib/python2.7/site-packages/rucio-1.14.8-py2.7.egg/rucio/common/config.py\", line 32, in config_get return __CONFIG.get(section, option) File \"/usr/lib/python2.7/ConfigParser.py\", line 607, in get raise NoSectionError(section) NoSectionError: No section: 'database' ", "634": "---------- FTS is currently supporting destination:S3 transfers only from source:gsiftp and source:srm endpoints. The natural compatibility of source:davs is not yet implement. ------------ Change SCHEME_MAPPING. Once FTS supports source:davs, the scheme can be extended again. ", "633": "----------  ", "629": "---------- code clean up ------------ create_protocol doesn't need to check the given domain and operation since it gets checked in select_protocol Expected behavior ----------------- ", "626": "---------- Add autogenerated documentation for bin/rucio command line tool Based on  ", "622": "---------- Update of con.fy to get the default readthedocs template ", "619": "", "612": "---------- The regular expression for naming convention is wrong ------------ Fix naming convention validation ", "604": "---------- There is sometimes wrong values in the FTS json used in check_rses_distance : instead of float, sometimes current = [0, 0] ------------ Handling of non-expected value in json in check_rses_distance ", "598": "---------- If a rule is created whose size goes over the freespace+secondary threshold of the RSE, the rule should be rejected. ", "597": "---------- Express all dependencies needed to build the documentation on readthedocs ", "596": "----------- Sphinx (with the autodoc module) cannot import properly the client packages in the readthedocs building environment because of the missing rucio configuration file and then fails in generating the pages. Some attempt to get the configuration file distributed by a rucio-clients-mock package in the virtualenv of readthedocs failed. ------------- Instrument the configuration part of rucio to not fail when it is imported through sphinx to build the documentation. Expected behavior ------------------- Get the documentation of the APIs. ", "587": "---------- The FTS probe checks the \"busy\" links (i.e. links with more than X files submitted). It can put some load on the FTS DB. ------------ Introduce a flag to enable/disable the check of busy link in FTS probe ", "586": "---------- Currently, the REST API frontend of Rucio is provided using the web.py framework. This framework is already quite dated and not in development anymore. On the other hand Flask is popular framework that is still actively supported. Overall, the effort to also add a flask frontend is not very high. Additionally, flask provides the possibility to autogenerate sphinx REST API documentation directly from comments in the code. ------------ Change the structure under `lib/rucio/web/rest` and add two subfolders `webpy` and `flask`. The old content goes into `webpy` and the flask interface will be ported to the other folder. The configuration of aliases in rucio.conf has then to be changed to the new folders. ", "576": "---------- If there is an issue invoking the WSGI script, it's possible the response will *not* contain a nicely-formatted JSON object. In such case, the Rucio base client will throw a parsing error and the HTTP status code (and any other data is lost). Expected behavior ----------------- Even if we can't discern an error message, return the status code to the user instead of a JSON parsing error. ", "575": "---------- The srm scheme currently requires space_token as extended_attribute, however when space_token is not set, rucio complains about extended_attributes and web_service_path not specfied. ------------ Happens in these lines:  Expected behavior ----------------- Ideally , space_token would be an optional attribute, and if not used, no spacetoken should be passed to FTS transfer job submission command. See also #574 ", "574": "At many CMS sites, space tokens are not used even when SRM is utilized. However, it's not currently possible to skip this setting. It may be as simple as setting the space token to the empty string (``), which could be done automatically if not specified. However, there might be hidden dependencies that want a non-empty-string space token. Needs investigation. ", "573": "---------- Following on from RUCIO-2622, the only way to declare all replicas of a file lost is to find all the replicas and declare each one lost. But this fails if replicas have different protocols: Detailed information in  Migrated from  ", "572": "This issue is used to track the changes and todos necessary for the general documentation overhaul for the workshop. # General tasks ## General information - [x] Concepts and Terminology (#566, @bari12) - [x] Architecture (@bari12) - Depending on how much needs to be done, some help might be needed here ## Developer documentation - [x] Contributer guide (from github) - should be only one line in the TOC (@vingar) - [x] Installing dev environment (@vingar) - Docker example written by @berghaus can be used here. - [x] REST API description (@tbeerman) - Some general information, some examples, rest auto generated - [x] API description auto-generated (@vingar) - [x] CLI description auto-generated (@vingar @cserf) for rucio, rucio-admin utilities and daemons ## User documentation - [x] Installing Rucio clients (@cserf) - Mostly already there, only docker part needs to be added - [x] Rucio Clients howto (#551, @cserf, @TomasJavurek) - Should be a generalized version of the ATLAS RuucioHowTo - [x] Demo section based on docker and different steps (#696 @tbeerman, @cserf) - Basically installing a server, daemons etc in docker and then doing some things in this env ## Operator documentation - [x] General information (@mlassnig) - Different ways of deploying rucio. Dependencies (DB, Transfertool, Messaging, Monitoring) - [ ] Docker deployment - [x] How to configure Rucio: rses, accounts, etc (#736, @cserf) - [x] How to version the DB with alembic, upgrade and downgrade instructions (@mlassnig) - [ ] (Puppet deployment) - [ ] (Kubernetes deployment) # Bits & Pieces - [x] Concepts needs to be expanded with information how a replica actually works on storage, with a dataset as example; how deterministic/non deterministic works; how scopes come into play etc. (@bari12) - [x] How to connect and configure a stroage element to Rucio (@cserf) - [ ] We also need something about dumps and ACLs in the op documentation - [ ] How consistency and recovery can be done - [x] List of return codes of bin/rucio (@mlassnig) ", "571": "---------- Could be nice to have a countdown timer to the workshop in the main rucio webpage ------------ There is a nice clock here, timezone aware...  Expected behavior ----------------- To display the clock in the rucio webpage. ", "570": "Previously, we introduced a mechanism to specify the algorithm used for LFN2PFN at the RSE. The options now are `default` and `identity`. The `default` algorithm creates subdirectories based on hashes of the LFN, while `identity` is the identity function: LFN is used directly. As new communities come in, each has their own twist on the LFN2PFN mapping. Currently, there's no good way to add community-specific code for the mapping. I propose we: * In the Rucio server configuration, provide a module that is loaded on startup which can register additional plugins (mapping the algorithm name to a Python callable). * Continue to allow the algorithm to be set per-RSE as an attribute. * In the Rucio server configuration, specify the default algorithm to use if not specified as an attribute. So, Rucio could continue to offer one or two stock algorithms -- but as experiments mature, this can become part of their policies. ", "567": "---------- Pilot2 test cases are parsing the output of rucio download in case of errors. The tests are failing because there are still cases in rucio download where return FAILURE is used instead of raising the appropriate exception. ------------ Change return FAILURE statements to raising the correct exceptions. Expected behavior ----------------- ", "566": "---------- Update concepts & terminology section of the documentation - [x] Rucio account - [x] Files, Datasets and Containers - [x] Rucio Storage Element - [x] Meta-data attributes - [x] Permission model - [x] Replica management - [x] Replication rule examples - [x] RSE Expressions - [x] Accounting and quota - [x] Notifications - [x] Subscriptions / Policies ", "563": "---------- The footer in the account_usage table is wrong ------------ Fix footer in account_usage table ", "560": "", "557": "---------- add auto-generated API description to the development section. ", "554": "---------- ``` rucio add-rule --grouping DATASET --comment \"site replica test from Eric\" cms:/gluinoGMSB_M2150_ctau10p0_TuneCUETP8M1_13TeV_pythia8/RunIISummer16MiniAODv2-PUMoriond17_80X_mcRun2_asymptotic_2016_TrancheIV_v6-v1/MINIAODSIM 1 T2_US_Caltech 2018-01-30 16:33:05,559 ERROR Provided RSE expression is considered invalid. Details: Expression does not comply to RSE Expression syntax ``` The same command works if Caltech is substituted with UCSD, but both are valid site names in CMS ------------ Propagate allowing lower case letters into the RSE attributes as well for the CMS schema Expected behavior ----------------- ", "551": "---------- Missing doc for the CLI ------------ Improve doc for the CLI ", "549": "---------- It would be good to have an easy way to setup a simple Rucio instance to be able to play around with the Rucio CLI tools. ------------ Modify the dev images and add a new script to fill the database with demo DIDs and accounts. ", "548": "---------- Currently rucio list-rules --account displays different rule attributes, but not the creation date. Add the creation date to this list. ", "543": "---------- If a site goes into a long downtime, but many transfers are already submitted to FTS, there will be a lot of failures. ------------ A probe collecting the downtimes in AGIS will cancel the FTS jobs if a site is supposed to be down for more than 24 hours. ", "535": "---------- At the moment read is used for source and write is used for destination, for both third_party_copy should be used. ", "533": "---------- Right now the different configuration/permissions (atlas, cms) are part of the core repository. This is not sustainable in the long term. There should be a better way to manage these e.g. separate packages and inclusion via entry points etc. ", "532": "---------- To be able to install directly from master. ", "531": "---------- If a file is suspicious and has more than 1 replica it can be auto-recovered. ", "530": "---------- Rucio rule-info should display the TTC. ", "529": "---------- Full support of heartbeats and automatic partitioning for the reaper. ", "525": "---------- To calculate the number of accesses of a file or dataset, kronos should also update this information. - [x] Schema change needed to ad `nbaccesses` column - [x] Changes in Kronos needed to update the column ", "524": "---------- At the moment there are two oracle procedures (cron jobs) being used to keep the data consistent. However, there needs to be a solution for non-oracle users. For Postgres it might be possible to also introduce PLSQL Cronjobs, but for mysql this is not possible, thus daemon implementations for this might be applicable. - Update collection replicas: Exists only as PL/SQL - Update of account counters: Exists as PL/SQL and in the Abacus daemon ", "523": "---------- At the moment downloading and uploading data is only possible via the command line interface. For the rucio movers it is necessary that this functionality exists via python callable methods. Thus the functionality should be added to the python clients; Afterwards the bin/rucio method can also use these methods for downloading. ", "522": "---------- The probe setting the naming convention (check_ami) is broken. The MERGE INTO statement updating the regexp does not seem to work. ", "519": "---------- Using the API methods in check_rse_attributes induces a lot of commits since we open a new session each time. ------------ Move the methods calls in check_rse_attributes from the API to the core and reuse the DB session. ", "514": "---------- It's possible to choose the list of activities on which the conveyor will run, but it's not possible to exclude some activities ------------ Will implement a --skip-activities opttion for the conveyor ", "513": "---------- The conveyor receiver does not consume messages at the moment. ------------ The 'vo' field moved from headers to messages and needs to be checked there. Also the check for `false` on `msg['job_m_replica']` and the check for `true` on `msg['job_m_replica']` needs to be lowered. ", "510": "---------- The heartbeat `live()` and `die()` calls in the kronos-dataset thread use different executable names and therefore the heartbeats are never removed from the table. ------------ Use the both time the same name. ", "509": "---------- rucio list-dids -h usage: rucio list-dids [-h] [--recursive] [--filter FILTER] [--short] did List the Data IDentifiers matching certain pattern. Only the collections (i.e. dataset or container) are returned by default. With the filter option, you can specify a list of metadata that the Data IDentifier should match. positional arguments: did Data IDentifier pattern optional arguments: -h, --help show this help message and exit --recursive List data identifiers recursively. --filter FILTER Filter arguments in form `key=value,another_key=next_value`. Valid keys are name, type. --short Just dump the list of DIDs. There is no list of available filter options ------------ rucio list-dids -h should expose the list of available options ", "508": "---------- Since the introduction of domains this information is not used anymore and deprecated; It was left in the rse_settings but is now reason for confusion. These keys should be removed from the rse_settings. ", "505": "---------- On the client side: >>> from rucio.client.ruleclient import RuleClient >>> r = RuleClient() >>> r.update_replication_rule('ebb0750fbb4b46678bb0d49cdba1f6c3',{'lifetime': 10000}) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/client/ruleclient.py\", line 123, in update_replication_rule exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content) File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 212, in _get_exception data = parse_response(data) File \"/usr/lib/python2.7/site-packages/rucio/common/utils.py\", line 213, in parse_response return json.loads(data, object_hook=datetime_parser) File \"/usr/lib64/python2.7/json/__init__.py\", line 351, in loads return cls(encoding=encoding, **kw).decode(s) File \"/usr/lib64/python2.7/json/decoder.py\", line 366, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/lib64/python2.7/json/decoder.py\", line 384, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded the JSON that cannot be decoded is \"internal server error\" Looking at the apache logs on server side: [Mon Jan 22 10:02:47.071115 2018] [:error] [pid 6857] [client 134.158.132.125:54212] update_replication_rule(rule_id=rule_id, options=options, issuer=ctx.env.get('issuer')) [Mon Jan 22 10:02:47.071119 2018] [:error] [pid 6857] [client 134.158.132.125:54212] File \"/usr/lib/python2.7/site-packages/rucio/api/rule.py\", line 171, in update_replication_rule [Mon Jan 22 10:02:47.071124 2018] [:error] [pid 6857] [client 134.158.132.125:54212] rule.update_rule(rule_id=rule_id, options=options) [Mon Jan 22 10:02:47.071128 2018] [:error] [pid 6857] [client 134.158.132.125:54212] File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 349, in new_funct [Mon Jan 22 10:02:47.071132 2018] [:error] [pid 6857] [client 134.158.132.125:54212] result = function(*args, **kwargs) [Mon Jan 22 10:02:47.071136 2018] [:error] [pid 6857] [client 134.158.132.125:54212] File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 1112, in update_rule [Mon Jan 22 10:02:47.071141 2018] [:error] [pid 6857] [client 134.158.132.125:54212] rule.expires_at = datetime.utcnow() + timedelta(seconds=lifetime) if options['lifetime'] is not None else None [Mon Jan 22 10:02:47.071146 2018] [:error] [pid 6857] [client 134 [5:07 PM] TypeError: unsupported type for timedelta seconds component: NoneType Following Vincent Garonne suggestion I fill an issue as this seems a bug. ------------ Expected behavior ----------------- ", "502": "---------- In version 1.14.7 `rucio download` always uses the same protocol from an RSE ------------ Expected behavior ----------------- `rucio download` should fail over to a different protocol when the first one fails. Fail over used to work until version 1.14.5 ", "501": "---------- ``` [root@9c1d917353f9 /]# rucio -v --verbose upload --scope user.ewv --rse T2_US_NEBRASKA --name /store/data/Run2017A/SingleMuon/MINIAOD/PromptReco-v3/000/296/980/00000/3C8C839C-3256-E711-AD6B-02163E01A717.root /tmp/test.root 2018-01-19 15:50:03,264 DEBUG Extracting filesize (2161712084) and checksum (86e5e384) for file user.ewv:/store/data/Run2017A/SingleMuon/MINIAOD/PromptReco-v3/000/296/980/00000/3C8C839C-3256-E711-AD6B-02163E01A717.root 2018-01-19 15:50:03,267 DEBUG Automatically setting new GUID 2018-01-19 15:50:03,346 DEBUG Using account ewv 2018-01-19 15:50:03,346 DEBUG Skipping dataset registration 2018-01-19 15:50:03,346 DEBUG Processing file user.ewv:/store/data/Run2017A/SingleMuon/MINIAOD/PromptReco-v3/000/296/980/00000/3C8C839C-3256-E711-AD6B-02163E01A717.root for upload 2018-01-19 15:50:03,364 INFO Adding replicas in Rucio catalog 2018-01-19 15:50:03,404 INFO Replicas successfully added 2018-01-19 15:50:03,404 INFO Adding replication rule on RSE T2_US_NEBRASKA for the file user.ewv:/store/data/Run2017A/SingleMuon/MINIAOD/PromptReco-v3/000/296/980/00000/3C8C839C-3256-E711-AD6B-02163E01A717.root 2018-01-19 15:50:06,514 ERROR list index out of range 2018-01-19 15:50:06,515 ERROR Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the available information to reproduce this error at  2018-01-19 15:50:06,519 DEBUG Traceback (most recent call last): File \"/usr/bin/rucio\", line 168, in new_funct return function(*args, **kwargs) File \"/usr/bin/rucio\", line 1082, in upload trace['protocol'] = rse_settings['protocols'][0]['scheme'] IndexError: list index out of range Completed in 8.8367 sec. ``` ------------ Expected behavior ----------------- ", "496": "---------- ``` [root@c936e632db24 /]# rucio update-rule --lifetime 360 0fac0679b1b042559d2a2ab5240c8c82 2018-01-19 15:01:30,483 ERROR 'int' object has no attribute 'lower' 2018-01-19 15:01:30,484 ERROR Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the available information to reproduce this error at  ``` The variable is forcefully casted to int() in argparse thus the .lower() of the variable fails. ", "493": "---------- rucio set-metadata returns status code 0 for non-existing DID, e.g. : rucio set-metadata --did mc16_5TeV:non_existing_dataset --key transient --value 0 echo $? 0 ------------ Should return non 0 or an error message ", "490": "---------- The shuffling makes sense if no client location can be detected, to distributed the load, however, if a client location can be detected, the list_replicas call returns the replica locations already in a sorted order and they must not be shuffled. ", "487": "---------- Approvers want to give a reason when denying a replication rule. ------------ This change needs two modifications: - [x] WebUI: A website where an approver can type in a short reason when denying a replication rule. This can use the same REST call to the server, just a comment is supplied as well. - [x] Core: Change of the deny method in core and change of the email templates, to deliver the reason to the user. ", "486": "---------- Currently, the server and daemon images are always using a  config with SSL. It should be configurable so that it is possible to easily deploy containers without SSL, e.g., for pods in a Kubernetes cluster. ------------ Add an ENV variable 'RUCIO_ENABLE_SSL'. The right config will be created when the container starts. ", "484": "---------- In the method where the target_rse is selected, bb8 should check if there already exists a collection replica or dataset lock and then skip this RSE. _Migrated from  ", "483": "---------- Sometimes we get the error in panda for our jobs: ``` ddm, 200: Could not add files to DDM: Details: [u\"One of the PFNs provided does not match the Rucio expected PFN : [u'gsiftp://dtn01.nersc.gov/project/projectdirs/m2015/atlasdatadisk/rucio/mc16_13TeV/3b/cd/log.11350637._033145.job.log.tgz.1'] vs [u'gsiftp://dtn01.nersc.gov:2811/project/projectdirs/m2015/atlasdatadisk/rucio/mc16_13TeV/3b/cd/log.11350637._033145.job.log.tgz.1'] ([ {'scope': u'mc16_13TeV', 'name': u'log.11350637._033145.job.log.tgz.1'} ])\"] ``` In this message it does not make it clear, which path we should be editing vs which one Rucio is expected. I found the message sitting in rucio/src/lib/rucio/core/replica.py around line 1023: raise exception.InvalidPath('One of the PFNs provided does not match the Rucio expected PFN : %s vs %s (%s)' % (str(pfns), str(expected_pfns), str(lfns))) So I see the first is probably what we set in AGIS and the second is what you are expecting. Can you make this clear in the error message? Thanks, Taylor _Migrated from  ", "480": "---------- Attempts to finish transfers end with this issue: ``` Database exception. Details: (psycopg2.ProgrammingError) operator does not exist: boolean = integer LINE 3: WHERE rses.deleted = false AND rses.deterministic = 0 AND (r... ^ HINT: No operator matches the given name and argument type(s). You might need to add explicit type casts. [SQL: 'SELECT rses.id AS rses_id, rses.rse AS rses_rse, rses.rse_type AS rses_rse_type, rses.deterministic AS rses_deterministic, rses.volatile AS rses_volatile, rses.staging_area AS rses_staging_area, rses.city AS rses_city, rses.region_code AS rses_region_code, rses.country_name AS rses_country_name, rses.continent AS rses_continent, rses.time_zone AS rses_time_zone, rses.\"ISP\" AS \"rses_ISP\", rses.\"ASN\" AS \"rses_ASN\", rses.longitude AS rses_longitude, rses.latitude AS rses_latitude, rses.availability AS rses_availability, rses.updated_at AS rses_updated_at, rses.created_at AS rses_created_at, rses.deleted AS rses_deleted, rses.deleted_at AS rses_deleted_at \\nFROM rses JOIN rse_attr_map ON rses.id = rse_attr_map.rse_id \\nWHERE rses.deleted = false AND rses.deterministic = %(deterministic_1)s AND (rses.availability = %(availability_1)s OR rses.availability = %(availability_2)s OR rses.availability = %(availability_3)s OR rses.availability = %(availability_4)s OR rses.availability = %(availability_5)s OR rses.availability = %(availability_6)s OR rses.availability = %(availability_7)s OR rses.availability = %(availability_8)s) AND (rses.availability = %(availability_9)s OR rses.availability = %(availability_10)s OR rses.availability = %(availability_11)s OR rses.availability = %(availability_12)s OR rses.availability = %(availability_13)s OR rses.availability = %(availability_14)s OR rses.availability = %(availability_15)s OR rses.availability = %(availability_16)s) GROUP BY rses.id, rses.rse, rses.rse_type, rses.deterministic, rses.volatile, rses.staging_area, rses.city, rses.region_code, rses.country_name, rses.continent, rses.time_zone, rses.\"ISP\", rses.\"ASN\", rses.longitude, rses.latitude, rses.availability, rses.updated_at, rses.created_at, rses.deleted, rses.deleted_at'] [parameters: {'availability_10': 1, 'availability_11': 2, 'availability_12': 3, 'availability_13': 4, 'availability_14': 5, 'availability_15': 6, 'availability_16': 7, 'availability_8': 7, 'availability_9': 0, 'deterministic_1': 0, 'availability_2': 1, 'availability_3': 2, 'availability_1': 0, 'availability_6': 5, 'availability_7': 6, 'availability_4': 3, 'availability_5': 4}] 2018-01-17 07:38:14,882 12917 WARNING Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/finisher.py\", line 118, in finisher __handle_requests(chunk, suspicious_patterns) File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/finisher.py\", line 196, in __handle_requests undeterministic_rses = __get_undeterministic_rses() File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/finisher.py\", line 311, in __get_undeterministic_rses rses_list = list_rses(filters={'deterministic': 0}) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 274, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. ``` ------------ Seems that the Postgres refuses to do an implicit cast from integer to boolean. The query in the finisher should do this before handing it to SQLAlchemy. ", "469": "---------- We added a REST API for manipulating the distance information between two links. However, we did not add the corresponding CLI. ------------ Add corresponding CLI. Expected behavior ----------------- Set very basic distance information (ranking and distance) via CLI. More detailed attribute manipulation can remain part of the REST API. ", "468": "---------- Similar to `rucio download` there should be a protocol failover mechanism in `rucio upload`, if the process does not work. This can probably be c/p in large parts. ", "467": "---------- The fix done in #83 to force GSI authentication did not work for gfal. The related merge request is #223 ", "464": "---------- The creation of INCOMPLETE messages for datasets whose first file was deleted does not seem to work. This is related to the development done in #69 There are no INCOMPLETE messages in the message_history to be found. Expected behavior ----------------- Generation of INCOMPLETE messages once the first file of a dataset is permanently deleted. ", "460": "From the CMS instance, I noticed this exception thrown in the `conveyor-transfer-submitter` after creating some minimal RSEs and trying to trigger a transfer ``` 2018-01-11 20:59:36,983 4217 CRITICAL Exception happened when trying to get transfer for request 8a818df9ca4047e3a6b36f95b98a5c7e: Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/core/transfer.py\", line 567, in get_transfer_requests_and_source_replicas scheme=current_schemes) File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 640, in find_matching_scheme dest_candidates = sorted(dest_candidates, key=lambda k: k['domains'][domain][operation_dest]) File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 640, in <lambda> dest_candidates = sorted(dest_candidates, key=lambda k: k['domains'][domain][operation_dest]) KeyError: None ``` ", "457": "---------- __declare_bad_file_replicas crash when one declare bad file replicas already declared bad on non-deterministic RSE because of the check : rowcount != len(declared_replicas) ------------ Remove the files already declared from the declared_replicas list. ", "453": "---------- Currently in R2D2 it possible to jump directly to the RSE selection tab without selecting any DID. Then, the quota check will hang without any error message. ------------ Prevent the RSE tab to be opened if no DID is selected. ", "450": "---------- With the necessary APIs in place we can now do smart selection of WAN and LAN replicas. ------------ - extend list_replicas to reply with replicas across all domains - select correct domain replica in clients based on location ", "447": "---------- The change done in  has changed the behaviour of the set_new_dids method. Before when new_flag was set to None, the value set in the DB was NULL, now it's 0. It is a problem because this column is indexed. ------------ Change the set_new_dids method to set NULL value instead of 0 ", "444": "---------- Lifetime is a valid parameter of add_container(), but is ignored ------------ Expected behavior ----------------- Should be passed through to add_did() ", "439": "---------- The bootstrap procedure mistakenly sets GSS values for SSH. ------------ Fix typo ", "436": "---------- In many Rucio REST APIs (but not all! Styling is a bit inconsistent between the newest and the oldest APIs), the scope name and the DID are encoded as part of the URL. This is problematic for CMS DIDs as there are HTML meta-characters utilized in our naming scheme (particularly `#` and `/`). It is probably other organizations looking to adopt Rucio will have a similar problem. ------------ The client should escape any scope and DID as a part of URL generation. Expected behavior ----------------- DIDs naming should be limited by the schema in place on the Rucio server, not URL rules. ", "433": "---------- In testing of `rucio upload`, I've sporadically hit cases where: 1. In a sequence of REST API calls, one call requires the client to retrieve a new Rucio token. 2. The client will contact the token issuer endpoint to retrieve a new token. 3. Since the client reuses the session object, an existing SSL session may be utilized. 4. If the existing SSL session wasn't created with an X509 user proxy (or established via TLS session reuse), then `mod_gridsite` isn't able to pass along the appropriate headers about the client DN. 5. Without the client DN, the issuer (correctly) refuses to issue a new token. ------------ Whenever the client receives an authorization failure, it should create a new session object prior to retrying the token retrieval. ", "430": "---------- The copyright footer currently reads 2012-2017. ------------ Change to 2012-2018. Expected behavior ----------------- Correct copyright footer in WebUI. ", "429": "---------- Auditor is not able to handle dumps on non-SRM endpoint ------------ Enable the dump download for gsiftp and davs sites ", "426": "---------- For rucio move_rule the permission check is not working, thus the default permission is used. ", "425": "- automatic mode skips sites, where is some active rebalancing rule in STUCK and REPLICATING status. For the moment, only REPLICATING shall be used and later, it shall count the number of bytes that are currently replicating. - test overlap between t2s and Nuclei, shall the rebalancing in these two groups completely seprate? - development of softer conditions for rules that can be rebalanced. - reduce background rebalancing, optimal set of params. - very low efficiency of t2 bg rebalancing:  ", "422": "---------- After I do some studies, I think python module requests can only use x509 certificate but can not use proxy certificate for SSL. (There is no problem if I use x509 or userpass authentication.) So it need mod_gridsite for Proxy authentication. But since www.gridsite.org is no longer available, I can not figure out how to use or check these setting. <LocationMatch /auth/x509_proxy> GridSiteIndexes on GridSiteAuth on GridSiteDNlists /etc/grid-security/dn-lists/ GridSiteGSIProxyLimit 16 GridSiteEnvs on GridSiteACLPath /opt/rucio/etc/gacl </LocationMatch> ------------ solved this issue by \"pip install urllib3[secure]==1.22\" It seems urllib did not enable certification verification by default. According to  I install the package and there is no SSL error. (PS. I did not found this error by run_tests.sh ) report from Wu Edward ", "420": "---------- Traceback (most recent call last): File \"recreate_protocol.py\", line 37, in <module> c.add_protocol(rse=rse, params=protocol) File \"/usr/lib/python2.7/site-packages/rucio/client/rseclient.py\", line 229, in add_protocol raise exc_cls(exc_msg) rucio.common.exception.RSEOperationNotSupported: RSE does not support requested operation. Details: Operation 'third_party_copy' not defined in schema. ------------ Add third_party_copy to the list of supported operations ", "413": "---------- set_metadata run on guid only updates guid in the did table, but no the content one ------------ set_metadata should update guid both in did and content table ", "412": "---------- If the computed eol_at differs from the recorded one, Atropos doesn't take into account the new value. It can lead to expire rules that shouldn't be expired. ------------ Check that computed eol_at is expired before expiring the rule ", "411": "If we want `/` to be a valid character in the LFN, URL escaping in the client suddenly becomes more relevant. For example, on the CMS test instance, I noticed the following GET request in the logs: ``` GET /dids/user.bbockelm//store/mc/SAM/GenericTTbar/AODSIM/CMSSW_9_2_6_91X_mcRun1_realistic_v2-v1/00000/AE237916-5D76-E711-A48C-FA163EEEBFED.root/meta HTTP/1.1\" ``` We probably wanted to issue this request: ``` GET /dids/user.bbockelm/%2Fstore%2Fmc%2FSAM%2FGenericTTbar%2FAODSIM%2FCMSSW_9_2_6_91X_mcRun1_realistic_v2-v1%2F00000%2FAE237916-5D76-E711-A48C-FA163EEEBFED.root/meta HTTP/1.1\" ``` ", "410": "When looking through the logs for something else, I noticed the following: ``` ==> /var/log/rucio/conveyor-finisher.log <== Database exception. Details: (psycopg2.ProgrammingError) column requests.estimated_started_at does not exist LINE 1: ..., requests.submitter_id AS requests_submitter_id, requests.e... ^ [SQL: 'SELECT requests.id AS requests_id, requests.request_type AS requests_request_type, requests.scope AS requests_scope, requests.name AS requests_name, requests.did_type AS requests_did_type, requests.dest_rse_id AS requests_dest_rse_id, requests.source_rse_id AS requests_source_rse_id, requests.attributes AS requests_attributes, requests.state AS requests_state, requests.external_id AS requests_external_id, requests.external_host AS requests_external_host, requests.retry_count AS requests_retry_count, requests.err_msg AS requests_err_msg, requests.previous_attempt_id AS requests_previous_attempt_id, requests.rule_id AS requests_rule_id, requests.activity AS requests_activity, requests.bytes AS requests_bytes, requests.md5 AS requests_md5, requests.adler32 AS requests_adler32, requests.dest_url AS requests_dest_url, requests.submitted_at AS requests_submitted_at, requests.started_at AS requests_started_at, requests.transferred_at AS requests_transferred_at, requests.estimated_at AS requests_estimated_at, requests.submitter_id AS requests_submitter_id, requests.estimated_started_at AS requests_estimated_started_at, requests.estimated_transferred_at AS requests_estimated_transferred_at, requests.account AS requests_account, requests.requested_at AS requests_requested_at, requests.priority AS requests_priority, requests.updated_at AS requests_updated_at, requests.created_at AS requests_created_at \\nFROM requests \\nWHERE requests.state IN (%(state_1)s, %(state_2)s, %(state_3)s, %(state_4)s, %(state_5)s, %(state_6)s, %(state_7)s) AND requests.request_type IN (%(request_type_1)s, %(request_type_2)s, %(request_type_3)s) AND requests.updated_at < %(updated_at_1)s ORDER BY requests.updated_at ASC \\n LIMIT %(param_1)s'] [parameters: {'updated_at_1': datetime.datetime(2017, 12, 19, 17, 11, 52, 245632), 'param_1': 1000, 'request_type_1': 'T', 'request_type_3': 'O', 'request_type_2': 'I', 'state_1': 'D', 'state_3': 'L', 'state_2': 'F', 'state_5': 'A', 'state_4': 'G', 'state_7': 'O', 'state_6': 'N'}] 2017-12-19 11:11:52,251 13154 ERROR Thread [1/1] : Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/conveyor/finisher.py\", line 109, in finisher thread=hb['assign_thread'], total_threads=hb['nr_threads']) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 274, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (psycopg2.ProgrammingError) column requests.estimated_started_at does not exist LINE 1: ..., requests.submitter_id AS requests_submitter_id, requests.e... ^ [SQL: 'SELECT requests.id AS requests_id, requests.request_type AS requests_request_type, requests.scope AS requests_scope, requests.name AS requests_name, requests.did_type AS requests_did_type, requests.dest_rse_id AS requests_dest_rse_id, requests.source_rse_id AS requests_source_rse_id, requests.attributes AS requests_attributes, requests.state AS requests_state, requests.external_id AS requests_external_id, requests.external_host AS requests_external_host, requests.retry_count AS requests_retry_count, requests.err_msg AS requests_err_msg, requests.previous_attempt_id AS requests_previous_attempt_id, requests.rule_id AS requests_rule_id, requests.activity AS requests_activity, requests.bytes AS requests_bytes, requests.md5 AS requests_md5, requests.adler32 AS requests_adler32, requests.dest_url AS requests_dest_url, requests.submitted_at AS requests_submitted_at, requests.started_at AS requests_started_at, requests.transferred_at AS requests_transferred_at, requests.estimated_at AS requests_estimated_at, requests.submitter_id AS requests_submitter_id, requests.estimated_started_at AS requests_estimated_started_at, requests.estimated_transferred_at AS requests_estimated_transferred_at, requests.account AS requests_account, requests.requested_at AS requests_requested_at, requests.priority AS requests_priority, requests.updated_at AS requests_updated_at, requests.created_at AS requests_created_at \\nFROM requests \\nWHERE requests.state IN (%(state_1)s, %(state_2)s, %(state_3)s, %(state_4)s, %(state_5)s, %(state_6)s, %(state_7)s) AND requests.request_type IN (%(request_type_1)s, %(request_type_2)s, %(request_type_3)s) AND requests.updated_at < %(updated_at_1)s ORDER BY requests.updated_at ASC \\n LIMIT %(param_1)s'] [parameters: {'updated_at_1': datetime.datetime(2017, 12, 19, 17, 11, 52, 245632), 'param_1': 1000, 'request_type_1': 'T', 'request_type_3': 'O', 'request_type_2': 'I', 'state_1': 'D', 'state_3': 'L', 'state_2': 'F', 'state_5': 'A', 'state_4': 'G', 'state_7': 'O', 'state_6': 'N'}] 2017-12-19 11:11:52,251 13154 INFO Thread [1/1] : Sleeping for a while : 29.9860031605 seconds ``` Is this a new column? Is it possible some schema evolution didn't get applied at the CMS instance (or maybe actually a \"real\" bug)? ", "407": "The work in #339 caused a regression if the `permission` section does not exist. From the logs: ``` mod_wsgi (pid=18803): Exception occurred processing WSGI script '/usr/lib/python2.7/site-packages/rucio/web/rest/did.py'. Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/web/rest/did.py\", line 22, in <module> from rucio.api.did import (add_did, add_dids, list_content, list_content_history, File \"/usr/lib/python2.7/site-packages/rucio/api/did.py\", line 19, in <module> from rucio.core import did, naming_convention, meta as meta_core File \"/usr/lib/python2.7/site-packages/rucio/core/did.py\", line 35, in <module> import rucio.core.rule File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 2888, in <module> def archive_localgroupdisk_datasets(scope, name, session=None): File \"/usr/lib/python2.7/site-packages/rucio/common/policy.py\", line 78, in policy_filter policy = get_policy() File \"/usr/lib/python2.7/site-packages/rucio/common/policy.py\", line 40, in get_policy policy = config_get('permission', 'policy') File \"/usr/lib/python2.7/site-packages/rucio/common/config.py\", line 30, in config_get return __CONFIG.get(section, option) File \"/usr/lib64/python2.7/ConfigParser.py\", line 607, in get raise NoSectionError(section) NoSectionError: No section: 'permission' ``` Seems that this exception type is simply not caught. ", "406": "---------- ``` # rucio erase user.ewv:Astore\\* 2017-12-18 15:48:14,645 INFO CAUTION! erase operation is irreversible after 24 hours. To cancel this operation you can run the following command: rucio erase --undo user.ewv:Astore* ``` Expected behavior ----------------- Fail with error message ", "403": "---------- If often happens that suspicious files have more than one replica. In that case an automatic recovery can be done without the need of a human confirmation. ------------ Automatic recovery of suspicious files that have more than one replica ", "402": "---------- On error, it's possible to get something like this: ``` 2017-12-18 21:48:29,429 ERROR Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the available information to reproduce this error at  ``` (linebreaks added by me for readability) Should this be just changed to GitHub? ------------ Modify `rucio` CLI as needed to selectively remove references to CERN JIRA (there may be some cases that are ATLAS-specific where this is warranted). Probably worthwhile to `grep` through the entire code base to see if there's any other place where this can be done. ", "399": "---------- Make the length of the name configurable ", "394": "------------ add # in cms regexp name ", "391": "---------- We have to force remove the newly rolled out ssh support on the clients, due to an ALRB python2.6 incompatibility. The ssh support will eventually be moved to rucio-clients[extras]. ", "388": "---------- One PostgreSQL test is failing. ``` ====================================================================== FAIL: DATA IDENTIFIERS (CLIENT): List dids by pattern. ---------------------------------------------------------------------- Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/nose/case.py\", line 197, in runTest self.test(*self.arg) File \"/opt/rucio/lib/rucio/tests/test_did.py\", line 187, in test_list_dids assert_equal(len(results), 2) AssertionError: 3 != 2 -------------------- >> begin captured logging << -------------------- urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): localhost urllib3.connectionpool: DEBUG:  \"POST /accounts/jdoe/scopes/mock_ab5a437f644947e5b456 HTTP/1.1\" 201 None urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): localhost urllib3.connectionpool: DEBUG:  \"POST /replicas HTTP/1.1\" 201 None urllib3.connectionpool: DEBUG: Starting new HTTPS connection (2): localhost urllib3.connectionpool: DEBUG:  \"POST /replicas HTTP/1.1\" 201 None urllib3.connectionpool: DEBUG: Starting new HTTPS connection (3): localhost urllib3.connectionpool: DEBUG:  \"POST /replicas HTTP/1.1\" 201 None urllib3.connectionpool: DEBUG: Starting new HTTPS connection (1): localhost urllib3.connectionpool: DEBUG:  \"GET /dids/mock_ab5a437f644947e5b456/dids/search?type=file&name=file_a_%2A HTTP/1.1\" 200 None urllib3.connectionpool: DEBUG:  \"GET /dids/mock_ab5a437f644947e5b456/dids/search?type=file&name=file_a_1%2A HTTP/1.1\" 200 None urllib3.connectionpool: DEBUG:  \"GET /dids/mock_ab5a437f644947e5b456/dids/search?type=file&name=file_%2A_1%2A HTTP/1.1\" 200 None --------------------- >> end captured logging << --------------------- ``` ", "383": "---------- Passed travis tests exit with 0. ", "380": "---------- In order to track all contributions to Rucio, a separate file \"AUTHORS\" is included top-level. It includes both individual personal contributors, as well as their associated institutes or organisations. ", "377": "Currently, `rucio upload` is a bit tied to the ATLAS concept that the filename on disk (`foo.root`) is going to match the filename in Rucio (`user.bbockelm:foo.root`). This is a bit broken by CMS, where the LFN differs significantly from the name on disk. We should allow the LFN to be specified via a CLI flag, just like the GUID. The `rucio upload` interface should, by default, retain the current behavior. ", "376": "---------- rucio list-files crashes if no GUID is set ------------ Protect against guid = None ", "375": "---------- rucio list-files crashes if no GUID is set `# rucio list-files mock:myDataset1 2017-12-14 19:50:52,569 ERROR 'NoneType' object has no attribute '__getitem__' ` ------------ Protect against guid = None ", "369": "------------ Imported the full Apache Licence 2.0 file ", "368": "_Migrated from  ", "367": "---------- More detailed report on [ATLAS  ", "363": "------------ Add the url and field description of the suspicious files dump to the dumps overview page of the WebUI. ", "359": "---------- cp: cannot stat '/opt/rucio/etc/docker/travis/rucio_postgresql.cfg': No such file or directory ------------ cp /opt/rucio/etc/docker/travis/rucio_postgres.cfg /opt/rucio/etc/rucio.cfg Expected behavior ----------------- postgresql tested in travis ! ", "356": "---------- Schema is not configurable per collaboration (cms use case) ------------ Extend the permission and schema part to allow [policy] permision = cms schema = cms from the configuration ", "351": "---------- Sorting in case of multiple tape replicas should be done in reverse order, 1 being best, 11 being worst. Also cases where the ranking is 0 should be skipped. ", "348": "---------- We wrongly forward to  instead of davs in the metalink link follow. ------------ Change string. ;-) ", "345": "---------- To ensure python 2.6 compatibility, the package paramiko was downgraded to an older version. However, it's dependencies, such as Crypto, need to be pinned to a Python 2.6 version as well to ensure compatibility. ", "344": "Instead of having to download a logfile archive manually, the WebUI will extract and preview logfile contents (below a certain filesize limit) directly in a new browser tab. ", "343": "At the moment it is not possible to download entire datasets at once. To make downloading datasets possible in a single step, the WebUI will be extended to create a metalink containing download links for all files contained in a dataset. ", "339": "Right now, the security policy is encoded into a separate Python module; the selected security policy for a given Rucio instance is part of the `rucio.cfg`. This allows Rucio to implement security policies that differ fairly drastically from ATLAS's default ones. Analogously, the current schema is somewhat specific to ATLAS: it would be useful if we had an override mechanism to provide a CMS-specific schema. For example, CMS site names are of the form `T2_US_Nebraska`; this is not currently a valid RSE as RSEs must be capitalized. Another example is CMS LFNs and dataset names, which liberally utilize the `/` character (as in `/store/generator/Fall13pLHE/BprimeTToBZ_M1200GeV_Tune4C_13TeV-madgraph-tauola/GEN/START62_V1-v1/00000/5EE73686-1651-E411-B447-02163E00EF58.root` and `/BprimeTToBZ_M1200GeV_Tune4C_13TeV-madgraph-tauola/Fall13pLHE-START62_V1-v1/GEN`, respectively). ", "338": "---------- Each CMS storage endpoint has a set of rules for generating their PFNs. Example:  With this list of regex-based transform rules, we can generate a PFN only knowing: - Desired protocol - RSE - LFN My understanding is that, currently, the input to the PFN generation function for the non-deterministic case is only \"desired protocol\" and \"LFN\". Note that only a few sites actually utilize the full power of the transform, versus utilizing the prefix-based approach (However, there's a strong overlap between \"important sites\" and \"complex sites\".) Hence, for a first approximation, we might be able to get away with simply marking the RSE as doing the \"identity transform\" where the PFN is generated as: ``` $(SCHEMA)://$(HOSTNAME):$(PORT)/$(PREFIX)/$(LFN) ``` E.g., no hash in the component; this is because the CMS LFNs are of the form `/store/generator/Fall13pLHE/BprimeTToBZ_M1200GeV_Tune4C_13TeV-madgraph-tauola/GEN/START62_V1-v1/00000/5EE73686-1651-E411-B447-02163E00EF58.root` and are already appropriately hashed. Proposed ------------ We'll likely need to modify the code here:  Expected behavior ----------------- RSEs should be able to carry an attribute describing the LFN-to-PFN process they utilize. ", "336": "---------- Some modules are not necessary in the clients and should be imported only if available or installed in the environment ------------ * Define a list of extra packages, check for their presence and import them if there * Add a new exception missingModuleException and raise it from the method which can not worked with the missing module ", "331": "---------- From the current contributing guide it's not clear that pull requests depending on their types should go against certain branches. In addition, usual and popular pull request tools like the github interface and the hub wrapper are badly covered. ------------ * Few corrections for the format of the pull requests * Remind the logic for patch and feature pull requests * Mention the github interface and the equivalent workflow with hub ", "330": "It seems that the only way to set distances between two RSEs is to directly manipulate the database (or write a python script that can execute against Rucio's DB). I would expect this to be exposed via: - the typical REST API, - Python client API (perhaps requiring a new permission to be created?), - `rucio-admin` CLI. ", "329": "---------- Documentation is still missing, the JSON schema is not (yet) self-explanatory. ------------ Add general documentation and link to the ATLAS specific parts on twiki. Also, change the numbers in the JSON sample file to be more realistic for real use cases (to reduce the possibility of confusion of units), and extend the schema file with a short description of the fields. Expected behavior ----------------- Users new to this hopefully get their questions answered ;-). This is handled in PR #319 . ", "327": "Currently, the only way to explicitly manage protocols is to write some python code. Example:  Now, eventually CMS or ATLAS-sized VOs will probably want to use an external service (such as AGIS) to maintain this information. However, during the evaluation phase (or even steady-state for VOs with a modest number of endpoints), it'd be really beneficial to have this piece of the API wrapped in the CLI. ", "325": "---------- Bash completion (e.g., hitting `<tab>`) provides a powerful way to improve the the user experience. With `argcomplete`, most of the static suggestions are present. However, dynamic items like `rucio list-rse-attributes <tab>` (which should suggest available RSEs) don't work. Let's add some smarter auto-complete logic! ", "323": "---------- Currently, direct users of the `rucio` tools must setup the `$X509_USER_PROXY` environment variable and/or set a line in their `rucio.cfg`. There are various standard ways to auto-discover the proxy in the user's environment. Let's use them! ", "322": "---------- When developing code it is much faster to just mount the modified code in the container for testing than to rebuild the container to test every change. ------------ - [x] Remove source code from docker image in: - [x] bin - [x] lib - [x] tools - [x] Provide docker-compose manifest that launches dev environment in `etc/docker/dev` - [x] Provide docker based pre-commit hook - [x] Document usage in `etc/docker/dev` (README.md) Expected behavior ----------------- The developer may use docker-compose with the manifest to launch dev container environment. The code may be tested in the container using `docker exec ...`. The pre-commit hook runs `pylint` analysis in a container based on the dev image. ", "321": "_Migrated from  ", "320": "---------- The closeness_sorter module seems to be dead code. ------------ Confirm that it is dead and remove it from the package. ", "317": "---------- Missing cx_Oracle dependency in DockerFile ------------ Add cx_Oracle ", "314": "---------- Pin the dependency **paramiko** to version 1.18.4 for python 2.6 compatibility. ", "313": "---------- ` Traceback (most recent call last): File \"/opt/rucio/lib/rucio/daemons/bb8/nuclei_background_rebalance.py\", line 80, in <module> rebalance_rse(source_rse['rse'], max_bytes=available_target_rebalance_volume, dry_run=False, comment='Nuclei Background rebalancing', force_expression=destination_rse['rse']) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 349, in new_funct result = function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/daemons/bb8/common.py\", line 251, in rebalance_rse raise error rucio.common.exception.RSEBlacklisted: RSE excluded due to write blacklisting. Details: RSE excluded due to write blacklisting. ` ------------ Catch the exception properly ", "312": "---------- The query against the locks takes too much time and Oracle fails with \"snapshot too old\" ` *************************** BB8 - Execution Summary Mode: DECOMISSION Dry Run: False *************************** scope:name rule_id bytes(Gb) target_rse child_rule_id (cx_Oracle.DatabaseError) ORA-01555: snapshot too old: rollback segment number 7078 with name \"_SYSSMU7078_1542158916$\" too small [SQL: \"SELECT r.scope, r.name, rawtohex(r.id) as rule_id, r.rse_expression as rse_expression, r.subscription_id as subscription_id, 0 as bytes, 0 as length FROM atlas_rucio.rules r\\nWHERE\\nr.id IN (SELECT rule_id FROM atlas_rucio.locks WHERE rse_id = atlas_rucio.rse2id(:rse) GROUP BY rule_id) and\\nr.state = 'O' and\\nr.child_rule_id is NULL\"] [parameters: {'rse': 'xxxx'] ` ------------ Use some dumps on Hadoop. The dumps must be grouped by rule_id, rse and state ", "307": "", "298": "append a trailing slash if missing ", "288": "---------- SAWarning: Textual SQL expression \"mod(abs(('x'||md5(rule_id...\" should be explicitly declared as text ", "285": "---------- The kerberos packages are blocking the doc generation on buildthedocs ------------ Modify the main setup.py to not include kerberos packages by default but in extra requires Expected behavior ----------------- Working documentation generation on buidthedocs ", "282": "---------- Need to provide postgres support ------------ Include postgres client into the CC7 docker image ", "281": "---------- Express the \"recommended\" dependencies for the db backend ------------ Change of the main setup.py script to include extra dependencies Expected behavior ----------------- pip install rucio[oracle] pip install rucio[postgresql] pip install rucio[mysql] or maybe define later some meta packages like rucio-oracle with install_requires = [\"rucio[oracle]\"] ", "280": "---------- All queries should be psql compatible now, so travis can also test it now. ", "279": "---------- If multiple tape replicas are available to the conveyor for sources the conveyor should select them based not only on distance, but also on the sizes of the staging queues. Expected behavior ----------------- 2 replicas, same distance, the one with less staging jobs in the queue should be used. ", "272": "---------- Test if RSEs are properly configured for upload/download of files using the rucio client (e.g. for stageout/stagein with the rucio sitemover in the pilot) ------------ Add new nagios probe Expected behavior ----------------- Loops over all protocols supported by target RSE Uses rucio client to upload and download a test file for each protocol Produces a report file in the format: RSE, action, protocol, result, timestamp ", "265": "---------- Currently there are two errors with postgressql which have to be fixed to make Rucio fully postgressql compatible. Expected behavior ----------------- All tests run with postgressql. ", "262": "---------- PostgreSQL is interpreting the SQL standard correctly, it's actually Oracle and MySQL which are doing non-standard things here. The problem you see is that for an outer join the nullable side cannot be locked for reading, so PostgreSQL stops. This is the last outstanding issue for PostgreSQL compatibility. ------------ Changing the query for add-rule from \"select outer join for update\" into something like \"select for update union select\". Expected behavior ----------------- add-rule will work on PostgreSQL. ", "261": "---------- In the XENON case, we have an origin server from which the files are uploaded to the source RSE. The origin server and source RSE are the same server. Rather than doing a `gfal-copy` it would be better to do a `mv`/`rsync` of the data. ------------ Add a flag to `rucio upload` or check the origin hostname whether the start and endpoint are the same machine. ", "258": "---------- The installation of the Oracle instantclient is not needed anymore in the docker build for travis and breaks the whole build. It needs to be removed ------------ Expected behavior ----------------- ", "255": "---------- There are some merge leftovers in the doc/source/conf.py file. ", "246": "---------- The DB part is used in server, daemons and UI image. Can be factorized by moving it to CC7 image ------------ Move the DB part to CC7 image ", "245": "---------- Some pages in the RucioUI are experiment-specific, make the generation of pages more fine-grained. ------------ When generating RucioUI pages, automatically include/exclude pages based on the defined policy (e.g., only show ATLAS pages in ATLAS instance). ", "244": "---------- ``` $ rucio -v add-dataset user.briedel:test_test_test --lifetime 100 2017-11-30 16:01:50,357 ERROR [No JSON object could be decoded] 2017-11-30 16:01:50,357 ERROR [Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the necessary information at  2017-11-30 16:01:50,359 DEBUG [Traceback (most recent call last): File \"/usr/bin/rucio\", line 161, in new_funct return function(*args, **kwargs) File \"/usr/bin/rucio\", line 423, in add_dataset client.add_dataset(scope=scope, name=name, statuses={'monotonic': args.monotonic}, lifetime=args.lifetime) File \"/usr/lib/python2.7/site-packages/rucio/client/didclient.py\", line 131, in add_dataset lifetime=lifetime, dids=files, rse=rse) File \"/usr/lib/python2.7/site-packages/rucio/client/didclient.py\", line 100, in add_did exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content) File \"/usr/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 191, in _get_exception data = parse_response(data) File \"/usr/lib/python2.7/site-packages/rucio/common/utils.py\", line 193, in parse_response return json.loads(data, object_hook=datetime_parser) File \"/usr/lib64/python2.7/json/__init__.py\", line 351, in loads return cls(encoding=encoding, **kw).decode(s) File \"/usr/lib64/python2.7/json/decoder.py\", line 365, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/lib64/python2.7/json/decoder.py\", line 383, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded ] Completed in 0.0916 sec. ``` ", "243": "---------- We should replace  by the github issue link in the client code 2017-11-30 16:01:50,357 ERROR [Rucio exited with an unexpected/unknown error. Please rerun the last command with the '-v' option and submit a ticket with all the necessary information at  ", "242": "---------- The CLI examples here no longer match the rucio client  ------------ So far I've found that \"add\" and \"list\" are no longer available, replaced by more specific commands, Expected behavior ----------------- ", "237": "", "236": "---------- Kronos already updates the accessed_at of each file. Additionally it should also update the # of accesses. ", "235": "---------- The AGIS probe doesn't remove manually added protocols in AGIS. It can lead to inconsistencies. ------------ AGIS probe will remove manually added protocols in Rucio ", "232": "---------- get_submitted_at_rucio function in forecast.py extension is buggy. ------------ The function have access to rucio database, but the query is tricky and need testing on production to see if the time to retrieve the submitted_at time stamps for a link in not an overkill. Expected behavior ----------------- Should return an array on time stamps ordered descendent (the oldest one at the end). ", "227": "---------- It is not needed and crashes the setup procedure. ", "214": "---------- Some packages appear several times in the various pip requires files. ", "213": "---------- Script which uses the github api to compile release notes based on milestones. ", "211": "---------- When the pilot uses the rucio mover the traces are sent out as type 'download' and with the account 'pilot'. This messes up the monitoring in the as they will show up as 'CLI download' in the Dashboard and we are also losing the user information. ------------ Add two new paramters the rucio download cli command: `--usrdn` and `--eventType` which will then be propagated to the traces. Expected behavior ----------------- ", "209": "---------- The current command to get the list of changed files for a PR, which was originally taken from the old build bot script, does not work with the travis workflow and has to be fixed. ------------ Expected behavior ----------------- ", "205": "---------- Minor cleanups in the repo. ", "204": "---------- The schema.sql file is full of ATLER table statements, instead of fully correct table signatures. ------------ The signatures must be corrected and the alter table statements removed. ", "199": "---------- The probe only checks if an account has quota on some SCRATCHDISK, thus for old accounts, they will never get quota on new scratchdisks. ------------ This has to be changed to actually check each quota specifically. ", "196": "---------- If a dataset is touched before the end of it's lifetime, the lifetime is not extended ------------ Increase lifetime to last_access + extension ", "195": "", "192": "---------- As per request from Yun-Ha the updated_at timestamp was added to the consistency datasets dump. The dumps description page in the WebUI now needs to be updated accordingly. ------------ Expected behavior ----------------- ", "191": "---------- The CLI command rucio list-account-usage does not return any output. ", "187": "", "185": "", "177": "---------- When the dataset-name is part of the file-name the parse_pfns method for gfal (Maybe also for other protocols) creates a faulty path. ------------ Fix the parse_pfns method so it does not create a faulty path. ", "174": "---------- Remove JIRA syntax from prepare-commit-msg ", "171": "---------- >>>from rucio.rse import rsemanager as rsemgr >>> rse_info = rsemgr.get_rse_info('MOCK') >>> proto = rsemgr.create_protocol(rse_info, 'write') >>> proto.parse_pfns(['srm://locahost:8443/srm/managerv2?SFN=/pnfs/data/atlas/atlasgrouptape/group/phys-higgs/mc15_13TeV/group.phys-higgs.mc15_13TeV.mc15c.MGHwpp_tHjb125_yt_minus1.MxAODFlavorSys.p2908.h015.totape_20170825.root/mc15c.MGHwpp_tHjb125_yt_minus1.MxAODFlavorSys.p2908.h015.totape_20170825.root' ]) {'srm://locahost:8443/srm/managerv2?SFN=/pnfs/data/atlas/atlasgrouptape/group/phys-higgs/mc15_13TeV/group.phys-higgs.mc15_13TeV.mc15c.MGHwpp_tHjb125_yt_minus1.MxAODFlavorSys.p2908.h015.totape_20170825.root/mc15c.MGHwpp_tHjb125_yt_minus1.MxAODFlavorSys.p2908.h015.totape_20170825.root': {'name': u'mc15c.MGHwpp_tHjb125_yt_minus1.MxAODFlavorSys.p2908.h015.totape_20170825.root', 'hostname': 'locahos, 'prefix': u'/pnfs/data/atlas/atlasgrouptape/', 'path': u'/group/phys-higgs/mc15_13TeV/group.phys-higgs.mc15_13TeV.', 'scheme': 'srm', 'web_service_path': '/srm/managerv2?SFN=', 'port': '8443'}} Expected behavior ----------------- mc15_13TeV.mc15c. vs. mc15_13TeV./mc15c ", "168": "---------- If for one RSE no protocol is defined the get_space throws an exception `Traceback (most recent call last): File \"/usr/rucio/tools/probes/common/check_storage_space\", line 137, in <module> retvalue = get_space(rsename, protocol='srm', client=CLIENT) File \"/usr/rucio/tools/probes/common/check_storage_space\", line 106, in get_space print \"Update RSE %s space usage (usedsize: %s, freesize: %s)\" % (rsename, usedsize, freesize) UnboundLocalError: local variable 'usedsize' referenced before assignment` ------------ Only set the usedsize if protocol is defined ", "165": "---------- setup.py file is needed in the repo for readthedocs. ------------ Copy the setup_rucio.py to setup.py and have both (as duplicates) ", "158": "---------- The flake8 command used in travis is currently wrong and excludes everything. Currently: `flake8 --ignore=E501 --exclude=\"*.cfg bin/* lib/ tools/*.py tools/probes/common/*\"` But it should be: `flake8 --ignore=E501 --exclude=\"*.cfg\" bin/* lib/ tools/*.py tools/probes/common/*` ", "155": "---------- There are several flake8 errors both on master and next which need to be fixed. `flake8 --ignore=E501 --exclude=\"*.cfg\" bin/* lib/ tools/*.py tools/probes/common/*` ", "148": "---------- In the manual fix of the next branch of the repo some alembic migrate_repo files are missing. ------------ Add the missing files. ", "145": "---------- The changes in schema.sql and a alembic upgrade file is missing for the identity length changes for identity, account_map and tokens table. ", "142": "---------- When the last file in a dataset is removed by the reaper, the reaper also tries to delete the dataset. However, in certain cases it can happen that there is a SUSPENDED/Waitint for approval rule with 0 locks on the dataset, thus the deletion always fails with a child_constraint error from the database. ------------ The reaper should check for rules when it is deleting the dataset and remove them in case there are any. ", "141": "---------- Provide Dockerfiles to create container ------------ Provide Dockerfiles to create container ", "140": "---------- Cloud storage solutions have opted to use MD5 to verify file integrity. To support cloud storage and ensuring file integrity the MD5 digests of files added to the data catalogue should be computed. ------------ - [x] Client computes md5 checksum on file upload and adds this to the file metadata - [x] Client download checks the file adler32 or md5 against the metadata - preference of adler32 Expected behavior ----------------- The MD5 field of the file metadata in the Rucio database will be filled with the MD5 digest of the file for new files. ", "137": "---------- In the _get_possible_protocols call the filtering (tbr) of protocols removes too many protocols, as the list is essentially done twice (per domain) and it looks like more is removed than is there, thus an exception is raised. ------------ Make proper filtering. Split the candidates in two lists per domain and do the filtering smarter. Expected behavior ----------------- Exception should not be raised. ", "136": "---------- Add md5 method to extract md5 checksum from files (ported back from gitlab/next) ", "133": "---------- The idea is basically the same principle as with BB8. A specific rule should be moved to another RSE, including all it's properties. _Migrated from  ", "123": "---------- configure_git.sh still points to gitlab ------------ Move to github ", "122": "---------- The partitioning in atropos is broken. The atropos workers ask for a partition before all the workers are started ------------ Introduce a sleep before starting the main loop ", "121": "---------- For the lifetime model, it would be good to expose simply who(account) uses a dataset(did) and the last time of access. Another requirement is to provide a way for users to express their interest for DIDs and use it wrt to the lifetime model and the list of future deleted data. ------------ Extend models with two tables. consumers/leechers: scope, name, account, created_at_ updated_at, hits to keep track of the number of access, the first access and last access for an account to a specific dataset. Populated from the traces. core only needed. followers: scope, name, account, created_at_ updated_at, expired_at to keep track of people interest for a dataset. This will need clients, rest, core. ", "119": "---------- Our setup.py files normally read the version information from the git tag, with github this does not work anymore as github does not use annotated tags. ------------ Change setup_*.py files to remove the version generation and just use the version mentioned in the vcsversion.py file. ", "116": "---------- Sometimes datasets are resurrected which are already expired but not deleted yet. In this case the call does not do anything, and the dataset will be deleted at a later point in time. ------------ Change resurrect method to remove expiration date of dataset. ", "113": "Sometimes it happens that the origin branches are a bit out of sync with the upstream. In these cases just rebasing upstream (what the script does now) will not make the situation better, instead it should make a full reset against upstream, those making the origin/upstream the same. ", "109": "Github allows to define issue template and pull request template in the repo ", "106": "", "104": ": Add a new state to reflect that a dataset(did) can be INCOMPLETE on the system. At least one file of the content not there ", "101": "Pre-filling the version file. ", "98": "Some minor fixes and links are not written consistently (sometimes URL, sometimes names) ", "91": "", "86": "_Migrated from JIRA  ", "85": "_Migrated from JIRA  ", "84": "_Migrated from JIRA  ", "83": "if there are krb5 tickets in the user environment then root server will automatically prefer this. can be circumvented by setting export XrdSecPROTOCOL=gsi _Migrated from JIRA  ", "82": "When upload multiple files, if the number of file is too much so that cause error like \"database: too many connections\" or \"database deadlock for MySQL server\" or \"can not allocate memory on server\". Some upload process may be interrupted at the middle, leaving some DIDs without rule. If one want to delete these DIDs, one will fail, because rucio-undertaker give replicas tombstone when deleting rules: > /opt/rucio/lib/rucio/core/did.py(604)delete_dids() -> rucio.core.rule.delete_rule(rule_id=rule_id, purge_replicas=purge_replicas, delete_parent=True, nowait=True, session=session) But if a DID does not have a rule, rucio-undertaker will go to /opt/rucio/lib/rucio/core/did.py(639)delete_dids() if file_clause: rowcount = session.query(models.DataIdentifier).filter(or_(*file_clause)).\\ filter(models.DataIdentifier.did_type == DIDType.FILE).\\ update({'expired_at': None}, synchronize_session=False) and only reset expired_at to None. As a result, one can not delete these DIDs. _Migrated from JIRA  ", "81": "_Migrated from JIRA  ", "80": "_Migrated from JIRA  ", "79": "T\u00b3C service for transfer time to complete prediction. ToDo: Rucio DB access to retrieve the submitted_at for all the active transfers for a (src,dst,act) link. _Migrated from JIRA  ", "78": "_Migrated from JIRA  ", "77": "Hi I find a import problems about /opt/rucio/lib/rucio/tests/test_lifetime.py This test will send request to trigger lib/rucio/web/rest/lifetime_exception.py but it seems a circular import occurs in this .py ? ``` (.venv) [root@rucio-test03 rucio]# python -i lib/rucio/web/rest/lifetime_exception.py Traceback (most recent call last): File \"lib/rucio/web/rest/lifetime_exception.py\", line 17, in <module> from rucio.api.lifetime_exception import list_exceptions, add_exception, update_exception File \"/opt/rucio/lib/rucio/api/lifetime_exception.py\", line 14, in <module> from rucio.core import lifetime_exception File \"/opt/rucio/lib/rucio/core/lifetime_exception.py\", line 22, in <module> import rucio.common.policy File \"/opt/rucio/lib/rucio/common/policy.py\", line 23, in <module> import rucio.core.did File \"/opt/rucio/lib/rucio/core/did.py\", line 35, in <module> import rucio.core.rule File \"/opt/rucio/lib/rucio/core/rule.py\", line 32, in <module> import rucio.core.lock # import get_replica_locks, get_files_and_replica_locks_of_dataset File \"/opt/rucio/lib/rucio/core/lock.py\", line 27, in <module> from rucio.core.lifetime_exception import define_eol ImportError: cannot import name define_eol ``` In python shell: ``` >>> import rucio.core.lifetime_exception Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/opt/rucio/lib/rucio/core/lifetime_exception.py\", line 22, in <module> import rucio.common.policy File \"/opt/rucio/lib/rucio/common/policy.py\", line 23, in <module> import rucio.core.did File \"/opt/rucio/lib/rucio/core/did.py\", line 35, in <module> import rucio.core.rule File \"/opt/rucio/lib/rucio/core/rule.py\", line 32, in <module> import rucio.core.lock # import get_replica_locks, get_files_and_replica_locks_of_dataset File \"/opt/rucio/lib/rucio/core/lock.py\", line 27, in <module> from rucio.core.lifetime_exception import define_eol ImportError: cannot import name define_eol ``` Best RegardEdward Wu ASGC _Migrated from JIRA  ", "76": "``` 2017-10-05 09:11:53,730 9234 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/judge/injector.py\", line 87, in rule_injector inject_rule(rule_id=rule_id) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 349, in new_funct result = function(args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 630, in inject_rule session=session) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct result = function(args, kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rule.py\", line 2674, in __create_locks_replicas_transfers session=session) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct result = function(*args, kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rule_grouping.py\", line 88, in apply_rule_grouping session=session) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 365, in new_funct result = function(args, *kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/rule_grouping.py\", line 429, in __apply_rule_to_files_dataset_grouping prioritize_order_over_weight=True) File \"/usr/lib/python2.7/site-packages/rucio/core/rse_selector.py\", line 115, in select_rse raise InsufficientAccountLimit('There is insufficient quota on any of the target RSE\\'s to fullfill the operation.') InsufficientAccountLimit: There is not enough quota left to fulfil the operation. Details: There is insufficient quota on any of the target RSE's to fullfill the operation. ``` _Migrated from JIRA  ", "75": "_Migrated from JIRA  ", "74": "_Migrated from JIRA  ", "73": "_Migrated from JIRA  ", "72": "_Migrated from JIRA  ", "71": "_Migrated from JIRA  ", "70": "Do we need two different messages? One for Dataset one for Container or is the same one sufficient? _Migrated from JIRA  ", "69": "- ~~Decide on Message format for ActiveMQ Message to AMI:~~ #70 - ~~Schema change for database to add Partially Available(Imcomplete) to state column~~ - ~~Setting dataset/container state to PA and send message to AMI when first file gets deleted in Dataset~~ #71 _Migrated from JIRA  ", "68": "Existed in PandaQueues as WAN sink limits. We need a new way to do this in DDM. _Migrated from JIRA  ", "67": "# Tracking ticket for Py3 migration - [x] Clients: #819 - [x] Daemons: #1924 - [x] Server: - [x] REST API: #1738 - [x] Core: #1912 ", "66": "_Migrated from JIRA  ", "65": "In the code it seems like this is not properly implemented. _Migrated from JIRA  ", "64": "----------- For local usage, a combination of wan/lan attributes with rse_expressions will provide the local or remote replicas to a site. ------------- added lan/wan in list_replicas (clients/rest/server). default wan ", "63": "- [x] Poller - [x] Submitter  _Migrated from JIRA  ", "62": "The idea is to add the domain to all method signatures so that clients/conveyor can specify which domain should be used to create protocols/pathes. - [x] First iteration: Add domain to method signature, without implementing any functionality yet. - [x] Second iteration: The right output is generated depending on domain input. If no domain is given, wan is used, if wan does not exist, lan is used - [x] Third iteration: Client / Conveyor have to call the methods with the right domain. _Migrated from JIRA  ", "61": "If the src_rse_id is pre-filled the throttler can release requests based on links, and not only based on destination. _Migrated from JIRA  ", "60": "Set the expiration date of the rule to 4h (~) in the future if the child_rule is not OK yet, so that there is no jam of rules in front of the queue. _Migrated from JIRA  ", "59": "_Migrated from JIRA  ", "58": "_Migrated from JIRA  ", "56": "Hermes does not protect against double selection of messages, specifically in a phase where the partitions are not well established yet, e.g. during a restart. It does retrieve the messages with a SELECT FOR UPDATE, but the retrieval only takes ms and after that the row locks are given up. For it to be effective the locks have to be held until the message is removed from the table. This might not be a good idea to do. One possible solution would be to have an additional state column and the retrieve_message function SELECTS the messages (with for update) and updates their state to SUBMITTING. In subsequent retrieve calls, only messages in state WAITING or which are in the SUBMITTING state for more than 10min are selected. To be discussed. _Migrated from JIRA  ", "55": "_Migrated from JIRA  ", "52": "_Migrated from JIRA  ", "51": "Based on fts RSE attribute : > fts:  _Migrated from JIRA  ", "50": "_Migrated from JIRA  ", "49": "_Migrated from JIRA  ", "48": "When inserting messages to hermes which are too long this throws a DB exception `DatabaseException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (cx_Oracle.DatabaseError) ORA-01461: can bind a LONG value only for insert into a LONG column [SQL: 'INSERT INTO atlas_rucio.messages (id, event_type, payload, updated_at, created_at) VALUES (:id, :event_type, :payload, :updated_at, :created_at)'] [parameters: {'id': , 'created_at': datetime.datetime(2017, 5, 18, 14, 26, 32, 316246), 'event_type': 'email', 'updated_at': datetime.datetime(2017, 5, 18, 14, 26, 32, 316240), 'payload': '{\"body\": \"A new rule has been requested for approval in Rucio.\\\\n\\\\nRule description:\\\\n\\\\n ID: 89593c3ea1e449cea3b2dbbd81033b06\\\\ ... (3749 characters truncated) ... TED MESSAG\\\\n\", \"to\": [\"damian.alvarez.piqueras@cern.ch\"], \"subject\": \"[RUCIO] Request to approve replication rule 89593c3ea1e449cea3b2dbbd81033b06\"}'}]` We have to look into if we can update the DB to allow longer messages, but in anyway, hermes should protect against this. _Migrated from JIRA  ", "47": "``` 4:24 PM Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/rse/protocols/s3boto.py\", line 180, in connect calling_format=OrdinaryCallingFormat()) File \"/usr/lib/python2.7/site-packages/boto/init.py\", line 141, in connect_s3 return S3Connection(aws_access_key_id, aws_secret_access_key, **kwargs) File \"/usr/lib/python2.7/site-packages/boto/s3/connection.py\", line 191, in init validate_certs=validate_certs, profile_name=profile_name) File \"/usr/lib/python2.7/site-packages/boto/connection.py\", line 569, in init host, config, self.provider, self._required_auth_capability()) File \"/usr/lib/python2.7/site-packages/boto/auth.py\", line 987, in get_auth_handler ready_handlers.append(handler(host, config, provider)) File \"/usr/lib/python2.7/site-packages/boto/auth.py\", line 144, in init HmacKeys.init(self, host, config, provider) File \"/usr/lib/python2.7/site-packages/boto/auth.py\", line 81, in init self.update_provider(provider) File \"/usr/lib/python2.7/site-packages/boto/auth.py\", line 148, in update_provider super(HmacAuthV1Handler, self).update_provider(provider) File \"/usr/lib/python2.7/site-packages/boto/auth.py\", line 85, in update_provider self._hmac = hmac.new(self._provider.secret_key.encode('utf-8'), AttributeError: 'tuple' object has no attribute 'encode' ``` _Migrated from JIRA  ", "46": "Add a flag --extract, which will automatically extract compressed files. To be precise: This should download a full zip file and extract a specific file needed from that archive. _Migrated from JIRA  ", "45": "ntplib etc _Migrated from JIRA  ", "44": "Hello, I'm the current DAST shifter and a user reported that hitting the 'Check quota' button on R2D2 hangs forever. I checked with firefox 52.0.2 on ubuntu 16.04 and I have the same problem. The debug console points to a java script error: \"option.errors s not a function\". Laurent _Migrated from JIRA  ", "42": "It would be very convenient for this command to declare all replicas of the files lost: ``` [dcameron@lxplus078] ~> rucio-admin replicas declare-bad --reason \"bad guid metadata\" mc15_13TeV:HITS.10672838._007007.pool.root.1 mc15_13TeV:HITS.10672838._007080.pool.root.1 No JSON object could be decoded Rucio exited with an unexpected/unknown error, please provide the traceback below to the developers. Traceback (most recent call last): File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.10.1/bin/rucio-admin\", line 107, in new_funct return function(*args, **kwargs) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.10.1/bin/rucio-admin\", line 730, in declare_bad_file_replicas non_declared = client.declare_bad_file_replicas(pfns=chunk, reason=args.reason) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.10.1/lib/python2.7/site-packages/rucio/client/replicaclient.py\", line 43, in declare_bad_file_replicas exc_cls, exc_msg = self._get_exception(headers=r.headers, status_code=r.status_code, data=r.content) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.10.1/lib/python2.7/site-packages/rucio/client/baseclient.py\", line 191, in _get_exception data = parse_response(data) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.10.1/lib/python2.7/site-packages/rucio/common/utils.py\", line 193, in parse_response return json.loads(data, object_hook=datetime_parser) File \"/usr/lib64/python2.6/json/_init_.py\", line 318, in loads return cls(encoding=encoding, **kw).decode(s) File \"/usr/lib64/python2.6/json/decoder.py\", line 319, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/lib64/python2.6/json/decoder.py\", line 338, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded ``` At least a proper error message should be returned instead of a stacktrace. _Migrated from JIRA  ", "41": "_Migrated from JIRA  ", "40": "_Migrated from JIRA  ", "39": "_Migrated from JIRA  ", "37": "When the latest replica (eligible for deletion) is removed, the file is removed from the content but not archived. _Migrated from JIRA  ", "34": "The documentation on the CLI  needs to be updated, e.g. : > Download specific files from a dataset > > This operation is still not supported by rucio, but will be available soon. > > Download a sample of n random files from a dataset > > This operation is still not supported by rucio, but will be available soon. _Migrated from JIRA  ", "30": "_Migrated from JIRA  ", "29": "_Migrated from JIRA  ", "28": "_Migrated from JIRA  ", "27": "_Migrated from JIRA  ", "26": "We need a protection for deleting a RSE. In particular, it should fail if some rules, collection replicas exist. _Migrated from JIRA  ", "23": "Lots of sites already support WebDAV downloads, so it would be a nice feature if we would have download directly from the WebUI. So when users list the files in a dataset their can just click a button and the files / one file would be directly downloaded to their machine. _Migrated from JIRA  ", "8": "We need a new submission script for pull-requests on github. ", "5": "test! "}