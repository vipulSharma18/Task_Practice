{"3395": [], "3388": [], "3383": [], "3377": ["I think that s a good idea. Not the highest priority though \ud83d\ude04 "], "3365": [], "3360": [], "3350": [], "3347": [], "3346": [], "3340": [], "3311": [], "3283": [], "3261": [], "3260": ["The ctrl c is already implemented in the downloadclient but there it is easy since it is multithreaded. If I understand correctly we want to apply something similar for the uploadclient which is not mult threaded. What about the other clients? Do we need it there? "], "3254": [], "3250": [], "3249": [], "3235": [], "3200": [], "3199": [], "3164": ["Yes the  default timeout is too short given the fact that we expect to always do bulky staging submissions to sites. A longer default timeout setting in FTS plus respecting the timeout passed by client e.g. rucio may just do it. Hope it helps Xin ", "Since we are discussing this also via mail We can increase this by changing the rucio.cfg bring online setting so we do not need to change the default in the Rucio code as well. "], "3148": [], "3147": [], "3135": [], "3124": [], "3115": ["Adding a None ssh identity in an attempt to avoid using default values in the bootstrap section also results in padding error. "], "3103": [], "3101": ["That is because traces were not meant to be used as return values for some time. They were just the data for the monitoring system. Actually I don t like that we start to expand this even more. Some time ago I discussed another approach with  were it would be possible to set callback functions for different steps of the download client. E.g. one for failed downloads one for successful downloads one for failed attemps For each event the download client would call this function and give the user the opportunity to react to the event. ", "This needs some more discussion next week. I am not sure if the callbacks alone solves this well enough. ", "In meanwhile we discussed with Tobi whether current downloadclient is threadsafe or not. The problem is in the list reference that I created in pilot for adding the traces According to the following article about locking appending to a list is threadsafe operation. "], "3097": [], "3096": [], "3084": ["Tests performed on the latest version. All tests have been performed on CERN PROD SCRATCHDISK and UNI FREIBURG SCRATCHDISK  upload with registration in several attempts  upload without registration to the same rses  upload of existing file that is not registered  upload of a file that is already registered  upload of non existing file All these tests performed well. Although it is not all we should be checking e.g. uploading corrupted file uploading to not existing rse etc. ", "Following the comments from TWAtGH I dropped the summary string form the NotAllFilesUploaded exception  The suggestion from Tobi is to add attributes directly to the exception whether as a dict or standalone attributes corresponding to the quantities in the summary needs to be discussed and decided. "], "3076": [], "3042": [], "3031": ["You skipped the modifications section in the description P So how should this be resolved? Is it related to  ? ", "Using urllib.quote plus for all but containers? ", "I don t think thjis is related to  There it is specifically if the trailing is mentioned in the did name like ATLAS prodsys was doing this in the past. Here the error must be something else. ", "I think this is rather an list replicas issue then. But if the physical filename or the dataset name contains a it will be problematic because it s the directory separator. We could change the download client to replace it by another character or create a subdirectory for each but I think the best thing would be not to have in the LFNs sweat smile ", "Creating subdirectories is fine. This is in fact how these files are stored on storage. The LFN just includes these structural directories. There is nothing wrong with having in the LFN. I would suggest we just create subdirectories. TWAtGH can you please try and confirm the exact issue. If it is in list replicas then this goes over to mlassnig but we should narrow down what the actual issue is. ", "Looks like the download is already implemented like this. But it s difficult to try because the upload doesn t support this. I assume that most client functions that do an REST API call won t work using a DID containing a slash. A little bit more info about the exact issue would be nice cserf and how can I create a DID containing a slash? Also how would double slashes be resolved? Just a single directory I guess. ", "Ok in fact it was the server which rejected the upload due to a missing AllowEncodedSlashes On But the server still don t allow to insert slashed DIDs. I think it happens in the add replicas call        DEBUG File DID does not exist        ERROR Provided object does not match schema. Details Problem validating dids u slash  does not match A Za   A Za   .   Failed validating pattern in schema items properties name description Data Identifier name pattern A Za   A Za   .   type string On instance  name u slash  ", "Yes you have to try this with a schema which allows slashes. Eg. the belleii schema. ", "TWAtGH is there any news on this? ", "Traceback rucio v download test grid belle ddm functional tests release        MCyy  dst        INFO Processing  item s for input        DEBUG num unmerged items  num dids  num merged items         INFO Getting sources of DIDs        DEBUG schemes None        DEBUG rse expression istape true        DEBUG num DIDs for list replicas call         DEBUG num resolved files         DEBUG unzip v returned with exitcode         DEBUG tar version returned with exitcode         DEBUG num list replicas calls         DEBUG Queueing file test grid belle ddm functional tests release        MCyy  dst         DEBUG real parents set test grid belle ddm functional tests release        MCyy  dst        DEBUG options test grid belle ddm functional tests release        MCyy  dst ignore checksum False transfer timeout  destinations set . False        DEBUG Traceback most recent call last File usr bin rucio line  in new funct return function args kwargs File usr bin rucio line  in download result download client.download dids items args.ndownloader trace pattern File usr lib  site packages rucio client downloadclient.py line  in download dids input items self. prepare items for download did to options merged items with sources resolve archives resolve archives File usr lib  site packages rucio client downloadclient.py line  in prepare items for download paths os.path.join self. prepare dest dir dest  dataset name file did name dest  file did name for dest in destinations File usr lib  site packages rucio client downloadclient.py line  in prepare dest dir os.makedirs dest dir path File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs makedirs head mode File usr   os.py line  in makedirs mkdir name mode OSError Errno  Permission denied grid        ERROR Errno  Permission denied grid        ERROR Rucio exited with an unexpected unknown error. "], "3023": ["cserf The columns are added to the table by jwackito You can add the filling of the values to the poller. ", "transfertool  bulk query method wasn t querying staging info that s why staging start and staging finished columns in the requests were not populated ", "jwackito this still doesn t seem to work. We have to check if we are losing this info somewhere in the workflow or if we just don t get the value from FTS. jwackito please try to debug this on one of the integration pollers. "], "2983": [], "2973": [], "2961": [], "2926": [], "2915": [], "2881": ["Thanks for the report. I m having a look. "], "2830": [], "2827": [], "2770": [], "2714": [], "2706": [], "2703": ["For what it s worth I would also find this very useful so I ll add my support! ", "This would indeed be very useful and much appreciated. "], "2686": ["i wonder what the most appropriate way to do this is. We can basically do a json dump of the post put data to the logfile or even ES. It still wouldn t be in the apache access log but it would be easy to correlate the access with the line in the logfile ES. ", "dumping the request json to the logfile seems the right thing to do to me. i ve had a look at the different parameters in the mod wsgi documentation but there s just no way you can write to the access log. we can however dump the access itself also in the error log and silence the access log so everything would be in a single file? "], "2639": [], "2637": [], "2636": [], "2635": [" part   part  ", "Main schema change  "], "2634": ["Yet another non existing web page referred to in rucio list rses help for expressions option "], "2631": ["Google to follow this activity. ", "Also related to  "], "2630": ["Google to follow this activity. "], "2621": ["Is it still valid ? ", "I don t think it is valid. There is a loop on activity so I don t understand what s wrong. Martin can you double check ? ", "I think the issue is that activity is not passed through to get next It loops over the activities but does nothing with it. "], "2613": ["See also ", "Please link from a menu tab as I only find the url in gmail. I expect my earlier comments are still relevant. "], "2582": ["I think we discussed this on Slack. I ll make a patch for this ", "Hi Eric this is still an issue. Did you figure out why this is happening? ", "Oh I ve complete forgotten about this. And I don t see a patch in CMSRucio for it where the script resides . I ll at least make an issue there. "], "2543": [], "2542": ["There should be also an easy way to configure the throttler in the config table . For example there could be a throttler mode in the config like throttler mode throttler mode SRC PER ACT to throttle per source RSE and per activity throttler mode DEST PER ACT throttler mode SRC DEST PER ACT throttler mode SRC ALL ACT throttler mode DEST ALL ACT throttler mode SRC DEST ALL ACT to throttle per source and destination RSE and over all activites Then each RSE would need a threshold value in combination with the mode to allow switching between modes like All Activities DEST ALL ACT MOCK  User Subscription SRC DEST ALL ACT MOCK  This would add the modes to values in the rse transfer limits table. Also each RSE should be configured to use FIFO or grouped FIFO depending on the mode DEST ALL ACT MOCK FIFO ", "Yes we would need some kind of mode specifier in the table. I wonder if this should be an additional column though. ", "Also there are some special cases to be considered if the throttler uses destination and source it could be that the destination RSE is configured to use FIFO and the source RSE grouped FIFO then its not clear what to use if the throttler uses destination and source it could be that a RSE is used as source and destination then the configuration would also need a field to specify that like All Activities DEST ALL ACT MOCK DEST  if MOCK is used as destination RSE ", "Yes we would need some kind of mode specifier in the table. I wonder if this should be an additional column though.  You mean in the rse transfer limits table? I thought that the throttler can only have one mode so I would set the value only once ", "Yes in rse transfer limits . It s just not super nice that all the configuration is part of a long string. For some things it s not possible otherwise but we should try to put as many things as possible into separate columns. ", "Ah yes makes sense. We should be maybe also check why the throttler thresholds are stored in the config table. I think the ones in the rse transfer limits table are unused and it would be better to use them and then to use seperate colums like you said ", "I think this schema for rse transfer limits would fit. RSE ID Activity Max transfers Transfers Waiting Direction Mode Volumn Deadline MOCK ALL    DEST FIFO  MOCK User Subscription    SRC FIFO MOCK User Subscription    DEST GFIFO  MOCK User Subscription    SRC GFIFO  Then depending on the throttler mode the thresholds and the release mode can be read out e.g the mode DEST PER ACT would read the third row DEST ALL ACT would read the first row and SRC PER ACT would read the second row ", "Yes I think that would work. Although Volum e instead of Volum n \ud83d\ude04 Right now there is also a column rse expression in the table. I am not quite sure why maybe this was just put there in preparation when the throttler was created. ", "In the new implementation the throttler lost the ability to set a limit for all rses only limits for individual rses are possible now. This needs to be added again. "], "2517": [], "2459": ["Is this a possibility? ", "Yes this shouldn t be too complicated to do for the deep False case. For the deep True case it is a bit more complicated as several tables are joined here but I assume that this one is not relevant for you anyway. I think we can extend the API with giving a list of scope names or add a new call but it wouldn t allow the deep True case. ", "And what s the difference between deep True and deep False? ", "There is a table storing all dataset replicas called collection replicas which is queried when doing the query with deep False this is a very efficient query just a PK lookup With deep True this is not resolved via the collection replica table but the result is generated based on the location of every single file replica in the dataset. This is an expensive query which is not needed in general as mostly you create replication rules on the datasets you are querying for. But there are certain workflows which do not result in a collection replica being created for the dataset as no rule was created for the dataset in this case deep True must be used. Just to illustrate a case where this might be necessary. You have a dataset A .  of the files of dataset A are in dataset B and  in dataset C . If you don t create a replication rule for dataset A but only for B and C no collection replica was created for dataset A . For the simple query deep False it will look like that the dataset has no full replica there although all files actually have a replica. In that case the deep query needs to be used. ", "I see thanks for this explanation. Indeed I believe we won t have need for using deep True but Eric knows those use cases better than me. ", " Something on our setup is not working correctly I think. Even making a rule directly on a dataset we don t really get replicas without the deep flag. From the CLI rucio list dataset replicas cms SingleElectron    MINIAOD      DATASET cms SingleElectron    MINIAOD      RSE FOUND TOTAL  US Wisconsin Test    DE DESY    US Nebraska Test   rucio list dataset replicas deep cms SingleElectron    MINIAOD      DATASET cms SingleElectron    MINIAOD      RSE FOUND TOTAL  US Wisconsin Test    DE DESY    US Nebraska Test   While the rules are on the dataset rucio list rules cms SingleElectron    MINIAOD      ID ACCOUNT SCOPE NAME STATE OK REPL STUCK RSE EXPRESSION COPIES EXPIRES UTC CREATED UTC  root cms SingleElectron    MINIAOD      OK     US Nebraska Test         root cms SingleElectron    MINIAOD      OK     US Wisconsin Test         sync  de desy cms SingleElectron    MINIAOD      OK    rse  DE DESY        Are we missing a trigger or procedure in Oracle that is supposed to fill in the collection replicas table? ", "Yes there is the COLL UPDATED REPLICAS procedure in But this works with virtual scopes which Gancho set up Basically there is a virtual scope for MC one for DATA one for Panda and one for everything else. Would need to check with Gancho how to exactly setup these virtual scopes in oracle But alternatively you can also use the abacus collection replica daemon This one does the same as the oracle procedure. ", "After I tried posted this I found that daemon. Running it does not work in that this line basically takes minutes and consumes  GB of RAM and causes not just my pod but my  node to crash. Maybe because this is because we ve got a lot built up there and have never done this? Is there an easy way to limit it? ", "Yes the daemon really seems to rely on the table being small. Possibly easiest would be to do a cleanup with the PL SQL first. I have a version without the virtual scopes which you could just execute once. Let me check ", "Hi  getting back to the original issue. In case you don t need any further information from my side would you have a rough ETA for considering this ticket? Even by the end of  would be helpful ", "Hi amaltaro I can t give you an exact ETA but it should be before the end of  This is not a difficult development I just need to find time for it or find someone who has time to do it \ud83d\ude04 "], "2417": [], "2414": ["Config file was changed according to David s proposal. Documentation needs to be done still. "], "2410": ["Alternatively a json column? I don t like string parsing too much "], "2393": ["Hi Rod At one point we were discussing of looking into third party copy support of zip constituents. xroot FTS would obviously have to support that. But if this is the case we could essentially create the replicas via transfer requests and just nee some logic around that. The timescale of this is a different question though. "], "2387": [], "2356": [], "2330": ["For the last point There are many scripts in there which do not belong in our core repo. We should think about collecting the useful ones in a rucio tools or rucio utils repo. Something to discuss today. "], "2325": [], "2318": [], "2312": [], "2311": [], "2310": ["Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again. Add a column which gives the reason of the failure if all accesses fail with same error. I am interested in Checksum issue since it does not require further check in contrary to problem to access file ", "When files are declared lost corrupted we reach a page almost empty. It would be usefull to have a button to go back to the same configuration period type to continue to check and declare other files. ", "All the requests can be implemented relatively easily but this one Could you add a button to claim that the file was checked and correct. Then it should disappear from the list except if transfer fails again. . It will require some significant changes. I need to evaluate how much work will be needed. "], "2282": [], "2264": [], "2263": [], "2258": ["The problem is with the value  that is not handled properly by tabulate table u TAIWAN  TAPE STAGING True u is stagingarea True u stresstestweight  print tabulate.tabulate table tablefmt plain Traceback most recent call last File stdin line  in module File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  lib  site packages tabulate.py line  in tabulate for c ct fl fmt miss v in zip cols coltypes float formats missing vals File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  lib  site packages tabulate.py line  in format return format float val floatfmt ValueError could not convert string to float True vs table u TAIWAN  TAPE STAGING True u is stagingarea True u stresstestweight  print tabulate.tabulate table tablefmt plain TAIWAN  TAPE STAGING True is stagingarea True stresstestweight  or table u TAIWAN  TAPE STAGING True u is stagingarea True u stresstestweight   print tabulate.tabulate table tablefmt plain TAIWAN  TAPE STAGING True is stagingarea True stresstestweight   ", "Sorry forgot to mention that I ve debugged it already to this point. But I won t have time to look further into it  ", "nikmagini TomasJavurek can you please have a look on this issue. See cserf comment ", "BTW the Strange error No section policy will be fixed here "], "2254": ["Testing a xrootd implementation root rucio nagios prod  rse rucio admin rse delete protocol scheme root RAL  ECHO SCRATCHDISK root rucio nagios prod  rse rucio admin rse add protocol hostname xrootd.echo.stfc.ac.uk scheme root prefix atlas scratchdisk rucio port  impl rucio.rse.protocols.gfal.Default domain json wan read  write  third party copy  delete  lan read  write  delete  RAL  ECHO SCRATCHDISK "], "2232": [], "2218": [], "2216": ["How many files is this list long? ", "Just following up on old tickets The chunking should be done on the auditor side. The list replicas method here is a core function which should not need protection from other core daemon calls only on the API level. Thus please chunk this in the daemon itself. "], "2213": [], "2159": [], "2143": [], "2104": [], "2063": [], "2029": ["mlassnig please have a look if this is still relevant Close if not. "], "1923": [], "1911": ["Note that importing the client at the top of the rucio CLI causes an authentication to be attempted even when you re just doing rucio help . ", "That doesn t seem to be problem only with importing downloadclient but Clients in genereal. The config is imported in baseclient. ", "Just importing the module should not read the config? Only when constructing the client object Then indeed it does read the config and even trys to authenticate against the rucio server. TomasJavurek can you please check the code path why this is the case even when doing rucio help ? In my opinion no client object needs to be constructed in that case. "], "1895": [], "1867": ["Example On an RSE there are  TB free dataset A is  TB and is already available on the RSE dataset B is  TB and not available on the RSE container C contains A and B . The e mail sent when a user requests to replicate C should report that the expected free space after the approval is  TB. Currently the e mail will report  TB. "], "1860": [], "1839": ["This should be consolidated. I would not be too afraid of backwards compatibility but maybe we should still add the new parameter leave the old one in but if it is used display some kind of message. A few feature versions later we then take it out. We used this concept already for several other CLI changes. ", "Much of it is historical due to the API itself but we can start changing the clients and bring the API REST and core slowly up to speed. At least for the RSE parameter I would suggest for conciseness to always use rse but always parse RSE expressions and get rid of the rse expression or expression extra arguments. ", "As rse expression can give a back a list i.e. rses looks more appropriate imo. starting with the same name for both rse expression and expression would be already a good step. ", "Sounds good to me rse if it is really only  RSE rses if it is an expression. Although rses suggests a bit that you can enumerate them which is not really true RSEA RSEB RSEC would work but not RSEA RSEB RSEC ", "But shouldn t we handle single RSE or multiple RSEs automatically? rse is a basically a subset of rses and having two different parameters is confusing. ", "We would have to go through it in most cases rses is ok. But in cases where you really only want one RSE e.g. marking a replica as bad lost rses doesn t really make sense as it suggests a list which doesn t work. ", "I would do rse everywhere and in case a list comes back instead of a single RSE fail to the client and tell hey you selected more than one instead of having two parameters. In a way like SQL cursors ", "ah but are you proposing to always resolve RSE by expression ? Most of the time the RSE expression corresponds to the RSE name. It might be worth to check the implementation as this will be the most popular call then ", "From a performance that should be fine the expression is cached server side so it s not even going to the DB too much. ", "it s worth to check that even without the caching it does a simple lookup first against the rse table. Not sure the caching is enabled if memcached is not there and the memoization fallback will be most likely in memory per thread process which won t reduce that much the query rate. "], "1834": [], "1830": [], "1829": [], "1817": ["It seems that it is not possible to catch exceptions inside the streaming. To reproduce run this minimal flask server. The exception gets raised on flask side but does not get catched. An internal server error is generated instead of the error string. python from flask import Flask Response jsonify app Flask name app.route def hello def generate for i in range   raise Exception yield  n try return Response generate except Exception as error return jsonify error error if name main app.run passthrough errors True FLASK APP file.py flask run One solution would be to rework the logic and check for possible problems like non existing account before querying the database. Another solution would be to call the same function inside a for loop but with a break after one loop where we check for exceptions. Afterward the streaming can start if there was no exception catched. ", "To me the accurate path looks like the one where we change the logic and checking of the APIs which use streaming. Thus we do all the conflict checking first and then just iterate the stream. Conceptually the error handling we do right now with streaming is wrong and possibly leads to malformed replies. Because the HTTP return code is calculated and sent! at the header anyway. So let s say this is a  whatever you stream afterwards and throws an exception is not properly reflected in the error code. But this will take time we probably have to more carefully select which APIs we offer as a streaming API list contents list replicas and not be so generous like we are now with them. ", "Worth mentioning here to think about proper support for HTTP Chunked Encoding. "], "1808": ["Just a small note as I ve been working with LIGO to utilize OSG and EGI based computing centers for multiple years now. While proprietary HPC clusters characterizes at least one LIGO resource there s actually a wide range of resources available. Regardless that s neither here nor there. The rest of the post looks good! ", "Hi Brian Thanks for the head up. Indeed I was just intending to highlight the differences between our two computing infrastructures not to be detailed ! Cheers! Gabriele ", "Hi guys fascinating discussion this is something that we have mulled in EGI Foundation for a bit as well. It would be out of place to make sweeping statements about DIRAC without the developers involved so maybe this issue could be pointed out to them if that hasn t already been done. My  is that there there are two patterns right now in developing these platforms  Build a core product discover need for some other functionality tack it on ??? Profit!!!  Build a core product discover need for some other functionality set up a contract with another set of services to do that ??? Profit We have often looked at DIRAC as an HTC solution but it s way more than that and just using it as an HTC solution is actually quite hard. I hazard to say that it works best when it s the primary interface for users and applications. Rucio on the other hand is forgive me for projecting my own perception here a fantastic data management system. It could is? tack on compute management as well. As a product we say EGI would like it to interoperate with other services like cloud compute HTC HPC etc via stable APIs and do it s data management thing. It would be nice to know if Rucio could be used as a drop in replacement data catalogue for DIRAC and more interesting to know if DIRAC could be used as a drop in compute orchestration service for Rucio. My personal feeling is that something that does compute orchestration only would be a better fit maybe HTCondor I don t have a great answer here sorry . Thanks! usual disclaimer of these opinions are mine and mine alone this does not represent the position of EGI EGI Foundation etc apply here wink ", "Hi Bruce I am pointing some DIRAC people to this issue! Cheers Gabriele ", "Hi I am DIRAC technical coordinator and right now its main developer. I ve been pointed here I will try to give some advice. As mentioned above DIRAC give you the possibility to work with different and even multiple catalogs. Just to mention some real life use case which are the ones working best CLIC uses DIRAC with the DIRAC File Catalog both as a Replica Catalog and as a Metadata Catalog  uses DIRAC with the LFC LCG File Catalog as Replica Catalog and AMGA as Metadata Catalog LHCb uses DIRAC with the DFC as Replica Catalog and the LHCb Bookkeeping as Metadata and provenance Catalog . Some years ago LHCb moved its production replica catalog from the LFC to the DFC all the other users that I know either use the DFC the majority or the LFC or some other combination. The DFC the LFC AMGA the LHCb Bookkeeping are all In DIRAC terminology in fact they are all Catalog plug ins as a DIRAC Catalog is such if it implements the same interface e.g. add file remove etc . All catalogs implement the same interface and inherit from You can have more than  catalog at the same time as obvious from the examples above. In this case the operations will be executed on all of them. So for example you can register files on BOTH the LFC and DFC at the same time. Basically each catalog plug in implements a certain operation e.g. the addFile operation following its own interpretation of what e.g. adding a file means for a certain catalog. So what may be interesting for you is implementing a RucioCatalogClient.py. The rest is purely configuration. ", "Hi fstagni thank you for joining the conversation. Indeed that was the solution thought about at first but I have some questions to ask  How is the synchronization between two separate catalogs the DIRAC one and the external one dealt with?  What are the required keys for the external catalog to implement to make it fully compatible with DIRAC? Is case the external catalog doesn t provide topological information how does DIRAC retrieves such details?  Is there a list of the write and read methods to be implemented in the FileCatalogClientBase.py custom derived interface?  What are the methods that are needed by DIRAC to make it efficient at least as with the native catalog? Thank you Gabriele ", " There s always a Master catalog this is defined in the DIRAC CS Configuration System . Just to be sure you don t need to necessarily have the DIRAC one . I repeat a catalog is a catalog as long as it behaves like one.  What s a topological information for you? The location of the replicas?  Each catalog plugin announce what it can do. Otherwise there s a really basic list of required methods just check the code for that.  DIRAC needs nothing at all in that sense. The DFC is just another service. ", " Awesome. I think the Rucio catalog can generate the missing info on the fly using functions hence it should be possible to plug it to DIRAC  yes for example. As far as I have understood DIRAC tries to perform the computation as close as possible to data. How does it figures out how to do that? Is the catalog providing some information or is it doe using some external metrics?  I saw the list of the mandatory READ METHODS hasAccess exists getPathPermissions . It seems to me and makes totally sense that DIRAC requires to be able to read data but the persistent output of data is not strictly required for the computing functionalities.  Several times in your publications there is an indication of a link between DIRAC s computing efficiency and the catalog information. Can you clarify a bit about that? In addition I would like to ask you if there is any example of a custom implementation of FileCatalogClientBase.py e.g. for LFC to read and understand more the integration process. Thank you Gabriele ", " I am not sure which are the missing info ? Can you elaborate?  This is not fully correct. DIRAC CAN make the computation close to the data but this is not a requirement. In fact you can run productions even in full mesh mode meaning jobs can in theory go everywhere independently of the location of the input files.  A replica catalog at least provides you with the location of the replicas. This location is a DIRAC Storage This info is used in several places but a DIRAC SE can simply be RucioSE if this is something you want.  DIRAC jobs decide if and where to store   N of their outputs. The object is what links the functionalities of FC and SE and it s often the starting point for simple DM operations  Well the DIRAC DFC is fast efficient practical customizable widely used and it s already there. I can t compare it to any other solution apart the LFC and I am not in a position of comparing it with the Rucio Catalog because I don t know much about it.  For examples just look in LcgFileCatalogClient.py is the LFC FileCatalogClient is the DFC and I would not suggest to look at the others because they are a bit less obvious . For LHCb is the LHCb Bookkeeping. ", " and  I see that DIRAC can operate in fully mesh mode but in any other case the locality of data and computing resources should be described. The  question is about that what information is needed for not to operate in fully mesh .  Correct me if I am wrong if I get to use Rucio catalog from DIRAC I should be able to publish output files to Rucio BY HAND using direct calls to Rucio APIs. To perform a basic test of the eventual FileCatalogClientRucio.py implementation it should be enough to use direct calls instead of a custom DIRAC plugin.  The question was more about how much of the DIRAC efficiency is due to custom information stored in computed from the DFC?  Thanks for the references ", " DIRAC would need to know where the input data is to make a proper job scheduling. Data is always in at least one Storage Element . The SE s needs to be described in the DIRAC CS.  Of course  TLDR nothing. DIRAC is a not small set of components. Each component has its own life and there s no need to install all of them. In fact many installations only install a small subset of them. The DFC is just a DIRAC component. In fact from a informed user s perspective it s just a URL. The interrogations and answers that it gives need to follow a certain contract nothing else. If another catalog respects that contract then you re done. The RucioFileCatalogClient.py file should be the only one of the whole DIRAC where you do import rucio . "], "1806": [], "1801": [], "1797": ["FTS is thinking about adding this to FTS On hold until confirmation "], "1773": [], "1771": ["For reference ", "Just came across the same problem any chance it will be fixed? ", "tbeerman what can we do ? "], "1760": [], "1759": ["Happens on Name MariaDB server Arch   Version  with MySQL python  Will have to try to reproduce this. ", "Still not able to reproduce. Abandon? ", "ping astroclark ", "Closing. ", "We re also seeing similar behavior with MariaDB  and MySQL python  In judge cleaner.log         DEBUG Deleting lock     tpc records   for rule  raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError  Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters     tpc records    xbb   xeb xbf  xed xdcx  jM       xafo   Background on this error at         ERROR Traceback most recent call last File usr lib  site packages rucio daemons judge cleaner.py line  in rule cleaner delete rule rule id rule id nowait True File usr lib  site packages rucio db sqla session.py line  in new funct raise DatabaseException str error DatabaseException Database exception. Details raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError  Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters     tpc records    xbb   xeb xbf  xed xdcx  jM       xafo   Background on this error at In undertaker.log         INFO Undertaker  Receive  dids to delete         INFO Removing did     tpc raw records  DATASET Data identifier not found. Details Data identifier archive raw records  not found         DEBUG Removing rule  for did     tpc raw records  on RSE Expression UC OSG USERDISK         DEBUG Deleting lock     tpc raw records   for rule  raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError  Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters     tpc raw records      xde        jM       xafo   Background on this error at         CRITICAL Traceback most recent call last File usr lib  site packages rucio daemons undertaker undertaker.py line  in undertaker logging.error Undertaker s Got database error s. worker number str error UnboundLocalError local variable error referenced before assignment ", " can we come up with a reproducable test for this? ", "I haven t looked into this personally. It would be good to understand under which conditions this shows up. Running the daemons with multiple threads or also one thread etc. astroclark jlstephen Then we could try to reproduce this condition. ", "From Handson Hi rucio team I\u2019m DDM admin from ASGC Taiwan site We have a rucio server to manger out data. Here is our version information Rucio  MariaDB  Python package SQLAlchemy  When I run rucio judge cleaner I got error like         INFO rule cleaner   Deleting rule  with expression TW           DEBUG Deleting lock ams user felixlee  for rule  Exception mysql exceptions.InterfaceError  in bound method SSCursor. del of MySQLdb.cursors.SSCursor object at  ignored raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError  Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters ams user felixlee  xbb xed   xadC      b?   xbfA xbf      xed          ERROR Traceback most recent call last File opt rucio lib rucio daemons judge cleaner.py line  in rule cleaner delete rule rule id rule id nowait True File opt rucio lib rucio db sqla session.py line  in new funct raise DatabaseException str error DatabaseException Database exception. Details raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError  Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters ams user felixlee  xbb xed   xadC      b?   xbfA xbf      xed  I google it and found this issue I do some test and found in the rucio lib core rule.py line  I use the Rucio  on git hub to be the stander of line number locks session.query models.ReplicaLock .filter models.ReplicaLock.rule id rule id .with for update nowait nowait .yield per  If there was a rule that contain over  locks which the number is the same with yield per input then the error will show up. I guess the session query ReplicaLock table get over  row but function yield per only return  for once and the rows which satisfy ReplicaLock.rule id rule id are still lock by function with for update. So on the rucio lib core rule.py line  if delete lock and update replica lock lock purge replicas rule.purge replicas nowait nowait session session rucio lib core rule.py line  lock.delete session session flush False it failed to delete the row on the ReplicaLock table. I\u2019m not sure my guess is right or not. Increase the number of yield per input can fix the problem temporarily. But I think that is not a good way. Is there a better way to fix it? Best regards Handson Peng ", "To my understanding the most likely reason why this is happening is that due to the yield per parameter the query is actually still running and the delete call line  changes the table thus somewhat conflicting the query since it is still running . This is not an issue on Oracle but it appears this brings MySQL into troubles. When you are changing the yield per to something bigger than the amount of locks this is not an issue as the query already ended at the moment we start deleting the locks. The only solution I see is to change the yield per to an all so the query finishes at the beginning of the loop. The downside of this is that the entire result has to be loaded into memory which for big rules might be an issue. Maybe we can do this conditional to the DB implementation. ", "The problem solved by changing the yield per to an all . The rule stuck before is around  locks with the same rule id. And there is no other problems. So I still don t know the limit of rule size. ", "There is no principal limit of the rule size however the repairer might load the entire rule into memory which for bigger rules requires a bit more. Thus if you run the repairer with a lot of threads in this uses all memory already under normal conditions these large rule peaks might bring it over the limit. "], "1752": ["I am not quite sure why you don t transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. This pattern of key attribute values is certainly used in more APIs I would have to check which ones though. We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a  API or some kind of option to return the result in this kind of way. ", "Martin the transformation implies full knowledge of all keys. Since it is not possible in this structure such process can be very tedious not only on code writing but on parsing level too. For instance how many keys exists will these keys stay the same over long time I don t think so since we may add discard RSEs for instance how many structure have this issue in all Rucio APIs what is complete schema of all APIs. The list can go on. Not to mention that custom transformation cost time and resources. I outlined the issue and presented use cases. Anyone tool who need to deal with aggregation or analytics of the results will face this issue. When you need to deal with dozens of services in real time it is a big issue. But of course I understand backward compatibility issue and ideally it should be fixed both on server and client sides. If is it feasible it is another story. Of course I can work around such cases but I m in favor that this issue should be fixed eventually. On  Martin Barisits notifications github.com wrote I am not quite sure why you don t transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. This pattern of key attribute values is certainly used in more APIs I would have to check which ones though. We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a  API or some kind of option to return the result in this kind of way. You are receiving this because you authored the thread. Reply to this email directly or view it on GitHub ", "Hi Martin Aren\u2019t at least a few of the APIs passed through a JSON schema verification? Or maybe I\u2019m just recalling some internal data structures? It\u2019d probably be useful to maintain some sort of published schema as new users of the API come along. Success breeds these sort of questions after all! Thanks Brian Sent from my iPhone On Nov   at   PM Valentin Kuznetsov notifications github.com wrote Martin the transformation implies full knowledge of all keys. Since it is not possible in this structure such process can be very tedious not only on code writing but on parsing level too. For instance how many keys exists will these keys stay the same over long time I don t think so since we may add discard RSEs for instance how many structure have this issue in all Rucio APIs what is complete schema of all APIs. The list can go on. Not to mention that custom transformation cost time and resources. I outlined the issue and presented use cases. Anyone tool who need to deal with aggregation or analytics of the results will face this issue. When you need to deal with dozens of services in real time it is a big issue. But of course I understand backward compatibility issue and ideally it should be fixed both on server and client sides. If is it feasible it is another story. Of course I can work around such cases but I m in favor that this issue should be fixed eventually. On  Martin Barisits notifications github.com wrote I am not quite sure why you don t transform the result into a data structure you can store? This should not be very resource intensive and you can guarantee this way that you store sanitised data. This pattern of key attribute values is certainly used in more APIs I would have to check which ones though. We cannot change the result of this API due to backwards compatibility with old clients. We certainly could make a  API or some kind of option to return the result in this kind of way. You are receiving this because you authored the thread. Reply to this email directly or view it on GitHub \u2014 You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub or mute the thread. ", "We only do the schema verification for the incoming data structures not for the outgoing one. But I fully agree we should maybe have something for the return structures too. Not so much a verification but more documentation. vkuznet I get your point I didn t mean to do the transformation for all eternity just for now The data structures right now are very much prepared in a way so that the client The rucio client that is can immediately deal with them efficiently. That s why the rses are keys so the client can quickly get all replica states for a certain rse in a long list of replicas I think what might be most appropriate is an option for the call to prepare the return object in an analytics ready way. We would have to look at all REST calls you are making and see if such an option is needed and keep it in mind if there are API changes in the future. ", "Martin it is not a stopper for me since I deal with these issues in other data services but eventually I ll be glad if this issue will be fixed one way or another. ", "So far I identified the following dynamic structures in replicas scope name API states states  US Nebraska AVAILABLE rses rses  US Nebraska gsiftp red gridftp.unl.edu user  pnfs unl.edu  cms store data  Charmonium MINIAOD         pfns pfns gsiftp red gridftp.unl.edu user  pnfs unl.edu  cms store data  Charmonium MINIAOD         client extract false domain wan priority  rse  US Nebraska type DISK volatile false ", "The pfns example is extremely bad if such records will be placed in DB and indexed on these keys since a pfn names are long strings b we have extremely long list of pfns in GRID universe. ", "Hi Valentin I m not really following you. What is your concern with the pfns? They are not stored in the database at all? ", "Yes but if we will dump data to HDFS as is and then create a dataframe the pfns will become attributes column names rather then values. On  Mario Lassnig notifications github.com wrote Hi Valentin I m not really following you. What is your concern with the pfns? They are not stored in the database at all? You are receiving this because you were mentioned. Reply to this email directly or view it on GitHub ", "Hi vkuznet I think we ve determined that this ticket is really about having a JSON schema for the Rucio APIs no? Since that comes down to a pretty major redesign of the APIs which FWIW is being discussed what should we do with this ticket? Brian ", "I think it is up to Rucio team. We can use it for progress or further examples as I did today or it can be turned into actions milestones. ", "Discussed in the development meeting on We decided to leave it like this until we have a  API Long term project ", "Should this become an real problem for wanting to dump data to HDFS it should be possible as an interim step to define the desired schema s for  and build a simple translator which would precede  right? ", "yes that s definitely an option. "], "1746": ["This are actually two different methods but there seem to be a problem with the regular expression of the rest endpoint in combination with the CMS SCOPE NAME regular expression so in both cases it calls list replicas. ? . datasets resolves to the list dataset replicas method and ? . ? to the list replicas method But the latter one seems to cover anything with datasets too so it should probably be changed to ? . So in conclusion It should be two different methods but in your case it is calling wrongly the same method. ", "Yet another mis behavior this call returns states  US Nebraska AVAILABLE pfns gsiftp red gridftp.unl.edu user  pnfs unl.edu  cms store data  Charmonium MINIAOD         domain wan rse  US Nebraska priority  volatile false client extract false type DISK   name store data  Charmonium MINIAOD         rses  US Nebraska gsiftp red gridftp.unl.edu user  pnfs unl.edu  cms store data  Charmonium MINIAOD         scope cms bytes   null while this call returns nothing. Is it the same regex issue? Please note that this time I used as a name a CMS dataset a b c pattern rather then CMS block a b c  one. ", "I think in this case it is correct The first query without datasets returns the file replicas. The second query datasets returns dataset replicas which essentially are just a more efficient way to get the replica status of all files of a dataset in the database we create a dataset replica which has synchronised counters However dataset replicas are not created in all workflows thus in this case everything seems correct. I think you are right in your previous query including the this seems to be somehow problematic with the REST regex. I will have a closer look on this in the next days. "], "1667": [], "1648": ["Should not be an issue with  "], "1636": ["I m not sure what the problem is here maybe related to the double in xrootd path . But add files to dataset definitely works without SRM. We tried with gsiftp at  and it works fine. "], "1583": ["In the transfer request the core creates we already create a ds scope and ds name attribute. This is mostly done for tape endpoints as it is used in the PFN generation. I think we can add this to the file metadata dict which is part of the fts request. However the ds name ds scope is usually the dataset the rule is defined on. E.g. if the file is also in different datasets only one will be given. Also if the rule is on a file directly none will be given even if the file is in a dataset. "], "1558": [], "1504": [], "1431": [], "1393": ["Beware that  is a really tricky character to work with especially in APIs that include the DID in the URL path itself. For example mod wsgi always converts these back to . How good is the unit test coverage in this area? ", "Test coverage is bad in this. Essentially we never tested with any in there not even the trailing one as for us the container names in ATLAS do not include the trailing . So we need to add some tests with or other special characters in the names. Solution will be most likely that we ask external applications in ATLAS not to submit the trailing since the other solutions impact communities using slashes in their names which is valid . "], "1382": ["Replicas controller GET calls is unused by the Rucio clients. We should deprecate it in the next feature release. ", "It s actually used by the ARC clients cannot deprecate. Still needs deep check if there are functionality differences between GET and POST ", "We keep it as it is but should put comment in code. "], "1351": ["We looked at this and it looks like a pure statically linked binary is not possible but perhaps we can provide a stripped down RPM that installs this binary. Note though that this will require some additional dependencies ROOT s libCore boost etc ", "another option would be to add a configuration option that lists the extensions for which GUIDs are extracted. the default could be extractGUID extensions .root which could be set to extractGUID extensions "], "1304": ["We discussed this couple of times and it s still not clear if this is a bug or not. Can we discuss this on Thursday cserf TomasJavurek  ? ", "Could be an upload of a replica on a different rse did exists This should add a rule on the RSEx "], "1248": [], "1245": ["Hi Alessandra I think that s possible but maybe cserf or TomasJavurek can comment. ", "Hi any answer here? It seems that after declaring those  files lost  times rucio still thinks it has them  . Unfortunately more than keeping on declaring all of them I cannot do in these conditions. thanks  ", "Hi That s possible. I ll try to implement it next week. Cheers Cedric "], "1213": ["After checking the code the necromancer do it the proper way. The problem is actually due to the fact that the ignore availabilty flag that is in the signature of the list replicas method is not working so reassign the ticket to core. "], "1202": [], "1157": ["Hmm it s technically not mandatory to use alembic to install the database so looking for the alembic version might not be the right approach. What we could do is to test if it is there and compare then I am not so sure if this is really helpful though. ", "It is a real pain to recover from this issues. I would really prefer that there is only one way to install and upgrade the DB. If we want to maintain the ability to install the DB without alembic you shouldn t be able to use alembic to upgrade the database. On Tue  May  at   Martin Barisits notifications github.com wrote Hmm it s technically not mandatory to use alembic to install the database so looking for the alembic version might not be the right approach. What we could do is to test if it is there and compare then I am not so sure if this is really helpful though. \u2014 You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread . Benedikt Riedel Scientific Programmer University of Chicago Computation Institute ", "We discussed this in the meeting today. We will add a check for this in all daemons on startup. If the alembic table is present check if the right schema version is installed if not fail. If the alembic table is not present do nothing for the moment. This line we will remove eventually "], "1152": ["Up ", "Some observations I made with a simluated timeout in the API  Using a  TimeOut Apache times out after  and returns a  Proxy error. !DOCTYPE HTML PUBLIC IETF DTD HTML  EN html head title  Proxy Error title head body  Proxy Error  p The proxy server received an invalid response from an upstream server. br The proxy server could not handle the request em a href proxy rses  GET nbsp proxy rses  a em . p Reason strong Error reading from remote server strong p p body html Strangely this occurred only with postman in my test but not with the client. The client received the correct response.  Using a  TimeOut The default timeout in the base client is  The client raises a requests TimeOut exception which does not get catched. I think it would make sense to change the default timeout in the base client to  and to catch the TimeOut exception to then raise an own TimeOut exception ? But I could not reproduce a case where the client interprets a request successfully when there was a timeout. ", "Just to follow up on this in the github a thon. Could you maybe try to generate a case where HTTP streaming is used? I could imagine a workflow where Client and server communicate via python requests with the streaming parameter. As the header of a stream is sent first it might be a HTTP  During the stream even if is just one line an exception might occur and actually kill the DB session. This could still be interpreted as a HTTP  as this was sent in the header first. Now most POST calls where a timeout matters do not really stream much but we should check if this can still happen if it is requested via python requests. ", "mlassnig TomasJavurek please have a look if we can still do anything with this one. If you thik it s not relevant anymore please close "], "1114": [], "1109": [], "1091": ["I am a bit concerned that the only info so far is a list of files inside a zip file. In order to see if a file is in a zip file you d need to search this table also when the file is not archived. It seems to me you need a special replica which is just the name of the zipfile. Then list replicas would also show the locations of the zip file. Adding files to the zip would mean adding a special replica to all the content files. Anyway I m glad some else has to worry about this stuff. Cheers Rod. ", "Without entering into all the details listing replicas without any additional entries is doable. About the format of the url replica for a file contained in a zip you proposed scheme zipfile  I m wondering if it s a well adopted semantic for this. like in web browser etc. ", "We have to see how we do this. Adding a special replica for each zip content is a possibility but this adds workflow complexity to keep everything consistent. I prefer the way we discussed in the dev meeting today by just incorporating the information we already have. The implication there though is performance as each list replicas call gets more complicated and we really have to make sure that this doesn t slow down things in general. ", "So any progress thoughts? ", "For  TomasJavurek will work on this no update yet. The transparent list replica support we discussed and we think it should be fine with the information which is already there without creating some fake replicas. Which would be better for consistency reasons. Essentially when you list the replicas for a file the information that it is also a constituent in a zip file is there thus we can list the parent replicas then. It might have some performance implications for the general workflow so this needs to be done carefully. We didn t discuss a development plan specifically but I was hoping that vingar could work on this? Maybe he can comment? ", "Hi rodwalker So we discussed the timeline of the the transparent archive support list replicas of files also exposes parent archives and we are aiming to have this by end of June Hopefully in the  release . ", "Just to follow up on the results so far There will be a presentation in the ATLAS S C week this Thursday about this as well rucio download constituent archive did archive issues an xrdcp streamed download of the constituent in the archive However the archive has to be named specifically for this to be possible. list replicas has been adapted if you do list replicas constituent that it also outputs the archive with the streaming option. This should enable the client to do a download of a constituent transparently. This will need an update of root as far as I know mlassnig ? as the streaming is currently only possible via xrdcp and not gfal. What is not there rucio add rule constituent makes a rule on the archive. This will be discussed on Thursday. With the list replicas change mlassnig did if the constituent is not on a root enabled storage the replica dictionary has a client extract True flag which tells the client to download the archive and extract the file. This functionality needs to be implemented. TWAtGH or TomasJavurek ? ", "I have updated the missing features bugs to the overview on the top. Please comment here in this ticket if anything else is missing. ", "I think we can remove the clients label from this issue  "], "1026": [], "1021": [], "857": [], "781": ["For  this needs a little bit of work as the error is not propagated to the locks. It s only the last error stored at rule level.  this information is in the requests history just needs to be added to the dictionary. "], "736": [], "713": [], "704": ["Up "], "689": [], "630": [], "609": ["More info ", "bbockelm Won t the workflow be similar with openId ? Something like Client rucio auth servers ext auth. service openId scytoken ask token forward token request token request store token store token in db and answer with it to the clients give token then interacts with Rucio co ", "Not quite because the Rucio server itself will be the issuer. If Rucio allows you to get an X Rucio Auth Token you can request a SciToken instead with the appropriate domains set based on VO policy . ", "in X Rucio Auth Token you expect to have the SciToken one ? ", "The client can request it in addition and use it to go to storage If the storage allows the domain . We can also think to use it as a replacement for X Rucio Auth Token with a custom Rucio Auth domain. ", "I d suggest starting out by having them alongside each other to understand how the library works then eventually replacing X Rucio Auth Token . ", "The clients will have to store both tokens and put in addition the sciToken one in the request header Authorization Bearer token for the next interactions rucio storage etc . Am I correct ? ", "Yeah at the beginning one would just use the SciToken for interaction with storage. In fact this is a strong argument for keeping multiple tokens you don t want the storage authorization token to additionally interact with Rucio. So your Rucio token would gain a new format which is convenient because it doesn t have to be serialized by Rucio and you d still separately handle a storage related token. While Rucio and the storage service would need to understand how to verify parse the token the client can continue to treat it as an opaque string. "], "595": [], "583": [], "536": ["During Rucio Community Workshop  this was also identified by DUNE as being very useful to handle rules on larger datasets containers. ", " ", " Finally a proper alternative of updating \u00b4updated at\u00b4 "], "534": ["The docstring headers are not handled properly by sphinx for the API documentation. The clients source files will be updated to Copyright c   CERN for the benefit of the ATLAS collaboration. Licensed under the Apache License Version  the License You may not use this file except in compliance with the License. You may obtain a copy of the License at Authors ", "Back to block comments Correct dates according to git are   I d also modify the first to lines to be a bit more explicit no need for c for example Copyright   European Organization for Nuclear Research CERN For the benefit of the ATLAS Collaboration also makes it nicer to modify the second line later on if necessary ", "yes back to block comments and we can hope that pylint will offer a way to mute the missing docstring at the top of the file I was thinking having the starting date based on the file creation date. According to The format must be c Copyright year CERN for the benefit of the Name of appropriate group Collaboration ", "Right then that s how it is ", "this one is not true everywhere ", "Ah I thought with your recent patches you iterated all files. ", "only the ones I ve modified to get the doc generation working "], "528": [], "527": [], "526": [], "485": ["However this does not add a second leading slash when supplied with one . Considering that all CMS datasets are primary secondary Tier this won t impact CMS. ", "yes I knew this but this was the best option I quickly managed to come with to satisfy the CMS naming convention. The proper behavior here would have been to reject the name as all CMS names start with a slash. ", "Which my python API did for something that did not start with but the CLI did not for some reason. Anyhow this is low no priority for CMS but may be useful to keep around. "], "479": [], "461": ["The verify False I vaguely remember was a fix due to a temporary issue but I might be wrong. Maybe mlassnig remembers. ", "verify False Indeed this looks like a remnant. For the session handling yes you re right. For the delegation this is done via a periodic cron outside the conveyor. We didn t want to couple this as a failure of one could affect the other tools delegate proxy fts.py btw once  releases their CLI which can set the delegation lifetime longer than  hours this script can be made much more simple. With this script we get a safety buffer of  hours. ", "mlassnig we really need to come back to this one but unfortunately I don t think I ll personally have time to tackle it . verify False is bad news. ", "We need to add a separate ca cert chain per FTS host eg. BNL hostcert vs UKeS vs CERN . Shouldn t be too difficult. I ll have a look next week. ", "I m not entirely sure that s necessary current version of the requests module really it s probably  that does this allows you to point at a directory such as etc grid security certificates . Additionally even older versions allowed multiple CAs per file simply concatenate them all together . In other projects that use clients that don t understand directory based CA layouts we ve just written scripts to do the concatenation at startup. ", "That makes it even easier then. Thanks for the heads up. ", "Hi can you take a look at and see if that can be of any use in this regards? I mean that as a starting point to get your feedback on how to proceed for the integration of fts delegation if that can be useful. ", "I have pushed this part of the changes needed for transfers in CMS but that is more general for FTS proxy delegation. I splitted it because I think that can be safely introduced and utilized for other purposes not CMS related. So I was thinking that for the other changes more CMS specific I could open an issue to better follow up without delaying further the proxy delegation stuff. How does it sound bbockelm mlassnig vgaronne  ? ", "Hi Diego I only had a quick look will look more in detail later but it looks good to me like that. I would propose you open a separate issue even for the proxy delegation and submit the PR already to rucio so we can comment there. No need to just keep it in your private repo. ", "Agreed I m doing that. Thanks "], "421": ["vingar mentioned that there is already signed URL support in Rucio and will be updating the ticket with some example code. Additionally here is some sample code for getting a macaroon from a dCache endpoint ", "Creating bearer tokens is already the basic authn authz scheme in Rucio where we delegate the verification of the requestor to separate entities Kerberos Servers VO CAs SSH pubkeys . Creating Rucio issued bearer tokens e.g. based on the SciTokens proposal which can then be given to rucio clients and transfertools should be doable by adding a new token generator which takes account identity and optionally RSEs as input. In case a challenge response handshake is needed there s a workflow in place as we need to do this for SSH based auth already and which can be adapted. This could even supersede the X Rucio Auth Token. To auth with storage generating signed URLs follows naturally from this as instead asking a third party to sign the URL Rucio could do it internally by passing the optional RSE to the token generator. Few things to take care about thrash rate on the token table length of the bearer tokens attributes of the bearer tokens which are not part of the token itself ", "Length can be a legitimate problem. Apache s limit tends to be around  A narrowly scoped token including a full PFN would be around  So if you issued a token to download a specific file it would not be a problem. The problem would crop up if you wanted to do something like a token for a long list of files in such a case you d probably have to do a more coarse grained scope e.g. at the directory level . Since the tokens are signed with a public key they don t need to be recorded in a manner analogous to the existing table. It is a good idea to have a log activity of tokens recently no need to record the tokens themselves issued for auditing purposes but that s more a log file than a DB table. For attributes for the pure file transfer use case that s fairly well specified already. If we wanted to include internal Rucio authorizations you d probably just embed the authorization name in the scp scope claim. ", "Scheduled for  "], "372": [], "366": ["       auditor worker DEBUG PID  Checking PIC MCTAPE        auditor worker ERROR PID  Check of PIC MCTAPE failed in  minutes  remaining attemps GError srm ifce err Communication error on send err SE Ls CGSI gSOAP running on rucio daemon prod  reports Could NOT import client credentials How auditor can suffer from erroron send ? "], "354": [], "112": ["Needs to be discussed with Paul ", "Try to figure out where the earliest possible exit in the codepath is. ", "For the upload one of the first things we do is query the rse information So directly afterwards we could check if any protocol implementation is available "], "57": [], "54": ["For combining weights we could do some kind of simple language. eg.   "], "53": ["We could apply the same logic Thomas das for monitoring to asynchronously prefill the sources in WAITING state. ", "Needs to be evaluated Maybe use Thomas algorithm for monitoring. "], "43": [], "38": [], "36": ["scope name for fake dataset other other ", "I checked the code and it looks like what is done in the conveyor is OK. But there might be a problem if the file has a parent DID with the wrong number of . . It will be fixed. In addition this part is very ATLAS specific  path always starts with but prefix might not end with naming convention rse attrs dest rse id .get naming convention None dest path construct surl dsn name naming convention We should have a way to have different conventions for different collaboration. "], "35": [], "31": [], "3389": [], "3382": ["I ll try to delete the storm protocol manually which removes both I suppose and add the correct one with the correct prefix to the attributes. ", "After running rucio admin delete protocol scheme storm INFN  DATADISK the correct protocol was removed but the wrong one remains. At least this is shown in rucio admin rse info However when I try to run the command for removing protocol again it states that there is no storm protocol. ", "root rucio nagios prod  test area rucio admin rse delete protocol scheme storm INFN  DATADISK RSE does not support requested protocol. Details RSE INFN  DATADISK does not support protocol storm Rucio exited with an unexpected unknown error please provide the traceback below to the developers. Traceback most recent call last File usr bin rucio admin line  in new funct return function args kwargs File usr bin rucio admin line  in del protocol rse client.delete protocols args.rse args.scheme kwargs File usr lib  site packages rucio client rseclient.py line  in delete protocols raise exc cls exc msg RSEProtocolNotSupported RSE does not support requested protocol. Details RSE INFN  DATADISK does not support protocol storm root rucio nagios prod  test area root rucio nagios prod  test area root rucio nagios prod  test area root rucio nagios prod  test area rucio admin rse add protocol hostname storm fe.cr.cnaf.infn.it scheme strom prefix storage gpfs atlas atlasdatadisk rucio port  impl rucio.rse.protocols.storm.Default domain json wan read  write  third party copy  delete  lan read  write  delete  INFN  DATADISK An object with the same identifier already exists. Details Protocol strom on port  already registered for INFN  DATADISK with hostname storm fe.cr.cnaf.infn.it . This means that you are trying to add something that already exists. ", "From Cedric this is a known feature. It should disappear when we move to CRIC new importer ", "Additionally there was a typo strom instead of storm . Nothing to do here. "], "3381": ["Hi ericvaandering Please go ahead with the changes. The code worked fine for us in our testing there is other changes still to be done though since the link finder is sometimes a bit slow. "], "3378": [], "3371": [], "3368": [], "3364": [], "3361": [], "3355": ["Oops I should know better than that!  is not None True bool  False "], "3348": [], "3337": [], "3334": [], "3324": [], "3320": [], "3315": [], "3314": [], "3313": ["New naming convention "], "3312": ["I implemented that just in procedural style. I am thinking about something nicer. The build blocks are  We don t want to call protocols if wan pfns already worked.  We check lan pfns but registration should still be done with wan pfns.  We raise an exception if nore wan neither lan pfns fit. ", "Hmm why is this working correctly at OU OSCER ATLAS then? Here we also have a wan pfn that s different from the lan pfn root  the external xrootd proxy SE vs. root dms.oscer.ou.edu the internal xrootd redirector . And as far as I can tell write lan works fine here and uses the internal one. Or am I misunderstanding the issue here? I very occasionally see a failed panda job with an error related to rucio not understanding root dms.oscer.ou.edu but those are very infrequent. Thanks Horst ", "I checked one job log and it looks like writes are using the write wan host         INFO copytool out   copy Event triggered BOTH  CORE COPY LIST ITEM file lscratch  atlas  PanDA Pilot    root   xrd atlasdatadisk rucio       I don t know why they don t use the internal redirector. SITE NAME is set correctly. But I think if they were using the internal redirector you would suffer the same problem. ", "Thanks David. I could ve sworn we had this working correctly after Mario and Martin helped me with that last year some time but maybe I m just not remembering this correctly. But we should get this working properly and automagically at any rate since it will greatly improve our bandwidth since using the internal redirector we basically have  Gbps to work with  storage servers while anything that goes through the proxy is currently limited to its  Gbps through flow . Thanks Horst "], "3308": [], "3292": [], "3289": ["TomasJavurek please have a look on this. I am not sure why this line was introduced It did work before Maybe it was just a mistake. ", "Is this also the reason gridftp fails and thus the reason we can t have a release with the wrote wan fix? Then it is urgent. Cheers Rod. ", "The line was introduced by berghaus on our request because TWAtGH discovered volatile behaviour of protocol.stat when it sometimes returned not correct answer. Thus we implemented retries we certain schedule but it should not exceed few minutes. "], "3286": ["gsiftp protocol is missing stat method has it actually ever worked? ", "This is still gsiftp via gfal. So the protocol scheme is the gfal.py file. It just routes the gsiftp calls through gfal. ", "yeah I know but gfal stat is anyway calling protocol implementation at the end or not? worked well for  root rucio nagios prod  test area rucio upload rse CERN PROD SCRATCHDISK scope user.ddmadmin protocol gsiftp         INFO Preparing upload for file         INFO File DID already exists        INFO Successfully added replica in Rucio catalogue at CERN PROD SCRATCHDISK        INFO Trying upload with gsiftp to CERN PROD SCRATCHDISK         INFO Trying upload with gsiftp to CERN PROD SCRATCHDISK        INFO Successfully uploaded file          INFO Successfully uploaded file          DEBUG Starting new HTTPS connection  rucio lb prod.cern.ch          DEBUG POST traces HTTP   None         DEBUG PUT replicas HTTP    root rucio nagios prod  test area rucio version rucio  ", "No this is somewhat hidden in the protocol definition of the RSE. It will specify the scheme somewhere Which will be gfal and then it will channel everything through the gfal.py. It will not touch the gsiftp.py one. It s a bit misleading I know ", " haven t had the GLOBALLY SUPPORTED CHECKSUMS implemented yet. This part of the stat method is different. ", "I tried to verify checksum at CERN PROD SCRATCHDISK with  that s one of the problems. ", "second problem is that .capitalize method capitalize only first leter who implemented that? ", "with upper instead it works well. Patching it. But still should really  be used for CERN PROD? I thought it is matter of few US sties. ", "the  wasn t actually perferred it was already the second attempt. First attempt failed because of the capitalize "], "3285": ["In case file doesn t exist still returns  as exit code while returned data contains just HTML code with  error e.g. vokac  capath cvmfs atlas.cern.ch repo ATLASLocalRootBase etc grid security emi certificates cert  USER PROXY X PROPFIND html body  atlas atlasscratchdisk rucio user mphipps   missing file.txt Not Found   body html Davix HttpRequest Error HTTP  File not found vokac  echo ?  Even though stdout is most probably valid XML last line comes from stderr even latest code just fails a bit later with Exception ExpatError? because you are trying to access d getetag element that doesn t really exists in a query for non existent file. Exception should be clear and is not optimal interface. I also think ATLAS capath should not be hardcoded in Rucio sources. "], "3284": [], "3282": ["Thanks David I will have a look. ", "I read the code again and I don t think this is an issue. What happens is that the weighting is skipped if any files in the dataset have replicas on any RSEs in the rule expression which make sense in principle because it means less data movement. So I wonder if the bias in the data carousel was because some sites had RAW files lying around and so those sites were preferred even though they had less free space. "], "3275": [], "3272": [], "3262": ["Only images can be attached in lining python code python ! usr bin python import os sys time import subprocess import select import fcntl def run cmd process cmd timeout None shell command parser with timeout param cmd shell command as a string param timeout in seconds return stdout xor stderr and errorcode Note PIPE hangs after filling  buffer continuously read stdout stderr output with select p subprocess.Popen cmd stdout subprocess.PIPE stderr subprocess.PIPE shell True fd p.stdout.fileno fl fcntl.fcntl fd fcntl.F GETFL fcntl.fcntl fd fcntl.F SETFL fl os.O NONBLOCK fd p.stderr.fileno fl fcntl.fcntl fd fcntl.F GETFL fcntl.fcntl fd fcntl.F SETFL fl os.O NONBLOCK stdout b stderr b readfds p.stdout.fileno p.stderr.fileno start time.monotonic while True readable writable exceptional select.select readfds timeout if timeout curr time.monotonic if len readable  and curr start timeout p.kill raise Exception timeout timeout curr start start curr for fd in readable if fd p.stdout.fileno stdout p.stdout.read if fd p.stderr.fileno stderr p.stderr.read TODO deal with situation when stdout stderr is infinite otherwise this will fail witn OOM if p.poll ! None break return p.returncode stdout stderr if name main handle run cmd process find proc rcode stdout stderr run cmd process find proc timeout  rcode stdout stderr run cmd process sleep  rcode stdout stderr run cmd process sleep  timeout  rcode stdout stderr run cmd process sleep  print CODE s rcode print STDOUT s stdout print STDERR s stderr ", "This is just super quick patch in order to get the output into the log. It will not stay like that. ", "I think best final solution would not use at all and should do something similar to ", "vokac i tried it like this in the beginning for some reason this was not behaving well with  maybe it s fine these days worth a try! ", "Code as simple as following one works fine for me with StoRM endpoint from  python import requests session requests.Session res session.request PROPFIND .format storm base lfn verify False timeout timeout cert   print res.status code print res.text session.close works fine for me. The main issue seems to be with protocol selection because StoRM storages works only if you specify  or don t use special adapter at all. ", "Comment from davidgcameron you have cmd capath   add in the next line something like logger.log cmd   can you also make the timeout configurable? "], "3258": ["this was fixed together with issue  "], "3257": [], "3253": [], "3236": [], "3228": [], "3225": [], "3209": ["Could be fixed with the no exception parameter. But to check the client config in a pure server functionality is already wrong. As you said the client config should not be necessary on a server. I think the right change is to do this similar to other optional extras modules. Check if they are on path and only load them then. Similar to ", "I am working on the issue. Just wanted to ask what should be done here Does this need to be changed too? ", "Hi Ruturaj no this one is fine. The server does also use the rucio config file. It just should not be assumed that there is a client section in the server config. This is somewhat contradictive. "], "3204": ["Hi rptaylor I don t think there are any concrete plans to do that. tbeerman or ericvaandering do you know any plans about that? ", "Not that I know of. ", "Ok I asked around a bit but nobody knows anyone planning to do this. I will close the ticket until we can find somebody interested in this development. "], "3201": [], "3196": [], "3193": [], "3188": [], "3187": [], "3182": [], "3181": [], "3178": ["Of course the right choice is hyphen case! Compare tpc commission davs True tpc commission root True vs. tpc commission davs True tpc commission root True vs. TPCCommissionDAVS True TPCCommissionROOT True ", "Let s finish the collection of all attributes in  first. Then we can organize a rename of the mismatched ones "], "3175": [], "3165": [], "3161": [], "3160": ["This problem also applies to get limits rucio admin account get limits dcameron CERN PROD DATADISK Client object has no attribute get account limit Rucio exited with an unexpected unknown error please provide the traceback below to the developers. Traceback most recent call last File home dcameron dev ddm rucio bin rucio admin line  in new funct return function args kwargs File home dcameron dev ddm rucio bin rucio admin line  in get limits limits client.get account limit account args.account rse args.rse AttributeError Client object has no attribute get account limit ", "I didn t include a fix for getting the account limits yet as that functionality still seems to be possible with rucio list account limits . ", "I think David is referencing  But this can be addressed in a separate PR. ", "Thanks for the patch bjwhite fnal! ", "Still seeing this in Rucio admin version  Is there something wrong with our setup? ", "Let me be more clear. There is still no get account limit in accountlimitclient ", "Yes these methods are in the accountclient . It is a bit confusing but I think historically they have always been there. In rucio admin we are still pointing to the wrong methods though I think. It should query the get local account limits and get global account limits . bjwhite fnal do you perhaps have time to have a look on this? ", "This is partly fixed in cserf PR  but not fully since we should still enable the querying of the global limits with rucio admin as well. "], "3153": ["I think we can just show the ones in DONE and FAILED already. But under normal conditions the requests will only be in this state very shortly "], "3138": [], "3134": ["there currently is no replica attribute that corresponds with the time the replica is physically created i.e. state AVAILABLE the base class created at attribute stores when the record in the database is created and hence is in particular for a tape stage in scenario not helpful at all best possible workaround is to filter on the updated at attribute instead this could lead to false positives for the motivation use case but these might need to be handled in any case due to a safety overlap in the poll cycle add updated after parameter to all list replicas functions and nested functions down to the place where the SQL query is composed list replicas list replicas list replicas for datasets list replicas for files also to the recursive call for archives add a .filter models.RSEFileAssociation.updated at updated after clause to the replica retrieval queries will need about  lines of code added changed "], "3129": [], "3128": [], "3127": [], "3121": [], "3118": [], "3111": [], "3108": [], "3102": [], "3093": [], "3089": [], "3086": [], "3085": [], "3081": [], "3078": [], "3074": [], "3071": [], "3070": [], "3054": [], "3053": [], "3050": ["Thanks TWAtGH can you please have a look. "], "3039": [], "3038": ["You can run tools run tests docker.sh i to do just the database init again. I ll add the rest to the docs. For logshow you are missing a pseudo terminal in docker are you sure you did t to docker exec ? Also make sure you run a recent docker version e.g. the default in CentOS is  you need at least  that s  major releases in between If you install the one from the linked repo in the docs you will get  . ", "Indeed I was using the default CentOS  version  After updating to  it works. Maybe you can add the requirement for a newer version to the doc as well? "], "3035": ["Reading through the thread the rule lifetime also seems appropriate to me. We are doing something similar for the pure STAGING requests for the STAGING AREA rules. The copy pin lifetime we are setting already to  . Do we need to add a parameter called desideredLifetime as well? ", "No the copy pin lifetime needs to be set to the rule lifetime then everything is fine. In lieu of a rule lifetime we can set a default of something like  hours. FTS will release the SRM pin after the transfer out of the tapebuffer is done. ", "sorry i still i don t understand what are you referring to here as desideredLifetime as parameter for FTS. The only parameter to set FTS side is the copy pin lifetime which is set as default to  hours now as Rucio fills it to  when submitting the job or it does not fill it at all . The srmBringOnline.desideredLifetime is set by FTS and it takes as value the copy pin lifetime value passed by the client. What is changing from FTS to FTS cause this is a parameter on  is the srmPrepareToGet.desideredLifetime but it does not affect the pin lifetime of the file on disk after the file has been staged via srmBringOnline though as this is taken from the copy pin lifetime. Anyway by changing  to avoid to run a srmPrepareToGet if the file is not online we also avoid to have different desideredLifetime if the file is then staged by the srmPrepareToGet. i know it s a mess but everything around SRM is a mess ", "just to mention that Castor does not take into account the pin lifetime as the pinning is disabled. But both Castor at CERN and RAL will be decommissioned and ATM they have huge buffer space so they don t have garbage collector issues. ", "We can assume that desideredLifetime Copy pin lifetime. I have followed too much the srm spec I ve modified the title too. ", "There is a pin leak in FTS For the Staging transfer case FTS creates two pins. One for bringonline and another for srmpreparetoget . On transfer completion only the pin generated by srmpreparetoget is released. The one from bringonline is kept. For now the pin lifetime from Rucio to FTS cannot be set to the rule lifetime as it will keep files for too long on the buffer. As the same time the current default pin lifetime is too short and doesn t protect the files Files are staged several times garbaged collected etc before the final transfer. This also impact site performance in general. ", "Since this was just discussed in the data carousel meeting If we don t set it to the rule lifetime since this might be too long is it sufficient to set it to  ", "So specifically changing to  "], "3032": [], "3028": [], "3022": [], "3019": [], "3013": [], "3011": [], "3006": [], "2996": [], "2993": [], "2992": ["rucio list files actually shows different outputs for an empty and non existing dataset. Issue dismissed. "], "2990": [], "2987": [], "2986": [], "2980": ["Actually this should already be implemented on Rucio side. There is a parameter called something like copy traces out . Or is there something missing? ", "I think this can be closed "], "2970": [], "2969": ["Need discussion ", "This issue was tagged erroneously. It is not related to the thread information contained in the messages. "], "2968": [], "2960": [], "2957": [], "2954": [], "2947": [], "2943": [], "2931": ["I decided to use TransferToolWrongAnswer so as not to make a whole new exception for this. If you instead prefer a dedicated TransferToolNoID exception let me know. "], "2925": ["Please assign to me ", "avec plaisir ", "I think there is a bit of duplication between   and this. Would be good to clarify what is done where. \ud83d\ude04 ", "Proposed changes here I didn t want to make a pull request yet since we should discuss how these changes may affect other clients if there are any? using the download and upload clients. I ve tested that the pilot with my latest changes that were merged and rucio download and upload CLI work ok. ", "thanks! TWAtGH can you please have a look ", "Why do we need to do changes to the API at all? Are the log strings not enough? I think it was implemented like this because Pilot expected Rucio to throw an exception if an error occurs. ", "Removing the exceptions might be a bit problematic yes. Why not put the info within the exception? Thus in the downloadclient line  and  Adding it to the logging is also fine but passing the error structurally on to the pilot is probably better within an exception? ", "Logs are not enough for callers of the API which need to pass error messages up to other systems i.e. pilot passing the error back to panda to display on the panda monitor . Exceptions are bad because in a bulk call you may not want to stop transfers just because one file failed. The exception also does not tell you which file failed unless you do some error prone string parsing. The doc for download pfns even says that it returns a dictionary with file states including FAILED so I suppose there were not meant to be exceptions originally. Also I don t think that a transfer failure is an exceptional or unexpected situation so throwing an exception is not really the right thing to do. ", "Alright but there needs to be an aggregated error message per API call that could be passed to other systems like panda? Or how will it be displayed if one file fails because of no source found another one fails because of checksum validation and a third one succeeds? Not all files downloaded sounds reasonable for me at this point. For the download the API will try to download all given files even if one fails. As Martin mentioned it should be possible to put one error string per file into the exception and this looks to me like the best and non API breaking solution. And I think this should then be done for every error and not just in case the protocol gives an error like in your code changes currently. ", "Summary of face to face discussion please correct anything I got wrong no change in API so as not to break backwards compatibility. There are two options for error propagation pass the file status dictionary as a keyword argument in the exception or use the information in the traces that is passed to download client traces copy out and filled with information. ", "The commits above revert the previous changes and instead fill the stateReason of the traces with error messages. I had to add a parameter to the uploadclient to support passing a reference to a traces list it was already in the download client . ", "Thanks a lot! Looks fine to me except some small comments In the download client you have this unused variable state reason And I think you missed at least two points which raise errors quite frequently  no sources available  failed checksum validation Is there a reason the upload client uses the state reason variable instead of writing it directly to the trace dict like the download client? And one less important point is stateReason as key name in the traces dict already used somewhere or could it might be changed to e.g. errorDescription ? That would also make clear why it s empty on success ", "Thanks for the comments. I fixed the minor issues. Is there a reason the upload client uses the state reason variable instead of writing it directly to the trace dict like the download client? The download client sends a trace for each attempt whereas the upload client only sends the trace after all attempts have finished so this is the reason to keep state reason outside the attempt loop. I m not sure if this difference is intentional or not. And one less important point is stateReason as key name in the traces dict already used somewhere or could it might be changed to e.g. errorDescription ? That would also make clear why it s empty on success Maybe tbeerman could answer that one it could be used for example for the automatic declaration of lost files. ", "Travis tests still fail due to if SUITE all then docker exec it rucio bin sh c opt rucio tools run tests docker.sh fi Error No such container rucio The command if SUITE all then docker exec it rucio bin sh c opt rucio tools run tests docker.sh fi exited with  Is that expected? ", "Did you try to restart or does it consistently fail? Sometimes this one shows up but I think this is more travis related. ", "The tests also fail in the pull requests but with an Oracle error which I think is unrelated to my changes. "], "2924": [], "2917": [], "2916": [], "2912": [], "2911": ["To me that makes sense but cserf is mostly an advocate of not making these commands too accessibly. \ud83d\ude04 But I think you can go ahead and implement that. "], "2907": [], "2905": [], "2899": [], "2896": ["We have to make a bit of a policy decision here. Significant operations like deleting an RSE or re creating it we always considered strictly manual operations since it can have quite severe impacts. cserf do you have an opinion how we should handle this with CRIC? Keep it manual? ", "Well as it is the importer right now it deletes all RSEs that are not part of the JSON. ", "i m still in favour of my old proposal that we should have an RSE state so not only exists not exists but also something like enabled suspended decomissioning removed from cric etc right now this can all be handled via RSE attributes but I think it would make sense to have this as a dedicated RSEState. ", "As discussed today we should make the merge strategy configurable here. If not present in CRIC delete RSE If not present in CRIC keep RSE and make deletion only manual hahahannes arisfkiaras can one of you have a look on this please. ", "Ok I guess you mean soft delete the RSE? Also how do we deal with a new RSE with the same name in the first case? Should we reactivate and then update it "], "2895": ["In the rebalancing script we need to update scopes account strings to InternalScope InternalAccount. This should be not too difficult to fix. dchristidis can you maybe have a look since TomasJavurek is not available at the moment? Thanks! "], "2886": ["The conversion is done in the permission API this is correct. The problem here is that sometimes we do comparisons such as if issuer in rse attr.get rule deleters .split Which does not work since the issuer object is looked for in a list of strings. This needs to be adapted to issuer.external . See There might be other places in the permission code which do that as well. ", "hahahannes Can you please have a look in the permission APIs and adapt these lines? Wherever issuer in is used. Thanks! "], "2882": [], "2878": [], "2877": [], "2874": [], "2872": [], "2866": [], "2865": [], "2861": ["mlassnig Any progress on this one? Please let me know if how I can help to solve this. ", "Hi Lionel sorry too much going on at the time can t raise the priority on this ticket right now. If you want to help out fixing the code there s a neat way now with the recently overhauled development dockers you can have a look here ", "I m looking into this again. I might have ideas finally. ", "ericvaandering This is great news. I confirm that the problem is still present. "], "2858": ["This patch also includes a fix for an error reported by the Undertaker         CRITICAL Traceback most recent call last File usr lib  site packages rucio daemons undertaker undertaker.py line  in undertaker delete dids dids chunk account InternalAccount root expire rules True File usr lib  site packages rucio db sqla session.py line  in new funct result function args kwargs File usr lib  site packages rucio core did.py line  in delete dids rucio.core.rule.archive localgroupdisk datasets scope did scope name did name session session File usr lib  site packages rucio common policy.py line  in new funct return function args kwargs File usr lib  site packages rucio db sqla session.py line  in new funct result function args kwargs File usr lib  site packages rucio core rule.py line  in archive localgroupdisk datasets rucio.core.did.set status scope archive name name open False session session File usr lib  site packages rucio db sqla session.py line  in new funct result function args kwargs File usr lib  site packages rucio core did.py line  in set status values bytes values length values events resolve bytes length events did scope scope name name session session File usr lib  site packages rucio db sqla session.py line  in new funct result function args kwargs File usr lib  site packages rucio core did.py line  in resolve bytes length events did did session.query models.DataIdentifier .filter by scope scope name name .one File usr   site packages sqlalchemy orm query.py line  in one ret self.one or none File usr   site packages sqlalchemy orm query.py line  in one or none ret list self File usr   site packages sqlalchemy orm query.py line  in iter return self. execute and instances context File usr   site packages sqlalchemy orm query.py line  in execute and instances result conn.execute querycontext.statement self. params File usr   site packages sqlalchemy engine base.py line  in execute return meth self multiparams params File usr   site packages sqlalchemy sql elements.py line  in execute on connection return connection. execute clauseelement self multiparams params File usr   site packages sqlalchemy engine base.py line  in execute clauseelement distilled params File usr   site packages sqlalchemy engine base.py line  in execute context e util.text type statement parameters None None File usr   site packages sqlalchemy engine base.py line  in handle dbapi exception util.raise from cause sqlalchemy exception exc info File usr   site packages sqlalchemy util compat.py line  in raise from cause reraise type exception exception tb exc tb cause cause File usr   site packages sqlalchemy engine base.py line  in execute context context constructor dialect self conn args File usr   site packages sqlalchemy engine default.py line  in init compiled for key in compiled params File usr   site packages sqlalchemy engine default.py line  in genexpr for key in compiled params File usr   site packages sqlalchemy sql type api.py line  in process return impl processor process param value dialect File usr lib  site packages rucio db sqla types.py line  in process bind param raise InvalidType Cannot insert to db. Expected InternalScope got string type. "], "2855": [], "2841": [], "2840": ["Hi arisfkiaras Yes please have a look! Thanks \ud83d\ude04 ", "Interesting enough the rucio add did meta seems not to be working at all on the rucio dev container. Even though the alembic version is the same as my production server  on the rucio dev db the table did meta does not include the updated at and created at columns while on my production db it does. ", "Yes I also noticed that and already opened  But couldnt find a fix so far ", "Thanks Hannes I had not seen that issue. Another interesting finding test did meta.py seem to succeed for any RucioException. I guess we can discuss this tomorrow in the meeting in conjunction with some generic metadata query times. "], "2835": [], "2823": ["Hmmm alembic runs fine for me in the container both down and up. ", "Or is this a problem with master next being out of sync when they shouldn t be? ", "I made some wrong changes on the next branch with the feature PR that removed the dots in some alembic files. "], "2822": ["The thought we had was that we could set and get a value in the cache with the Dogpile interface to evaluate the status of the caching system when the regions are created. However we were not sure where in the codebase to place such functionality. I was thinking in core but there is not an obvious core module for placement of such a utility. ", "Maybe lib rucio common utils.py is a good place it has a couple other dogpile related functions in there already. ", "In principle I think raising a warning is a good idea so that there is at least a trace for this but this might spam the log quite a bit. We always considered the caching backends as something optional so we should not spam the users who decide not to run any. Make the caching part of rucio.cfg so we can track this caching decision? See  ? "], "2819": [], "2812": ["Hopefully Travis doesn t break this time. "], "2805": [], "2802": [], "2799": [], "2788": [], "2787": [], "2777": [], "2772": [], "2771": ["I would like to see a fix for this as well. I checked out the latest Rucio source this morning and found the bug with the setup demo.sh script to still be present. ", "We are moving away from the demo containers in favor of the dev containers which are much more advanced. mlassnig is the error above an issue for the dev ones? I don t think so as it uses root containers for storage emulation? ", "demo needs to be removed it s unmaintained. see here ", "Demo has been removed "], "2769": ["After chatting with Stephane we figured out what is actually needed Since XCache supports we should allow xcache usage not only for root URLs but also for URLs. ", "this also means forcefully mangling our typical davs into "], "2764": [], "2763": [], "2760": [], "2755": ["Yes there is already an ongoing discussion in  about this. hahahannes proposed to switch to statsd too. ", "Great. Looks like that has progressed to a pull request even so let s close this. "], "2754": ["I just confirmed with GFAL people that GFAL indeed does not create any parent directories. ", "I think this is just a mistake. I think my intention was to ensure that the the directories are not created for RSEs with URL signing enabled in which case the protocol is and self.renaming is true . So the condition should be not self.renaming and dest  instead. I will submit a pull request to correct this. "], "2747": [], "2737": [], "2736": [], "2732": [], "2731": ["Hi can you maybe check if there is some exception output in Also try to rebuild the Docker image. I had the same problem when I build it on the next branch and then changed to another branch based on master where some dependency was missing. ", "Mon Jul      error pid  client   mod wsgi pid  Target WSGI script opt rucio lib rucio web rest authentication.py cannot be loaded as Python module. Mon Jul      error pid  client   mod wsgi pid  Exception occurred processing WSGI script opt rucio lib rucio web rest authentication.py . Mon Jul      error pid  client   Traceback most recent call last Mon Jul      error pid  client   File opt rucio lib rucio web rest authentication.py line  in module Mon Jul      error pid  client   from rucio.api.authentication import get auth token user pass Mon Jul      error pid  client   File usr lib  site packages rucio api authentication.py line  in module Mon Jul      error pid  client   from rucio.api import permission Mon Jul      error pid  client   File usr lib  site packages rucio api permission.py line  in module Mon Jul      error pid  client   from rucio.core import permission Mon Jul      error pid  client   File usr lib  site packages rucio core permission init .py line  in module Mon Jul      error pid  client   from .atlas import NOQA pylint disable wildcard import Mon Jul      error pid  client   File usr lib  site packages rucio core permission atlas.py line  in module Mon Jul      error pid  client   import rucio.core.did Mon Jul      error pid  client   File usr lib  site packages rucio core did.py line  in module Mon Jul      error pid  client   import rucio.core.rule Mon Jul      error pid  client   File usr lib  site packages rucio core rule.py line  in module Mon Jul      error pid  client   import rucio.core.replica import get and lock file replicas get and lock file replicas for dataset Mon Jul      error pid  client   File usr lib  site packages rucio core replica.py line  in module Mon Jul      error pid  client   from rucio.core.credential import get signed url Mon Jul      error pid  client   File usr lib  site packages rucio core credential.py line  in module Mon Jul      error pid  client   import  Mon Jul      error pid  client   ImportError No module named  Mon Jul      ssl debug pid  ssl engine io.c  client    Connection closed to child  with standard shutdown server localhost  So I pip installed it and it runs through. It is still running so I can t report it to go through to the end yet but the first issue seems to be tackled by the extra pip install inside the container FYI I run the container using docker compose which basically pulls it from docker hub I haven t tested building it using the Dockerfile in the dev subdirectory . ", "Cheered a bit too soon. I think one of the last tests seems to fail as well. ERROR REPLICA RECOVERER Testing declaration of suspicious replicas as bad if they are found available on other RSEs. Traceback most recent call last File usr lib  site packages nose case.py line  in setUp try run self.inst setup setUp File usr lib  site packages nose util.py line  in try run return func File opt rucio lib rucio tests test replica recoverer.py line  in setUp assert true exitcode  AssertionError False is not true FAIL REAPER DAEMON Test the reaper daemon. Traceback most recent call last File usr lib  site packages nose case.py line  in runTest self.test self.arg File opt rucio lib rucio tests test reaper.py line  in test reaper nose.tools.assert equal exitcode  AssertionError  !  begin captured stdout rucio reaper run once rses MOCK bin sh rucio reaper command not found n end captured stdout begin captured logging root INFO main starting processes root INFO Reaper This instance will work on RSEs MOCK root INFO Starting Reaper Worker  child  will work on RSEs MOCK root INFO Reaper   Running on RSE MOCK None root DEBUG RSE MOCK source for total space storage source for used space storage root INFO Reaper   Space usage for RSE rse type DISK region code None ASN None city None ISP u Brookhaven National Laboratory rse u MOCK created at datetime.datetime       deterministic True availability  time zone None longitude None continent u NA updated at datetime.datetime       staging area False volatile False deleted False country name u United States latitude None deleted at None id  max being deleted files  needed free space None used None free None root INFO Reaper   free space is above minimum limit for MOCK root DEBUG Reaper   list unlocked replicas on MOCK for  bytes in  seconds  replicas root INFO Reaper   No replicas to delete MOCK. The next check will occur at       root INFO Graceful stop requested root INFO Graceful stop done end captured logging Seems like rucio reaper doesn t exist in this version. ", "Confirming that using a dev container built from the repository itself solves the first issue. So adding the build instructions to the documentation or pushing a new dev container to docker hub would fix that one. Second reaper issue is still there. ", "Yes the new docker image should be already pushed. I will have a look for the reaper issue ", "Now it complains that Sync rse repository Traceback most recent call last File tools sync rses.py line  in module json data open rse repo file IOError Errno  No such file or directory etc rse repository.json Failed to sync! Probably I can fix it by linking etc in in the docker compose but I don t know if that s the intention. ", "Sorry I moved the dev container config to our container repository to setup automated builds on docker hub but I forgot this file. Should be fixed now if you pull again from docker hub. ", "ygrange Does it work for you? ", "I\u2019m on leave now. Will have a look at it early next week! Y. Grange Op  jul.  om   heeft Hannes Hansen notifications github.com het volgende geschreven ygrange Does it work for you? \u2014 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread. ", "Still can t find reaper . I decided to updte the checked out version to  and I get another bunch of errors looking line this FAIL SCOPE REST send a POST to create a already existing scope to test the error Traceback most recent call last File usr lib  site packages nose case.py line  in runTest self.test self.arg File opt rucio lib rucio tests test scope.py line  in test scope duplicate assert equal   AssertionError  !  begin captured stdout ", "I think the reaper error is related to it not having a bin script like the other daemons. For the reaper we tried to build this on install of the package which might not happen like this in the container. We probably should go back to bin scripts ", "just passed and ready for merge this fixes the reaper issue "], "2730": [], "2727": [], "2725": [], "2720": [], "2717": [], "2715": [], "2707": [], "2702": [], "2701": [], "2691": [], "2681": [], "2678": [], "2672": [], "2671": [], "2664": [], "2661": ["In case of cli I get root rucio nagios prod  rucio v download file.that.does.not.exist.txt        INFO Processing  item s for input        DEBUG Processing item file.that.does.not.exist.txt        DEBUG RSE Expression istape False        DEBUG Splitted DID file file.that.does.not.exist.txt        ERROR Data identifier not found. Details Data identifier file file.that.does.not.exist.txt not found        DEBUG This means that the Data IDentifier you provided is not known by Rucio. In case of pilot logs Not easily feasible with the Exceptions that we have. Imagine you have a bulk of files some of them you download and some of them not. Than only thing that you can raise at the end is Not all of the files have been downloaded. We can introduce a new exception where I list all the files that have not been downloaded and the reason for that. ", "Hmmm Can you check with Rod which release he uses ? ", "My problem was interactive with the client not from the pilot. ", "Which release? Do you have an option to try with some of the newest ones? Like the testing release at ALRB? ", "current one. try it with a non existent file. Cheers Rod. "], "2656": [], "2653": [], "2652": [], "2649": [], "2640": [], "2638": ["submitted to probe repo "], "2633": [], "2632": [], "2625": [], "2622": [], "2620": ["This test is wrong it should be if pfn and is deterministic . But also is it a good idea to switch to no register here? this might be more confusing than just failing. ", "I think this is correct. In case of non deterministic sites pfn is obligatory option and only way to upload there. However one sometimes still want to register such a dids. ", "The problem could be with the is deterministic flag how it was set for given RSE that you want to upload to? In case of the ATLAS it is defined in AGIS CRIG . ", "There s an earlier check that the pfn is defined for non deterministic RSEs. The test I quoted says if pfn is provided and the RSE is non deterministic then set no register True . That s the opposite of what the warning message says. And it means that for a non deterministic RSE the file will be uploaded but not registered. And the non deterministic flag was set correctly first thing I checked ", "I managed to reproduce the problem. It occurs only when register after upload is used. ", "register after upload is a slightly different problem without it on a non deterministic RSE the upload works but the file isn t registered rucio v upload rse FNAL DCACHE PERSISTENT lifetime  scope  reco keepup pfn gsiftp   pnfs fnal.gov usr dune persistent RSE dunepro protodune  beam output detector full reconstructed      raw    reco     raw    reco           INFO Preparing upload for file  raw    reco           WARNING Upload with given pfn implies that no register is True except non deterministic RSEs        INFO Trying upload with gsiftp to FNAL DCACHE PERSISTENT        INFO Successfully uploaded file  raw    reco    Completed in  sec. rucio list file replicas rse FNAL DCACHE PERSISTENT  reco keepup  raw    reco    SCOPE NAME FILESIZE  RSE REPLICA with register after upload        INFO Preparing upload for file  raw    reco           WARNING Upload with given pfn implies that no register is True except non deterministic RSEs        ERROR An unknown exception occurred. Details no error information passed status code  internal server error server error o    Completed in  sec. ", "I ve updated the pull request to fix the register after upload issue as well. But I can still see a few potential issues On a deterministic RSE if you provide a pfn it will automatically switch to no register mode. This appears to be what was intended only it was doing it for non deterministic RSEs instead but it seems dangerous to me I think it d be better to make these fail unless no register is explicitly given. With register after upload if the file exists it tests if it already registered and skips it if it is. But the check is if the DID exists not if there s a replica at this RSE. So if the DID has been used before it won t be added to the RSE even though the file is there. This is getting off the scope of this ticket a bit but rsemanager.exists fails badly with an unknown error exception for non deterministic RSEs if there is no file replica at the RSE. This is inconsistent with deterministic RSEs where it just returns false. ", "I doesn t work even with api. Furthermore the registration doesn t take into account name of the file and rather attempt to register according to the name of the source file. ", "I ve updated the pull request to fix the register after upload issue as well. But I can still see a few potential issues On a deterministic RSE if you provide a pfn it will automatically switch to no register mode. This appears to be what was intended only it was doing it for non deterministic RSEs instead but it seems dangerous to me I think it d be better to make these fail unless no register is explicitly given. With register after upload if the file exists it tests if it already registered and skips it if it is. But the check is if the DID exists not if there s a replica at this RSE. So if the DID has been used before it won t be added to the RSE even though the file is there. This is getting off the scope of this ticket a bit but rsemanager.exists fails badly with an unknown error exception for non deterministic RSEs if there is no file replica at the RSE. This is inconsistent with deterministic RSEs where it just returns false.  our WM system uses pfn without registration. If an user attempts to upload something with pfn it will be soon deleted by the agent that deletes non registered data.  yes. I think it is better to check DID. If we check replica only it could attempt to register did that is already present at different rses. We don t allow reuse of the same dids.  thx to be fixed. ", "our WM system uses pfn without registration. If an user attempts to upload something with pfn it will be soon deleted by the agent that deletes non registered data. Does it use no register or the API equivalent ? Because I m arguing that automatically changing between registration no registration depending on the RSE type is potentially error prone. If it s explicit then there s no problem. ", "our WM system uses pfn without registration. If an user attempts to upload something with pfn it will be soon deleted by the agent that deletes non registered data. Does it use no register or the API equivalent ? Because I m arguing that automatically changing between registration no registration depending on the RSE type is potentially error prone. If it s explicit then there s no problem. yes no register flag in the API. But right it doesn t sound generic enough. ", "excluding not from if pfn and not is deterministic solves the problem with registration. the register after upload is however still failing. ", "ah ok I am bit behind. register after upload doesn t work exactly because rsemgr.exists is failing for non deterministic sites that illingwo already mentioned above. That needs to be fixed first. I ll open separate ticket for that. ", "was this the travis issue why it failed? I ll restart the pr from illingwo as well an that one would be preferred to approve. "], "2616": ["Hi Boris you can try to set the rule to stuck rucio update rule stuck which should issue a recount of the locks.     tpc raw is a dataset with all  files part of the dataset? So not a container with multiple levels of containers datasets? ", "I tried the rucio update rule stuck id before. And now it is shame on me obviously it just took quite long to repair the file miss count. But I can confirm that the observed bug from Rucio version  about the wrongly counted files for a certain rule is gone with  "], "2614": [], "2612": ["This issue will be used only for modifications of rucio core libraries. Second issue will be created for the IAM prototype script. "], "2611": [], "2607": [], "2606": ["Hi Lionel Thanks for reporting We will have a look! "], "2601": [], "2600": ["The last commit is meant for discussion. Especially I am not sure whether it is not better to  place and propagate external traces directly in the methods of Download client in place of making it parameter of the class.  not to create separate method update traces in place of adding it to send trace method. It might be enough just to rename it to send or update trace did is populated directly in the traces but we still can make it dict did trace if needed for pilot. ", "The list is used as an object to which the reference is propagated from pilot to the client. I could propagate directly the trace objects from pilot but that wouldn t be generic enought in my opinion. I don t see any issues with datasets bulks archives but TWAtGH might be of different opinion. "], "2597": [], "2592": ["You still call new configuration third party read write and I m not completely sure what exactly does this mean. For XRootD it is necessary that destination endpoint supports TPC because only pull mode is available but for WebDAV you can either contact destination endpoint and use pull mode or it is also fine if you contact source endpoint and use push mode. Currently FTS use for WebDAV pull mode first and than automatically fallback to push it is possible to configure FTS not to try push this makes things even more interesting in case of multisource transfers and I have no idea how FTS currently deals with such TPC transfers in case of push fallback. I guess new third party read write configuration doesn t try to cover all possible scenarios push model . "], "2591": [], "2588": ["Looks like they are trying to get the oracle xe docker image back to dockerhub or to give travis access to it. I think that might be the only solution "], "2579": [], "2576": [], "2560": [], "2556": [], "2553": [], "2548": ["AFAIK the repo directory should be mounted with v pwd to get the setup.py file available. ", "You cannot mount anything during the build process. It is mounted when the container is actually started. For the setup.py to work during the build process the whole rucio directory would have to be copied into the container. It was working before without setup.py and I will revert it to that. If you need to run setup.py it can still be done after container has been started. ", "yes I realised my confusion afterwards. COPY . etc rucio should fix it otherwise. ", "Doest it work with dockercompose etc docker dev docker compose.yml ? I.e. rucio opt rucio This was how I tested and didn t see this error. python setup develop was also put there to deploy properly the package and dependency within the container. ", "No the mounting in docker compose doesn t work. Also the change of the RUCIOHOME from opt rucio to etc rucio introduces some problems e.g. the aliases for are using opt rucio . You were able to build it and get it running? ", "yes it worked for me. I might have changed the aliases as well. I should check the pull request. ", "After some checks docker compose file etc docker dev docker compose.yml up d works for me and root  rucio more opt rucio etc web aliases  Rucio REST WSGIScriptAlias accounts opt rucio .venv lib  site packages rucio web rest account.py points against my env. "], "2545": ["Another place with hardcoded protocols is here but it is perhaps irrelevant for this case because storm won t get on protocol level. ", "Last place is here introducing new protocol might fail the check. ", "It works now but keeping the ticket open for following actions The requirements I remember now were to print the command and stdout err to the log when it fails. check existence of the file to link to check using archive works. Make link to zip and extract required file to workdir. more generally for all protocols optimize common case where multiple files from the same archive are to be staged. allow configurable retries in API bulk download command for all inputs for job parallel retries traces return result object to pilot ", "There is more concrete ticket now "], "2541": [], "2536": [], "2535": [], "2528": ["C P from email Hi Since this is R D I don t like the idea of exposing this in the full API I mean in the python replica client server sided it s fine . Hence I like the b approach a bit more. Or alternatively we do not add a parameter to the method but make this configurable via a cfg parameter. Server side it is more flexible we can just do a wrapper around list dataset replicas which enhances the result with the vp query. Cheers Martin ", "Just to elaborate on this If it is just something we want to try out let s either do a separate function like Ilija suggested or hide the activation of this somehow. We shouldn t pollute the public API with a parameter we use for testing Even if it defaults to something Especially since nobody else can use the VP service anyway. Once this is production ready we can decide how to integrate it. "], "2525": ["tbeerman can you please du the etc docker part of the cleanup "], "2521": [], "2520": ["I think mlassnig did not add these on purpose since mysql previously did not support check constraints. If this is supported now we should add them again. ", "Correct. MySQL did not support check constraints. I leave the glory of adding them to the migrate scripts to you ", "btw no need to edit the old scripts just add a new one which adds them all in one go ", "I would add them in the appropriate migrate scripts. It s more work but this way we can roll back to a specific table version also with mysql. ", "Seems like they should get created automatically by sqlalchemy as they are the enum types described in the models.py. Needs to be investigated why they dont get created. "], "2519": [], "2518": ["Hi dciangot We had quite some issues the last days with the  rest api package since it does quite a hacky workaround with including the  package which was killing the building of the entire rucio package. This is now temporarily fixed with  but it will for sure come up later thus I would prefer if we can get the  rest api package out of the dependency tree completely. Right now it is used in the delegate function inside the transfertool  and also used by some other methods in transfertool  . Thus if at all this is only used by CMS since ATLAS also doesn t use this kind of delegation. If you have some time on your hand it would be really great to rewrite this functionality so we can remove the dependency. Or remove it for now and add it back later if not used Thanks! Cheers Martin "], "2514": [], "2511": ["After some discussion the approach will use the core.config module. ", "Originally I wanted to support this as well rucio admin config set section root proxy internal option  value root my.proxy.host  but I could not figure out even with AllowEncodedSlashes and the quote plus trick to make mod wsgi keep two slashes together. So for now we will have to do it like this rucio admin config set section root proxy internal option  value my.proxy.host  and prepare the root prepend in list replicas . "], "2505": ["I just noticed this as well. It seems there is a new version of  since a few days which is a dependency of  rest API. Need to check what the actual problem is. Some threads suggest to additionally install swig but this doesn t seem a good solution to me. ", "I fixed the demo build by adding this line in the Dockerfile before the rucio package resolution starts RUN pip install ignore installed ipaddress RUN pip install   RUN pip install rucio rucio webui This will trick pip into not installing the latest version. Whether it works or not runtime I still didn t check ", "I fixed this directly in our dependency file to fix  to  This should also fix the demo issue. "], "2502": [], "2501": [], "2500": [], "2498": ["It s not particularly relevant to the question but Standard gridftp do not support multiple checksum algorithms without the installation of additional plugins This statement is incorrect.  support was added in  see Older GridFTP servers were also quite annoying in that they would provide the  sum regardless of what algorithm you requested. It s still good to handle multiple checksum algorithms better in Rucio for other reasons ", "To me it feels logical to add the possibility to specify the checksum algorithm s to be used for a destination RSE. This could just be an RSE attribute with an enumerated list. Right now Rucio only supports  and  and basically prefers  over  but it is more flexible to make this decision per RSE. This somewhat overlaps with the discussion we had in  where we discussed to basically offer arbitrary checksum support if communities want to import their historic crc checksums. ", "It s not particularly relevant to the question but Standard gridftp do not support multiple checksum algorithms without the installation of additional plugins This statement is incorrect.  support was added in  see Older GridFTP servers were also quite annoying in that they would provide the  sum regardless of what algorithm you requested. It s still good to handle multiple checksum algorithms better in Rucio for other reasons Hi Brian Yeah that s right. I have to rephrase that statement to something in the lines of some older gridftp versions do not . \ud83d\udc4d Thanks for the heads up! ", "Hi all For the records the branch I am working in to implement this functionality is here Cheers Gabriele "], "2496": [], "2494": [], "2493": ["Removed from tools pip requires test It is still pulled in as a sub dependency from Flask though but with  "], "2488": [], "2485": [], "2482": [], "2480": [" Maybe we should change the  test to check all changed files now instead of only files with the  header "], "2479": ["Fixed by  "], "2469": [], "2468": ["TWAtGH this will still need an extension in the downloadclient. list replicas replies with in the json case already now and after this patch with ?xml version  encoding UTF  ? metalink xmlns urn ietf params xml ns metalink metalink in the metalink case. "], "2465": [], "2462": ["Just did some testing. The database is initialized with did meta having the columns created at and updated at but then the following migration is applied and the fields are missing. My suggestion would be to explicitly add the fields on the DIDMeta model and create a new migration. ", "But in that case it is enough to just add a new migration to fix it since the columns are created from the models.py correctly But dropped added wrongly by the migration ", "So I have fixed it in my own branch and also re enabled the tests that were set to always pass. I could open a pull request for review or do a combined one when I have ready. "], "2460": ["I have marked a few dependencies in the issue text where I am not sure why we still need this. Does anyone remember? ", "You can drop also pygeoip  GeoIP API "], "2456": [], "2446": [], "2445": [], "2444": [], "2443": [], "2440": [], "2436": [], "2434": ["The original point was to have a ChangeLog there the copyright is just put to the header of each file. But since we are not using the ChangeLog I agree we should remove it. ", "maybe keep it and mention in it that the changes are described in instead. ", "OK I can do this vingar s suggestion ", "Great thanks! \ud83d\udc4d "], "2433": ["with defining a state root rucio nagios prod  grid testing python update state bug.py Traceback most recent call last File update state bug.py line  in module rucio client.update replicas states rsename scope scope name filename state A File usr lib  site packages rucio client replicaclient.py line  in update replicas states raise exc cls exc msg rucio.common.exception.UnsupportedOperation The resource doesn t support the requested operation. Details State AVAILABLE for replica user.ddmadin test    on UNI FREIBURG DATADISK cannot be updated but right this gives different error message ", "Is it possible that this replica was already in state A ? ", "hmmm my bug as usual. Scope is wrong. But still the exception should be turned to something more understandable. Like did doesn t exist or so. ", "It s an optimisation of queries here. We don t check if the DID exists we just execute an UPDATE query and check if it affected a row. If not the error is raised. Which could be replica was already in that state replica is in a state which should not be changed replica does not exist etc. It s difficult to give a specific error here without additional queries. I am not sure if we should change that since this is also not really a command a simple user is exposed to. "], "2432": [], "2421": ["had to revisit this  support must not be lost "], "2413": [], "2412": ["Done with my part. I also addressed the Protection of sources too strict "], "2411": [], "2408": ["Isn t this a duplicate from  ? ", "Might be. Not sure yet. This is in traces while the information can be still somewhere in the logs. It even might be that this is actually more ticket to pilot than Rucio. ", "But thx for pointing to the other ticket here. Can we link them together somehow? ", " is about the logging messages while this one aims for more details in the traces. We could maybe do a single issue for this. I think the complete error handling in the download and upload client should be overhauled. ", " Can we close this? Log verbosity should actually be fine. "], "2403": [], "2402": ["The conveyor should probably just stop starting with an error if no scheme is specified. ", "I would have expected to have the conveyor to figure out automatically which schemes protocols should be used for third party transfers out of the ones available for the replicas here it was gsiftp davs srm . Note also here the non expected behaviour of the judge repairer after the configuration has been corrected to use gsiftp . ", "It s also an option If no scheme is given just use all compatible ones. For the judge repairer I don t think anything has to be changed. The issue here is in the conveyor as the request probably stays in it s state forever and never get s failed. Thus this is never reported back to the rule. For the judge it looks like that the request is still in submission. ", "One observation after such error the request stays in then N NO SOURCES state and then disappeared. The judge repairer keeps it in the S state forever. ", "Is it possible that there is no conveyor finisher running? The finisher should process the NO SOURCES ones and mark them as failed. ", "The finisher runs too. ", "Can you track if the replicas affected by this bug are actually handled by the finisher? But in any way this has to be fixed in the submitter than the workflow through the finisher repairer is fine again. ", "I m seeing some messages in the finisher like requeing etc. Don t you have enough information to reproduce the problem and investigate more from your side ? ", "I only tested the NO SOURCES one and this works fine in my finisher. But I didn t test the full error yet. But yes information is sufficient \ud83d\udc4d "], "2399": ["The PFN download doesn t query the Rucio server for PFN s so it doesn t have the checksums. Thats why ignore checksum True by default. And in my opinion it should stay like this. ", "I would like to see chacksum after any download no matter on download method. But I agree that we don t want to add another query to Rucio DB. So only option would be to make it optional and pass the checksum from pilot user etc. Pilot should have the src checksum stored in fspec object. "], "2398": ["Hello could you also make sure that the message is clear in this case copy on TAPE which cannot be accessed with rucio download . In the previous case it is just that the RSE does not exist.  rucio v download rse RAL  MCTAPE            INFO Processing  item s for input        DEBUG Processing item            DEBUG RSE Expression RAL  MCTAPE istape False        DEBUG Splitted DID            DEBUG  DIDs after processing input        DEBUG Processing resolve archives True name   did     rse RAL  MCTAPE istape False no subdir False nrandom None transfer timeout  scope   type u FILE force scheme None base dir .        DEBUG ?xml version  encoding UTF  ? metalink xmlns urn ietf params xml ns metalink        DEBUG Traceback most recent call last File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  bin rucio line  in new funct return function args kwargs File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  bin rucio line  in download result download client.download dids items args.ndownloader trace pattern File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  lib  site packages rucio client downloadclient.py line  in download dids input items self. prepare items for download items File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  lib  site packages rucio client downloadclient.py line  in prepare items for download files with pfns self. parse list replica metalink metalink str File cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  lib  site packages rucio client downloadclient.py line  in parse list replica metalink raise error ExpatError no element found line  column         ERROR no element found line  column         ERROR Strange error No section policy  rucio list file replicas     SCOPE NAME FILESIZE  RSE REPLICA      GB  BNL  DATADISK srm dcsrm.usatlas.bnl.gov  srm  pnfs usatlas.bnl.gov  rucio            GB  RAL  MCTAPE srm srm atlas.gridpp.rl.ac.uk  srm  castor ads.rl.ac.uk prod atlas simRaw atlasmctape   AOD        NNLOPS nnlo                 GB   DATADISK srm    srm  pnfs uchicago.edu atlasdatadisk rucio       ", "I think we could add an extra message if the selected RSE is skipped because it is a tape RSE. That shouldn t be too difficult to add. TWAtGH Line  in the downloadclient should also be rewritten similar to line  ", "Hey the error message for a metalink parsing error did change with  The error here is that the metalink returned by list replicas is incomplete I guess an exception in the REST frontend . If list replicas doesnt t return any replicas for a file the message is something like No sources found . This message should be enough  no? In the case of this ticket we could print a more user friendly message like Error in server response .  we already discussed in a Rucio meeting that the code of line  can be removed  ", "BTW regarding this should raise RSENotFound . I m not sure about that. It s currently not really expected that list replicas raises exceptions for reasons like this. Would need to check this. ", "Problem confirmed in list replicas REST frontend. Metalink starts streaming then later the exception is raised causing the client to think everything is in order. "], "2390": [], "2379": [], "2378": ["Please let me know if I can begin working upon this if the contributors feel this is a worthy change. ", "Yes please go ahead \ud83d\ude09 "], "2375": ["I would like to be assigned to this issue. "], "2370": ["please look into this pr. "], "2367": ["i have tweaked the dockerfile to remove any isssues during installation "], "2363": [], "2362": [".venv   twegner twegner dev rucio rucio download               INFO Processing  item s for input        INFO Getting sources of DIDs        INFO Using main thread to download  file s        ERROR None of the requested files have been downloaded. could you please give debug output? ", "rucio v download        rse EISCAT        INFO Processing  item s for input        DEBUG num unmerged items  num dids  num merged items         INFO Getting sources of DIDs        DEBUG schemes davs gsiftp root srm file        DEBUG rse expression EISCAT istape true        DEBUG num DIDs for list replicas call         DEBUG num resolved files         DEBUG unzip v returned with exitcode         DEBUG tar version returned with exitcode         DEBUG num list replicas calls         DEBUG Queueing file               DEBUG real parents set        DEBUG options        ignore checksum False transfer timeout  destinations set . False        DEBUG Prepared sources num sources   num non cea sources  num cea ids         ERROR d format a number is required not NoneType        DEBUG This means the parameter you passed has a wrong type. Completed in  sec. ", "Thanks. The commit should fix it. Difficult to test though. ", "It fixes the error message but it still doesn t download the file. It s a regression as a previous rucio download version can download the file e.g.  . ", "That s correct. It cannot be downloaded because there are no PFNs. I thought this is maybe expected. Could you show me a list file replicas metalink please? ", "bash  rucio list file replicas metalink        ?xml version  encoding UTF  ? metalink xmlns urn ietf params xml ns metalink file name     identity        identity hash type   hash size  size glfn name  rucio        glfn url location NDGF domain wan priority  client extract false gsiftp preprod srm.ndgf.org   rucio          url url location NDGF domain wan priority  file metalink ", "Thanks. For the download you used rse EISCAT but these rerplicas are on NDGF . ", "Nope one replica is on EISCAT rucio list file replicas        SCOPE NAME FILESIZE  RSE REPLICA         GB  EISCAT         GB  NDGF gsiftp preprod srm.ndgf.org   rucio          "], "2359": ["i would like this to be assigned to me. Sorry uploaded a pr before hand "], "2355": [], "2352": [], "2344": [], "2336": [], "2327": [], "2322": [], "2319": [], "2315": ["As discussed the RSE selector in core rse selector.py needs to updated to first check local quotas and then global quotas. If a RSE does not have quota left from either local or globa quotal it will get removed from the list of possible destination RSEs. It still needs to be discussed how the free space should be calculated displayed to the user. ", "How should I modify the CLI commands to display the limits and usage? rucio list account usage root RSE USAGE LIMIT QUOTA LEFT rucio list account limits root RSE LIMIT Should I just leave it as it is and only show local quota because it might be confusing to mix it with global quotas? On the other side it might be weird to see a rule failing but to also have enough local quota ", "We could add the rse expressions of the global quotas there as well similar rows like the others but this might be confusing. It might be better to separate this completely and have essentially two tables. ", "Yes I was thinking the same. It is too messy in one table also because the global quota is independent from one specific RSE "], "2314": [], "2313": [], "2306": [], "2305": [], "2304": ["I ll try to tackle this one. "], "2299": [], "2298": [], "2291": [], "2290": [], "2288": [], "2285": [], "2277": [], "2266": [], "2265": ["dchristidis are you also taking care of this one? ", "Issue  was actually fixed by cserf. "], "2262": ["Can be closed as duplicate of  "], "2261": [], "2257": [], "2253": ["Should I also add it to the account abacus ? ", "Yes please if we do it for one we might as well do it for both. \ud83d\ude04 ", "Should I add the history table to models.py or use models.RSEUsage. history mapper .class to access the history table? I think adding it to models.py is better as we are not using the sqlalchemy versioned table? ", "The plan was to change all of these tables with  anyway. However I think we should change them explicitly in this separate PR and not intermix them just get s easier to track. So for now keep using the history mapper. "], "2249": [], "2248": [], "2245": [], "2240": [], "2238": [], "2237": [], "2233": [], "2227": [], "2226": ["This is no longer needed due to  ? ", "In the current implementation the download priories the root download no explicit extraction if  the file is not in a zip that will be downloaded anyway e.g. zip is prio  for another file  the file is not in a zip with at least  other files that are needed This decision is made there "], "2221": [], "2220": [" I wonder if we also need something in the submitter since the decision should be on the tuple src dst dataset . Right now I don t see it but we should keep it in mind.  The deadline until datasets from tape are to be released should be configurable via core.config with a default of  hour. ", "Oh the deadline can be different per RSE. ", "How should the chosen strategy be configured? In the cfg file or config table or as daemon argument? ", "Seems like MySQL has problems with the original version of core request release waiting requests as well my new query for grouped FIFO. LIMIT and IN not supported together This version of MySQL doesn t yet support LIMIT IN ALL ANY SOME subquery It seems like mysql never supported the query for core request release waiting requests UPDATE requests SET state s updated at s WHERE requests.id IN SELECT requests.id AS requests id nFROM requests nWHERE requests.dest rse id s AND requests.state s ORDER BY requests.requested at ASC n LIMIT s FOR UPDATE this means that the throttler is not working with mysql Updating table while selecting from it You can t specify target table requests for update in FROM clause "], "2219": [], "2217": ["Please also add a test case for this. "], "2212": ["This still isn t quite right dune rucio prod ALTER TYPE BAD REPLICAS STATE CHK RENAME TO BAD REPLICAS STATE CHK OLD ERROR syntax error at or near BAD REPLICAS STATE CHK LINE  ALTER TYPE BAD REPLICAS STATE CHK RENAME TO BAD REPLICAS Those need to be double quotes not single or omitted entirely . ", "It s worse than that. This won t work at all ALTER TYPE BAD REPLICAS STATE CHK RENAME TO BAD REPLICAS STATE CHK OLD ALTER TABLE bad replicas ADD CONSTRAINT BAD REPLICAS STATE CHK CHECK state in B D L R S T ALTER TABLE bad replicas ALTER COLUMN state TYPE BAD REPLICAS STATE CHK DROP TYPE BAD REPLICAS STATE CHK OLD It renames the enum tries to add a check constraint to the column which fails because the column type is still the enum then it would try to change the column type to a non existent type which would fail if it ever got that far. ", "I think this works if context.get context .dialect.name postgresql For Postgres the ENUM Type needs to be changed to the new one and the old one needs to be dropped op.execute ALTER TYPE BAD REPLICAS STATE CHK RENAME TO BAD REPLICAS STATE CHK OLD pylint disable no member new type sa.Enum B D L R S T name BAD REPLICAS STATE CHK new type.create op.get bind op.execute ALTER TABLE bad replicas ALTER COLUMN state TYPE BAD REPLICAS STATE CHK USING state text BAD REPLICAS STATE CHK pylint disable no member op.execute DROP TYPE BAD REPLICAS STATE CHK OLD pylint disable no member else drop constraint BAD REPLICAS STATE CHK bad replicas type check create check constraint name BAD REPLICAS STATE CHK source bad replicas condition state in B D L R S T ", "Thanks illingwo! I m currently rewriting all the alembic migrate scripts now properly via and was stumped by this as well. "], "2207": [], "2203": [], "2200": [], "2199": [], "2194": [], "2186": [], "2185": [], "2181": [], "2180": [], "2177": [], "2174": [], "2173": [], "2168": ["The upload client part will be improved with  The rsemgr protocol part will be improved with the rsemgr overhaul. So I close this one "], "2166": [], "2161": ["I wonder if we should make this configurable via the config table? ", "I wonder if we should make this configurable via the config table? We should. IMHO all configuration that is not absolutely necessary to make a connection to the rucio server client side and to make the database connection daemon server side should be moved into the config tables. New configs should not go into the .cfg ", "The fts source strategy makes sense as an override but per default I think it just should come from the config table. But re reading the ticket I guess that s what cserf meant anyway. ", "I was thinking about completely getting rid of the fts source strategy and just use the config table. If one has  ways to specify the source strategy that s not obivious which one has the precedence on the other. ", "It might make sense to have both ways if we just want to test a strategy at one conveyor without impacting all of them. Thus command line has precedence But I am not really sure if it is needed for the source selection strategy. "], "2160": ["I don t think this can be made generic but I m happy to be proven wrong The symlinking itself is simple enough but for StoRM you ll need the additional webdav etag lookup. So something like storm should be perfectly reasonable unless you can find a nice way to make it generic then it would be something like ln "], "2156": [], "2155": [], "2154": [], "2149": ["Before you go through the effort we should decide if we want to move all the bin scripts to console scripts or everything back to bin Is there any major added benefit of the console scripts? We had one issue where a dependency conflict was only affecting the console scripts not the bin scripts. So we should get this consistent in one way or the other. "], "2144": [], "2142": ["hahahannes can you please have a look on this. In we probably need to join models.IdentityAccountAssociation with models.Identitiy to add the email there. "], "2141": [], "2140": [], "2137": [], "2133": [], "2128": [], "2127": [], "2126": [], "2122": [], "2118": [], "2115": [], "2112": [], "2107": [], "2105": [], "2103": ["Duplicate of  It s fixed in  "], "2102": ["This is a duplicate of  it s already merged. \ud83d\ude04 "], "2095": ["Fixed by  "], "2090": [], "2089": [], "2080": [], "2076": ["Note this seems to affect also other options stored in merged options e.g. ignore checksum "], "2069": [], "2065": [], "2054": [], "2051": [], "2048": ["This needs to be fixed in the downstream monitoring. Not a bug on our side we are reporting correctly the site. "], "2045": ["I had a quick look on this without conclusion and think this has something to do with the protocol scheme caching in the conveyor. The spacetoken is part of the extended attribute it might be that for some reason it gets cached without one and recalled later expecting one to be confirmed. ", "Due to session reuse in FTS. Not a Rucio bug "], "2040": [], "2039": [], "2038": ["No feedback success ", "Yep like rm cp or mv \ud83d\ude04 "], "2030": ["Could you maybe have a look on  as well? It sounds quite related. ", "Duplicate of  "], "2028": ["Notes pip requires numpy There are some complications with numpy and wsgi this needs to be handled separately in another issue. numpy stays at  pip requires client pylint Upgrading pylint to  instead of  .  is the last version which is  compatible everything else only   and pycodestyles Leaving them at their respective version as a new version generates some errors. This will be handled in a separate PR. "], "2022": ["Thinking a bit more about my implementation it is probably not the best correct one because it doesn t really apply in case DownloadClient UploadClient is called through API. I ll first have a look how it is used in pilot and than I could come back with different solution for use case described above. ", "Yeah I was about to comment that. I think it really has to be added to the DownloadClient and Uploadclient. ", "Now I m even thinking if we should polute rucio code with this functionality or if it is job that should be done directly in pilot I ll discuss that with TomasJavurek ", "I briefly discussed how to filter RSE protocols used by jobs on specific site and this functionality should go directly in pilot. So drop this request. "], "2021": ["Would be good to remove this and get it into the  release as it creates some confusion. There is some inconsistency there though as we remove rsemanager.download moved to downloadclient but still use rsemanager.upload Should make a plan to remove it there too. ", "I think we can just remove it without touching any other part of the code. ", "Maybe but please check bin rucio for something like archive file option ", "right ", "download file from archive has been removed. rsemanager.download is still there needs to be removed later. "], "2020": ["the error coming from  bin rucio admin rse seems to be caused by a change how argparse handles errors in   python import argparse parser argparse.ArgumentParser subparsers parser.add subparsers first subparser subparsers.add parser first help help first second subparser first subparser.add subparsers third parser second subparser.add parser second help help second third parser.add argument version help version args parser.parse args print args shell python file.py first error too few arguments  file.py first Namespace "], "2015": [], "2010": [], "2007": [], "2006": ["One more seems like the exception handler is borked  is lowercase now but cert does not exist rucio env rjoshi rucio bastion rucio v a rjoshi S  whoami given client cert opt rucio etc web client.crt doesn t exist        ERROR Cannot authenticate. Details  authentication failed        DEBUG Traceback most recent call last File opt rucio env bin rucio line  in new funct return function args kwargs File opt rucio env bin rucio line  in whoami account client get client args File opt rucio env bin rucio line  in get client elif auth type  proxy UnboundLocalError local variable auth type referenced before assignment        ERROR local variable auth type referenced before assignment        ERROR Rucio exited with an unexpected unknown error. Please rerun the last command with the v option to gather more information. If it s a problem concerning your experiment or if you re unsure what to do please followup at alastair.dewhurst cern.ch If you re sure there is a problem with Rucio itself please followup at Completed in  sec. "], "2001": [], "2000": [], "1990": [], "1987": [], "1986": ["After discussion with cserf The blocking of these requests is essentially wanted. allow tape is reserved for admins. Regular users should not download from tape uncontrolled staging requests to the robots and rather transfer the data to a scratchdisk. So everything works as expected. Closing "], "1985": [], "1977": ["Reason was identified as a problematic query which changes the query plan. Plan stabilisation needed "], "1976": ["The check if a rule exists uniqueness is already done. The missing part is the uniqueness of list child datasets "], "1973": [], "1970": [], "1967": [], "1965": ["This adds a new RSE limit parameter MaxSpaceAvailable . An alternative would be to use the free attribute of the rse counter entry for the relevant RSE but there d have to be some race free way of setting it. "], "1961": [], "1958": [], "1955": [], "1954": [], "1953": ["Now FTS issue a  when a double submission is done. Will modify the conveyor to handle this. "], "1952": [], "1948": [], "1945": [], "1942": [], "1937": ["Not needed "], "1932": [], "1929": [], "1924": [], "1922": [], "1918": [], "1917": [], "1912": [], "1910": ["Kerberos is handled as an extra in setup.py so you would have to do a pip install rucio kerberos which installs kerberos  pykerberos  requests kerberos  I am not sure why it trys to load kerberos modules in the baseclient it basically checks if requests kerberos is available and if yes loads HTTPKerberosAuth. Might be some issue with a very old requests kerberos version? ", "Ah ha! Looks like an old client has snuck into PYTHONPATH . Line numbers above don t match with the current code. "], "1909": ["Adding high priority flag cause this is causing major pain now "], "1901": ["Created PR  Maybe we may want to test it on the CERN server before merging. ", "Hi Andrea You created this PR against the next branch Thus it is treated as a feature . This will then only go in the Rucio  release in February  Is this fine? Otherwise please re create as a patch. See ", "Hi Martin ok sorry. I m recreating the PR. ", "Created  ", "That s against the hotfix branch \ud83d\ude04 I ll create the two PRs for master and next . ", "Although I just saw you based your development off the hotfix branch instead of master . It s not super clean to merge it like that. It would probably work fine but we should stick to the normal workflow here. ", "I m really sorry I ve read the doc too quickly and understood patches go in hotfix I do not know how since is clearly stated . I ll recreate  PR one for next and one for master both from a branch based on master if I understand correctly ", "Hi Andrea no worries. The workflow we have is sadly not very simple for somebody new since it is somewhat different than most projects do it so I can understand the confusion. Just ping me on Slack if you have any question about the setup. \ud83d\ude04 "], "1898": [], "1888": ["You can ignore  It is deprecated and will be removed in less than  months. ", "downloadclient can be ignored also because the last  problem is the round function that behaves different in   but we decided to ignore this because of the incompatibility of the fix from builtins import round see  "], "1885": ["I don t think we need the reaper preparer. I mean the key difference here is that the main reaper does the partitioning different on hash of dids and not RSEs. But it would work exactly the same if the selection happens on the expired replicas instead of the ones marked as BEING DELETED by the preparer. The thing we have to get right here is not to overload an RSE. We probably would need some kind of cross communication of the reapers here. Another thing where we have to be careful It s probably fine but just raising the point is the whole remove the dataset once the last replica is deleted workflows as there might be race conditions if different reapers delete on the same RSE. ", "The problem if you hash on DIDs is that it can be you can get millions of files on the top of the queue on the same RSE. In that case there will be no deletion on the other RSEs and one can quickly go into trouble. What I propose is to build a list of replicas to delete where each RSE gets a share e.g. it can be needed free space . It would be something like rep to delete tot share sum of all needed free space on all rses for rse in rses get a list of unlocked replicas on rse take needed free space tot share replicas from the previous list and insert into rep to delete return rep to delete Of course all of this can be done in list unlocked replicas but that makes this method a bit complicated that s why I was thinking about a new daemon. ", "Another proposal was chosen "], "1882": [], "1879": ["This is a duplicate of  \ud83e\udd23 "], "1878": ["Actually I thought that this table gets automatically filled by sqlalchemy? ", "Nope apparently this was a procedure as well which we deactivated in  ", "The new procedure has been implemented. "], "1874": [], "1871": ["Isn t this a duplicate of  I also don t think this should be a probe but a service daemon since it is really a central functionality. Or do you suggest to do a probe first and the service later? ", "Ah right I forgot about the other ticket. I m not sure it deserves to be a daemon. ", "I think we should aim to have these functional parts which are potentially useful for a wider audience as daemons We should try to get away from probes for these things. "], "1870": [], "1864": [], "1861": [], "1853": [], "1852": ["Fixed by "], "1849": [], "1848": [], "1845": ["Not sure why this was not caught by the Python  test though? ", "I will remove the imports and add a basic test to the  client test to check at least imports. Also the pylint check for  has to be changed to ignore round errors. "], "1842": [], "1841": [], "1836": ["destination or source ? ", "Destination ", "See also  ", "How would you change the behaviour? By a value in the config table ? ", "I think this can be handled after  because the grouped FIFO will also throttle over all activites per destination. So it will be easy add it to the current FIFO way. ", "According to  this is implemented. hahahannes can you confirm please ? ", "This should be fixed by  because there I add the new way of changing between all the different modes. "], "1835": [], "1833": [], "1825": [], "1818": ["As discussed in  If there is an API CLI to access history information we should add an index per default. If there is no API CLI we still might want to add one if this information is frequently queried by operations. "], "1814": [], "1811": [], "1810": [], "1809": [], "1807": [], "1805": ["Issue to be solved with FTS people the submission python api is missing the following file attributes selection strategy and the following job attribute  Opening a ticket to FTS support FTS Jira ", "Also made a pull request to allow bulk status requests from the client ", "Requiring also to implement the following function in the FTS bindings update priority query latest set se status version Needed before completing the migration ", "This is superseded by  Closing "], "1804": ["This is actually a change which is more relevant to  since the algorithm needs to decide if to include greey RSEs or not. ", "The new reaper relies on the RSE attribute greedyDeletion so I propose to close this ticket. "], "1803": ["In the medium term the plan was to get these VO specific modules schema policy permissions  algorithms out of the core package. We discussed this some time ago and we want to offer two different ways to do that. It s summarized it in  If you can wait for that ticket to make progress I would just leave it like this otherwise I would suggest we move it to rucio lib rucio common schema until we make progress with  ", "I think I d like to get into schema while we wait for  This avoids us having to build our own containers and chase versions. It would allow us to just use rucio containers. Maybe we can get a CMS person to look into  "], "1792": ["Traceback most recent call last File opt rucio lib rucio daemons   background rebalance.py line  in module rebalance rse source rse rse max bytes available target rebalance volume dry run False comment  Background rebalancing force expression destination rse rse File usr lib  site packages rucio db sqla session.py line  in new funct result function args kwargs File usr lib  site packages rucio daemons  common.py line  in rebalance rse for scope name rule id rse expression subscription id bytes length in list rebalance rule candidates rse rse mode mode File usr lib  site packages rucio db sqla session.py line  in new funct raise DatabaseException str error rucio.common.exception.DatabaseException Database exception. Details cx Oracle.DatabaseError ORA  divisor is equal to zero SQL u SELECT dsl.scope as scope dsl.name as name rawtohex r.id as rule id r.rse expression as rse expression r.subscription id as subscription id d.bytes as bytes d.length as length FROM atlas rucio.dataset locks dsl JOIN atlas rucio.rules r ON dsl.rule id r.id JOIN atlas rucio.dids d ON dsl.scope d.scope and dsl.name d.name nWHERE ndsl.rse id atlas  rse and n r.expires at sysdate  or r.expires at is NULL and nr.created at sysdate  and nr.account IN panda root ddmadmin and nr.state O and nr.copies  and nr.did type D and nr.child rule id is NULL and nd.bytes is not NULL and nd.bytes d.length    and nd.is open  and nd.did type D and nr.grouping IN D A and  SELECT count FROM atlas rucio.dataset locks WHERE scope dsl.scope and name dsl.name and rse id dsl.rse id nORDER BY dsl.accessed at ASC NULLS FIRST d.bytes DESC parameters u rse u OU OSCER ATLAS DATADISK Background on this error at ", "The original problem was that I set manually the limit on fsize calculated as bytes length. This was done only localy at prod  I removed that and rather ordered the query by fsize. There is no division by  anymore. "], "1791": ["In we did this with the SCOPE NAME REGEXP There is an issue with this way though see  but in this case it should be fine. I think just submitting these as params is not a bad idea but due to backward compatibility I would go the regexp way. ", "Ok thanks I missed the SCOPE NAME REGEXP thing. I ll go with that then. "], "1788": [], "1787": ["One difference between the Google Cloud Storage signature plugin and  swift ones is the need to call an external service to generate the signed url. To no affect the performance on one of the most called APIs list replicas the signed urls operation has ben isolated in a dedicated endpoint. cf. \u00b4lib rucio web rest objectstore.py\u00b4. For example with the degradation of this signed url external service the responses time can increase together with the number of simultaneous db sessions resulting in less slots available on frontends snowball effect . A solution might be between the two approaches. ", "There s the possibility to sign  offline without having to contact an external service. This should work for both private  servers because we control the private key and for AWS similar to the delegated credential loading we do for GCS . ", "Interesting. It looks new. For which  version ? spec ? ceph the most popular  object stores for ATLAS is usually behind for new features unfortunately. And for swift ? ", "I m not sure if it s mentioned in the documentation but my experimentation shows that the signing operations in the  API work fully offline they still work if I disable the network on my VM but the ones in the old boto API currently used by Rucio don t. So it might be necessary to use  if we want offline url signing unless it can somehow be made to work with boto  "], "1786": ["dciangot Which access pattern would you see to retrieve this information ? requestId is very internal to Rucio and exposed at few places. Drafting the corresponding REST APIs and workflow would help. ", "vingar In fact the idea is that a system using Rucio clients e.g. CRAB for CMS user data wants to keep track of the FTS job ID IDs for a certain DID against a user defined RSE only for debugging purpose at the moment. In this sense I think the access pattern could be as complicated as it needs to be e.g. first retrieve request ID for a DID and then to a second call to retrieve the request details. An optimal solution instead but probably too specific for the use case would be that the details of the last request will be shown providing a set of did source rse destination rse . Does it make any sense for you? P.S. using the MQ would be a possibilty too for this problem but I fill like it is a general enough problem that one may want to pass it directly through the client. ", "yes it makes perfect sense. I think the workflow will be from DID RSE Rule RequestIds FTS jobs Ids. Few things are already there at least in the rucio UI rules CLIs for this purpose. It should be checked. ", "For what I have understood the only missing part there is RequestIds FTS jobs at list from CLI point of view. So a minimal solution here is essentially the porting of RequestIds FTS jobs step into CLI python bindings from where it is already available in Rucio core. At least this is my understanding opinion at the moment. ", "I had a look at the UI code showing the status of a rule and there it calls get request by did which corresponds to the REST call I have the feeling that s what you need. Maybe some convenient clients and wrappers missing. ", "Yes I think that what we are looking for is to integrate the info of get request by did with the info coming from query request in some form. ", "mmm btw probably get request by did it is enough alone. If I understand well the query made there. So probably it is really something minor that is missing. But I think that it is now more or less clear what I have to check. ", "So summarizing with a working plan  add a call here or in another client if you prefer like def get request by did self name rse scope None this will use the rest call to  Problem is that I have just checked that due to this the request rest call is broken for CMS DIDs containing . So this is to be fixed before  that is pretty straight forward can work. How does it sound? Did I miss anything? ", "About  With the current per resource logic it should go in a new rucio client requestclient.py file.  looks fixable ", "About  With the current per resource logic it should go in a new rucio client requestclient.py file. ok then I ll put in rucio client requestclient.py  looks fixable yeah indeed do you prefer a separate issue for that? ", "Hi yes agree on a new requestclient . For  a new ticket would be helpful thanks. ", "I think a new requestclient would be good as well. The concern I see a bit also what Vincent said that previously the requestId was a rather internal concept but since we are exposing it already via the REST For the WebUI fts id linking I think it is good to expose it properly via a client. Also it s a proper usecase to be able to access fts ids for external monitoring. ", "from the described workflow it looks not needed to interact with requestId ", "Yes you are right it s not interacting but exposing primarily for the fts id . "], "1782": [], "1777": ["This is a duplicate of  Closing "], "1774": [], "1772": ["Just on a side note The way we manage the PKs for history tables in the models.py is wrong. For Oracle we do not create PKs only in the models.py we create them but as new experiments create their tables with the models.py install script it should be handled differently. ", "I think it s historical because sqlalchemy enforces the definition of PK on table otherwise it fails. This explains why PKs are defined in models but not in Oracle. ", "Yes sqlalchemy expects a primary Key otherwise the whole mapping from rows to addressable objects does not make sense. What is a little bit better is to define a fake PK like in the replication rule history table. With a history id as PK Which does not get created in Oracle but this is still kind of wrong for experiments which create their schema from models.py because they will still end up with a PK. But thats more of a long term change. In this specific case the merge just doesnt work because merging on a table without a PK doesnt make much sense. ", "yes in that case the merge is wrong. ", "Fixing this with a PR now more for the other tables in  "], "1766": ["Hmmm This one is a blocker for Kronos. I did a manual patch on the nodes and I expected it would be in  ", "Actually this has been fixed by  "], "1765": ["I guess the second sentence should read in the absence of messages in the queue "], "1758": ["In Rucio what we call a block is going to be called a dataset. Beyond that I m having kind of a hard time understanding what you re looking for. Rucio itself does not require that datasets Rucio definition live all at one site they can be scattered file by file. We don t plan to do this in CMS though. ", "The dataset replicas API is GET replicas scope name datasets that s correct. We don t have an API for dataset replicas for only a single RSE. Should be trivial to implement though. ", "Eric we use the following query in DAS block dataset a b c site XYZ where block is a b c  patterns. So I must pass to API a b c and get back a b c  whatever you call them . The site is part of response so we can filter but it would be nice to add it as a parameter to the query. The later can be problematic due to REST logic. I can imaging this replicas cms rse name where we can supply both rse and dataset block names or replicas cms name rse but it may be hard to implement since API path should be parsed properly. REST goes well with a single argument which is appended to the path of API but handling multiple parameters in REST is harder and solution may require fallback to URL parameters e.g.     etc. Valentin. On  Eric Vaandering notifications github.com wrote In Rucio what we call a block is going to be called a dataset. Beyond that I m having kind of a hard time understanding what you re looking for. Rucio itself does not require that datasets Rucio definition live all at one site they can be scattered file by file. We don t plan to do this in CMS though. You are receiving this because you authored the thread. Reply to this email directly or view it on GitHub ", "The first should give you the mapping you want. I don t know what the client is doing internally in the second call if its one REST API call or a dozen. root client   rucio list content cms Charmonium    MINIAOD SCOPE NAME DID TYPE cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET root client   rucio list dataset replicas cms Charmonium    MINIAOD DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC    US Nebraska Test   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US Nebraska Test   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   ", "Can we translate this into REST API calls? My understanding that it is replicas cms Charmonium    MINIAOD datasets which produces nothing. It aligns with your output which shows  found results. Then the question is why we see files on these sites but not blocks in CMS terminology or DATASET in Rucio one . Is it artefact or something else? On  Eric Vaandering notifications github.com wrote You can do it in two calls or maybe just one the second one root client   rucio list content cms Charmonium    MINIAOD SCOPE NAME DID TYPE cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET cms Charmonium    MINIAOD      DATASET root client   rucio list dataset replicas cms Charmonium    MINIAOD DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC    US Nebraska Test   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US Nebraska Test   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   DATASET cms Charmonium    MINIAOD      RSE FOUND TOTAL  US NERSC   You are receiving this because you authored the thread. Reply to this email directly or view it on GitHub ", "This is still an outstanding issue. The dataset replicas are currently updated only by an PL SQL procedure on the Oracle database. This PL SQL I believe is not run on the CMS instance hence the dataset replica counters not being updated ", "vkuznet that s why I have suggested a couple of times that you install the rucio client and instrument it to see which REST calls are being made. It s a fool proof way. Relying on me or others to read the code is time consuming and error prone. ", "With  the daemon to update the collection replicas has been added. It s in  release Thus with this one or the PL SQL procedure it will update the collection replicas to the correct stats. "], "1755": [], "1749": ["This is potentially tricky. Changing the primary key is easily possible however this won t move the data already on the tablespace it will just check that the new primary key constraint is still valid. What we actually want is for the data to be moved. So this will need an intermediate tmp replicas table and thus some downtime as the DBA manually shovels data around. Changing the primary key can be done with the alembic upgrades but shoveling the data can only be done manually if we don t want tmp replicas to pollute our models.py ", "We can do the primary key change anyway though. And we could provide a script for all databases to do it offline. Then it s the instance admins choice when how they apply it if they want to do it ", "It is fine to do it offline but we need to document it somehow. I am surprised that this never showed up but in smaller installations the queries will just be quick enough without going against the PK. And from a uniqueness point of view this PK is correct as well. "], "1739": ["briedel is this an icecube requirement? Because if not then it s almost certainly a mistake. ", "Per briedel in Slack there s no need to allow single quotes. Probably an error. illingwo any chance you want to put in a PR? "], "1738": [], "1731": ["It s exactly one hour but we can add this explicitly. Proposal  in UTC X Rucio Auth Token Expiration      ", "from here Date format All dates returned are in UTC and are strings in the following format RFC  ex RFC  Mon  May     UTC In code format which can be used in all programming languages that support strftime or strptime a d b Y H M S UTC ", "Hmmm that might be the content of the docs but in reality we are always returning  in our JSONs e.g. curl H X Rucio Account mlassnig H X Rucio Auth Token created at      . Maybe better to update the docs instead to reflect this. For the HTTP headers that s a completely different story there  makes sense since it s also what the web servers return. ", "Strange for      RFC  ex RFC  DATE FORMAT a d b Y H M S UTC is the format used in dumps loads json cf. utils.py "], "1730": ["I guess it corresponds to answer with Content Type application x json stream . To stream JSON we used objects delimited by newlines. cf. ", "According to JSON specs from specs In JSON they take on these forms An object is an unordered set of name value pairs. An object begins with left brace and ends with right brace . Each name is followed by colon and the name value pairs are separated by comma . An array is an ordered collection of values. An array begins with left bracket and ends with right bracket . Values are separated by comma . END OF QUOTE Since your APIs provide an answer as a list of records which are dicts they need to be separated by comma and embraced with brackets. The provided output of REST APIs e.g. curl H Content Type application json H X Rucio Auth Token token account das type USER email null account dciangot type USER email null does not satisfy the JSON specs. That s why the standard JSON parsers in python or other languages will fail to parse it. What you yield in APIs is a list of strings even though they re represented dicts but it is not a valid JSON document. I provided an example of python parser flow which you can use to tests your documents curl H Content Type application json H X Rucio Auth Token token account.json parse docs in python import json data json.load open accounts.json And it fails to parse the document obtained from the REST API. I can easily substitute a file with file descriptor which is established in network call and json.load fileDescriptor should work. In order to stream a data in JSON format someone needs to do the following obtain json serialized object objectData json.dumps data send objectData to the wire ", "vkuznet isn t that compatible with what Vincent said? That is the API returns Content Type application x json stream . A JSON stream is not JSON e.g. ", "bbockelm according to Rucio REST API page The server answer can be one of the following content type in the Header Content type application json Content Type application x json stream In the last case it corresponds to JSON objects delimited by newlines streaming JSON for large answer id  foo bar id  foo baz The MIME media type for JSON text is application json and it is defined by The data with MIME type application x json stream is not the same as data with MIME type application json . The former can be defined as JSON objects delimited by newlines and later should be strict JSON document. Vincent is correct about application x json stream but that is not what I was looking for. I asked about application json which right now returns data in application x json stream format. ", "Hi to explain we stream the results from the database directly to the clients that s why we do JSON streams the server ignores client requests for different MIME types . Now if for a particular call you d like to have a single JSON document then we will have to add support for a client requesting MIME application json . It s not really a problem but the implications are of course that any wide spread use of that will quickly exhaust the memory on our servers since the JSON has to be assembled there. For example doing this for list replicas is a no go whereas for many other calls it would be ok. ", "Mario I m not sure I understand the technical issue here. Why do you need to assemble a JSON? If you have your stream from DB the change I would expect is to add opening bracket to the response replace newline delimiter n to comma and end response with closing bracket . It can be easily done by adding response wrapper decorator which will take whatever stream API you have and perform these three actions. Here is a brief sketch of the program flow which does not assemble the JSON in memory but rather add additional wrapper around rucio data stream and yield proper JSON format ! usr bin env python import json def json stream func JSON streamer which yield correct JSON object yield for idx doc in enumerate func if idx sdoc yield sdoc sdoc json.dumps doc yield sdoc yield def rucio data Provide JSON data from Rucio server doc srv rucio for idx in range  doc counter idx yield doc def main print Rucio JSON stream stream for doc in rucio data print doc stream json.dumps doc n try data json.loads stream except ValueError print ERROR unparseable JSON stream print nRucio JSON object stream for doc in json stream rucio data print doc stream doc data json.loads stream print parsed json object s data if name main main If you ll run it you ll get this output Rucio JSON stream counter  srv rucio counter  srv rucio counter  srv rucio ERROR unparseable JSON stream Rucio JSON object counter  srv rucio counter  srv rucio counter  srv rucio parsed json object u counter  u srv u rucio u counter  u srv u rucio u counter  u srv u rucio ", "Hi Valentin sure. If we agree that all JSON streams can also be returned as a JSON list instead then that is easy. ", "In your example you are not testing the serialisation between the server and the client through the network which was our problem at that time. That s what we went with newlines which was the way to stream json. Maybe now there is some better way. ", "vingar of course I don t know the details of your code but example clearly demonstrate the idea. I would expect that you organize the pipe between your DB layer and streaming API. You consume data on your DB part and yield it back with proposed modifications to the streaming API. ", "mlassnig Mario you can leave JSON stream as is and they can be returned with application x json stream type but my request was to fix the data when client requests application json . ", "vkuznet we started by what you proposed but got issue with the client requests etc not being able to consume the streamed entries. ", "Valentin you mentioned you used Content Type. Shouldn\u2019t that be Accept? ", "vingar indeed it s probably easier for the client side to consume the JSON streaming format not to mention things like handling HTTP chunk encoding better! but if the client explicitly asks for JSON instead I guess we have to assume they know what they are doing! ", "yes good points. Feel free to post pull requests ", "Ok I guess I figured out why Valentin expected that he could request a JSON with Accept since the statement in the doc is ambiguous The server answer can be one of the following content type in the Header However this doesn t mean that the client has a choice Anyway adding an Accept parser to the REST frontend is easy. ", "When the HTTP Header Accept application json is there proper json should be returned. One caveat is to check the rucio clients regarding this BTW listing accounts is not the most memory usage critical operation and do no need streaming imo. ", "Indeed the proper request header should be Accept sorry for confusion. ", "Yet another issue with ambiguity of documents returned by server. Imaging that one query will return a single record. Using x json stream we have I took record as an example accessed at null name Charmonium    MINIAOD      rse  US NERSC created at Wed  Oct     UTC bytes  state UNAVAILABLE updated at Wed  Oct     UTC available length  length  scope cms available bytes  By parsing this doc we ll get a Python dictionary data type. While if server return multiple records e.g. accessed at null name Charmonium    MINIAOD      rse  US NERSC created at Wed  Oct     UTC bytes  state UNAVAILABLE updated at Wed  Oct     UTC available length  length  scope cms available bytes  accessed at null name Charmonium    MINIAOD      rse  US NERSC created at Tue  Oct     UTC bytes  state UNAVAILABLE updated at Tue  Oct     UTC available length  length  scope cms available bytes  it will be converted assuming parser can do it into a list data type. As you can see a single API can produce different data type using this format. Without application json we end up with ambiguity of data type from APIs. I hope you can take care of this ambiguity at a server level and return data such that they can be decoded into consistent data type. ", "For me the main issue here is that the documentation is a bit ambiguous and the server handling of the requested Content Type is not strict enough. If the application requests Content Type application json or any other content type that is and the server cannot supply it it should raise a HTTP  Not Acceptable See instead of just replying with whatever it thinks is useful. For me the handling is fine If no specific type is requested we return application x json stream By keeping it like this we also ensure possible back compatibility issues with old clients. What we should change is if a different type is requested e.g. application xml application pdf we should raise a  And we can also add the application json handling as asked by Valentin it shouldn t be too hard to add. Let s discuss this at the Dev Meeting on Thursday and then create the respective issues. ", "This was discussed in the development meeting from Essentially we decided that if a content type is requested which the server cannot reply we will raise HTTP  However we will try to add application json type additionally to application x json stream . ", "How do we stand on this issue? I found yet another problem with records using default application x json stream mime type. It does not provide newline n for the last record i.e. if we have multiple records in response they are separated by n except last one. Response with single record does not have n either. This requires to properly write a parser for x json stream which should separate record based on newline and collect last output till the end of the stream. According to the line delimited JSON should have n for every record but it is unclear from the description if last record should have the delimiter or not. ", "It s still in the plan but as this is non blocking it doesn t have the highest priority. hahahannes Can you please look into the raising a  in case of non supported content type. We can discuss this on Monday. To also allow application json for certain queries should be not difficult. For the n for the last record. Just quickly looking into it I couldn t find a definitive answer on this. For JSON Which is part of the Line delimited JSON it says The last character in the file may be a line separator and it will be treated the same as if there was no line separator present. "], "1729": ["It seems to me that the right logic here is to pick the most recent entry from rse usage that isn t rucio for each rse and use that to sort the free space. The catch is that it isn t very easy to express this in SQL. You can do it with a window function like this select rse staging area id from rses left outer join select rse id free from select rse id free row number over partition by rse id order by updated at desc as rownum from rse usage where source ! rucio as ranked free where rownum  as recent usage on rses.id recent usage.rse id where deleted false order by free but I don t know how well MySQL supports this and SQLite only added window function support this September. "], "1726": [], "1725": [], "1718": [], "1710": [], "1703": ["we never delete RSE. It s on purpose a soft deletion with the deleted column instead ", "Ah alright thanks. ", "but \u00b4rse exists rse name True\u00b4 should give back False instead if the flag is deleted. "], "1702": ["Not a bug. This is CERN Megabus force overwriting the Rucio stomp.py dependency. "], "1701": [], "1698": [], "1695": [], "1692": ["So the issue here is that the pip dependency resolver is not all that good We need to define stricter dependency ranges for this. "], "1689": ["Related to RSE deletion it might make sense to have a look at  If the RSE is not empty a RSEOperationNotSupported should be raised. "], "1686": [], "1685": ["Maybe set account property set account parameter or something similar instead? So that it is aligned with the attribute functions del account attribute add account attribute . Then I would also replace the get account status with get account property . ", "Also I am not sure how I shall handle the permissions. For example I think the permission for set account status should apply for changes on all account properties ? Then I would also update the permission files. "], "1682": [], "1677": ["This also links to  "], "1672": [], "1669": ["How? ", "Needs to be checked. ", "We can check again but I think the registration is working fine now "], "1668": [], "1664": [], "1663": [], "1656": ["Depends on  "], "1651": [], "1647": ["Shall I also add ASN availability and usage as there are also missing. "], "1646": [], "1639": [], "1638": ["This was discussed in the dev meeting on ", "New repository The probes will be migrated "], "1637": [], "1635": [], "1632": [], "1625": [], "1624": [], "1623": [], "1622": ["I think this is dependent on  ", " is merged now. I think this bug should be fixed by that but please check. ", "It gets downloaded correctly using the current master and  . It s also done correctly in case the whole zip is downloaded. "], "1619": ["It seems that the request to fails because of a broken check for the oracle should be session.connection instead session.connection . I think it is not usable at the moment because of the missing oracle upgrade. So I would change the error message to a more meaningful like Could not retrieve DID meta informations in case this request fails but it would be still shown on top of the page. ", "get did meta should never be called at the moment It needs the oracle version upgrade . I strongly suspect that we should call meta here like in all other cases. I think this was changed as an error. The connection is a different bug and should be fixed can you please create a ticket for this? ", "Okay linking the ticket for the connection bug "], "1614": [], "1613": ["Not only filemeta is returned wrong but also the identity tags .venv   twegner twegner dev rucio rucio list file replicas metalink expression NIKHEF ELPROD DATADISK   DRAW   metalink xmlns urn ietf params xml ns metalink file name DRAW   identity   DRAW   identity file metalink .venv   twegner twegner dev rucio rucio list file replicas metalink   DRAW   metalink xmlns urn ietf params xml ns metalink file name DRAW   identity   DRAW   identity file metalink I d prefer that the identity tag always contains the constituent DID "], "1610": [], "1606": [], "1601": [], "1600": [], "1599": ["Priority increased "], "1598": [], "1593": ["Link to  as only one of them has to be implemented. ", " has been implemented and merged. Therefore all downloads with eventType get sm or get sm a shouldn t report any suspicious files "], "1590": [], "1589": [], "1584": [], "1582": ["one more error came         DEBUG Traceback most recent call last File usr lib  site packages rucio tests daemons Automatix.py line  in upload client.add dataset scope dsn scope name dsn name rules account account copies  rse expression rse grouping DATASET activity Functional Test meta meta lifetime dataset lifetime File usr lib  site packages rucio client didclient.py line  in add dataset lifetime lifetime dids files rse rse File usr lib  site packages rucio client didclient.py line  in add did raise exc cls exc msg RucioException An unknown exception occurred. Details u cx Oracle.DatabaseError ORA  Primary key constraint DELETED DIDS PK violated nORA  at ATLAS RUCIO.CHECK DID UNIQUENESS line  nORA  error during execution of trigger ATLAS RUCIO.CHECK DID UNIQUENESS n ", "I m not sure what is the best thing to do. The patch that modifies the output of the upload method has comment rsemanager dirty dirty fix of upload method return  . So I would like to know if the dirty fix will stay or will it be replaced by something cleaner. ", "Need to check with TomasJavurek and TWAtGH "], "1579": ["hahahannes can you please have a look on this. I expect it should be an easy fix. ", "It is I was just doing a pull request ", "i just tried this in various ways and can t reproduce it. rucio v whoami given client cert data rucio  USER PROXY doesn t exist        ERROR Cannot authenticate. Details  authentication failed respectively Details Cannot find a valid  proxy not in tmp    USER PROXY not set and  proxy not set in the configuration file. or        ERROR PEM routines PEM read bio no start line SSL routines SSL CTX use certificate file PEM lib really interested in that pull request ", "Ahh sorry \ud83d\ude04 Thanks a lot! "], "1578": ["Discussed in Dev Meeting on We should do it like planned. "], "1569": [], "1568": ["I ll try to repeat it. ", "Behaviour noticed from August. ", "I was able to repeat the problem on lxplus arcproxy S atlas while arcls L rucio rucio lb prod.cern.ch replicas     do sleep  done wait until it exits with ERROR Failed to obtain information about file No such file or directory HTTP error when contacting server Not Found To get more debug info you can add d DEBUG to arcls. ", "Ok I had this running now for quite some time several thousand requests both against server direct and through the loadbalance and I could reproduce it with arcls but not with curl. Just by chance is arcls using the GET replicas interface? This worked for me thousands of times while true do curl L si capath etc grid security certificates H X Rucio Account mlassnig H X Rucio Auth Token d dids scope   name   X POST grep HTTP done ", "Just to be precise the GET interface works as well for me using curl. ", "Both replicas SCOPE NAME GET and replicas list POST use the same API in the back. That should be the same. ", "do you mean POST replicas POST replicas list ? ", "Yes Will edit the comment. \ud83d\udc4d ", "the debug mode gives a bit more info DEBUG No security processing check requested for incoming DEBUG HTTP   OK DEBUG Date Thu  Oct     GMT DEBUG Server Apache DEBUG Access Control Allow Origin None DEBUG Access Control Allow Headers None DEBUG Access Control Allow Methods DEBUG Access Control Allow Credentials true DEBUG Access Control Expose Headers X Rucio Auth Token DEBUG Cache Control no cache no store max age  must revalidate DEBUG Cache Control post check  pre check  DEBUG Pragma no cache DEBUG X Rucio Auth Token DEBUG X Rucio Host voatlasrucio auth prod.cern.ch  DEBUG Content Length  DEBUG Keep Alive timeout  max  DEBUG Connection Keep Alive DEBUG Content Type application octet stream DEBUG Acquired auth token for DEBUG Module Manager Init DEBUG Loaded DEBUG Loaded usr  arc libmccsoap.so DEBUG Loaded usr  arc libmcctcp.so DEBUG Loaded usr  arc libmcctls.so DEBUG Loaded usr  arc libmccmsgvalidator.so VERBOSE Trying to connect rucio lb prod.cern.ch   DEBUG Loaded MCC tcp.client tcp DEBUG Loaded MCC tls.client tls DEBUG Loaded MCC DEBUG TCP client process called DEBUG No security processing check requested for outgoing DEBUG No security processing check requested for incoming VERBOSE Using cipher ECDHE RSA  SHA DEBUG Linking MCC tls.client tls to MCC tcp under empty DEBUG Linking MCC to MCC tls under empty VERBOSE Peer name DC ch DC cern OU computers CN rucio lb prod  VERBOSE Identity name DC ch DC cern OU computers CN rucio lb prod  VERBOSE CA name DC ch DC cern CN CERN Grid Certification Authority DEBUG No security processing check requested for outgoing DEBUG GET HTTP  Host rucio lb prod.cern.ch  Connection keep alive user agent ARC x rucio auth token DEBUG No security processing check requested for incoming DEBUG HTTP   Not Found DEBUG Content Type text plain charset utf  DEBUG X Content Type Options nosniff DEBUG Date Thu  Oct     GMT DEBUG Content Length  ERROR Failed to obtain information about file No such file or directory HTTP error when contacting server Not Found It corresponds to while true do curl L si capath etc grid security certificates H X Rucio Account RUCIO ACCOUNT H X Rucio Auth Token X GET grep HTTP sleep  done which doesn t fail with a  . One difference is that a token is acquired for each operation. ", "One issue is with the server instance on  supposed to get  of the load arcls rucio rucio  int xxxxx.cern.ch  replicas     ERROR Failed to obtain information about file No such file or directory HTTP error when contacting server Not Found Maybe some caching are done in the arc clients making that worst. ", "Fixed now on kubernetes side. "], "1561": [], "1560": ["I think we should just change the switch to no resolve archives. If not set resolving is true by default. ", "maybe shorter just no archives ? ", "Hmm it might be a bit confusing as one could assume that no archives means there are not archive replicas in the result. Although to some extend no resolve archives could be interpreted the same "], "1559": [], "1555": [], "1554": ["Duplicate "], "1551": [], "1550": ["Not so simple. Imagine a file is marked as temporarily UNAVAILABLE and there is only one other copy of this file that is declared BAD to Rucio in the meantime. In that case the necromancer won t see any AVAILABLE copy of the file and will set a Epoch tombstone on all the UNAVAILABLE replicas. ", "Yes it s a quite heavy change touching all systems. That s why I think it would have to be a new TEMPORARYUNAVAILABLE state so it is obvious to other daemons that this is not a normal UNAVAILABLE replica. "], "1543": ["That should be fine. I think how JSON is implemented in sqlalchemy is that if the database does not support it it just creates a blob or big varchar thus we can switch to this even before  is available you just can t use the json functionality. I think it should be fine to have this in the next feature release. "], "1535": ["Solved by  "], "1532": [], "1529": [], "1528": [], "1527": [], "1524": [], "1523": ["Reminder For ATLAS we need to remove the puppet monkey patch were we update usr lib  site packages  contrib pyopenssl.py "], "1522": [], "1516": [], "1512": ["This dataset is only available at CERN PROD DERIVED which has the istape attribute set to True . Download is not allowed from tape endpoints by non admin accounts. Because of this no file is considered for download. Anyway this is probably already fixed with  Needs to be checked though. ", "Thanks for the quick reply! Yes I should have said that I figured out this much myself as well. I think the output is still wrong and should be fixed. Best Christian ", "With  this case is checked and the NoFilesDownloaded exception is thrown. The CLI exit with a return code !  There is no error message though. ", "TWAtGH Can you add an error message for this case? ", "Yes I m currently working on the downloadclient anyway "], "1506": ["After a closer look I think it s exactly the same problem as  ", "Ok I will close this one as a Duplicate of  then. "], "1505": ["Here are two files which are not  compatible modified lib rucio rse protocols gfal.py try import urlparse except ImportError from urllib import parse as urlparse modified lib rucio rse protocols protocol.py try import urlparse except ImportError from urllib import parse as urlparse try from ConfigParser import NoOptionError NoSectionError except from configparser import NoOptionError NoSectionError ", "Hi hahahannes can you please prepare some kind of mini proposal for the Dev meeting in  weeks  Thanks \ud83d\ude04 ", "As discussed in the meeting on We should tag the files in the header if they are  compatible. This script should be added as a separate test in the travis test matrix. If a file which is already tagged  compatible fails the pylint test the test should fail. ", "List of  compatible files directories according to the command pylint  d no absolute import path I removed warnings about missing absolute Only the files in bin are compatible when it is activated. Not sure if this mandatory for  ? bin lib rucio client accountclient.py lib rucio client accountlimitclient.py lib rucio client baseclient.py lib rucio client client.py lib rucio client configclient.py lib rucio client lifetimeclient.py lib rucio client lockclient.py lib rucio client metaclients.py lib rucio client objectstoreclient.py lib rucio client pingclient.py lib rucio client replicaclient.py lib rucio client rseclient.py lib rucio client ruleclient.py lib rucio client scopeclient.py lib rucio client subscriptionclient.py lib rucio client touchclient.py lib rucio client uploadclient.py lib rucio api account.py lib rucio api account limit.py lib rucio api authentication.py lib rucio api config.py lib rucio api credential.py lib rucio api heartbeat.py lib rucio api identity.py lib rucio api lifetime exception.py lib rucio api lock.py lib rucio api meta.py lib rucio api permission.py lib rucio api replica.py lib rucio api request.py lib rucio api rse.py lib rucio api rule.py lib rucio api scope.py lib rucio api subscription.py lib rucio api temporary did.py lib rucio common config.py lib rucio common constants.py lib rucio common exception.py lib rucio common log.py lib rucio common rse attributes.py lib rucio common schema atlas.py lib rucio common schema cms.py lib rucio common schema icecube.py lib rucio common schema generic.py lib rucio common dumper consistency.py lib rucio core config.py lib rucio core distance.py lib rucio core identity.py lib rucio core lock.py lib rucio core message.py lib rucio core nongrid trace.py lib rucio core quarantined replica.py lib rucio core rse selector.py lib rucio core scope.py lib rucio core staging.py lib rucio core temporary did.py lib rucio core transfer limits.py lib rucio core volatile replica.py lib rucio daemons sonar  distribution distribution daemon.py lib rucio daemons  utils dataset cache.py lib rucio daemons  utils expiring dataset cache.py lib rucio daemons  utils expiring list.py lib rucio daemons  utils popularity.py lib rucio daemons  utils timeseries.py lib rucio daemons  collectors agis.py lib rucio daemons  collectors free space.py lib rucio daemons  collectors jedi did.py lib rucio daemons  collectors mock did.py lib rucio daemons auditor hdfs.py lib rucio db sqla constants.py lib rucio db sqla sautils.py lib rucio db sqla types.py lib rucio db sqla migrate repo env.py lib rucio rse rsemanager.py lib rucio rse protocols cache.py lib rucio rse protocols dummy.py lib rucio rse protocols  lib rucio rse protocols gsiftp.py lib rucio rse protocols mock.py lib rucio rse protocols ngarc.py lib rucio rse protocols posix.py lib rucio rse protocols  lib rucio rse protocols sftp.py lib rucio rse protocols xrootd.py lib rucio transfertool  myproxy.py lib rucio web ui main.py lib rucio web rest webpy  config.py lib rucio web rest webpy  heartbeat.py lib rucio web rest webpy  lifetime exception.py lib rucio web rest webpy  meta.py lib rucio web rest webpy  ping.py lib rucio web rest webpy  request.py lib rucio web rest webpy  scope.py lib rucio web rest flaskapi  config.py lib rucio web rest flaskapi  heartbeat.py lib rucio web rest flaskapi  lifetime exception.py lib rucio web rest flaskapi  lock.py lib rucio web rest flaskapi  meta.py lib rucio web rest flaskapi  ping.py lib rucio web rest flaskapi  request.py lib rucio web rest flaskapi  scope.py ", "I see I think it is a bit too clustered to do this based on directories. It might be better if we add a tag such as  COMPATIBLE in the header of the file below the authors for example and the test essentially only tests if these tagged files stayed compatible. We just have to be consistent in the future to keep tagging the newly compatible files. "], "1497": [], "1494": ["The PR for rsemgr.upload was merged  rsemgr.download will be removed since it isn t used anymore. ", "Ok Closing with reference to  "], "1490": ["We should check if there is a common implementation to do this. "], "1487": ["We have hole somewhere in rsemgr upload method. What is certainly wrong that we don t have same structure on return once list and once dictionary The case of upload to CERN PROD ES leads to the empty list ret so it returns gs True but this should actually never happen. "], "1486": [], "1485": [], "1484": [], "1483": ["suggestion from xcache devs   glfn name atlas rucio mock another.one.zip?xrdcl.unzip zippedfile   glfn url location APERTURE MOCK LOSSXLHIQX domain zip priority  client extract false root root.aperture.com  test chamber mock   another.one.zip?xrdcl.unzip zippedfile   url ", "Hi Mario Indeed that was what I recommended today for Wei conundrum. In general we want to know the base file not the archive within it as we will be cachin the whole zip file. That said there are a lot of devious issues involved here especially when dealing with intermediate servers along the way. So I can t say we have addressed all the issues. However as a first take what you say is the most reasonable option. Andy On Wed  Aug  Mario Lassnig wrote suggestion from xcache devs   glfn name atlas rucio mock another.one.zip?xrdcl.unzip zippedfile   glfn url location APERTURE MOCK LOSSXLHIQX domain zip priority  client extract false root root.aperture.com  test chamber mock   another.one.zip?xrdcl.unzip zippedfile   url You are receiving this because you were mentioned. Reply to this email directly or view it on GitHub ", "This was fixed automatically through  "], "1480": [], "1479": ["add vhost support ", "I would probably be able to provide a pull request for this although my ability to test anything is limited since I m still trying to get it working at all . ", "We can test and confirm it. So if you want to provide the PR please go ahead. ", "I have this working at least for the hermes and cache daemons which are the only ones I m currently using . But I have a question about  the existing implementation doesn t use it since it only looked up the DNS A record. But getaddrinfo will return both unless instructed otherwise. For the time being I ve forced  to keep the behaviour consistent but that s not very future proof. Would it be better to use the  addresses if available? I see atlas mb.cern.ch does have them. ", "I think there s standard policy and procedure we can follow Use  if that s available. If not use  Extra credit If both  and  are available then after waiting for  for the  connection to finish opportunistically try a  connection as well. Take the first one to successfully connect. I think I ve only seen this recommendation executed in big codebases such as browsers. I saw at the GDB today that  of LHC data is transported using  this suggests the simple case of use first record available is probably fine for most R D networks. ", "After having so much trouble with  on the CERN network I consider the extra credit option as mandatory unfortunately. ", "One problem in this case is with the multiple responses to the name lookup I don t think it s possible to match the  and  addresses for the same host. So you could easily end up connecting twice to the same broker which doesn t seem like a good idea. How about a disable  option? it d be simpler to implement. ", "That s perfectly viable. If we feel the need to support  properly down the line we can always do it. ", "illingwo did you manage to progress on this? if you already have some lines of code written I can integrate them during our coding camp next week ", "I never got around to doing the  part. I can turn what I do have  only with the system resolver into a pull request. ", "Ok cool! I ll take care of the  part then later on. Thanks! "], "1476": ["I have updated  and will close this ticket. Right now the clients are compatible with Python  but fail with  . ", "Duplicate of  "], "1475": ["The attack vector is for Elgamal which we are not using. One option for pycrypto is to change to To be investigated. ", "We should be able to remove pycrypto. It s a secondary dependency and  will automatically fallback to pyOpenSSL which we have anyway. "], "1470": ["TWAtGH during the creation of the PR we saw that UploadClient and DownloadClient are not inherited from BaseClient. Is there a special reason for this? ", "I don t really remember to be honest. But looking at it now the download and upload client don t really fit into the api like the other classes do. The two classes only need to use the client and don t do rest calls themselves. I guess that were my thoughts when creating these classes. I probably wasn t aware of how the client object is meant to be used. However changing this now will be a little bit challenging since the init calls are getting some custom parameters and should be downward compatible. ", "I would suggest to close this one. As I ve written above adding these classes to the main client is not necessary and could be quite problematic "], "1464": [], "1463": ["I think this is just a typo. I don t think there is a verison operator in setuptools. ", "Which version of setuptools are you using? ", "Setuptools  although the failure doesn t involve setuptools at all. There is actually a triple equals operator but I m not sure if it s expected to work in this context or not see ", "Yes I just saw the operator is not directly described in setuptools but apparently has been patched in in version  I cannot re produce this on my machine neither with  nor with  which we use on our machines. ", "It works with pip  that comes with RH  but I m not sure it s patched more that the older version of pip is simpler. You should be able to reproduce with a virtualenv illingwo  python mvirtualenv testdir New python executable in cloud login illingwo testdir bin python illingwo  testdir . bin activate testdir illingwo  testdir pip install upgrade pip Collecting pip Using cached Installing collected packages pip Found existing installation pip  Uninstalling pip  Successfully uninstalled pip  Successfully installed pip  testdir illingwo  testdir pip install rucio Collecting rucio Exception Traceback most recent call last File cloud login illingwo testdir lib  site packages pip internal basecommand.py line  in main status self.run options args File cloud login illingwo testdir lib  site packages pip internal commands install.py line  in run resolver.resolve requirement set File cloud login illingwo testdir lib  site packages pip internal resolve.py line  in resolve self. resolve one requirement set req File cloud login illingwo testdir lib  site packages pip internal resolve.py line  in resolve one set req to install.extras set dist.extras File cloud login illingwo testdir lib  site packages pip vendor pkg resources init .py line  in extras return dep for dep in self. dep map if dep File cloud login illingwo testdir lib  site packages pip vendor pkg resources init .py line  in dep map self. dep map self. compute dependencies File cloud login illingwo testdir lib  site packages pip vendor pkg resources init .py line  in compute dependencies reqs.extend parse requirements req File cloud login illingwo testdir lib  site packages pip vendor pkg resources init .py line  in parse requirements yield Requirement line File cloud login illingwo testdir lib  site packages pip vendor pkg resources init .py line  in init raise RequirementParseError str e RequirementParseError Invalid requirement parse error at  ", "Confirmed. \u2705 related to pkg ressources in the pip version. I think we can change the to a that should be sufficient to fix this since we do not need that strict comparison anyway. "], "1457": [], "1455": [], "1454": [], "1446": ["I think I ve found them all. rucio core account counter.py also uses  on a uuid but the concat function coerces its arguments to text anyway so that one should be okay. "], "1443": [], "1434": [], "1433": [], "1432": [], "1430": ["Proposing to close as duplicate of  "], "1427": ["Another problem with this is that the download API is currently designed to allow to specify parameters per DID e.g. rse expression schemes resolve archives while list replicas takes those arguments per call. Solutions are  Merge the DIDs by the parameters to reduce the list replicas calls to a minimum  Create a new method in the API that is more strict in specifying the parameters but fits to list replicas  Drop the support of specifying parameters per DID in the current download API  Change list replicas so that it is possible to specify parameters per DID ", "Change for list replicas Return multiple fields in metalink parent did parent Return single list field in json parents   Convention is not to resolve all parents only the ones the user is requesting. "], "1423": [], "1422": ["Increased the priority to high. It can potentially generate high load on the servers. ", "What was the reason for changing this to ALL again? I remember that it was on COLLECTION in the beginning? "], "1415": ["rucio download metalink my  scope name "], "1412": ["Decided against. Spikes memory and network on server node. Will provide palnilsson a function to generate metalink pilot side from the list replica reply. "], "1411": [], "1410": [], "1405": [], "1404": ["suggestion allow tape "], "1394": [], "1390": ["Still does not work ", "So this probably needs to wait until pycurl is an extra in the  rest api package. For now although only the requirements.readthedocs.txt should be used it seems like rucio as a package is pulled in as well which then requires pycurl via  rest api crashing readthedocs build. Needs a version  "], "1379": ["ok confirmed the bug. this is not properly taking into account that there are two root doors one internal one external and thus takes the higher priority one of the internal door thus jumping in front of srm when it should just ignore it and instead take the priority of the external door which is left out completely at actual priority  since there already is a bad root priority assigned "], "1378": ["Would be solved by "], "1377": ["TWAtGH isn t this fixed with  ", "No.  will fix this. "], "1376": [], "1375": [], "1370": [], "1367": [], "1366": [], "1361": ["This doesn t seem to be my fault with python  CannotAuthenticate Cannot authenticate. ", "Although it can be that certain activities can be processed only by certain accounts and travis had not the rights to run this activity? "], "1360": ["TomasJavurek or TWAtGH can one of you please have a look on this? ", "Also adding nikmagini as he had a look on the email already \ud83d\ude04 ", "We werent able to reproduce this and the problem is probably solved with  because the upload code was completely overhauled. So I m closing this issue. ", "I got the problem with  In my case It was due to the rsemgr sending back a different value then the expected one. I vaguely remember a dict with True instead of something else. One side remark The logging in the debug mode of the traceback error showing where the error is exactly happening in the upload function is missing and will be more than useful. ", "Good call thanks. I ve fixed it with  There were also another bug. "], "1359": [], "1355": [], "1354": ["I m going to disable multi threaded download in case that multiple files should be downloaded from the same archive guess it s not too unlikely for datasets? How shall we handle the archive file? I have different approaches in mind A  download archive  extract file  go to  for next file B  download archive  extract file  delete archive  go to  for next file C  download archive if it doesn t exist  extract file  go to  for next file  delete all archives Or any of these approaches with downloading archives into tmp ", "How about never downloading the archives into tmp ?  Sort all files needed by archive name. This way you only download each archive once.  If  module is present For each archive needed read the last  of the archive parse out the central directory of the zipfile then read out only the bytes corresponding to files you need. If  module is not present download whole archive to the output directory then open with python unlink the file and start extracting the files needed. Should be relatively straightforward to avoid downloading the whole archive in the end ", "Do I have the archive name? This ticket should only handle the case when the protocol cannot stream the single file from an archive. I was told to use an external program like unzip in this case.  ", "Even if I had the archive name sorting would not be that easy because this would probably break the PFN order ", "Yes only the actual local extract case should be handled here streaming the content of the uncompressed zip is already handled in list replicas. The result of list replicas be it metalink or not has the PFN of the archive. Right now the loop is going logical file by file and then do the actual physical download. What you can do Basically Brians suggestion is to take the ones which only have replicas saying client extract True out remove the duplicates from this list and only download these replicas. Afterwards you extract the files wanted re adding the logical files whose duplicate replicas you removed . I know this breaks the current simple one loop solves it all concept but it shouldn t be all too complicated to add. ", "Can t list replicas return a file that has some PFNs with client extract True and some with False? So if its always either True or False per file it should make things easier. But in this case client extract should be a file attribute ", "Otherwise I would be afraid of cases like metalink  url client extract True prio  same archive pfn url url client extract False prio   url file  url client extract False prio   url url client extract True prio  same archive pfn url file metalink You say in such cases I should skip the prio  pfn of  ? ", "TWAtGH right now client extract is set per file current decision is based purely on if protocol root ", "It should be set per replica. It s a valid case to have a file which has normal replicas but is also a constituent of an archive. Thus some replicas of this file would have client extract True some would not. "], "1353": [], "1352": ["mlassnig not sure if you have seen this ticket would be great if you can have a look. ", "ok figured it out. zipped replicas which are not part of the rse expression are not correctly removed in the response to the client. "], "1346": [], "1338": [], "1335": [], "1334": ["The order of this configuration was changed some time in the past. So what s currently in the demo rucio.cfg is wrong. Changing the config as file as you proposed will fix it. No need to fix the config reader. "], "1329": [], "1324": [], "1323": [], "1318": [], "1317": [], "1310": [], "1307": [], "1306": [], "1305": ["The associated RSE for resolved archives is not correctly updated it sets the RSE of the existing replica instead. "], "1302": ["How far do we have to downgrade so it doesn t ask for the specific requests version? ", "Just the previous versoin which is from  but it s not on PyPI anymore. This is starting to get tricky. "], "1299": [], "1298": [], "1295": [], "1288": [], "1283": [], "1282": [], "1281": ["scope ? ", "Scope might make sense I ll have a look. ", "Up ", "Can we increase the priority of this ticket ? It s mainly for AMI "], "1276": [], "1273": ["We should really think about the syntax in filter In the meanwhile we could add a exclude empty ", "Has there been progress on this? ", "Hi Alex no news on this. These days development time is thin as many people are on vacation and there were also some conferences and meetings in the last weeks. We will keep you posted in this ticket but I don t have a specific ETA yet. ", "Understandable. Thanks for the reply. ", "Maybe it would be useful to implement the possibility to filter more generally. For example size.gt to filter out containers with a size greater than  And of course size.lt for less than. shell rucio list dids user bla filter size.gt  type CONTAINER "], "1272": [], "1269": [], "1268": [], "1263": [], "1262": [], "1251": ["cserf mlassnig I can take the lead on this one if you want? ", "This is more of a deployment issue since from a code point of view a distributed memcached can be used. Moving this to ATLAS jira. "], "1250": [], "1249": ["mlassnig nikmagini TomasJavurek can one of you take over this ticket? It would actually be nice if we can get this into  on Monday . If nobody has time I can take it as well. "], "1241": ["Hmm this is basically the same as  and maybe also partly  we should close some of them. ", "Actually  can stay open but I would close this one and leave  "], "1240": [], "1228": [], "1227": [], "1226": ["Hi can you also add a chart manifest to show how to configure the client using the rucio rucio client image. I tried and failed by bind mounting a rucio.cfg copying the ca.crt from ATLAS cvmfs atlas.cern.ch repo ATLASLocalRootBase   rucio clients  etc ca.crt into RUCIO HOME etc ca.crt setting a providing a proxy file and setting  USER PROXY RUCIO AUTH TYPE and RUCIO ACCOUNT env vars tree . \u251c\u2500\u2500 etc \u2502 \u251c\u2500\u2500 ca.crt \u2502 \u2514\u2500\u2500 rucio.cfg \u2514\u2500\u2500   docker run rm it v PWD tmp e  USER PROXY tmp   e RUCIO AUTH TYPE  proxy e RUCIO ACCOUNT lheinric rucio rucio clients latest bash some files get downloaded but I also see some issues like        INFO Thread   File group.phys exotics group.phys   trying from CA WATERLOO  SCRATCHDISK        WARNING The requested service is not available at the moment. Details An unknown exception occurred. Details Could not open source Failure Neon Server certificate verification failed issuer is not trusted after  attempts ", "The last error is more a storage issue. If you get rucio ping working it s most likely fine. Otherwise there is the image rucio clients atlas If you are willing to test it we can update it and update the instructions on dockerhub as well ", "Hi vingar yes I m very happy to test an ATLAS use case. For the ATLAS image would you be willing to also add the  auth e.g. voms proxy init etc such that inside the container one could do voms proxy init voms atlas rucio download In Kubernetes this could then be run as part of a initContainer ", "yes that s doable. But to do voms proxy init voms atlas one should mount the .globus directory as well or the variables  USER CERT  USER KEY. ", "yes my idea was to mount this via secrets. e.g. apiVersion  kind Secret type Opaque data gridpw host.cert privkey.pem metadata name atlas auth namespace default "], "1223": ["Pinging hahahannes "], "1222": ["This is not a bug but a missing feature. Rucio download does respect the order but it s not able to determine if lan or wan should be used. Instead it always uses the default wan . "], "1219": [], "1218": ["So basically list account usage but instead of per user account per rse? ", "Yes ", "Hi is it available in lxplus? how can I use it? Thanks Best Jose ", "It is as TESTING so you have to install the testing release with lsetup There is a new option for list rse usage show accounts ", "It is as TESTING so you have to install the testing release with lsetup There is a new option for list rse usage show accounts Great!! Thanks! Just a detail is there a way to get the response in a different format? Thanks again Jose ", "I am sure we can change the formatting a bit. What would be your suggestion? ", "I am sure we can change the formatting a bit. What would be your suggestion? I was just thinking about a line per user instead. I can parse the current output easily but just wondering if that was possible. Thanks again ", "Oh I thought it is already  line per account but I just tried it and saw that it is multiple lines. We will fix that and make it one line per account. Will be fixed in  "], "1216": [], "1215": [], "1214": ["The wrong traces were sent by download method in downloadclient that is going to be deprecated the other method download dids should send the traces correctly. Only missing piece is to add correct format to method download pfns where the traces are not introduced at all. This is not problem for now because pilot is sending traces alone the switcher that distinguish between copytools is not active . The possibility of propagating the trace created by pilot to the rucio clients is being discussed. ", " Done  still missing ", "Any update? "], "1208": ["TWAtGH FYI "], "1204": [], "1195": ["I ll be taking this up ", "I m currently still having troubles getting Oracle  reliably running on travis i.e. you will need to add a switch in the code to enable and disable this feature based on the version of Oracle. So if the version is  then disable json support otherwise enable it. This will be necessary anyway since our production DB is still  and we also would run tests both against  and  "], "1193": ["Duplicate of  Closing "], "1190": [], "1185": ["berghaus You recently did some changes in this code I think. Can you maybe have a look on this issue? \ud83d\ude03 "], "1184": [], "1179": [], "1177": [], "1176": ["This is fixed with Rucio  ", "Can we confirm that this works correctly with the pilot? The test version of rucio on the ALRB is  so this should be possible. "], "1173": ["PS I will not push it as far as having the email too but that is yet another separate action if I don t know the account I have to go to the CERN directory to find the user email. Sometimes the user doesn t exist because they have left for good. ", "Hi Alessandra I think the Dump you actually want to use is the Datasetlocks dump which should give you exactly this information. You might have to filter it a bit though as in case where multiple rules exists for the dataset you will have multiple entries. I am not sure at the moment how to access this dump maybe tbeerman can comment? ", "Hi do you mean the dataset consistency one? All in all there is a Dataset per RSE datasets consistency with the account name in it and a Dataset per RSE Incomplete datasets per rse without account name. Is there a reason why they cannot both have the account name? The dump with incomplete datasets contains everything that is on disk I assume so it is a better choice to check what is on disk at sites and who owns them. It also contains the creation and expiry date which I need to know what is the oldest stuff. While the datasets consistency one contains other things that are not particularly useful to a system administrator. ", "other things that are not particularly useful to a system administrator unless those number I see in datasets consitency are unix epochs i.e. create date guid ? expiry date ? could be either number of replicas but more likely  secondary  primary? ", "Ok so while consistency datasets is actually useful and more easily scripted it does only account for a fraction of the TB in the RSE. aforti  awk sum   END print sum UKI NORTHGRID MAN HEP LOCALGROUPDISK.txt sort   aforti  awk sum   END print sum UKI NORTHGRID MAN HEP LOCALGROUPDISK  sort   I need to have all the files the account name and if  and  stand indeed for primary and secondary that would be useful too as is unix epochs. In short consistency datasets format but for the datasets in datasets per rse Sorry for all the replies to myself. I know it is annoying trying to quit the habit . ", "The different dumps are coming from two different DB tables and only one has the account information dataset locks . For every rule on a dataset a dataset lock is created with the account that requested the rule. But the lock is only the storing the logical size and the dump only includes datasets which have an OK rule. This one cannot be used to check disk usage and it s more an overview of which user requested what to be replicated. The other tables collection replicas keeps track of the actual number of files and the size of a dataset replica at an RSE with or without a rule and rule. But there is no account here as there can also be replicas without a rule secondaries . So this one will have the correct space usage number but not the accounts. What I can do now is to join both dumps to get the account information from the dataset locks and the actual space usage from collection replicas. But this will then also mean that there will be replicas with and account. But it should be minimal for LOCALGROUPDISK as replicas should be deleted directly after the rule is deleted. ", "Hi Thomas thanks a lot will you keep also the dates? I think they are created expiration and last access if possible it would be useful to keep them all also knowing which is which without guessing . On      Thomas Beermann wrote The different dumps are coming from two different DB tables and only one has the account information dataset locks . For every rule on a dataset a dataset lock is created with the account that requested the rule. But the lock is only the storing the logical size and the dump only includes datasets which have an OK rule. This one cannot be used to check disk usage and it s more an overview of which user requested what to be replicated. The other tables collection replicas keeps track of the actual number of files and the size of a dataset replica at an RSE with or without a rule and rule. But there is no account here as there can also be replicas without a rule secondaries . So this one will have the correct space usage number but not the accounts. What I can do now is to join both dumps to get the account information from the dataset locks and the actual space usage from collection replicas. But this will then also mean that there will be replicas with and account. But it should be minimal for LOCALGROUPDISK as replicas should be deleted directly after the rule is deleted. \u2014 You are receiving this because you authored the thread. Reply to this email directly view it on GitHub or mute the thread Respect is a rational process. Fatti non foste a viver come bruti ma per seguir virtute e canoscenza Dante For Ur Fascism disagreement is treason. U. Eco But but but her emails covfefe! ", "Yes I ll keep the dates. But you don t really have to guess anything it is all described at ", "Hi Alessandra the new dump is now available at The new fields are RSE scope name owners size created at updated at accessed at rule ids and state There can be multiple owner separated by if there are multiple rules on the same dataset. Then there will also by multiple rule ids. The rule ids will be in the same order as the owners. If there is no rule the owner will be None and the rule ids will be empty. But for LOCALGROUPDISK there should only be very few with this. Please let me know if this is now ok for you. ", "Hi Thomas looks better! thanks Some of the entries have only two dates. Do you know which one might be missing every now and then? For example UKI NORTHGRID MAN HEP LOCALGROUPDISK archive    Zmumu.merge.DAOD         bhauruth             A UKI NORTHGRID MAN HEP LOCALGROUPDISK     Main.merge.DAOD      jcrane bhauruth                   A ", "Hi Alessandra created at and updated at will always be set but accessed at can be empty if this dataset replicas was never accessed. ", "ok thanks. ", "Hi Thomas I was concentrating on the format and only now realised the data is not complete. There are  in the RSE but if I sum the size of what is in the dump I only get a tiny fraction  . Was this just a test? awk sum  END print sum  UKI NORTHGRID MAN HEP LOCALGROUPDISK.txt  thanks cheers alessandra ", "You have a typo in you awk code. It should be awk sum  END print sum  ", "Thanks. ", "For me we can close the ticket. I now have everything I need to know who s using the space and how old are their data and if it was accessed or not plus their rules id if I need to take action. thank you. ", "Hi together the in tree documentation still lists the old format without the newly available fields should I open a new ticket about this? Cheers Oliver ", "Hi olifre No let s use this ticket. tbeerman can you please update the template. Cheers "], "1172": ["I discussed this a bit with Tobi already yesterday can I have a look? ", "Yes! Thanks a lot TomasJavurek "], "1169": [], "1168": [], "1163": [], "1160": ["Can t reproduce? rucio add rule  hi NTUP    NDGF  MWTEST DATADISK        ERROR RSE excluded due to write blacklisting. Details RSE excluded due to write blacklisting. and against  rucio H add rule  hi NTUP    NDGF  MWTEST DATADISK        ERROR RSE excluded due to write blacklisting. Details RSE excluded due to write blacklisting. ", "Identified. when raising DuplicateRule the error is not passed to the exception "], "1159": [], "1158": ["I think for this it should be fine if filename gets added to the schema. This is related to the change Brian did to be able to have filenames different than the name I am just wondering why this only shows up now or if basically no rucio upload works since this? mlassnig vingar Can one of you please fix this so we have it in the release next week? ", "TomasJavurek TWAtGH isn t it fixed ? ", "I tried to reproduce this but can t. Closing "], "1156": [], "1155": [], "1149": [], "1144": [], "1143": [], "1138": [], "1137": ["The code for bin rucio TWAtGH will add. "], "1128": [], "1127": ["When is this going in production? UI is still on  I thought this would be quicker. ", "The usual release cycle is  weeks. However we did not deploy  due to a problem thus we are still on   will be deployed tomorrow. "], "1118": [], "1117": [], "1116": [], "1115": [], "1104": [], "1093": [], "1092": [], "1086": [], "1083": [], "1080": [], "1078": [], "1077": [], "1060": [], "1057": [], "1056": ["Hi cserf rucio list file replicas already has the expression option to pass RSE expressions does it do what you need? Cheers N. ", "Yes that s it. Closing the ticket. "], "1055": ["See for an example implementation of a timeout wrapper ", "As discussed in the development meeting on Investigate if it is a possibility to kill e.g. gfal threads via a watcher thread. ", "Maybe its worth to check if you can use the gfal context object in a watcher thread to abort the download. The  object has a method called cancel that cancels running operations . import   unbound method  ", "Is there any update on this? It s quite high priority as that happens quite often on the grid. ", "Maybe its worth to check if you can use the gfal context object in a watcher thread to abort the download. The  object has a method called cancel that cancels running operations . import   unbound method  Thanks for the tip unfortunately I Tried  on a root file transfer from EOS but it blocks until the transfer is completed so it looks useless for cancelling a stuck transfer in general From what I see async cancel is implemented only for gridftp I ll double check with gfal devs ", "From what I see async cancel is implemented only for gridftp I ll double check with gfal devs On the other hand gfal cancel worked fine on gridftp and transfers in my tests. Checking with gfal devs what s wrong with xrootd cancel. ", "Thanks Nikolo! Let s check with gfal devs about it but unless we cannot cancel consistently all or the at least the major protocols I think we have to look for another way. Let s discuss this in the dev meeting tomorrow. ", "Andrea Manzi commented that it looks like a gfal issue with xrootd cancel. Anyway for the other protocols it seems to work for now I ve submitted a pull request to implement it note currently it won t do anything due to a bug in propagation of transfer timeout in the client submitting a separate bug report "], "1054": [], "1051": [], "1048": [], "1045": [], "1044": [], "1040": [], "1039": ["This is on hold until a new version of  is released which supports pycodestyle  Only support for  at the moment. "], "1038": [], "1036": [], "1032": [], "1029": [], "1025": [], "1024": [], "1023": [], "1022": ["I am not sure if we really need to add this in the days of containers. The argument for sqlite was always that it is easy as you do not need to install mysql etc for quick development. But now in the days of containers the point is not really valid anymore. If the code runs fine on mysql postgres oracle it is fine I think. "], "1020": [], "1019": [], "1018": [], "1017": [], "1012": ["This is a duplicate of  ", "Perhaps we should create a second issue so we fix the  eventually and don t completely forget about them. ", "yes  "], "1009": [], "1007": [], "1005": [], "1004": ["This should probably be fixed by doing a MERGE INSERT into the history table. That should solve the issue. It s not high priority though \ud83d\ude04 Thanks for the report. "], "1001": [], "999": ["After some investigation it looks like this is actually a docker compose issue on my dev environment. See ", "After further investigation it seems that adding the correct docker py version to the virtualenv solves the problem. ", "Thanks Bruce. Should we expand this on in our documentation? ", "Hi mlassnig I m working on a PR just checking a few things smile ", "Cool thanks "], "997": [], "992": ["it s easier to do it for the daemons first as there are no tests using the clis. cf. "], "985": [], "976": ["It s in both? ", "tests "], "973": [], "965": [], "964": [], "963": [], "961": ["Additional one in the poller         ERROR Failed to query FTS info Traceback most recent call last File usr lib  site packages rucio daemons conveyor poller.py line  in poll transfers resps transfer core.bulk query transfers external host xfers  timeout File usr lib  site packages rucio core transfer.py line  in bulk query transfers fts resps  external host request host .bulk query transfer ids transfer ids timeout timeout File usr lib  site packages rucio transfertool  line  in bulk query raise Exception Failed to parse the job response s error s str jobs str error Exception Failed to parse the job response Response  error Expecting value line  column  char  ", "For the first one RequestNotFound should be raised instead of generic RucioException in transfer.py and the exception should be caught in common.py The second one should catch JSONDecodeError "], "954": [], "949": [], "943": [], "942": [], "941": [], "938": [], "935": [], "930": ["This works for me in Python  ", "not for me on  pip doesn t find the last version and that s why I relaxed the version requirement. "], "929": ["The port  is taken by the docker proxy application. I didn t see that before for me. This application might be the rucio one actually. If not one naive comment should be to stop the application taking the port  Did you try to list the running docker containers docker ps ? Maybe the rucio one has been already started which explains this conflict. ", "You could try changing the exposed port. Just change line  of etc docker demo docker compose.yml to something like   ", "I faced the same problem yesterday. A simple sudo service docker restart worked for me. ", "I used docker ps and found that other application was using the port  Stopping that app and restarting docker service resolved the issue. I did not try the option given by tbeerman but I think that would work too. Thank you all. "], "928": ["Closing this It s basically  "], "913": [], "912": [" I would like to take this up if possible. Where exactly do the release notes have to be added? ", "Part of the work is to replicate the changelog from here into individual rst files. I guess you can help on this but first  has to define the directory and file name conventions. ", "Hi I have already started with this and prepared a structure and kind of template. PR for that will come today If you would like to you could then fill all the missing release notes. I would be very thankful for that \ud83d\ude04 ", "Sure  I will wait for your PR then to get started. ", "The PR is added now. Let s wait a bit of some people have comments to the general structure etc. Once it is merged I think you can go through the github releases and generate the releases notes similar to the  I already created. You can use the tools generate release notes release rst script for the rst output Thanks ", "When I run the script it asks for a Github token file. How am I supposed to get that? From what I understand it is needed for authentication to use the Github API. ", "Go here and create a new one and put it in the .githubtoken file ", "Thanks mlassnig it s working now. everyone If I understand the task correctly I am to create rst files for ALL releases on If so I should also club clients webui releases in the same file. And do I create files for each milestone number or separate files for hotfix releases as well? ", "Yes essentially create one file for each release including webui only client only and hotfix releases . If you want you can create a subset of the files first and submit the PR so we can comment them so you won t have to change all your files if there are some repeating issues. \ud83d\ude04 ", " mlassnig vingar I have added a few files via PR  Is the formatting correct? I wrote a small script to create these files. Also I am unable to generate release notes for tags of type  How do I proceed? Also the release notes tool is not returning anything for releases like  Do I skip these or enter them manually? ", "Hi tushar  The formatting is fine \ud83d\udc4d Let me check what s broken with the release notes script. ", "Ah I just realised. For the releases where there was a hotfix release afterwards there are two releases in the repository two tags . A normal one such as  and a  However there is only one milestone the  that s why the generate release notes script is not returning anything. In this case just ignore the hotfix  ones and just create the notes for the normal ones. For certain release client releases such as  there is also no milestone these you can also ignore. ", "I added all the files but the travis ci test has failed. I have no clue why that happened as I only added  rst files. ", "This is a problem on the current master. Once solved a rebase will solve the errors on your PR as well. "], "910": ["Need confirmation for the mentioned link which contains the RSE expressions docs I would do the necessary change. ", "This is the correct page it should reference rse expressions.html it s an upper lowercase problem Thanks! ", "All right.! I found similar link error in other places created PR for this. "], "906": [], "886": ["Is there anyway I can propose just one request with all changes across multiple files? ", "Sure just add multiple changed files to the same pull request. ", "mlassnig Done. Sorry for the mix up before "], "875": [], "870": [], "867": [], "862": [], "850": [], "849": [], "847": ["No comment? Alright I ll try out what I am suggesting The other option I was able to think of is to create a separate function in the protocols that get the checksum. ", "Hi Frank sorry for the late reply. \ud83d\udc4d for the approach you are describing. Don t think we need the separate function in protocols. ", "Thanks Martin I ll get started with it "], "819": ["There is a general ticket tracking the  migration. You can use this for general tracking. ", "BTW I found that the six python module makes these kinds of things easier during the transition period It is also easier to find them later searching for six whenever the project is ready to go Python  only. ", "I think all the client files are now  compatible. hahahannes can you confirm ? ", "The last ones missing are the didclient fileclient  and the downloadclient. According to pylint there are just small changes needed. I can fix them. "], "811": [], "808": [], "807": [" Should be indeed datetime.timedelta hours   Should be tomorrow datetime.utcnow timedelta  "], "806": [], "805": ["First we ll also need to make sure that all the transfer logic is indeed in the transfer tool last I checked some FTS code had snuck into the conveyor itself. ", "Yes there are some parts especially related to activemq messages which are part of the conveyor directly. this needs to be properly disentangled and put into the transfertool. ", " is related to this. ", "This has been implemented in  "], "800": [], "799": [], "796": [], "793": [], "782": [], "769": [], "763": [], "762": ["this should go also in alembic and schema "], "761": [], "757": [], "754": [], "752": [], "749": [], "742": [], "738": ["I found same problem in core request.py if total processes   if session.bind.dialect.name oracle bindparams bindparam process number process bindparam total processes total processes  query query.filter text ORA HASH rule id total processes process number bindparams bindparams elif session.bind.dialect.name mysql query query.filter text mod  rule id s s total processes  process elif session.bind.dialect.name postgresql query query.filter text mod abs x  rule id text bit  int s s total processes  process if total threads   if session.bind.dialect.name oracle bindparams bindparam thread number thread bindparam total threads total threads  query query.filter text ORA HASH rule id total threads thread number bindparams bindparams elif session.bind.dialect.name mysql query query.filter text mod  rule id s s total threads  thread elif session.bind.dialect.name postgresql query query.filter text mod abs x  rule id text bit  int s s total threads  thread ", "In fact I found the same in many other places rucio  core grep  .py grep filter grep v name account counter.py query query.filter mod  concat account rse id s s total workers  worker number account counter.py query query.filter mod abs x  concat account rse id bit  int s s total workers  worker number message.py subquery subquery.filter mod  id s s total threads  thread message.py subquery subquery.filter mod abs x  id bit  int s s total threads  thread quarantined replica.py query query.filter mod  path s s total workers  worker number  quarantined replica.py query query.filter mod abs x  path bit  int s s total workers  worker number  replica.py  .with hint models.DataIdentifier INDEX dids DIDS PK oracle .filter condition request.py query query.filter text mod  rule id s s total processes  process request.py query query.filter text mod abs x  rule id bit  int s s total processes  process request.py query query.filter text mod  rule id s s total threads  thread request.py query query.filter text mod abs x  rule id bit  int s s total threads  thread request.py sub requests sub requests.filter text mod  rule id s s total processes  process request.py sub requests sub requests.filter text mod abs x  rule id bit  int s s total processes  process request.py sub requests sub requests.filter text mod  rule id s s total threads  thread request.py sub requests sub requests.filter text mod abs x  rule id bit  int s s total threads  thread rse counter.py query query.filter mod  rse id s s total workers  worker number rse counter.py query query.filter mod abs x  rse id bit  int s s total workers  worker number rule.py  bytes  session.query  models.RSEFileAssociation.bytes  .filter models.RSEFileAssociation.scope lock.scope temporary did.py query query.filter text mod abs x  path bit  int s s total workers  worker number  transfer.py query query.filter text mod  rule id s s total processes  process transfer.py query query.filter text mod abs x  rule id bit  int s s total processes  process transfer.py query query.filter text mod  rule id s s total threads  thread transfer.py query query.filter text mod abs x  rule id bit  int s s total threads  thread transfer.py sub requests sub requests.filter text mod  rule id s s total processes  process transfer.py sub requests sub requests.filter text mod abs x  rule id bit  int s s total processes  process transfer.py sub requests sub requests.filter text mod  rule id s s total threads  thread transfer.py sub requests sub requests.filter text mod abs x  rule id bit  int s s total threads  thread ", "Hi we are usually testing against PostgreSQL  this looks like a backward incompatibility. Can you try to apply your fix and run the test suite to see if it fixes your problem? The automatic checker in Travis will do the same for  so we can immediately see if there s a regression. ", "I applied the change to the request.py and transfer.py and tested it and it works fine with Postgres  ", "imandr can you make a pull request please? thanks! tools create patch branch  database postgres incompatibility emacs git add git commit tools submit pull request ", "Cf. ", "when I do tools submit pull request I get this rucio  rucio cern . tools submit pull request Traceback most recent call last File . tools submit pull request line  in module  warnings AttributeError module object has no attribute  ", "Maybe some incompatibilities in the environment. You can try from a virtualenv with python tools install venv.py source .venv bin activate ", "rucio  rucio cern python tools install venv.py which no virtualenv in usr  qt  bin opt puppetlabs bin usr kerberos sbin usr kerberos bin usr local bin usr bin usr local sbin usr sbin opt puppetlabs bin home rucio .local bin home rucio bin Failed to install virtualenv with curl. Creating venv via curl rucio  rucio cern source .venv bin activate bash .venv bin activate No such file or directory ", "Your hostmachine needs to have virtualenv installed either via pip apt get or yum that should solve it. ", "virtualenv is not in your environment. pip install virtualenv or get it from ", ".venv rucio  rucio cern . tools submit pull request Loading github OAUTH token ERROR No github token file found at home rucio rucio cern .githubtoken ", ".venv rucio  rucio cern ls la total  drwxrwxr x.  rucio rucio  Feb    . drwxr xr x.  rucio rucio  Feb    rw rw r .  rucio rucio  Feb    AUTHORS.rst drwxrwxr x.  rucio rucio  Feb    bin rw rw r .  rucio rucio  Feb    ChangeLog rw rw r .  rucio rucio  Feb    CONTRIBUTING.rst drwxrwxr x.  rucio rucio  Feb    doc drwxrwxr x.  rucio rucio  Feb    etc drwxrwxr x.  rucio rucio  Feb    externals rw rw r .  rucio rucio  Feb     drwxrwxr x.  rucio rucio  Feb    .git rw rw r .  rucio rucio  Feb    .gitignore rw rw r .  rucio rucio  Feb    ISSUE TEMPLATE.md drwxrwxr x.  rucio rucio  Feb    lib rw rw r .  rucio rucio  Feb    LICENSE rw rw r .  rucio rucio  Feb    MANIFEST.in.client rw rw r .  rucio rucio  Feb    MANIFEST.in.rucio rw rw r .  rucio rucio  Feb    MANIFEST.in.webui rw rw r .  rucio rucio  Feb     rw rw r .  rucio rucio  Feb    PULL REQUEST TEMPLATE.md rw rw r .  rucio rucio  Feb    pylintrc rw rw r .  rucio rucio  Feb    README.client.rst rw rw r .  rucio rucio  Feb    README.rst rw rw r .  rucio rucio  Feb    README.rucio.rst rw rw r .  rucio rucio  Feb    README.webui.rst rw rw r .  rucio rucio  Feb    requirements.readthedocs.txt lrwxrwxrwx.  rucio rucio  Feb    rucio lib rucio rw rw r .  rucio rucio  Feb    setup.cfg rw rw r .  rucio rucio  Feb    setup.py rw rw r .  rucio rucio  Feb    setup rucio client.py rw rw r .  rucio rucio  Feb    setup rucio.py rw rw r .  rucio rucio  Feb    setup webui.py drwxrwxr x.  rucio rucio  Feb    tools rw rw r .  rucio rucio  Feb    tox.ini rw rw r .  rucio rucio  Feb    .travis.yml drwxrwxr x.  rucio rucio  Feb    .venv ", "If you go here you can create a github access token Generate a new token just put all permissions for simplicity s sake and write the token to a new file .githubtoken That should solve it.  we need to add this to the development guidelines ", "I created the token .venv rucio  rucio cern ls lad .git drwxrwxr x.  rucio rucio  Feb    .git rw .  rucio rucio  Feb    .githubtoken rw rw r .  rucio rucio  Feb    .gitignore and now I am getting this error .venv rucio  rucio cern . tools submit pull request Loading github OAUTH token OK Checking if current branch is a patch feature hotfix branch OK Username for imandr Password for Pushing the feature patch hotfix branch to origin ERROR remote Permission to rucio rucio.git denied to imandr. fatal unable to access The requested URL returned error  ", "Did you fork the repository and clone your forked repository imandr rucio ? This looks like you tried to clone the upstream repository rucio rucio . Submission to the upstream repository can only be done through merge requests from private forked repositories. ", "No I did not initially but now I did. I went to GitHub and forked yours into mine. Then I cloned it git clone my forked Then I installed virualenv cd my forked python tools install venv.py Then I activated it source .venv bin activate Now I am trying to create the patch branch and I am getting new error .venv rucio  my forked tools create patch branch  database postgres incompatibility Switching to master Already on master Updating master Fetching origin Already up to date. Rebasing master fatal ambiguous argument upstream master unknown revision or path not in the working tree. Use to separate paths from revisions like this git command revision file Can t rebase to master. Unstaged changes? ", "Ok never mind I added upstream remote with same URL as origin which is the URL of my fork and that error is gone now. "], "731": [], "730": [], "725": [], "722": ["Ok it seems in certain versions of postgres only a date instead of a datatime is returned when querying the db. With a date object this check becomes quite useless. I will disable it in case a date is returned. "], "721": ["Wont t be done cause the grafana information does not provide what we want. "], "718": [], "717": ["Files to re add .venv garvin rucio garvin cat doc source api upload.rst Upload Methods upload automodule rucio.client.uploadclient members undoc members show inheritance .venv garvin rucio garvin cat doc source api download.rst Download Methods download automodule rucio.client.downloadclient members undoc members show inheritance just adding them in the repo generates errors warning when doing python setup.py build sphinx "], "716": ["JJ volunteered can you give him more details pls. ", "Should this be a rucio admin command? I was thinking about something like rucio admin rse export RSE NAME o output rse.json default to rse name.json rucio admin rse import rse name.json Which attributes should be mandatory in order to make the method fail proof? For distances maybe an update method that support json files will do the trick. rucio admin update distances from json distances.json May also work for protocols. ", "The original idea of this came from Ale and what he had in mind was really a full importer. Thus essentially everything rses attributes protocols distances can be represented as one big json and this can be imported by rucio to update create delete etc. I think we will need some further  discussion on this. ", "I would propose to add the endpoints import and export endpoints to the API where the admin can upload the full json file. Instead of different rucio admin commands there would been one command like rucio admin import data.json . ", "Other option maybe as rse configuration with associated GET POST and DELETE? ", "I think a more general endpoint like configuration is better if we want to add more export functionality in the future. For example with users or scopes if this would be useful later. ", "I think a separate endpoint might be better. It gives us more flexibility to widen the scope of the importer exporter later on. ", "What endpoint name should we use in this case? Maybe configuration is a bit confusing as there already is an endpoint config . Also I would like to reopen the PR to use the export distances function. "], "707": [], "705": ["Hi I am working on this bug and have setup an instance in docker. I can identify that the key type is actually an enum in the database and F which is being passed to the database is not among valid values. Valid values are ALL CONTAINER DERIVED COLLECTION DATASET FILE . I m not sure how the tuple F FILE as in constants.py is being turned into string F when evaluated. from rucio.db.sqla.constants import DIDType DIDType.FILE. dict description FILE name FILE value F cls class rucio.db.sqla.constants.DIDType I could use some context here. Thank you. ", "This conversion is done by sqlalchemy I think. If you are preparing a patch I am assigning the ticket to you \ud83d\ude04 Thanks a lot! ", "Hi  I decided to fire up a debugger and have identified the source of this conversion. It is being handled in DeclEnumType class in lib rucio db sqla enum.py For some reason the migrations create all functions are creating this SQL statement CREATE TABLE did keys key varchar  NOT NULL is enum tinyint  DEFAULT  key type enum ALL CONTAINER DERIVED COLLECTION DATASET FILE DEFAULT NULL value type varchar  DEFAULT NULL value regexp varchar  DEFAULT NULL updated at datetime DEFAULT NULL created at datetime DEFAULT NULL PRIMARY KEY key There is an inconsistency between the database and the application. The database expected values ALL CONTAINER DERIVED COLLECTION DATASET FILE The application expected values F D C A X Y Z In fact manually executing below SQL statement brings the system in consistent state. alter table did keys modify column key type enum F D C A X Y Z default null and then successfully allows running the following script. from rucio.core.meta import add key from rucio.db.sqla.constants import DIDType add key datatype DIDType.FILE I think that either the SQLAlchemy s create all function or Alembic s migrations are the issuing incorrect SQL. There is also possibility that DeclEnumType class is handling the conversions incorrectly but the code looks correct to me. Also I may be wrong as the codebase is big and I keep on finding surprises in it. Please let me know if that is the case. Next I ll try to find out what is issuing the incorrect SQL. Thank you. ", "I think it s just a specific dialect conversion missing somewhere. We need to have a test for this in lib rucio tests test meta.py to reproduce this. shreyanshk Would it be fine for you to do a pull request with a test ? as such we can have a deeper look on this. ", "vingar Please have a look at the opened pull requests and CI logs. I added a test to test meta.py and hopefully it is what was asked. I was fully expecting tests on MySQL to fail but surprisingly all tests passed. What is even more surprising is that tests on PostgreSQL failed ! . I didn t touch application logic. So I am having hard time following it. What do you think? ", "The test error should be unrelated as we have some instabilities in the test with dbs. I ve relaunched the failed tests manually in travis. Thanks for the PR ! "], "700": [], "697": [], "696": ["cserf Here s my feedback for the demo all done on Debian needs to have at the beginning a link that one needs to git clone the repo dockerd was not running for my automatically had to start manually compose ended with ERROR for rucio Cannot start service rucio driver failed programming external connectivity on endpoint demo rucio   Error starting userland proxy listen tcp   bind address already in use changing it to  made it work which was weird anyway because i have nothing running on  other than that it worked ", "The part describing the installation is nice a really good work guys! . I ve extended and rst formated the README in  plus adding it in the main index of the documentation with a new Tutorial demo section. For me the remaining part is to extend doc source rucio demo.rst with more examples from rucio ping to how to configure rucio rse accounts etc and then how to use rucio find files upload download etc. Merge and integrate some examples from cli admin examples.rst and cli examples.rst "], "688": [], "684": [], "679": [], "678": [], "671": [], "670": ["This will also fix these issues     "], "669": ["This will also fix this issue    "], "668": ["Actually this is not done yet but got intermixed with the replica lan wan issue. Reopening and moving to  "], "667": [], "666": [], "665": [], "656": ["High ranking is actually ok as the probe sets it inversely to the agis distance. There are some inaccuracies and minor mistakes there which will be followed up in a different ticket. "], "646": [], "641": [], "634": [], "633": ["Do we want this? I already had to turn off the slack github notifications just too spammy ", "I like our github channel ", "I m finding the github notifications quiet useful for me and having the report from travis will be also useful ", "Ok maybe tbeerman can have a look and add it to travis \ud83d\ude04 ", "Sure ", "it doesn t look to hard ", "it looks quiet easy actually. just click click on slack I m submitting a PR ", "Done! "], "629": ["This will happen with the coming rse manager overhaul "], "626": [], "622": [], "619": [], "612": [], "604": [], "598": ["This is sufficiently covered by  No additional development necessary. "], "597": [], "596": [], "587": [], "586": [], "576": ["Fixed by   . Someday I ll get the GitHub linkage correct. "], "575": ["Note this is fixed in next but not in master . So I think this becomes a backport exercise. "], "574": ["Currently Rucio unconditionally passes spacetoken json parameter to FTS job. No matter what value is passed this causes fts error on CMS sites which do not support spacetokens. The desired behavior is if space token is not in the extended attributes do not add spacetoken to fts json. ", "Lines that need fixing ", "Hi bbockelm how do I now define an RSE so that it would not pass the spacetoken to FTS? Adding srm protocol still requires a non empty spacetoken argument rucio admin rse add protocol hostname  scheme srm prefix cms store test rucio web service path srm  port  impl  domain json wan read  write  delete  third party copy  space token  IT Pisa Error space token and web service path must be provided for SRM endpoints. "], "573": ["I think it was implemented by nikmagini  bin rucio admin Nicolo Magini         Group file list in separate sublists for different schemes  bin rucio admin Nicolo Magini         bad files pfns.sort  bin rucio admin Nicolo Magini         bad files pfns grouped groupby bad files pfns lambda f f f.find  bin rucio admin Nicolo Magini          bin rucio admin Nicolo Magini         for sublist in bad files pfns grouped  bin rucio admin Nicolo Magini         for chunk in chunks list sublist   Can you confirm ? ", "Fixed "], "572": [], "571": [], "570": ["Fully agree with all Brian wrote \ud83d\ude04 This also hooks in a little bit with the way we import external policies. eventually we should have a better mechanism to import the policies without the need of them being part of the rucio package but lets have a separate issue for this Just one side note as I looked at the code recently. For the  transfers a different method construct surl is used to build the PFN for a  transferred replica. This has to be changed and the method should also inherit the algorithm from the  function of the respective policy. ", "Ok I have a work in progress branch for this will be testing this out for LIGO where this is most urgent . ", "Alright I think this one is good to go! "], "567": ["As discussed today this should probably be coordinated with TomasJavurek as he is already working on the download function. "], "566": [], "563": [], "560": [], "557": [], "554": [], "551": ["HI there seems to be a typo in the example for adding protocols underscores are used instead of dashes in web service path ", "Hi Natalia Can you send a PR in for that change? Thanks! Brian ", "not a big change Just sent two PRs about it   I m sure nataliaratnikova will find other inconsistencies and submit new PRs ", "Hi Brian Vincent already corrected this one I believe. I noticed that Rucio client is not very consistent about using and in the options and names. It s not always clear to me what the intended conventions are and this particular code is under rapid development. May be it is worth going over the whole client code once it is stable and the conventions are clarified. Thanks Natalia. On      PM Brian Bockelman wrote Hi Natalia Can you send a PR in for that change? Thanks! Brian \u2014 You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread ", "Hi yes I did that one as the example value was not appropriate as well. I would be happy if you report more such inconsistency. Vincent ", "The convention is that minus should be used in CLI arguments instead of underscore . Anything else is a typo mistake. ", "Submitted with two more fixes hopefully all are caught now. On      AM Mario Lassnig wrote The convention is that minus should be used in CLI arguments instead of underscore . Anything else is a typo mistake. \u2014 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread "], "549": [], "548": [], "543": [], "535": [], "533": ["This was discussed in the Dev Meeting on Two things should be added Mentioning a module in the Rucio CFG which should be loaded. This module can be part of a config package e.g. from ATLAS or CMS which is installed via PyPi Alternatively also directly mentioning the path of a python file should be possible which is loaded More suitable for small experiments who do not want to package their configs ", "I ve written some code to load the experiment specific stuff from an external module It will definitely need more work but I thought it might be a useful starting point for discussion. ", "Thanks James I m going to have a look. ", "I am not sure how best to take this forward. My main concern is how we would switch to the new policy packages without breaking existing installations. We would need packages to be available for all the VOs that need them I could create the initial packages but I don t think I am the right person to maintain them since I have no connection with most of the VOs. We also need to make sure the right packages get installed so that everything continues to work including for the tests. There is also the multi VO issue. I think we will want each VO to be able to have a different policy package but this would need quite significant changes to the code and I am not sure how best to do this. ", "I think for the beginning we need some kid of dual stack solution. We would keep the current policies we have in the core package cms atlas belle and use them and notify these organisations that we will take the policies out of the core package within one or two feature releases. Thus   month time We would also show them how to produce their own policy packages. So we need good documentation for that During that time they can either provide a policy package in the config or still use the files included in the repo. I don t think we need to prepare these for everyone just show them how to create them and have some best practice recommendation. The Multi VO I would leave out of the picture for now. The current multi VO development does not support multiple policies it should eventually but I would do this step by step Let s get the single VO packages working first and then also see with Eli who will continue the Multi VO development in February. ", "That all sounds sensible. I ll work on making my current code into a dual stack version. The pull request for this already contains some documentation about building the policy packages. "], "532": ["the openstack package pbr is nice for this and can be used or at least as a source of inspiration cf. ", "Implemented the following pip install git git github.com vgaronne rucio.git rucio version rucio     ", "The   is just based on the commit id? ", "not completely. Have a look on the corresponding pull request. ", "Ah I see. Did you solve the issue with the installation of dependencies? ", "I didn t have a look on it. On next it is solved "], "531": ["This is a duplicate of  "], "530": [], "529": ["Closed this comes implicitly with Reaper  "], "525": [], "524": ["hahahannes this one might also be good for you. We can discuss the details one of these days. "], "523": [" I think this issue can be closed. We have   and  for the next steps. "], "522": [], "519": [], "514": ["Not needed anymore "], "513": [], "510": [], "509": [], "508": [], "505": ["I cannot reproduce this bug from rucio.client.ruleclient import RuleClient r RuleClient r.update replication rule  lifetime  True I wonder if this is due to some version difference. What client server version are you using? ", " I m re building the docker to get up to date stuff then I ll re test ", "well after updating I ended up with the very same version. But I still see the same error from rucio.client.ruleclient import RuleClient r RuleClient r.update replication rule  lifetime  Traceback most recent call last File stdin line  in module File usr lib  site packages rucio client ruleclient.py line  in update replication rule exc cls exc msg self. get exception headers r.headers status code r.status code data r.content File usr lib  site packages rucio client baseclient.py line  in get exception data parse response data File usr lib  site packages rucio common utils.py line  in parse response return json.loads data object hook datetime parser File usr   json init .py line  in loads return cls encoding encoding kw .decode s File usr   json decoder.py line  in decode obj end self.raw decode s idx w s  .end File usr   json decoder.py line  in raw decode raise ValueError No JSON object could be decoded ValueError No JSON object could be decoded ", "Ok I need to do some more digging then \ud83d\ude04 ", "I still cannot reproduce this \ud83d\ude1e I suspect this is a problem with the json decoder or apache. Can you add a print options before and show the output from the log. Alternatively you can also give me access to the machine. ", "HI I m confused. I cannot find the rule.py file. Are you talking of client side or server side? ", "Server side depending on the location of the code it should be in the lib rucio core subdirectory. ", "OK. I ve no access to server. Maybe we should ask Brian? One point is not clear to me. This is a client side problem right or at least it is in the client the difference that generate the problem ? Have you tried to reproduce it using Eric docker client environment? Maybe different versions of packages? ", "I have only tested with the same version on my dev machine but not the docker client yet. I am not sure if it is a client or server problem both is possible on the server it could be some combination between and the json decoder of the exact python version which leads to a different unpacking of this dictionary. On the client side it could also be the python version and json encoder which packs the data somehow differently so the server cannot unpack it. But it is just guessing basically. What I see in the log is that on the server the dictionary gets unpacked in a way that lifetime is a NoneType so not None but NoneType which is a json thing . I have to try it with the docker images. ", "Hi sartiran This ticket is quite old now but just to make a quick follow up on it did you experience this bug in any of the newer version  ? ", "I m quite sure it is fine now. I ll double check asap and if it is I ll close the issue ", "Hi sartiran I am closing this now. If the problem shows up again please open another ticket \ud83d\ude04 "], "502": [], "501": ["most likely due to no protocols registered at the destination RSE "], "496": [], "493": ["Actually this is a core issue. For some metadata it checks that the metadata was updated  python if not rowcount raise exception.UnsupportedOperation key s for scope s name s cannot be updated locals But for others no. ", "I m afraid that if a proper exception is raised now It ll break certain third party programs The core method can return rowcount to the clients and the client can use this value to give the right status code. Or check the presence of the dataset ", "The core change will be intercepted by the web rest layer and eventually the client? It could be properly handled there and make the client not except out into userspace. "], "490": [], "487": [], "486": [], "484": [], "483": [], "480": [], "469": [], "468": ["rucio download first tries on the same site with different protocols then hops to another site and tries the protocols available there. for the upload we only want the different protocols we don t want to automatically upload to another site. ", "Comment about the c p suggestion are you sure that the protocol failover in rucio download actually works? From a few tests and looking at the code I don t think it s doing anything AFAICT only the site failover works in rucio download but the protocol failover will always select the default though hard to test for sure since I need to find a site where protocol  fails and then protocol  works ", "pinging TWAtGH the download one should work with both protocol and rse failover. something for monday ", "You mean because the protocol is assigned to the read protocol attribute and then read protocol is ignored afterwards? I think you re right but for any reason its working in my tests rucio v download ndownloader  user.mlassnig user.mlassnig.pilot.test.multi.hits        DEBUG Thread    possible protocol s for read        DEBUG Thread   Trying protocol davs at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING The requested service is not available at the moment. Details An unknown exception occurred. Details Could not open source Connection timed out        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING The requested service is not available at the moment. Details An unknown exception occurred. Details Could not open source Connection timed out        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        DEBUG Thread   Trying protocol gsiftp at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING The requested service is not available at the moment. Details An unknown exception occurred. Details Could not open source globus xio Unable to connect to lapp   globus xio System error in connect Connection timed out globus xio A system call failed Connection timed out        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING The requested service is not available at the moment. Details An unknown exception occurred. Details Could not open source globus xio Unable to connect to lapp   globus xio System error in connect Connection timed out globus xio A system call failed Connection timed out        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        DEBUG Thread    possible protocol s for read        DEBUG Thread   Trying protocol srm at INFN NAPOLI ATLAS DATADISK        INFO Thread   File     trying from INFN NAPOLI ATLAS DATADISK        INFO Thread   File     successfully downloaded from INFN NAPOLI ATLAS DATADISK Anyway this should be patched maybe. ", "Hi TWAtGH which version of the rucio client are you using? For me protocol fallback works until rucio client version  see  However it doesn t work anymore with the latest rucio client  see  where davs is always used. At first guess it may have been broken in this commit I ll open a separate ticket for this Cheers N  nmagini  ruciotest lsetup rucio  Requested rucio Setting up emi    Skipping grid middleware already setup from UI Setting up rucio  Info Setting compatibility to  Information for user emi Your proxy has    remaining nmagini  ruciotest rucio verbose download rse  LAPP MWTEST DATADISK            DEBUG RSE Expression  LAPP MWTEST DATADISK istape False        DEBUG Starting  download threads        INFO Thread   Starting the download of            DEBUG Waiting for threads to finish        DEBUG Thread   Potential sources u  LAPP MWTEST DATADISK        DEBUG Thread    possible protocol s for read        DEBUG Thread   Trying protocol davs at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        DEBUG Thread   Trying protocol gsiftp at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details globus ftp client the server responded with an error   Command failed. System error in name No such file or directory  A system call failed No such file or directory   End.        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details globus ftp client the server responded with an error   Command failed. System error in name No such file or directory  A system call failed No such file or directory   End.        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        ERROR Thread   Cannot download file            DEBUG All threads finished Download summary DID     Total files  Downloaded files  Files already found locally  Files that cannot be downloaded  Completed in  sec.  nmagini  lsetup rucio  Requested rucio Setting up emi    Setting up rucio  Info Setting compatibility to  Info Set RUCIO AUTH TYPE to  proxy Information for user emi Your proxy has    remaining nmagini  cd work nmagini  work cd ruciotest nmagini  ruciotest ls ltra total  drwxr xr x.  nmagini zp  Jan    drwxr xr x.  nmagini zp  Jan    rucio drwxr xr x.  nmagini zp  Jan    . drwxr xr x.  nmagini zp  Jan      nmagini  ruciotest rucio verbose download rse  LAPP MWTEST DATADISK            DEBUG RSE Expression  LAPP MWTEST DATADISK istape False        DEBUG Starting  download threads        INFO Thread   Starting the download of            DEBUG Thread   Potential sources u  LAPP MWTEST DATADISK        DEBUG Waiting for threads to finish        DEBUG Thread    possible protocol s for read        DEBUG Thread   Trying protocol davs at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        DEBUG Thread   Trying protocol gsiftp at  LAPP MWTEST DATADISK        INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          INFO Thread   File     trying from  LAPP MWTEST DATADISK        WARNING Source file not found. Details Source file not found. Details Failure HTTP  File not found while readding after  attempts        DEBUG Thread   Failed attempt          DEBUG Thread   sending trace        ERROR Thread   Cannot download file            DEBUG All threads finished Download summary DID     Total files  Downloaded files  Files already found locally  Files that cannot be downloaded  Completed in  sec. ", "Hi I was using the current cvmfs version  . With rucio testing  it doesnt work anymore. Seems like it is always retrying the first protocol for me it was srm . I ll do some more tests. Cheers Tobi ", "Hi TWAtGH I opened a separate issue for the download and provided a fix if it looks fine I ll apply the same logic to upload. Cheers N ", "Hey how is the progress here? Because I already started refactoring the upload since it will be moved to the library. ", "I got the failover in the upload working in  but I need to fix the exit statuses in case of error I ll have time to do it mid next week. If you refactor the upload I can restart from your new version no big loss. ", "I also started replacing return FAILURE statements with exceptions. My plan is now to finish the upload part of  so that Tomas can push it in his repo. I will also put in the failover during this and a couple of bugfixes and cleanups. "], "467": ["This is the statement that should be working but doesn t self. ctx.set opt string XROOTD PLUGIN XRD.WANTPROT gsi Might be necessary to follow this up with the gfal people. ", "Hi Mario yes it seems to be an issue with the gfal library. If I put xrd.wantprot explicitly in the root URL I can force gfal to use GSI auth on xrootd  However if I set XRD.WANTPROT as an env option gfal falls back to  auth  I ll follow up with gfal devs. Cheers Nicolo  import  ctx  context ctx.filecopy root eosatlas.cern.ch eos atlas atlasdatadisk rucio  valid ff    xrd.wantprot gsi file tmp       cryptossl  Your identity C IT O INFN OU Personal Certificate L Genova CN Nicolo Magini Enter PEM pass phrase  import  ctx  context ctx.set opt string XROOTD PLUGIN XRD.WANTPROT gsi  ctx.get opt string XROOTD PLUGIN XRD.WANTPROT gsi ctx.filecopy root eosatlas.cern.ch eos atlas atlasdatadisk rucio  valid ff    file tmp   ", "Great thanks! ", "Hi all summary of the investigation  Indeed there s a bug in the current  version  with the XRD.WANTPROT option which is not passed in the root TURL. The  team provided a fix which should be released in the next  version   However even with this fix adding xrd.wantprot to the root TURL doesn t seem to work on EOS transfers fail with Network is unreachable . Actually it s not  specific I get this error even with the xrd client. I ll continue to follow up with gfal and eos devs Cheers N. nmagini  xrdcopy root eosatlas.cern.ch eos atlas atlasdatadisk rucio  valid ff    gsi tmp      s Run ERROR Server responded with an error  Unable to open file eos atlas atlasdatadisk rucio  valid ff    Network is unreachable ", "Note about the change this is needed to make sure that xrd.wantprot works on EOS. However it s not sufficient to disable the fallback to  for this we also need to wait for the release of gfal  as mentioned in the previous comment ", "More details about the reason for the change from gfal devs On EOS we need to pass both gsi for auth to the headnode and unix for auth between headnode and FSTs On other xrootd storages gsi is sufficient adding unix shouldn t do harm since as second option it won t be selected ", "Thanks for the thorough investigation! "], "464": [], "460": ["Here s the protocol definition for the available destination RSE Protocols gsiftp extended attributes None hostname  prefix hadoop oppo domains u wan u read  u write  u third party copy  u delete  u lan u read  u write  u delete  scheme gsiftp port  impl  and the source RSE Protocols gsiftp extended attributes None hostname red gridftp.unl.edu prefix dropfiles cms store domains u wan u read  u write  u third party copy  u delete  u lan u read  u write  u delete  scheme gsiftp port  impl  ", "This is the suspect line of code When I remove the domain None and just let it use the default keyword domain wan everything works. However that definitely wouldn t explain why transfers work for other folks though! ", "This looks like a regression from the latest WAN LAN domain support patch! We actually have a domain called third party copy which we would like to use in the future because it might be different than WAN data access priority but it s not used yet. "], "457": [], "453": [], "450": [], "447": [], "444": [], "439": [], "436": [], "433": [], "430": [], "429": ["This is a duplicate of  ", "Actually that s a different problem than  . Reopening "], "426": [], "425": [], "422": ["Oops I do the same on another server the SSL error still exist opt dicos rucio lib rucio client baseclient.py  get token  result self.session.get url headers headers cert cert Pdb opt dicos rucio lib rucio client baseclient.py  get token  verify self.ca cert Pdb p self.ca cert etc grid security certificates ASGCCA  Pdb p url Pdb self.session.get url headers headers cert cert verify self.ca cert SSLError HTTPSConnectionPool host rucio  port  Max retries exceeded with url auth  proxy Caused by SSLError SSLError bad handshake Error SSL routines  read bytes  alert unknown ca Pdb p cert opt dicos rucio proxy Pdb ", "Looks like the remote host isn t verifying the proxy? You probably want to increase the debugging on the Apache side and look for errors in the error log there should be a lot of gridsite based chatter. ", "Dear All I change another way to test. There are two servers one is old version rucio  and another rucio  is new. I do curl on rucio  I can success to authenticate rucio  but can not success on rucio  with same CA proxy I use curl to test proxy .venv root rucio  rucio curl i cert tmp   twgridpil cacert etc grid security certificates ASGCCA  capath etc grid security certificates H X Rucio Account chwu X GET curl  Peer does not recognize and trust the CA that issued your certificate. .venv root rucio  rucio curl i cert tmp   twgridpil cacert etc grid security certificates ASGCCA  capath etc grid security certificates H X Rucio Account chwu X GET HTTP   OK Date Mon  Jan     GMT Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  Access Control Allow Origin None Access Control Allow Headers None Access Control Allow Methods Access Control Allow Credentials true Access Control Expose Headers X Rucio Auth Token Cache Control no cache no store max age  must revalidate Cache Control post check  pre check  Pragma no cache X Rucio Auth Token chwu C TW O AS OU GRID CN Chi Hsun Wu  unknown  Content Length  Content Type application octet stream Then I use certificate and key both server is success .venv root rucio  rucio curl i key opt rucio .globus userkey nopass.pem cert opt rucio .globus usercert.pem cacert etc grid security certificates ASGCCA  capath etc grid security certificates H X Rucio Account chwu X GET HTTP   OK Date Mon  Jan     GMT Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  Set Cookie GRIDHTTP PASSCODE  domain rucio  path secure Access Control Allow Origin None Access Control Allow Headers None Access Control Allow Methods Access Control Allow Credentials true Access Control Expose Headers X Rucio Auth Token Cache Control no cache no store max age  must revalidate Cache Control post check  pre check  Pragma no cache X Rucio Auth Token chwu C TW O AS OU GRID CN Chi Hsun Wu  unknown  Content Length  Content Type application octet stream .venv root rucio  rucio curl i key opt rucio .globus userkey nopass.pem cert opt rucio .globus usercert.pem cacert etc grid security certificates ASGCCA  capath etc grid security certificates H X Rucio Account chwu X GET HTTP   OK Date Mon  Jan     GMT Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  Set Cookie GRIDHTTP PASSCODE  domain rucio  path secure Access Control Allow Origin None Access Control Allow Headers None Access Control Allow Methods Access Control Allow Credentials true Access Control Expose Headers X Rucio Auth Token Cache Control no cache no store max age  must revalidate Cache Control post check  pre check  Pragma no cache X Rucio Auth Token chwu C TW O AS OU GRID CN Chi Hsun Wu  unknown  Content Length  Content Type application octet stream Recall that the error if I use python requests opt dicos rucio lib rucio client baseclient.py  get token  verify self.ca cert Pdb p url Pdb p headers X Rucio Account chwu Pdb p cert opt rucio proxy Pdb p self.ca cert etc grid security certificates ASGCCA  Pdb self.session.get url headers headers cert cert verify self.ca cert SSLError HTTPSConnectionPool host rucio  port  Max retries exceeded with url auth  proxy Caused by SSLError SSLError bad handshake Error SSL routines  read bytes  alert unknown ca I then check CA by echo openssl s client showcerts connect www.your.host.example  I use diff to found that two server only different on certificate or hostname. The CA part is the same. In ssl log of rucio  the server where proxy authentication is good Mon Jan      ssl info pid  client    Connection to child  established server rucio   Mon Jan      ssl debug pid  ssl engine kernel.c  client    SSL virtual host for servername rucio  found Mon Jan      debug pid  canl mod gridsite.c  set GRST save ssl creds Mon Jan      ssl debug pid  ssl engine kernel.c  client    Protocol  Cipher ECDHE RSA  GCM    bits Mon Jan      ssl debug pid  ssl engine kernel.c  client    Initial  HTTPS request received for child  server rucio   Mon Jan      authz core debug pid  mod authz core.c  client    authorization result of Require all granted granted Mon Jan      authz core debug pid  mod authz core.c  client    authorization result of RequireAny granted Mon Jan      debug pid  canl mod gridsite.c  Examine ACL file opt rucio etc gacl from ACL path opt rucio etc gacl Mon Jan      debug pid  canl mod gridsite.c  After GACL Onetime evaluation GRST PERM  Mon Jan      authz core debug pid  mod authz core.c  client    authorization result of Require all granted granted Mon Jan      authz core debug pid  mod authz core.c  client    authorization result of RequireAny granted Mon Jan      debug pid  canl mod gridsite.c  After GACL Onetime evaluation GRST PERM  Mon Jan      info pid  client   mod wsgi pid  process application rucio   auth Loading WSGI script opt rucio lib rucio web rest authentication.py . Mon Jan      ssl debug pid  ssl engine io.c  client    Connection closed to child  with standard shutdown server rucio   But in rucio  where proxy authentication is failed Mon Jan      ssl info pid  client    Connection to child  established server rucio   Mon Jan      ssl debug pid  ssl engine kernel.c  client    SSL virtual host for servername rucio  found Mon Jan      ssl info pid  client    SSL library error  in handshake server rucio   Mon Jan      ssl info pid  SSL Library Error error  SSL routines  get client certificate certificate verify failed Mon Jan      ssl info pid  client    Connection closed to child  with abortive shutdown server rucio   I am wondering about if the version of Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  cause problems. Could I know what is the version on your server ? Thanks. Here is the configure for rucio  LoadModule ssl module LoadModule wsgi module LoadModule gridsite module Listen  WSGIPythonHome opt rucio .venv WSGIPythonPath opt rucio .venv lib  site packages VirtualHost rucio   ServerName rucio   ServerAdmin ph adp ddm lab cern.ch SSLVerifyClient optional SSLVerifyDepth  SSLOptions StdEnvVars SSLEngine on SSLCertificateFile etc grid security hostcert.pem SSLCertificateKeyFile etc grid security hostkey.pem SSLCACertificatePath etc grid security certificates LogLevel debug ErrorLog TransferLog Include opt rucio etc web aliases  IfModule mod ssl.c ErrorLog var log  ssl engine.log LogLevel debug IfModule Directory opt rucio Require all granted Directory LocationMatch auth  proxy GridSiteIndexes on GridSiteAuth on GridSiteDNlists etc grid security dn lists GridSiteGSIProxyLimit  GridSiteEnvs on GridSiteACLPath opt rucio etc gacl LocationMatch VirtualHost And rucio  LoadModule ssl module LoadModule wsgi module LoadModule gridsite module WSGIPythonHome opt rucio .venv WSGIPythonPath opt rucio .venv lib  site packages Listen  VirtualHost rucio   ServerName rucio   ServerAdmin ph adp ddm lab cern.ch SSLEngine on SSLCertificateFile etc grid security hostcert.pem SSLCertificateKeyFile etc grid security hostkey.pem SSLCACertificatePath etc grid security certificates LogLevel debug ErrorLog TransferLog Include opt rucio etc web aliases  SSLVerifyClient optional SSLVerifyDepth  SSLOptions StdEnvVars Directory opt rucio Require all granted Directory LocationMatch auth  proxy GridSiteIndexes on GridSiteAuth on GridSiteDNlists etc grid security dn lists GridSiteGSIProxyLimit  GridSiteEnvs on GridSiteACLPath opt rucio etc gacl LocationMatch VirtualHost \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com      Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com Looks like the remote host isn t verifying the proxy? You probably want to increase the debugging on the Apache side and look for errors in the error log there should be a lot of gridsite based chatter. \u2014 You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread . ", "Then I realized that the proxy create by voms proxy init is actually self signed certificate. So the CA should be itself. .venv root rucio  rucio curl i cert tmp   twgridpil key tmp   twgridpil cacert tmp   twgridpil capath etc grid security certificates H X Rucio Account chwu X GET HTTP   OK Date Mon  Jan     GMT Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  Access Control Allow Origin None Access Control Allow Headers None Access Control Allow Methods Access Control Allow Credentials true Access Control Expose Headers X Rucio Auth Token Cache Control no cache no store max age  must revalidate Cache Control post check  pre check  Pragma no cache X Rucio Auth Token chwu C TW O AS OU GRID CN Chi Hsun Wu  unknown  Content Length  Content Type application octet stream But lxplus proxy authentication works well. Could I know the necessarily setting for auth proxy? Here is what I have client rucio host auth host auth type  proxy username ddmlab password secret ca cert etc grid security certificates ASGCCA  client cert root .globus usercert.pem client key root .globus userkey nopass.pem client  proxy  USER PROXY account chwu request retries  ", "The logs on rucio  indicate that GridSite is not being loaded or used while GridSite is doing the authentication on rucio  . On rucio  the mod ssl module is passing the client chain directly to OpenSSL which then fails. This strongly suggests there s nothing wrong on the client side but mod gridsite is not being loaded or used for some reason on rucio  . I d suggest looking through that configuration in more detail and or earlier in the startup sequence to see why mod gridsite isn t being used. ", "Hi But after I use curl I viewed log on rucio  I can see mod gridsite is working ? .venv root rucio  rucio curl i cert tmp   twgridpil key tmp   twgridpil cacert tmp   twgridpil capath etc grid security certificates H X Rucio Account chwu X GET HTTP   OK Date Tue  Jan     GMT Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  Access Control Allow Origin None Access Control Allow Headers None Access Control Allow Methods Access Control Allow Credentials true Access Control Expose Headers X Rucio Auth Token Cache Control no cache no store max age  must revalidate Cache Control post check  pre check  Pragma no cache X Rucio Auth Token chwu C TW O AS OU GRID CN ShoaTingTwgrid Cheng  unknown  Content Length  Content Type application octet stream Log Tue Jan      ssl info pid  client    Connection to child  established server rucio   Tue Jan      ssl debug pid  ssl engine kernel.c  client    SSL virtual host for servername rucio  found Tue Jan      debug pid  canl mod gridsite.c  set GRST save ssl creds Tue Jan      debug pid  canl mod gridsite.c  store GRST CRED AURI  dn C TW O AS OU GRID CN ShoaTingTwgrid Cheng  Tue Jan      debug pid  canl mod gridsite.c  store GRST CRED AURI  dn C TW O AS OU GRID CN ShoaTingTwgrid Cheng  CN  Tue Jan      ssl debug pid  ssl engine kernel.c  client    Protocol  Cipher ECDHE RSA  GCM    bits Tue Jan      ssl debug pid  ssl engine kernel.c  client    Initial  HTTPS request received for child  server rucio   Tue Jan      authz core debug pid  mod authz core.c  client    authorization result of Require all granted granted Tue Jan      authz core debug pid  mod authz core.c  client    authorization result of RequireAny granted Tue Jan      debug pid  canl mod gridsite.c  Using identity dn C TW O AS OU GRID CN ShoaTingTwgrid Cheng  from SSL TLS Tue Jan      debug pid  canl mod gridsite.c  Examine ACL file opt rucio etc gacl from ACL path opt rucio etc gacl Tue Jan      debug pid  canl mod gridsite.c  After GACL Onetime evaluation GRST PERM  Tue Jan      authz core debug pid  mod authz core.c  client    authorization result granted no directives Tue Jan      debug pid  canl mod gridsite.c  Using identity dn C TW O AS OU GRID CN ShoaTingTwgrid Cheng  from SSL TLS Tue Jan      debug pid  canl mod gridsite.c  After GACL Onetime evaluation GRST PERM  Tue Jan      ssl debug pid  ssl engine io.c  client    Connection closed to child  with standard shutdown server rucio   I restart and can see something in Tue Jan      info pid  mod wsgi pid  Python home opt rucio .venv. Tue Jan      info pid  mod wsgi pid  Initializing Python. Tue Jan      info pid  mod wsgi pid  Attach interpreter . Tue Jan      info pid  mod wsgi pid  Attach interpreter . Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. Tue Jan      proxy debug pid  proxy util.c   initializing worker proxy reverse shared Tue Jan      proxy debug pid  proxy util.c   initializing worker proxy reverse local Tue Jan      proxy debug pid  proxy util.c   initialized single connection worker in child  for Tue Jan      info pid  mod wsgi pid  Python home opt rucio .venv. Tue Jan      info pid  mod wsgi pid  Initializing Python. Tue Jan      debug pid  canl mod gridsite.c  Cutoff time for ssl creds cache  Tue Jan      info pid  mod wsgi pid  Attach interpreter . Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. Tue Jan      info pid  mod wsgi pid  Attach interpreter . Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. Tue Jan      debug pid  canl mod gridsite.c  Cutoff time for ssl creds cache  Tue Jan      debug pid  canl mod gridsite.c  Cutoff time for ssl creds cache  Tue Jan      info pid  mod wsgi pid  Attach interpreter . Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. There are also something about mod gridsite . \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com      Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com The logs on rucio  indicate that GridSite is not being loaded or used while GridSite is doing the authentication on rucio  On rucio  the mod ssl module is passing the client chain directly to OpenSSL which then fails. This strongly suggests there s nothing wrong on the client side but mod gridsite is not being loaded or used for some reason on rucio  I d suggest looking through that configuration in more detail and or earlier in the startup sequence to see why mod gridsite isn t being used. \u2014 You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread . ", "Hi Edward Would it be possible to post the rucio  logs for the python requests based interaction? Brian ", "Hi Brian Here is the log after request sent Tue Jan      ssl info pid  client    Connection to child  established server rucio   Tue Jan      ssl debug pid  ssl engine kernel.c  client    SSL virtual host for servername rucio  found Tue Jan      ssl info pid  client    SSL library error  in handshake server rucio   Tue Jan      ssl info pid  SSL Library Error error  SSL routines  get client certificate certificate verify failed Tue Jan      ssl info pid  client    Connection closed to child  with abortive shutdown server rucio   Here is my request .venv root rucio  rucio rucio whoami opt rucio lib rucio client baseclient.py  get token  result self.session.get url headers headers cert cert Pdb p url Pdb p headers X Rucio Account chwu Pdb p cert tmp   twgridpil Pdb n opt rucio lib rucio client baseclient.py  get token  verify self.ca cert Pdb p self.ca cert etc grid security certificates ASGCCA  Pdb result self.session.get url headers headers cert cert verify self.ca cert SSLError HTTPSConnectionPool host rucio  port  Max retries exceeded with url auth  proxy Caused by SSLError SSLError bad handshake Error SSL routines  read bytes  alert unknown ca By the way I also found some log under for CURL but nothing recorded when I use python request. Tue Jan      debug pid  grst canl    starts Tue Jan      debug pid  grst canl   Look for CA root file etc grid security certificates  Tue Jan      debug pid  grst canl   Loaded CA root cert from file Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Initialise chain Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Cert sig check  returns  Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Cert sig check  returns  Tue Jan      debug pid  grst canl   Found included VOMS cert in  Tue Jan      debug pid  grst canl   Look for CA root file etc grid security certificates  Tue Jan      debug pid  grst canl   Loaded CA root cert from file Tue Jan      debug pid  grst canl    check issued returns  Tue Jan      debug pid  grst canl    starts Tue Jan      debug pid  grst canl   Look for CA root file etc grid security certificates  Tue Jan      debug pid  grst canl   Loaded CA root cert from file Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Initialise chain Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Cert sig check  returns  Tue Jan      debug pid  grst canl   Process cert at depth  in chain Tue Jan      debug pid  grst canl   Cert sig check  returns  Tue Jan      debug pid  grst canl   Found included VOMS cert in  Tue Jan      debug pid  grst canl   Look for CA root file etc grid security certificates  Tue Jan      debug pid  grst canl   Loaded CA root cert from file Tue Jan      debug pid  grst canl    check issued returns  Tue Jan      debug pid  grst gacl.c  GRSTgaclAclLoadFile starting Tue Jan      debug pid  grst gacl.c  GRSTgaclAclLoadFile parsing GACL Tue Jan      info pid  mod wsgi pid  Create interpreter rucio   auth . Tue Jan      info pid  mod wsgi pid  Adding opt rucio .venv lib  site packages to path. Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com Hi Edward Would it be possible to post the rucio  logs for the python requests based interaction? Brian \u2014 You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread . ", "Hi Edward this is quite mysterious. I assume you have the same version of apache mod ssl and mod gridsite on both  and   alert unknown ca means that the server is actively and purposefully rejecting the client. Maybe there s a magic dependency between the configurations they look similar to me but not quite . Would it make sense to copy the working one over from  to  including the respective etc ", "mlassnig  alert unknown ca is often the error you receive when a grid proxy is utilized to authenticate with a service that doesn t understand grid proxies. The proxy itself is an  certificate signed by a user acting as a CA OpenSSL is confused as the user isn t on its list of valid CAs. Of course it can also be a more mundane error but that s why I was trying to figure out why mod gridsite didn t appear to be processing his proxy cert. It does seem somewhat client dependent I wonder if there is some configuration difference causing the python requests client to map to a different virtual host? ", "Hi In fact the version of each server is different rucio  .venv root rucio  rucio uname a Linux rucio      SMP Mon Dec     UTC        GNU Linux .venv root rucio  rucio cat etc redhat release CentOS Linux release  Core Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  rucio  .venv root rucio  dicos rucio uname a Linux rucio      SMP Tue Sep     UTC        GNU Linux .venv root rucio  dicos rucio cat etc redhat release CentOS Linux release  Core Server Apache  CentOS OpenSSL  fips mod auth kerb  mod wsgi  Python  mod gridsite  I have tried to downgrade gridsite  to gridsite  but it is not working. I also remove and delete all config then copy config from rucio  to rucio  but it is still the same. I setup another virtualhost on rucio  with same version of rucio  and it is working. Since there are so many packages that are different in version between  and  I decide to make a new server in  and tried again. Can I also know the OS version openssl version gridsite version and which repo do they came from epel?umd  in your server. And to install  I also put globus toolkit Do you install  by this ? Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com mlassnig  alert unknown ca is often the error you receive when a grid proxy is utilized to authenticate with a service that doesn t understand grid proxies. The proxy itself is an  certificate signed by a user acting as a CA OpenSSL is confused as the user isn t on its list of valid CAs. Of course it can also be a more mundane error but that s why I was trying to figure out why mod gridsite didn t appear to be processing his proxy cert. It does seem somewhat client dependent I wonder if there is some configuration difference causing the python requests client to map to a different virtual host? \u2014 You are receiving this because you commented. Reply to this email directly view it on GitHub or mute the thread . ", "edwardwufast what version of python requests are you using? I got this exact same error on a newly configured hosts with python requests at version  I downgraded to an older version  and everything worked well. ", "Actually I did two things. I downgraded the python requests as noted above and switched python  back to the one provided by  it had previously been installed via PyPI . It may be possible that the python  is the culprit here and that RedHat is adding some special patch to make it work? ", "The rucio auth nodes in production have the following  requests    All installed from pip. The CentOS base python    and python requests.noarch    are not used. ", "edwardwufast can you post the versions you are using compared to the ones Mario posted? ", "Hi For python package there are the same because I installed by pip requirement under tools. For OS I have tried two version but all the same .venv root rucio  rucio cat etc redhat release CentOS Linux release  Core root rucio cat etc redhat release Derived from Red Hat Enterprise Linux  Source root rucio uname a Linux rucio     SMP Fri Mar     UTC        GNU Linux I also modify openssl version to  on  which is the same version as rucio  which is the working one. I really can not found the problems in configuration and CA. What if I provide the password and let you login to check? \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com      Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com edwardwufast can you post the versions you are using compared to the ones Mario posted? \u2014 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread . ", "Hi I then downgrade my rucio in rucio  and rebuild virtualenv then SSL problem disappear. I am wondering that maybe there is somethings to do with python packages. I then upgrade to  and tried again. There is the pip require in old version which is work for me .venv root rucio  rucio cat tools pip requires All dependencies needed to run rucio should be defined here SQLAlchemy  db backend alembic  Lightweight database migration tool for SQLAlchemy Mako  Templating language python editor  Programmatically open an editor capture the result flup  Needed to deploy web.py in web.py  Python web framework python memcached  Quick and small memcached client for Python ntplib  NTP library anyjson  Wraps the best available JSON implementation available in a common interface jsonschema  JSON Schema repoze.lru  LRU least recently used cache implementation. python dateutil  Extensions to the standard datetime module ordereddict  Ordered Dictionary   an HTML XML templating system for Python  pysftp  forces installation of paramikoi and pycrypto paramiko   protocol library pycrypto  Cryptographic modules   Package built from gearman  Used only gor emulation framework stomp.py  ActiveMQ Messaging Protocol dnspython  To resolve ActiveMQ broker alias pystatsd  Needed to log into graphite with more than  Hz pygeoip  GeoIP API    API for  support maxminddb  extension for reading the MaxMind DB format threadpool  threadpool cffi  Foreign Function Interface for Python calling C code cryptography  Cryptographic recipes and primitives   Python  Enum backported gcloud  API Client library for Google Cloud googleapis common protos  Common protobufs used in Google APIs A comprehensive HTTP client library idna  Internationalized Domain Names in Applications IDNA ipaddress    manipulation library   OAuth  client library protobuf  Protocol Buffers grpcio  Package for gRPC Python. pyOpenSSL  Python wrapper module around the OpenSSL library    modules  A collection of  based protocols modules pycparser  C parser in Python rsa  Pure Python RSA implementation setuptools  Easily download build install upgrade and uninstall Python packages retrying  general purpose retrying library to simplify the task of adding retry behavior to just about anything six  Python  and  compatibility utilities   explicitly needed on  .venv root rucio  rucio cat tools pip requires client All dependencies needed to run rucio client should be defined here argparse  Command line parsing library argcomplete  Bash tab completion for argparse kerberos  pykerberos  Kerberos high level interface requests  requests kerberos  A Kerberos authentication handler for python requests   HTTP library with thread safe connection pooling and file post support wsgiref  WSGI PEP  Reference Library dogpile  Caching API dogpile.core  Caching API plugins dogpile.cache  Caching API plugins nose  For rucio test server boto   boto protocol python swiftclient  OpenStack Object Storage API Client Library tabulate  Pretty print tabular data progressbar  Text progress bar   Read and write  compressed files. python magic  File type identification using libmagic .venv root rucio  rucio cat tools pip requires test All dependencies needed to develop test rucio should be defined here pinocchio  Extensions for the nose unit testing framework Paste  Utilities for web development in pyton   backport of unittest lib in python  coverage  Nose module for test coverage Sphinx  required to build documentation   template engine Provides a Sphinx domain for describing RESTful HTTP APIs stub  Temporarily modify callable behaviour and object attributes PIL  !!! This library does not exist anymore Pygments  Python Syntax highlighter docutils  Needed for sphinx   checks for  code style compliance pyflakes  Passive checker of Python programs   Wrapper around PyFlakes  mccabe  McCabe checker plugin for  pylint  static code analysis. Last release compatible with python  virtualenv  Virtual Python Environment builder tox  Automate and standardize testing in Python pytest  pytest xdist  py.test xdist plugin for distributed testing and loop on failing modes xmltodict  Makes working with XML feel like you are working with JSON pytz  World timezone definitions modern and historical Babel  Internationalization utilities Best Regard Edward Wu ASGC      GMT   Wu Edward edwardwufast gmail.com Hi For python package there are the same because I installed by pip requirement under tools. For OS I have tried two version but all the same .venv root rucio  rucio cat etc redhat release CentOS Linux release  Core root rucio cat etc redhat release Derived from Red Hat Enterprise Linux  Source root rucio uname a Linux rucio     SMP Fri Mar     UTC        GNU Linux I also modify openssl version to  on  which is the same version as rucio  which is the working one. I really can not found the problems in configuration and CA. What if I provide the password and let you login to check? \u4e0d\u542b\u75c5\u6bd2\u3002www.avast.com m       Best Regard Edward Wu ASGC      GMT   Brian Bockelman notifications github.com edwardwufast can you post the versions you are using compared to the ones Mario posted? \u2014 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread . ", "Was there any conclusion on this? ", "I compare with three servers I setup in asgc and use wireshark to check ssl process. But still fail to get a conclusion.i will prepare what I have tested later.Now I just use one of the server that is working and put the problemic one aside.    Martin Barisits notifications github.com \u5beb\u9053\uff1a Was there any conclusion on this? \u2014 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or mute the thread . "], "420": ["third party copy has been added to the schema in December  This should be fine now. "], "413": [], "412": [], "411": ["For example I think we ll need a lot of patches along this line a lib rucio client didclient.py b lib rucio client didclient.py     Martin Barisits martin.barisits cern.ch   from urllib import quote plus from json import dumps from requests.status codes import codes     class DIDClient BaseClient param scope The scope name. param name The data identifier name. path .join self.DIDS BASEURL scope name meta path .join self.DIDS BASEURL quote plus scope quote plus name meta url build url choice self.list hosts path path r self. send request url type GET if r.status code codes.ok Notes Not sure if we want to quote the scope or keep that relatively restricted. There s a corresponding Apache side change to allow encoded slashes in the URL. ", "Ok some progress here. Once the client properly escapes to  and Apache doesn t change the URL the next step is mod wsgi this again escapes and automatically cleans up characters. So dids user.bbockelm foo bar baz meta will with some regex hackery invoke the right function with scope user.bbockelm and DID foo bar baz note missing prefix . Since Rucio already has a pre load hook one can correct the path and make WSGI do the right thing . Example code for the hook request uri ctx.env.get REQUEST URI script name ctx.env.get SCRIPT NAME if request uri.startswith script name path part request uri.split ?  path part normpath path part len script name while path part.startswith path part path part  ctx.path unquote path part This causes the right handler to be invoked with scope user.bbockelm and DID foo bar baz . It works with the caveat that one has to tweak the regex for the WSGI application. Instead of . . meta Meta one needs . meta Meta see URLS global in lib rucio web rest did.py So that s a modest project to do minor fixups to all APIs that include a DID with an ugly hack in the load hook . Definitely an option but not a particularly appealing one. The second option is to completely rework the APIs to allow them to accept either query style or path style information. That is have dids meta?scope user.bbockelm did foo bar baz equivalent to dids user.bbockelm foo bar baz meta One can keep backward compatibility with existing clients. It s more work but seems cleaner to me. Thoughts? ", "one third alternative is to have . meta Meta and a function splitting the obtained string in scope name assuming that scope doesn t have a . we can stay backward compatible with this change. going for dids meta?scope user.bbockelm did foo bar baz will need to introduce an api version api version variable in the header or in the url  dids meta? . ", "vingar no unfortunately that won t work due to the substitution of the the into . The resulting DID is foo bar baz instead of foo bar baz . You end up having to do the same loader hook trick I highlighted above which again it does indeed work without client changes just I find it ugly . Now having an API version is a solid idea regardless but it s not clear why dids meta?scope user.bbockelm did foo bar baz would require one? That s not a valid request currently. Or do you mean that we d want to introduce an API version so the client can more easily auto detect the version of the server and hence the API it wants to use ? ", "hi I need to go in the details of the code to know what is doable. Unfortunately this discussion doesn t concern only the metadata part. The same logic has been applied to several resources dids replicas rules etc and operations put get post del . ", "In the meantime I m trying to figure out if we can configure mod wsgi not do the slash escape. ", "bbockelm This looks like it ", "Another option which should keep backward compatibility with the clients with special lookahead in regexp we can extract the delimiter from the regexp. Example re.match ? . meta user.bbockelm foo bar baz meta .groups user.bbockelm foo bar baz one possibility then is to make certain regexp policy configurable. "], "410": ["It s a column which was introduced with the  feature release. It s in the  pull request. ", "reseting the db should solve this. ", "Gotcha will close for now. "], "407": [], "406": ["Will it be enough to just check in bin rucio erase if there is a wildcard given and then print an error? I m wondering why set metadata isn t rising an exception for the key lifetime and in did name case. ", "Yes the error should be raised on server side though. "], "403": ["Don t we have this behaviour already for files marked as BAD? So wouldn t it be enough to mark a suspicious file as bad? ", "gumond has volunteered for this task ", "This was implemented as a probe in  but it should probably be it s own daemon. ", "I have not had a look at this task yet. I considered it closed. When I come back tomorrow I will see with you Cedric how rucio daemons are implemented and what could I do about creating this one. ", "bin rucio replica recoverer Contains in an epilog variable my current procedure for manual testing. Once I learn more about rucio should I write a test for this daemon ? lib rucio core replica.py Although there are some differences the two functions get suspicious files and the new get available suspicious replicas seem to be aiming for similar outputs. Could we merge the two into one ? "], "402": ["I already had a PR for this submitted but reverted it as it ties in with the discussion about the whole policy permissions thing. The idea was to have something like this in rucio.cfg common vo support email address rucio support and depending on the error show the appropriate followup. ", "pull request  issue duplicated with  Proposal policy organization support rucio support in the pull request the error messages are generated client side which forces to have the right parameters with the clients in addition to the hostnames etc . Server side would have been more flexible. ", "It is more flexible but some errors might be generated client side and then you would have to contact the server to know the addresses. Overall not a big issue I think as the config has to be distributed anyway. I am closing this let s follow up in  "], "399": [], "394": ["I don t think this patch worked. I try to create a dataset with the name  PromptReco       and it returns OK as if it worked however root  rucio list dids user.ewv SingleMuonLRun SCOPE NAME DID TYPE user.ewv  PromptReco  DATASET So the dataset name was truncated at the character. ", "I suspect the patch worked but hit client escaping issues. Can you try creating a different dataset with a CMS block name and note the timestamp when you did it? I can then poke around the Apache logs. ", "From the log  Dec      POST dids user.ewv  PromptReco  HTTP    the client generates the right url but then server side the url is changed and names with hash mark should be urllib.quoted ? ", "If its on the client I can try to find and fix it. Please assign to me. I ll let you know if I can t. ", "no sure it s only on the clients. This might need some change in apache wsgi and server code. ", "Ah hash is the HTML anchor. ", "ericvaandering if you want to tack a whack at this you probably want to edit something like this replacing path .join self.DIDS BASEURL scope name with path .join self.DIDS BASEURL scope urllib.quote plus name However I strongly suspect we should re close this issue as this really appears to be a client problem not the schema. ", "Yes this is not enough. Encoding the URL is not sufficient. It still comes back as dataset exists dropping the and everything after it seems . ", "Re closing this issue as I can create these datasets on the CMS instance with the client patches in  applied. "], "391": ["This has been done? "], "388": [], "383": [], "380": ["I appear twice in AUTHORS Joaqu\u00edn Bogado jbogado linti.unlp.edu.ar   Joaquin Bogado jbogadog cern.ch   Only the first is valid. Also I think UNLP Universidad Nacional de La Plata Argentina should appear as institution. ", "jwackito it s because of the mail address associated to your commits and the difference of accent in the first name. Could you try to stick to one mail address ? ideally the one associated to you github account to get the activity history linked to it. ", "vingar I will do. Thanks! "], "377": ["I m trying to wrap my head around the implications of this. This mapping of DID path name on disk LFN CMS namespace needs to be stored somehow. If you just give a different LFN to the DID that you re storing how will you be able to use download it later again? as the lookup is against the DID and not against an LFN . ", "If the LFN on disk differs rucio upload can take a PFN attribute LFN on disk . But this works for non deterministic endpoint. For deterministic the function LFN PFN is needed. ", "bbockelm can you maybe write down an example or a few how the data identifiers and paths on storage look right now and the different conventions in building the path I remember you linked some XML file on slack Just to give us an easier understanding of what we want to achieve. ", "sorry was out sick on Friday Let s assume for now that we call the CMS scope cms currently everything in the experiment is organized into a single namespace and socially we re not likely to change this . Then for the LFN we have now store generator  BprimeTToBZ    madgraph tauola GEN          we would use the following DID cms store generator  BprimeTToBZ    madgraph tauola GEN          Currently when rucio upload is invoked the file name on the local disk is not necessarily the same as the filename in the LFN      in this case it s really a mistake in the case of CMS to attach too much value to the component after the last in the LFN as it s not really the file s name . mlassnig is right this will need a corresponding patch for rucio download so the user can specify an output file name. However downloading a file is a nearly unused case in CMS which is why I wasn t worried about it at the outset here. In almost all use cases the files are accessed via the physics application not the workflow layer there s no concept of a mover for stagein as in ATLAS .  here s an example of what the PFN generation rules look like The discussion might really belong in  but we can get most of the use cases demonstrated by simply concatenating the LFN to the prefix. ", "Oh as an example here s likely how CMS would want to be able to upload files rucio verbose upload scope user.bbockelm rse  US NEBRASKA USERDISK myGreatFile.root lfn store mc SAM GenericTTbar AODSIM CMSSW      realistic         ", "Ah I see more clearly what you mean now. I feel though that this mixes Rucio concepts a bit up in your example above the did Or in some sense the Rucio LFN would be user.bbockelm myGreatFile.root but we additionally would have to store the CMS LFN store mc SAM GenericTTbar AODSIM CMSSW      realistic         plus the path on actual storage as this cannot be derived just by the did user.bbockelm myGreatFile.root itself. It is possible to expand the schema like this but I wonder would you ever address the file with the Rucio LFN did user.bbockelm myGreatFile.root ? Because if not why not make the Rucio did in the first place user.bbockelm store mc SAM GenericTTbar AODSIM CMSSW      realistic         We would have to extend the column and allow in the name part of the did and tinker with the client a bit but it seems like the more natural workflow to me? Please correct if I understand anything wrong \ud83d\ude04 ", " nope I m not sure that s correct. In this example myGreatFile.root is some random set of characters produced by the workflow management system which have no particular meaning once the job ends. CMS refers to the file as store mc SAM GenericTTbar AODSIM CMSSW      realistic         forever and ever after that. Then given that LFN we construct the corresponding PFN usually via concatenation of a known prefix gsiftp red gridftp.unl.edu user cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         Despite the strongly suggesting a directory hierarchy it is really treated as an opaque name. For example at RAL s Echo instance I believe we have a pool group named cms and the object name is store mc SAM GenericTTbar AODSIM CMSSW      realistic         . So for a Rucio DID I think this file would be referred to as the concatenation of the scope cms and the LFN cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         ", "Ok so the mapping will then be simply DID cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         and the replica at RAL ECHO would generate this PFN gsiftp red gridftp.unl.edu user cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         That s certainly doable. I guess at a later stage you can explore the use of scopes in your workflow? But for now like this your workflow management could continue as is. ", "mlassnig that s almost right s RAL ECHO  US Nebraska and it ll be  correct. Given how the workflows are architected and the CMS computing project is organized I think it s likely we ll use a single scope for  of our data. A few places where we might do additional scopes Testing data for things like load tests and scaling campaigns. Temporary outputs from the workflow management that are later combined into official files. Release validation is often run as quite separate from the rest of CMS. But these are not a bit second order for now. ", "Great that s what I thought too that after the upload the original filename myGreatFile.root is basically meaningless to you. I would suggest I think most changes are there already? that we adapt rucio in a way to make the did cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         If you want to use multiple scopes or not is really up to you for Rucio there is no difference. This should be much easier to manage as we don t need to take care of the CMS LFN separately as it is already encoded in the did. For the determinism I think we could overload the default handling with the hashes in the cms policy file with the lookup in the xml such that the did cms store mc SAM GenericTTbar AODSIM CMSSW      realistic         gets transformed to the PFN easily via the xml lookup. Plus some changes in the client to deal with the input filename and the did with slashes correctly. ", " for having alternate determinism policies it seems the alternatives are  Have a RSE protocol attribute that enables this and have the appropriate LFN to PFN function look for the attribute and behave appropriately. That s what I did in this case you can imagine a full implementation would have a list of rules instead of a single boolean.  Have a cms mode specified in the policy a third policy? where CMS can provide its own  function which would internally lookup the associated RSE protocol and implement the mapping. This could then be stored outside the Rucio source tree. Did I capture that correctly? Honestly my preference would be to start with  as it is done boolean toggle here instead of doing the full XML simply do the concatenation without the hashing and then do  later on in the evaluation. What s done here will allow us to use probably  of sites I hope! ", "Yes in the long run I would really prefer the second method. My thinking was to overwrite the current default with the two hashes via the CMS policy file could have the advantage that you do not have to set the determinism type for each RSE. It just works correctly out of the box. So Rucio won t do it wrong if you create an RSE and forget to set the setting. You could do this at first with this simple method you already implemented and later on expand in the full XML rule mode. But if you feel more comfortable to do it first with option  and later on only change to option  this is completely fine. ", "\ud83c\udf88 \ud83c\udf7e "], "376": ["duplicate with "], "375": [], "369": ["Would be very useful GitHub metadata doesn t currently pick up the license. ", "yes I realized it when creating a new repo "], "368": [], "367": ["Duplicate of  ? ", "Yes I ll close the other one. "], "363": [], "359": [], "356": [], "351": [], "348": [], "345": [], "344": ["There is really no need to self reference the issue number in the issue itself \ud83d\ude04 "], "343": [], "339": ["vingar is this fixed by  ? ", "Yes it is. Closing \ud83d\ude04 "], "338": ["Fixed by  "], "336": [], "331": [], "330": ["An example of where this is useful is a hosted Rucio instance where the VO may only interact via the REST API the existing rucio.core.distance module wouldn t be usable in this case. ", "Proposed specification for the REST API Resource rses distances source destination Operations POST DELETE PUT GET ", "That API looks good to me! How hard is it to implement? Is there a similar REST API I can look at? ", "Yes The method path is always core api rest network client. So to make the rest api you will have to implement the api as well. But all these are very simple and analogous. The api is basically the layer in between where permissions etc. are checked in the core itself everything works without permissions. I think a good stack to look at are the identities Each account has different identities such as username pass  certs etc. So the API file is The REST interface is You will also have to add the new apache endpoint to and so that apache mod wsgi can find it. "], "329": [], "327": ["Fixed by  "], "325": [], "323": [], "322": ["Sounds like a good modification to me but then I don t use the docker dev env so someone who does should confirm ", "Definitely a good idea. ", " ", "Great I ll get to it smile The docker build looks for a  config  which isn t in the repo. The  has a  section though. I just took the liberty to move the  section to its own file. ", "Apologies I had forgotten to format my commit messages hence the rewrite. ", "May I presume that we will build a container called rucio dev that will be available from the rucio docker repository i.e. so one could pull it like this docker pull rucio rucio dev ", "yes we should replace the current rucio container with rucio dev "], "321": [], "320": [], "317": [], "314": ["One option is to drop support for Python  it s not received any security updates for four years and is little used. Here s the pip installs for rucio from PyPI for the last month via pypinfo percent pip rucio pyversion python version percent download count          "], "313": [], "312": ["The solution is not streightforeward since we don t have dump of lock table in hadoop. "], "307": ["Motivation Badly merged conf.py file makes sphinx failing Running Sphinx  making output directory Traceback most recent call last File home docs checkouts readthedocs.org user builds rucio blueprint envs latest local lib  site packages sphinx cmdline.py line  in main opts.warningiserror opts.tags opts.verbosity opts.jobs File home docs checkouts readthedocs.org user builds rucio blueprint envs latest local lib  site packages sphinx application.py line  in init confoverrides or self.tags File home docs checkouts readthedocs.org user builds rucio blueprint envs latest local lib  site packages sphinx config.py line  in init raise ConfigError CONFIG SYNTAX ERROR err ConfigError There is a syntax error in your configuration file invalid syntax conf.py line  Configuration error There is a syntax error in your configuration file invalid syntax conf.py line  ", "This is a duplicate of  ", "closed "], "298": [], "288": [], "285": [], "282": [], "281": [], "280": [], "279": [], "272": [], "265": [], "262": ["I suggest raising the priority on this. "], "261": ["Can you give a concrete example how this would look like especially how the two paths would look like for the mv command? ", "We could do this with a very specific mv rsync protocol however the workflow in Rucio does not support something like this as that protocol would only work at one specific host. In ATLAS there is a similar workflow where workernodes copy their output themselves with whatever way they think is correct to the specific path on storage and then just register the already existing data in Rucio via the python clients didclient.py add did with the RSE attribute . Would this workflow be an alternative for you? ", "in the CLI it would be something like rucio upload dataset files scope scope local internally you can pass gfal copy file original path to file file RSE path to file or rsync original path to file RSE path to file ", "As discussed I would add a local argument like rucio upload source path local destination path and perform a mv or rsync command in the uploadclient. An own protocol is not feasible if I understood correctly? ", "There are some changes planned for this in regards to local moving protocols storm "], "258": [], "255": [], "246": [], "245": [], "244": ["What s the exact command you did? ", "it s there but hidden in the code formatting rucio v add dataset user.briedel test test test lifetime  "], "243": ["I wonder if we should have something VO specific here configurable in rucio.cfg. ", "Rucio support URL and Experiment support URL. Makes sense to me. ", "maybe some rucio yaml ini etc file to provide some string substitution variable based on the named policy ", "Yes. A lot of these errors could be VO specific so it is not bad to have the first line VO support filter before it goes to the Rucio github. And secondly some errors might include partially ? confidential information where we might not want to encourage posting them on a public issue tracker. "], "242": ["Thanks for the report ericvaandering we are currently rebuilding the entire documentation to get rid of errors like this. ", "This is a duplicate now of  Closing this ticket as we use the other one to track the changes. "], "237": [], "236": ["tbeerman I can see the access cnt is not null for certain DIDs. So I guess it has been implemented. Can you confirm please ? ", "Yes it s implemented but I used the wrong issue number on the PR so it s not linked here. But the issue can be closed. "], "235": ["Will be replaced by json import from CRIC.  "], "232": [], "227": [], "214": [], "213": [], "211": ["Proposal for consistency trace user trace event type "], "209": ["Having a simple test on all files to check that we don t have pylint errors would be good as well. I quickly googled travis pylint and few options exists apparently in travis but I didn t have time to investigate more. Something to look ? ", "Running it over all files can be enabled easily will just take a bit longer. ", "There is two things For the errors testing everything is fine because there really should not be any error in the code. Testing for score though you should do file by file. If you get a score for everything the few files changed in the PR will not really move the general score although the files themselves might have a very bad one. "], "205": [], "204": [], "199": [], "196": [], "195": [], "192": [], "191": ["Cannot reproduce rucio list account usage usage rucio list account usage h rse RSE usage account rucio list account usage error too few arguments rucio list account usage xyz RSE USAGE LIMIT QUOTA LEFT xxx  GB  B  B yyy  GB  B  B zzz  GB  B  B ", "Weird I don t know what I did then. Can be closed I guess. "], "187": [], "185": [], "177": [], "174": [], "171": [], "168": [], "165": ["Git can handle symlinks alright so no need to duplicate the file right? ", "yes git can but the issue is more with the release procedure as this file is changed several times and how readthedocs accesses it ", "True good point! ", "For the release procedure it is fine it is just overwriting the symlink but never commiting back the code. If readthedocs can handle the symlink it is fine for the release procedure. ", "try the symbolic link then "], "158": [], "155": [], "148": [], "145": ["alembic is fixed by another patch thanks Martin "], "142": [], "141": [], "140": [" ", "Hi Frank just to confirm as nobody is assigned to this ticket You are planning to implement this right? \ud83d\ude03 ", "There is no one assigned to the ticket because I can t assign anyone. I am planning to implement this. Sorry about the long delay I got absorbed by something else for a moment. Back at it now starting with dealing with the repo reset ", "I just added you as collaborator for the repo You should be able to assign now. ", "That worked thanks! ", "Going to work on  first to set up my dev environment if you guys like the suggestions that is . "], "137": [], "136": [], "133": [], "123": [], "122": [], "121": ["Lets discuss in the next dev meeting. The interest part sounds a bit redundant to the lifetime exceptions to me. ", "Just to note what we discussed in the meeting. We decided to follow up with DAST what they think of the idea and then move from there. ", "Lets try to discuss with them in Hamburg ", "Decided to keep it Possibly a student project. ", "Can I work on solving this issue? ", "Hi pujanm Yes this can be part of your gsoc project. Let s discuss this on Slack so you can put together a proposal. ", "Duplicate  "], "119": [], "116": ["Raising UnsupportedOperation looks also a good option if the expiration date now ", "Right now DataidentifierNotFound is raised in these cases and I think we can keep it the same. This is really only for the case where a dataset is not deleted yet but logically it already should be. "], "113": [], "109": [], "106": ["Hi I just tried the AUTHORS link from and it took me here Saying that the page does not exist. Should AUTHORS AUTHORS.rst become something like AUTHORS or AUTHORS AUTHORS.rst ? Cheers Jaroslav ", "It should point to "], "104": [], "101": [], "98": [], "91": [], "86": [], "85": [], "84": [], "83": [], "82": ["One way to identify them dumps query with index etc. One way to get rid of them rucio admin replica tombstone delete etc. ", "hahahannes the ticket text is a bit misleading. The idea of the ticket is a rucio admin command which can put a tombstone on a specific replica which has no locks etc. We can discuss details in person. "], "81": ["What is the status ? ", "already implemented "], "80": [], "79": [], "78": ["I guess this is a duplicate for  and  ", "pinging tbeerman ", "well in JIRA it was the main task and  and  where the sub tasks ", "Ah ok what you can do in github which is a bit similar to epics in jira is create tasklist inside the ticket which you could use as an epic. a b c ", "Closing as duplicate "], "77": [], "76": ["cserf I wonder what the correct behaviour is there. I cannot just catch it and ignore it because then the rule will stay in the queue probably forever. On the other hand if I just remove the rule than the user will never know that it was deleted due to it s async nature. Maybe the right thing is to put it to SUSPENDED. Opinions? ", "The behaviour should be that the rule is not replicated to the new RSE and kept at the current RSE ", "But there is no current RSE? ", "Ahhh sorry. I thought it was  didn t recover from my jet lag fro Gex p SUSPENDED would be the right thing to do "], "75": [], "74": [], "73": ["Right now also schemes compatible to the source schemes are used which at the moment works out fine but this should not be done as it could result in conflicts. "], "72": [], "71": [], "70": ["Is it related to the INCOMPLETE message ? Do we have another issue related to it ? Motivation When a dataset starts being deleted If its state change to INCOMPLETE from a previous state ! INCOMPLETE then a message is produced add message INCOMPLETE scope scope name name ", "Yes the epic issue for this is  I just noticed though we added the INCOMPLETE state in the schema.sql but not in models.py or constants.py yet. ", "I checked on next and could not find the constants change ? models.py is not necessary. ", "Yes as I said it s not there \ud83d\ude04 ", "but I did the merge request  sure ", "It doesn t seem to be in there. Only schema.sql and the migrate repo file. ", "But just coming back to the original question. I think one message is enough no need to have one for DATASET and one for CONTAINER. ", "did type in the message ? ", "Yes I think so. To some extent it is redundant as e.g. AMI would know it anyway but maybe this is consumed by other services too which do not necessarily know. ", "issue  is about Protect storage from too many requests ? ", "You mean about the epic issue link above? I accidentally linked  first but then corrected to  Maybe in your eMail it still says the first version. "], "69": ["I had a deep look in the code and it seems we overlooked the pros and cons When the last replica is removed the file is also removed from the content. Thus generating a INCOMPLETE state and message for a complete dataset from the system point of view is conceptually wrong. E.g. A dataset has  files when the reaper deletes the last replica it removes it from the content. When listing all replicas the  files defined in the content are there which contradict the INCOMPLETE state. Flagging the file as unavailable and then set the dataset to incomplete would have been more consistent. The counter part is that we will keep accumulating datasets with unavailable files. When all files are unavailable in the dataset this one is deleted by the reaper ? This will require more changes. One scenario is to communicate datasets which have no rules anymore against them to AMI. The unlocked datasets can be used by the event index but with the risk that they disappear. Possible changes in the judge cleaner ? ", "We discussed this in the meeting and I think conceptually this is fine. It really depends how you interpret the availability column of the did. For me this is not really a data management related metadata but physics metadata thus from a physics point of view the dataset is INCOMPLETE. As for data management related things we do not need the availability column anyway. The rule scenario would work but it is really the same as also in this case we need to store somewhere that the dataset is INCOMPLETE. From a workflow though it is better to do it in the reaper as it is the last possible moment and the reaper needs to query anyway if this was the last replica while for the judge this would be a completely new workflow. ", "incomplete is maybe not the correct semantic. unlocked or similar makes more sense. message and flag can be generated from the reaper now and later we can think about moving it elsewhere. ", "Unlocked for me is a bit too confusing with lock rule related things. Personally I feel incomplete is fine but I agree there might be a better terminology for it. But as you said we can move it later too. ", "I m still fighting with the concept. availability LOST DELETED AVAILABLE A file is LOST if there are no known replicas of the file in Rucio while at the same time at least one account requested a replica a file is DELETED if no account requested a replica otherwise the file is AVAILABLE. This is a derived attribute. complete True False A dataset container where all files have replicas available is complete. Any dataset container which contains files without replicas is incomplete. This is a derived attribute. ", "A dataset is INCOMPLETE if there is at least one LOST or DELETED file in the dataset. The fact that this file is not in the main table only in the deleted one anymore is fine in my opinion. Don t get me wrong I fully understand what you mean that conceptually the DELETED LOST file should be kept in the table dataset but from this workflow we walked away a long time ago. We do remove lost deleted files in the table dataset and move it to the deleted one. "], "68": [], "67": ["Linking  as it is relvant here too. "], "66": ["Fix in an earlier patch "], "65": ["Should be dropped! "], "64": ["Just to link this to  where I layed out the plan for this. In the first iteration I will add empty signatures for domain to all the RSEManager methods. At that point the list replicas call can basically already use them. What we said today is that list replicas for the client in general should use domain wan . If an RSE is found witch matches with the client location it should query again with domain lan for the RSE. ", "i will extend it to list replicas domain wan once the signature for create protocol is merged. then we can gradually start making the clients domain aware "], "63": ["Multiproces in Python  "], "62": ["this is starting to affect sites with different LAN WAN protocol settings "], "61": [], "60": ["cserf Do you know when this actually showed up? I just checked the code and the cleaner actually does not read any rules which have an active child rule id set. The workflow is that once the child rule is OK the parents child rule id is removed which makes it eligible for deletion. ", "Pinging cserf \ud83d\ude04 ", "If a child rule is expired and replicating it cannot be removed as the FK constraint of the parent rule is active. In this case the child rule id is removed from the parent rule and the parent rule is set to infinite lifetime. "], "59": [], "58": ["x Poller x Submitter x Receiver Finisher "], "56": [], "55": ["Duplicate of  "], "52": ["If no sources are available maybe rule should inform user that this is permanent terminal. "], "51": ["Not needed Too dangerous "], "50": ["I think this just needs to be activated in the menu. ", "hahahannes can you please have a look on this. ", "It is indeed already but it seems to be visible only for admins. ", "This page is a bit different. It shows the the usage of all the users on an RSE. I think the initial idea of this page was a page where a user sees how much quota he is using on all RSEs. ", "Okay then I will create a new page because I couldn t find one. ", "I could find a page under the link account rse usage which has no links directing to it yet. Also it shows a search field. Should it be removed so that automatically the user account gets choosen or is this page relevant for admins? "], "49": [], "48": ["Conclusion of todays meeting was to talk to Gancho about the impact of switching to BLOB. ", "Gancho just confirmed. We will get varchar  with the next oracle release. sqlite and mysql are  already psql is unlimited size. Will add new column payload clob for oracle which we can drop again as soon as oracle gets  varchar support. ", "it looks like a good use case for the json column type. ", "i also thought about that but we don t select on the message it s just a payload holder. so the json would just be overhead. ", "storing a json payload into a json column doesn t look too exotic even if we don t select on it. Something to add on the gsoc proposal as a first contribution ? ", "This could have implications both on INSERT as well as on SELECT. For CLOB at least oracle has to make context switches into pl sql to execute this. For JSON I imagine this could be the same especially since oracle might try to create some structure of the stored json? But I don t get the point of doing this since we are never trying to search inside the payload it s always the selection of the full row. It s an optimisation for something which we don t use and I doubt that it makes the current workflow faster \ud83d\ude04 ", "The json type is quite new and it can be the opportunity to get more experience with it on a simple use case support on different db sqlachelmy measure the overhead save some json dumps load operations etc. I would just put it as part of the gsoc project and part of the project ", "Up ", "Discussed in the dev meeting on Add new column CLOB Column should only be used if the normal column is NULL "], "47": ["this one is fixed "], "46": ["So more suggestion from Rod on the archive workflow support It might be instructive to put together a rucio client archive command. One might archive a dataset check for files or download zip to N GB archives register archives and contents upload archive mark individual replicas secondary ", "xrdcp has the option to just pull a file from a zip archive. It was a bit broken for dpm at least but fixed and in the next release I guess. It works for eos though xrdcp f zip   root eosatlas.cern.ch  eos atlas atlasscratchdisk rucio user walkerr    tmp pants Cheers Rod. ", "I summarized issues we have with extracting the files here "], "45": ["Non exhaustive list gearman  git review  Command line tool for Git Gerrit idna  Internationalized Domain Names in Applications IDNA ipaddress    manipulation library repoze.lru  LRU least recently used cache implementation. ", "I ran pip extra reqs Here is the list returned anyjson in requirements.txt argcomplete in requirements.txt argparse in requirements.txt babel in requirements.txt cffi in requirements.txt coverage in requirements.txt cryptography in requirements.txt docutils in requirements.txt  in requirements.txt flup in requirements.txt  in requirements.txt futures in requirements.txt gcloud in requirements.txt gearman in requirements.txt git review in requirements.txt googleapis common protos in requirements.txt grpcio in requirements.txt in requirements.txt idna in requirements.txt ipaddress in requirements.txt  in requirements.txt mako in requirements.txt maxminddb in requirements.txt  in requirements.txt ntplib in requirements.txt numpy in requirements.txt  in requirements.txt ordereddict in requirements.txt paste in requirements.txt pinocchio in requirements.txt protobuf in requirements.txt  in requirements.txt  modules in requirements.txt pycparser in requirements.txt pycrypto in requirements.txt pygments in requirements.txt pyopenssl in requirements.txt pytest xdist in requirements.txt python editor in requirements.txt python memcached in requirements.txt python swiftclient in requirements.txt repoze lru in requirements.txt rsa in requirements.txt setuptools in requirements.txt six in requirements.txt tox in requirements.txt Some modules like argparse and argcomplete appear in this list because they are already in  by default "], "44": [], "42": ["Hi all IIUC the desired behavior is that all existing replicas of a file should be declared BAD when the LFN is passed as argument instead of the PFN correct? Should the file itself also be flagged in some way if all its replicas are BAD? Thanks N. ", "Yes exactly. On the file itself I don t think that this has to be flagged I think the necromancer daemon will do that then. cserf maybe you can comment? ", "You just has to resolve the LFNs to the list of PFNs list replicas then pass this list to declare bad file replicas ", "OK. Additional point I can make the client resolve automatically if the user has passed LFN vs PFN. But please let me know if you prefer instead that the user should specify this explicitly through an option. "], "41": [], "40": [], "39": ["How is this done with the rse usage history Should be a daemon? ", "Done by  "], "37": ["Needs to be checked ", "Needs to be done \ud83d\ude04 ", "Starting today I will be marking each time DDM Ops is affected by this issue. fire "], "34": ["Duplicate of  "], "30": ["There seems to be a limit of  dids when registering files to dataset One option would be to have the client split into chunks of  "], "29": ["Hi if you don t mind I will take this too since I need it for the new api upload. ", "Hi sure sorry for the delay. Maybe you can simply reuse my changes to rsemanager.py here Cheers N. ", "Ah great! Thanks for the hint! ", "Done in bin rucio but not in uploadclient ", "It is ", "So this has to be prepared in the list of dicts given to the uploadclient? ", "Exactly. Most options are file specific in the API. ", "I see thanks \ud83d\udc4d . Closing "], "28": [], "27": ["Proposed implementation  For protocols tools that natively support timeout e.g. gfal pass the timeout to the underlying command. Targeting this for the next client release.  Later for all protocols implement a watchdog in rucio download upload that kills the transfer after the timeout expiration. The plan is to do this only for the new downloadclient uploadclient libraries. ", "Checking the code the gfal protocol already has a hardcoded  s timeout "], "26": ["I am not sure how to efficiently check if the rse is empty. If I understood correctly I could either check for rules like rule for rule in list rules for rse in parse expression rule rse expression if rse rse rse  But there I would have to iterate through each rule. Or to check if there are file or collection replicas like session.query models.CollectionReplicas .filter by rse id rse id .count  session.query models.RSEFileAssociation .filter by rse id rse id .count  ", "Hi The best way would be to check against the replicas table directly thus session.query models. RSEFileAssociation .filter by rse id rse id .count  This query goes against the Primary Key directly so it should be rather fast especially if the rse is empty. ", "Okay thanks was not sure because of the big size of the RSEFileAssociation table. ", "For this it should be fine as deleting an RSE is a really rare thing to do. "], "23": [], "8": [], "5": []}