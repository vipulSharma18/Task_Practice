{"3395": "---------- With `rucio.rse.protocols.posix.Default` implementation, the replicas with \"file\" schema always have port `0` in them, eg `TEST1: file://:0/tmp/testfile` ------------ Override `lfns2pfns()` in the posix protocol. I will submit a pull request for this shortly. ", "3388": "---------- Chained subscription use case for Belle II : - RAW data produce at T0 - 1st subscription exports the RAW to one DATADISK at site A - 2nd subscription exports the RAW from DATADISK at site A to DATATAPE at the same site ", "3383": "---------- Currently Rucio WebUI does not offer a user the possibility to choose how they authenticate before authorized to use the WebUI. The WebUI authentication method has to be pre-defined in the rucio.cfg file. ------------ changes in code under rucio/lib/rucio/web/ui/ ", "3377": "---------- The current ways of specifying a client config file are not very flexible and don't make it easy to use two different rucio systems from the same machine. ------------ Use an environment variable like `RUCIO_CFG` to specify a config file, and add a command line option like `-c CONFFILE`. ", "3365": "---------- ```rucio get-metadata O3:K-K1_llhoft-1266855936-4096.gwf``` ```adler32: 030853e9``` leads to the following: ```rucio list-file-replicas O3:K-K1_llhoft-1266855936-4096.gwf +---------+---------------------------------+------------+------------+-------------------------------------------------------------------------------------------------------------------+ | SCOPE | NAME | FILESIZE | ADLER32 | RSE: REPLICA | |---------+---------------------------------+------------+------------+-------------------------------------------------------------------------------------------------------------------| | O3 | K-K1_llhoft-1266855936-4096.gwf | 322.137 MB | 3.0853e+13 | .... |``` Because there are no letters, the client interprets the string as a number in scientific notation (next to last character is an e). ", "3360": "---------- Upgrade of dependencies. ------------ - pip-requires-client - `setuptools>=36.8.0,<41.0.0` change to `setuptools>=36.8.0,<45.0.0` (45.0 only Py3 compatible) - `argcomplete>=1.9.0,<1.10.0` change to `argcomplete>=1.9.0,<1.12.0` - `requests>=2.20.0,<2.22.0` change to `requests>=2.20.0,<2.24.0` - `urllib3>=1.24.2,<1.25` change to `urllib3>=1.24.2,<1.26` - `dogpile.cache>=0.6.5,<0.7.0` comment that `0.7.0` is only Py3 - `progressbar2>=3.39.0,<3.40.0` change to `progressbar2>=3.39.0,<3.41.0` - `six>=1.12.0<1.13.0` change to `six>=1.12.0<1.15.0` - `boto3>=1.9.130,<1.10.0` change to `boto3>=1.9.130,<1.13.0` - pip-requires - `SQLAlchemy==1.3.7` change to `SQLAlchemy==1.3.13` - `alembic==1.2.1` change to `alembic==1.4.1` - `jsonschema==3.1.1` change to `jsonschema==3.2.0` - `python-dateutil==2.8.0` change to `python-dateutil==2.8.1` - `stomp.py==4.1.22` change to `stomp.py==4.1.23` - `geoip2==2.9.0` change to `geoip2==3.0.0` - `redis==3.3.11` change to `redis==3.4.1` - `numpy==1.14.2` change to `numpy==1.16.6` (1.17.0 only Py3) - `1.16.6` did not work. Using `1.14.2` again since numpy is not that critical - `paramiko==2.6.0` change to `paramiko==2.7.1` ", "3350": "---------- Move the config.yml file from its current place at ./lib/rucio/transfertool/ to ./etc where the other Rucio configuration files are stored. ------------ Modify the code and documentation that references the location of config.yml: ./lib/rucio/transfertool/globusLibrary.py ./doc/source/configure_globus_transfertool.rst ", "3347": "---------- The `setup.py` for the main Rucio package is not Python 3 compatible. ------------ - [ ] Fix compatibility - [ ] Add Python3 Test for server to travis ", "3346": "---------- Allow client to query globus storage via rse protocol wrapper ------------ Adding list and exists methods to GlobusRSEProtocol ", "3340": "----------  This should not be done in reverse but in a normal sort. ------------ Remove `reverse=True` ", "3311": "---------- The reaper seems to skip the deletion of the collection replica when the last replica of a dataset is deleted on a RSE ", "3283": "---------- The configuration set method doesn't work properly if '.' is provided as a value : `rucio-admin config set --section automatix --option separator --value '.'` But when trying to retrieve it : ``` [Wed Feb 05 03:19:09.493072 2020] [:info] [pid 3368:tid 139652002477824] [client 188.184.29.109:36844] mod_wsgi (pid=3368, process='', application='rucio-server-int-02.cern.ch|/config'): Loading WSGI script '/usr/lib/python2.7/site-packages/rucio/web/rest/config.py'. [Wed Feb 05 03:19:11.931687 2020] [:error] [pid 3368:tid 139652002477824] 'NoneType' object has no attribute 'lower' [Wed Feb 05 03:19:11.931765 2020] [:error] [pid 3368:tid 139652002477824] <type 'exceptions.AttributeError'> [Wed Feb 05 03:19:11.932690 2020] [:error] [pid 3368:tid 139652002477824] Traceback (most recent call last): [Wed Feb 05 03:19:11.932702 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/common.py\", line 103, in decorated [Wed Feb 05 03:19:11.932705 2020] [:error] [pid 3368:tid 139652002477824] return f(*args, **kwargs) [Wed Feb 05 03:19:11.932708 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/common.py\", line 155, in decorated [Wed Feb 05 03:19:11.932710 2020] [:error] [pid 3368:tid 139652002477824] return f(*args, **kwargs) [Wed Feb 05 03:19:11.932713 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/config.py\", line 68, in GET [Wed Feb 05 03:19:11.932715 2020] [:error] [pid 3368:tid 139652002477824] for item in config.items(section, issuer=ctx.env.get('issuer')): [Wed Feb 05 03:19:11.932718 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/api/config.py\", line 132, in items [Wed Feb 05 03:19:11.932734 2020] [:error] [pid 3368:tid 139652002477824] return config.items(section) [Wed Feb 05 03:19:11.932737 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f [Wed Feb 05 03:19:11.932739 2020] [:error] [pid 3368:tid 139652002477824] return Retrying(*dargs, **dkw).call(f, *args, **kw) [Wed Feb 05 03:19:11.932741 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call [Wed Feb 05 03:19:11.932744 2020] [:error] [pid 3368:tid 139652002477824] return attempt.get(self._wrap_exception) [Wed Feb 05 03:19:11.932746 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get [Wed Feb 05 03:19:11.932748 2020] [:error] [pid 3368:tid 139652002477824] six.reraise(self.value[0], self.value[1], self.value[2]) [Wed Feb 05 03:19:11.932750 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call [Wed Feb 05 03:19:11.932752 2020] [:error] [pid 3368:tid 139652002477824] attempt = Attempt(fn(*args, **kwargs), attempt_number, False) [Wed Feb 05 03:19:11.932755 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 284, in new_funct [Wed Feb 05 03:19:11.932757 2020] [:error] [pid 3368:tid 139652002477824] return function(*args, **kwargs) [Wed Feb 05 03:19:11.932759 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/core/config.py\", line 192, in items [Wed Feb 05 03:19:11.932761 2020] [:error] [pid 3368:tid 139652002477824] return [(item[0], __convert_type(item[1])) for item in items] [Wed Feb 05 03:19:11.932763 2020] [:error] [pid 3368:tid 139652002477824] File \"/usr/lib/python2.7/site-packages/rucio/core/config.py\", line 267, in __convert_type [Wed Feb 05 03:19:11.932766 2020] [:error] [pid 3368:tid 139652002477824] if value.lower() in ['true', 'yes', 'on']: [Wed Feb 05 03:19:11.932768 2020] [:error] [pid 3368:tid 139652002477824] AttributeError: 'NoneType' object has no attribute 'lower' ``` The value is converted to None. The value is set properly if one use directly the set value in core. Problem probably at the REST level ", "3261": "---------- The resolve query takes a lot of time in cases where the RSE is not connected. ------------ Need to introduce thresholds to speed up the query. ", "3260": "---------- It's impossible to figure out if rucio-clients are hanging due to external libraries (e.g., libgfal.so). Make the client forcefully kill -9 itself upon repeated presses of CTRL-C. ", "3254": "---------- This needs to be changed in various parts of the code from `third_party_copy` to use `third_party_copy_read` and `third_party_copy_write`. In a later feature release, the `third_party_copy` column can then be removed. ", "3250": "---------- Following an example of download, add tests of upload to the list of tests:  ------------ - add site name to the test rses, important for client location and lan/wan decissions - almost copy paste download, it is already using upload functionality - enhance that for several test settings: fake file, fake site, corrupted file, etc. - ensure that it is running under both: travis/docker and dev docker env. ", "3249": "---------- In order to avoid an upload with .rucio.upload tmp name extension, we need to be able to set it in protocol (extended) attributes at rse level. It is not causing direct issue now since gfalv2.py sets renaming attr. by default:  but this is bit implicit. Once somebody changes this all switch impl. to gfal.py, uploads to rses where overwrite is not allowed will fail. ------------ if 'extended_attributes' in protocol_attr: self.renaming = protocol_attr['extended_atributes'].get('renaming', False) ", "3235": "I would like to make the following suggestions: 1. Use the synchronous API in case the number of files is below some thresholds. We had a case where a request to mark a large amount of files as temporarily unavailable was causing long delays for single-file requests which should have had higher priority. 2. Change the message to something other than `replicas successfully declared`. This is misleading as the PFNs could be invalid and the user will move on thinking that everything is done. 3. Do some checking on the PFNs, at the very least that they are valid URLs. It should be noted that on the side of Minos this isn\u2019t handled gracefully. ``` Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/badreplicas/minos.py\", li ne 182, in minos dict_rse[rse_id].extend(tmp_dict_rse[rse_id]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 206, in call return attempt.get(self._wrap_exception) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 284, in new_funct return function(*args, **kwargs) File \"/usr/lib/python2.7/site-packages/rucio/core/replica.py\", line 437, in ge t_pfn_to_rse split_se = surl.split('/')[2].split(':') ``` ", "3200": "---------- I have set myself 1MB quota on type=DATADISK, but the CLI cannot show it. ``` > rucio list-account-usage dcameron +--------------------------------+------------+------------+--------------+ | RSE | USAGE | LIMIT | QUOTA LEFT | |--------------------------------+------------+------------+--------------| | CERN-PROD_SCRATCHDISK | 964.600 kB | 20.000 TB | 20.000 TB | | NDGF-T1_DATADISK | 99.740 MB | 600.000 MB | 500.260 MB | | NO-NORGRID-T2_LOCALGROUPDISK | 2.894 MB | 150.000 TB | 150.000 TB | | SE-SNIC-T2_LUND_LOCALGROUPDISK | 141.105 kB | 0.000 B | 0.000 B | +--------------------------------+------------+------------+--------------+ 2019-12-16 15:59:11,401 ERROR Access to the requested resource denied. Details: Account dcameron can not list global account usage. ``` ------------ ", "3199": "---------- Running as an unprivileged user, it is expected that this fails but the exception should not be an internal server error: ``` >>> c.set_global_account_limit('dcameron', 'type=DATADISK', 1000000000) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/home/dcameron/dev/ddm/rucio/lib/rucio/client/accountlimitclient.py\", line 107, in set_global_account_limit raise exc_cls(exc_msg) rucio.common.exception.RucioException: An unknown exception occurred. Details: no error information passed  status code: 500 ('internal_server_error', 'server_error', '/o\\\\', '\\xe2\\x9c\\x97')) ``` ------------ The exception should be the same as I see when setting local account limits: ``` >>> c.set_local_account_limit('dcameron', 'NDGF-T1_DATADISK', 1000000000) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/home/dcameron/dev/ddm/rucio/lib/rucio/client/accountlimitclient.py\", line 63, in set_local_account_limit raise exc_cls(exc_msg) rucio.common.exception.AccessDenied: Access to the requested resource denied. Details: Account dcameron can not set account limits. ``` ", "3164": "---------- The `Bring_online` parameter is the timeout for the FTS polling to check that the file is online. The current default value defined in Rucio is 43200s / 12 h. At the last data carousel exercice, many transfers reached this timeout and the corresponding FTS requests failed (error message in Rucio 'Stage was cancelled'). Such staging request are then rescheduled on the same site, participating to the long tail effect. Based on the values in FTS about the max staging request pers site, e.g., 200000 requests  the avg file size 2.5 GB and the observed throughput (e.g., from 300 MB/s to 1GB/s), the timeout value of 12 hours is too small and lead to some unnecessary overheads and scheduling operations. ------------ I would suggest either to Increase the bring_online timeout to reflect the storage / tape performance wrt the number of queued staging request or set to the rule lifetime with some potential renormalisation or set it to a bigger constant value for all staging requests. ", "3148": "---------- 1) decommission mode not fully/correctly working 2) DB access vs. dumps 3) configuration - several things hardcoded at the time being 4) logging - needs to be correctly implemented 5) oop - bb8 evolved into quite complex tool. We might start to think about upgrading it. add 1) Difficult issue, see some limiting factors here:  ", "3147": "---------- 1) we are maintaing two independent codes at the moment, clients and rsemgr. The aim is to have as much as possible at single place: clients. 2) the rsemgr evolved organically, cleanup is needed. 3) preparation for new features like bulk mode ------------ 1) moving upload functionality to clients:  2) several small updates:  3) commissioning and testing in parallel with the old rsemgr, checking if functionality is conserved. ", "3135": "---------- rucio list-rules-history \"cms:/ttZJets_13TeV_madgraphMLM/RunIISummer16NanoAODv5-Nano1June2019_102X_mcRun2_asymptotic_v7-v1/NANOAODSIM#8c635224-3900-4ad1-83cf-d1036546ae63\" 2019-11-26 17:16:16,670 ERROR expected string or buffer ------------ Martin Barisits 10:29 AM I think this is due to missing quote_plus on  Maybe you can try to do this similar to  quote_plus around scope and name ", "3124": "---------- Currently, the monitor module only supports statsd but the CERN Kubernetes clusters come with a Prometheus server that can also be used for auto-scaling. ------------ Make the monitor module configurable to either report to statd or to prometheus. ", "3115": "---------- The base64 decoder requires width-padded strings, i.e., some strings need extra = padding elements in the string. Some keygens don't add them. Properly handle this corner case. ", "3103": "---------- Decommissioning of sites with problems take too much human intervention. Automation is needed to cover the corner cases (around 10% of rules fail not because the site to be decommissioned but for other factors). ------------ New daemon/service that keep track of the rules and transfers exiting the site to be decommissioned. ", "3101": "---------- As of today, one can pass a reference to a list where the traces from Rucio can be uploaded. If the other party already provided some traces, we have no way to update them so far, they would need to update the traces according to the information we provided. There is an option for trace_pattern that can be used for traces that Rucio is producing, but thah seems to be an option only for a single trace. ------------ This is matter of discussion. Either the other party provides list of references that they created and Rucio just updates them, or Rucio send a traces and the other party merges them with the parts of traces coming from Rucio. ", "3097": "---------- Quite often people ask DDM support \"what happened to my dataset?\" for datasets that have been deleted. It would be good to be able to query the metadata of these datasets, eg time it was deleted. ------------ Add an option to the CLI like `--deleted` to query the deleted DIDs table. ", "3096": "---------- Currently there are several interfaces to address metadata in Rucio. This should be unified into one interface which intelligently selects which backend to use. (Currently: Hardcoded columns or JSON columns) In the future this enables us to extend Rucio to external metadata catalogs which are accessed via the unified frontend. ", "3084": "---------- Some usless duplicate functionality. Rather keeping in the Client. ------------ Keeping basic functinalities, but: 1) removing the file loop 2) duplicate check for existence of a file 3) renaming - we can do better, duplicate code 4) adding proper exceptions 5) input validation ", "3076": "---------- Error is thrown when fts_hosts is empty string. The 'fts' attribute is empty for Globus transfers resulting in an error. ------------ I made 2 changes to fix here but cannot sure if they're ideal for FTS transfers.  ", "3042": "---------- Currently, `DESY-ZN_DATADISK` is blacklisted for deleting, but the Dark Reaper does not take that information into account. ``` 2019-10-25 15:13:28,661 23609 WARNING Dark Reaper 12-20: Deletion NOACCESS of data18_13TeV:data18_13TeV.00355529.physics_Main.merge.AOD.f948_m1999._lb0353._0005.1 as davs://lcg-se0.ifh.de:2880/pnfs/ifh.de/data/atlas/atlasdatadisk/rucio/data18_13TeV/5b/28/data18_13TeV.00355529.physics_Main.merge.AOD.f948_m1999._lb0353._0005.1 on DESY-ZN_DATADISK: The requested service is not available at the moment. ``` ------------ Mimic the behaviour of the regular Reaper. ", "3031": "---------- rucio download is not able to download files if the scope or name contains \"/\" ", "3023": "---------- Transfers with the source files on tape are done in several steps by FTS: The staging and then the transfers to the final destinations. Now, only the later it is reported in Rucio. It might be be good to add the beginning and end of the staging as reported by FTS in order to do further analysis, e.g. staged_at, staging_at. Both values are reported as \"staging_finished\" and \"staging_start\" by FTS when querying the file transfer statuses. ", "2983": "---------- MySQL5 is still the default version on many distributions, so we cannot force upgrade to MySQL8. Make it compatible with both version. ", "2973": "---------- The implementation here can be simplified:  *This should not be done until GFAL 2.16.4 reaches EPEL.* ------------ Do not make a separate call to GFAL to create the destination path. Use the `create_parent` transfer parameter instead. ", "2961": "---------- From Rod (See  > This file > data15_13TeV:data15_13TeV.00280673.physics_CosmicCalo.merge.RAW._lb0080._SFO-ALL._0001.1 > has 3 tape replicas, plus in an archive on tape and disk. > rucio list-file-replicas shows only the replica from the staged zip file on IN2P3-CC_DATADISK > not the archive on IN2P3-CC_DATATAPE and not the 3 constituent replicas on tape. > The see the latter again, > --no-resolve-archives > The python client shows all 5 replicas. > I don`t think it will break prod - just confusing. > I had another example with no replicas, before adding --no-resolve-archives > data17_13TeV.00341294.physics_Background.merge.RAW._lb0130._SFO-7._0001.1 > Cheers, > Rod. ", "2926": "---------- rucio upload --rse MOCK4 --scope mock file mock:ds rucio upload --rse MOCK4 --scope mock file2 mock:ds rucio download mock:ds mock:file2 The replica of mock:file2 seems to be added to list_replicas twice. ------------ ", "2915": "---------- Traceback : ``` [Thu Sep 19 08:30:41.397728 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] Traceback (most recent call last): [Thu Sep 19 08:30:41.397762 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 239, in process [Thu Sep 19 08:30:41.397766 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] return self.handle() [Thu Sep 19 08:30:41.397774 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 230, in handle [Thu Sep 19 08:30:41.397777 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] return self._delegate(fn, self.fvars, args) [Thu Sep 19 08:30:41.397780 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 462, in _delegate [Thu Sep 19 08:30:41.397783 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] return handle_class(cls) [Thu Sep 19 08:30:41.397786 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/web/application.py\", line 438, in handle_class [Thu Sep 19 08:30:41.397789 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] return tocall(*args) [Thu Sep 19 08:30:41.397791 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/rucio/web/rest/account.py\", line 509, in POST [Thu Sep 19 08:30:41.397794 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] raise  'Duplicate', error.args[0]) [Thu Sep 19 08:30:41.397797 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] File \"/usr/lib/python2.7/site-packages/rucio/common/utils.py\", line 309, in  [Thu Sep 19 08:30:41.397800 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] if len(str(exc_msg)) > 15000: [Thu Sep 19 08:30:41.397803 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] UnicodeEncodeError: 'ascii' codec can't encode character u'\\\\xed' in position 89: ordinal not in range(128) [Thu Sep 19 08:30:41.397823 2019] [:error] [pid 3042:tid 139805664990976] [client 188.185.80.147:50832] ``` ", "2881": "---------- After recreating an Identity using the following steps the RUCIO allows the token creation using Creating the test identity **fernando** `rucio-admin identity add --account carlos --type USERPASS --id fernando --email cgamboa@bnl.gov --password secret Added new identity to account: fernando-carlos` Listing identities for account **carlos** `[root@2b8957951db8 rucio]# rucio-admin account list-identities carlos Identity: fernando, type: USERPASS` Deleting the Identity recently created: `[root@2b8957951db8 rucio]# rucio-admin identity delete --account carlos --type USERPASS --id fernando Deleted identity: fernando` Creating same Identity with a different password `rucio-admin identity add --account carlos --type USERPASS --id fernando --email cgamboa@bnl.gov --password othersecret` I can still connect using the old password the one set when the identity was created. Token is created for the this password. The token is not created for the **identity** that was deleted and then reused/recreated with a different password. We are using RUCIO 1.20.2 ------------ ", "2830": " ", "2827": "---------- psycopg throws new kinds of exceptions when running on PSQL11. ", "2770": "---------- All three methods below should work equivalently: ``` [root@20920a55b4bc rucio]# rucio -S userpass -u asdf -pwd zxcv whoami cannot get auth_token 2019-07-31 15:11:27,139 ERROR Cannot authenticate. Details: userpass authentication failed ``` ``` [root@20920a55b4bc rucio]# RUCIO_ACCOUNT=mario rucio -S userpass -u asdf -pwd zxcv whoami cannot get auth_token 2019-07-31 15:11:30,424 ERROR Cannot authenticate. Details: userpass authentication failed ``` ``` [root@20920a55b4bc rucio]# rucio --account mario -S userpass -u asdf -pwd zxcv whoami status : ACTIVE account : mario account_type : USER created_at : 2019-07-31T14:38:57 updated_at : 2019-07-31T14:38:57 suspended_at : None deleted_at : None email : None ``` ", "2714": "---------- Some function based indexes are missing in models.py. For example the index for `REPLICAS_STATE_IDX` is missing, while it is defined in the schema.sql for oracle. For other indices this exists. e.g. `REPLICAS_TOMBSTONE_IDX` is available, although it is function based. ------------ 1. Need to check which function based indices are missing in the models.py 2. They need at least to be added (non function-based) to the models.py 3. Investigate if it is possible to add function based indices with SQLAlchemy, if yes, all of them need to be re-written. ", "2706": "---------- To help tape systems with colocation, we will pass two metadata hints per file: `storage_class`:`<classname>` as a single value, and `group_hints`:`['<mostimportant>', .... , '<leastimportant>']` as an ordered list of string values. ", "2703": "---------- Would be great to support text file upload, or command line method to submit exceptions for the lifetime model. Currently it requires user to pick up samples one-by-one (with wildcard but still, not that convenient), which is low efficient, and very easily to introduce mistakes there. ------------ ", "2686": "---------- One cannot see the parameters used in the POST/PUT requests making it very difficult for debugging ", "2639": "---------- Creation date of rule in the future, injector will only inject it then. ", "2637": "---------- A way to track (visualise) what happened to a did. - Check historic information - Transfers - etc. ", "2636": "---------- As discussed at the Community workshop. We want to establish some kind of repository for operator documentation / recipes which allows convenient ways of contribution (Wiki or similar) ", "2635": "---------- Overview ticket for Multi VO development. ", "2634": "I have noticed that in several places on CERN intranet and presentations/slides there is a link to Rucio HOWTO page  which does not exist anymore. If the page has been moved or replaced it would be nice to have a redirect to a new location. Also the output from `rucio list-file-replicas --help` shows a link to  that page is not there too. ", "2631": "---------- Documentation page listing all possible config table and RSE attribute functional parameters. ", "2630": "---------- 1. Need to specify a policy about which information goes where. 2. Needs to be enforced an current inconsistencies need to be removed. 3. We need to write down in a concise documentation page what are the (optional) configuration parameters for both .cfg and config table (Also need a page for functional RSE attributes) ", "2621": "---------- Most likely because `activity_shares` is missing here:  ", "2613": "---------- From Rod: > Hi, > I was recently introduced to >  > where I can manually declare suspicious files bad, which is required for last replicas. It is nice, but I have a couple of feature requests. > 1) select all button, and ideally select range with shift key. prodtask-dev can do this. > 2) link to dashbd transfer errors for each file, example below. > 3) a filed with the last data.reason_text from the dashbd > Cheers, > Rod. >  ", "2582": "@ericvaandering ---------- I am using the CMS rucio-dev instance. I have a couple of 'test' RSEs which are not controlled by any centralised scripts. The only changes to these RSEs should be manual changes. However, the distance/ranking values between my RSEs and others seem to be 'lost' in between sessions. I cannot view them (using _get-distance_) or add a rule making a file transfer between them, until I have used _update-distance_ to redefine the values. ``` rucio-admin rse get-distance T1_FR_CCIN2P3_Disk_Test T3_CH_CERN_EOS_Test %d format: a number is required, not NoneType This means the parameter you passed has a wrong type. ``` Nevertheless the distance/ranking is still set in some form, as I cannot use _add-distance_. ``` rucio-admin rse add-distance --distance 1 --ranking 1 T1_FR_CCIN2P3_Disk_Test T3_CH_CERN_EOS_Test An object with the same identifier already exists. Details: Distance from 48bd136e99a445398fe2075a8da0eb05 to f3c12eb9f9bf4a3ea5484743de0ba1a3 already exists! This means that you are trying to add something that already exists. ``` Within the same session, these values seem to persist, but next time they need redefining again. ------------ ", "2543": "---------- Upgrade version of pyflakes, flake8, pycodestyle and pylint ", "2542": "---------- There are multiple throttler tickets for different use cases: FIFO: - [x] throttle per activity - source RSE #2611 - [x] throttle per activity - destination RSE - [ ] throttle per activity - destination and source RSE #53 - [x] throttle per all activities - destination RSE #1836 - [x] throttle per all activities - source RSE #2611 - [ ] throttle per all activities - source and destination RSE #53 Grouped FIFO: - [x] throttle per all activities - destination RSE #2220 - [x] throttle per all activities - source RSE #2611 - [ ] throttle per all activities - destination and source RSE #53 ------------ ", "2517": "---------- The list_replica command when no replicas are there matching the requirements, returns a generator with 1 element which is an empty string ``` >>> list(rcli.list_replicas([{'name': '/ST_t-channel_top_4f_InclusiveDecays_TuneCP5_PSweights_13TeV-powheg-pythia8/RunIIFall17DRPremix-PU2017_94X_mc2017_realistic_v11-v1/AODSIM#6f7f91ba-4c38-4a31-af33-aa99287a9839', 'scope': 'cms'}], rse_expression='T2_FR_GRIF_IRFU')) [u''] ``` I may be wrong but I think it used to return an empty generator in previous versions (current 1.19.7) ------------ return an empty list ", "2459": "---------- Given that a Rucio dataset corresponds to a CMS block, it would be extremely helpful if the rucio-clients `list_dataset_replicas` API would accept a list of dataset names, such that we can fetch all CMS block replicas that belong to a CMS dataset in a single call (thus having a better performance at multiple layers). For the record, I've placed the same issue under the CMSRucio project:  FYI @ericvaandering ------------ Support a list of datasets in the `list_dataset_replicas` API, from the rucio-clients package. ", "2417": "---------- The query to get suspicious files is not optimal. Sometimes Oracle switch from Nested Loop to Hash Join that can result to a huge slowing down of the query. ------------ Use the following query : ``` WITH bad_repl AS ( SELECT scope, name, rse_id, COUNT(*) FROM atlas_rucio.bad_replicas WHERE created_at > :created_at_1 GROUP BY scope, name, rse_id HAVING COUNT(*) > :count_2 ) SELECT /*+ cardinality(bad_repl, 100) INDEX(replicas replicas_pk) */ atlas_rucio.replicas.scope AS atlas_rucio_replicas_sco_1, atlas_rucio.replicas.name AS atlas_rucio_replicas_nam_2, atlas_rucio.replicas.rse_id AS atlas_rucio_replicas_rse_3, MIN(atlas_rucio.replicas.created_at) AS min_1 FROM atlas_rucio.replicas, bad_repl WHERE atlas_rucio.replicas.scope = bad_repl.scope AND atlas_rucio.replicas.name = bad_repl.name AND atlas_rucio.replicas.rse_id = bad_repl.rse_id GROUP BY atlas_rucio.replicas.scope, atlas_rucio.replicas.name , atlas_rucio.replicas.rse_id; ``` ", "2414": "---------- The current configuration template for atlas at `etc/rucio.cfg.atlas.client.template` doesn't work out of the box. ------------ A working template (client section only): ``` [client] rucio_host =  auth_host =  ca_cert = client_cert = $X509_USER_PROXY client_key = $X509_USER_PROXY client_x509_proxy = $X509_USER_PROXY request_retries = 3 auth_type = x509 ``` It would also be good to explain the configuration in the docs, I couldn't find anything about it there. ", "2410": "---------- Right now Rucio only supports adler32 and md5 checksums via two specific columns for these checksums. This creates problems if a community has different checksums (e.g. CMS historically used crc checksums) and wants to store them. On the dev meeting on  it was discussed to extend Rucio with a generic checksum column of string type. Multiple checksums can be added (comma-separated) to this column, always specifying the checksum type. E.g. `md5:1234,adler32:5678,crc:123` We would have to specify a strategy in migrating to this new checksum column. ", "2393": "---------- It might be desirable to recreate the original dataset from the zip files, or at least amusing in a low-priority kind of way. All the metadata for constituent files and dataset exists. ------------ rucio client pulls the zip file, unpacks, verifies and then stores the constituent replicas. It would take the constituent dataset as the argument and cycle over the zip files so as to not use too much disk space. It should be recursive, so only recovering the files without a replica. To do this at scale would need some prodsys development, but I am not sure we will ever need that. Some way to do it manually might come in handy though. Cheers, Rod ", "2387": "---------- The tests right now the database testing workflow foresee the following: 1. Setup database with current `models.py` 1. Do full downgrade (all revisions) with alembic 1. Do full upgrade (all revisions) with alembic 1. Run the actual test cases Now the fact that the tests run through point to the fact that the database is actually equal to the one defined in the current `models.py`. However, it is not guaranteed. What we would need is an extra step which compares the database schema before and after. How this actual check works is a bit difficult though. It could be a table describe for each table and then just compare strings, but this would require the order of constraints, indices etc. to be kept the same. ", "2356": "---------- We sometimes have problem with sites running dual stack IPv4/6 storage. Sometimes the IPv6 interface is broken whereas the IPv4 works. One would need a switch (e.g. RSE attribute) to force the use of IPv4. ", "2330": "---------- - [ ] Default alembic.ini location should not be main rucio directory, but should point to etc/alembic.ini like the rest of the configuration. - [ ] Remove tox, everything is properly covered by Travis - [ ] Some hosts are pointing to cern.ch servers in the default template, replace with fake hostnames - [ ] Remove useless scripts from tools/ (e.g., sweep-srm, conftest, ...) ", "2325": "---------- The current main page of the WebUI is a generic one. It would be useful to have a main page that shows the most interesting informations for the end-user. Non-exhaustive list : - Table with available space and quotas and/or bar chart showing the available space by RSE - List of replicating/stuck rules - List of requests for exceptions to the lifetime model ", "2318": "---------- DPA needs a too to get the history of bad files ", "2312": "---------- This is an overview ticket for the Rucio redirector. - Need to assess the status of the service - Need to expand tests to ensure usability ", "2311": "---------- At the moment we only have very limited upload/download tests either using mock calls or direct posix protocols. One possibility to expand these to actualy functional tests would be to use xrootd docker containers and start up two xrootd storages in the test environment. This would allow us to test upload/download also with different protocols, such as gfal, which would also help enormously to validate gfal for python 3.6. ", "2310": "---------- Some users provided feedback to improve the suspicious files view in the WebUI ------------ - If the RSE pattern is not OK, do not run the loop forever. Instead break with clear pattern that RSE expression is wrong - I have declared some files as bad. After Could you put a flag to declare that the file was declared lost. ", "2282": "---------- The log files that have been declared suspicious a lot of times can be safely declared `BAD`. For now, this is done as a probe, but eventually it must be done by a dedicated daemon that apply different policies based on the project/datatype/nb_replicas. ", "2264": "---------- The demo image contains reference to service hosted at CERN. Need to be removed ", "2263": "---------- Currently, the Auditor expects to be able to list directory entries in order to identify the latest dump. This is not possible on object storages. ------------ Investigate and implement an alternative, such as: 1. Try the last _n_ days, based on the pattern 2. Use another file as an index 3. Use a static location and obtain the date from its metadata ", "2258": "---------- Since the merge of #1842 `rucio list-rse-attributes` crashes for staging endpoints: ``` [15:45][twegner@lxplus113 ~]$ rucio --version rucio 1.19.0.post2 [15:45][twegner@lxplus113 ~]$ rucio -v list-rse-attributes TAIWAN-LCG2_TAPE-STAGING 2019-02-26 15:45:25,391 DEBUG Traceback (most recent call last): File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.0.post2/bin/rucio\", line 159, in new_funct return function(*args, **kwargs) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.0.post2/bin/rucio\", line 1479, in list_rse_attributes print(tabulate.tabulate(table, tablefmt='plain')) File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.0.post2/lib/python2.7/site-packages/tabulate.py\", line 1301, in tabulate for c, ct, fl_fmt, miss_v in zip(cols, coltypes, float_formats, missing_vals)] File \"/cvmfs/atlas.cern.ch/repo/ATLASLocalRootBase/x86_64/rucio-clients/1.19.0.post2/lib/python2.7/site-packages/tabulate.py\", line 768, in _format return format(float(val), floatfmt) ValueError: invalid literal for float(): True 2019-02-26 15:45:25,391 ERROR invalid literal for float(): True 2019-02-26 15:45:25,392 ERROR Strange error: No section: 'policy' ``` ------------ ", "2254": "---------- We tried to switch implementation for xrootd protocol at ral-echo_scratchdisk, and it didn't work from obvious reasons. 1) and not startswith('root') needs to be added to line:  2) stat method returning checksum needs to be implemented mlassnig@lxplus106:~ $ xrdfs root://eosatlas.cern.ch:1094 query checksum //eos/atlas/atlasdatadisk/rucio/mc15_13TeV/6a/54/HITS.06828093._000096.pool.root.1 adler32 5d000974 ", "2232": "---------- Whenever a lock is being repaired, this `lock_repair_cnt` is incremented by 1. As a second condition (Next to the 2 week timeout) the repairer can then also SUSPEND rules based on the `lock_repair_cnt`. ", "2218": "---------- This would be a possibility to test new releases without having to deploy them on CVMFS. ", "2216": "---------- ``` >>> from rucio.daemons.auditor import process_output >>> process_output('\u2026') Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/lib/python2.7/site-packages/rucio/daemons/auditor/__init__.py\", line 188, in process_output lost_pfns = [r['rses'][rse][0] for r in list_replicas(lost_replicas) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 327, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (cx_Oracle.DatabaseError) ORA-00600: internal error code, arguments: [qcscbndv1], [65535], [], [], [], [], [], [], [], [], [], [] [SQL: u'SELECT /*+ INDEX(DIDS DIDS_PK) */ atlas_rucio.dids.scope AS atlas_rucio_dids_scope, \u2026 ``` Note that this is not an ordinary case. The file used in this example has about 41k lost files. Originally I was chunking on the Auditor\u2019s side, but I was recommended against that. ------------ ", "2213": "---------- When the last replica of a file is removed in the reaper, it should only be removed from the dataset if it is not a constituent of an existing archive replica. This also implies, if a archive replica is removed and there are no replicas on the constituents, the constituents need to be removed from their parent datasets. ", "2159": "---------- Currently it is not clear why a certain request is REPLICATING, but did not issue a FTS transfer yet. We should expose that a request is in WATING (or even MISMATCH) state. What would also be good to maybe show the queue length, although this might be more difficult as it is depending on throttle configuration. ", "2143": "---------- Zenodo  is a research data repository, especially nice for linking publications to the datasets they used. Investigating integration/interfacing with Zenodo might be useful. ", "2104": "---------- There are double counts in the lost files reports when the same lost file is in many datasets. ------------ easy ", "2063": "---------- There are currently 7 history tables which are defined by using the SQLA model base `Versioned`. The advantage of this is that the history table does not have to be created by hand, but it is just mirrored by the primary base. However, by doing this we loose full control about the indices and primary keys of the history table, as it is also just mirrored from the primary model. This already creates inconsistencies, as the history tables we define on oracle, do not have indices and primary keys. ------------ Two possibilities: - Find a way to adapt indices/PKs for versioned tables (AFAIK not possible) - Do not use Versioned at all and define the history tables explicitly; This is already done for some tables anyway. ", "2029": "---------- Assuming that the DID exists, this command returns a broken XML where the closing tag is missing. It is caused by using a RSE expression that returns no RSEs ```shell ReplicaClient().list_replicas([{'name': 'file_LMXZHCXRHS', 'scope': 'data13_hip'}], rse_expression='test=True', metalink=True) <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<metalink xmlns=\"urn:ietf:params:xml:ns:metalink\">\\n ``` ------------ ", "1923": "---------- - Wrong title in lifetime_exception should be fixed - Need a button to bulk update the WAITING requests ", "1911": "---------- Even on a bad installation or typos in the configuration system, `rucio --help` ought to work. The current version of `rucio` imports the `DownloadClient` at the module level. If there is an error in the `rucio.cfg` that causes the `DownloadClient` constructor to throw an exception, you can't get to `--help`. Example: ``` $ rucio --help Traceback (most recent call last): File \"/home/cse496/bbockelm/software/rucio-doma/bin/rucio\", line 63, in <module> from rucio.client.downloadclient import DownloadClient File \"/home/cse496/bbockelm/projects/rucio/lib/rucio/client/downloadclient.py\", line 34, in <module> client = Client() ... stack trace goes on for miles ... File \"/home/cse496/bbockelm/software/rucio-doma/lib/python2.7/site-packages/requests/adapters.py\", line 412, in send self.cert_verify(conn, request.url, verify, cert) File \"/home/cse496/bbockelm/software/rucio-doma/lib/python2.7/site-packages/requests/adapters.py\", line 227, in cert_verify \"invalid path: {0}\".format(cert_loc)) IOError: Could not find a suitable TLS CA certificate bundle, invalid path: /etc/ca.crt ``` In my case, the problem was caused by an invalid path in `rucio.cfg` for the CA bundle. ------------ Failures to create the clients should be swallowed until they are actually used. Potentially we shouldn't even import the clients until needed. ", "1895": "---------- This is needed to let the development of user data transfers code separated from the RUCIO one. ------------ Talking in particular of using `importlib` (using configuration param)  for the following methods:: - user to DN mapping - myproxy interactions I would go with importing just the methods, since the case of mapping a user to a DN and then retrieving the corresponding proxy may have a more general use. In any case a feedback on this is very welcome. ", "1867": "---------- The calculation does not take into account the replicas that are already available on the destination. ------------ Use `rucio.core.replica.list_replicas()`. The operation can be expensive, but since it\u2019s not done frequently, it should be fine. ", "1860": "---------- For rules on containers, when datasets are detached from the container, the respective DATASET_LOCK does not seem to be removed properly. There were some cases found where this was the case. Needs to be investigated and fixed. ------------ TBD ", "1839": "---------- It\u2019s difficult memorising command-line options when they are named differently across commands. `delete-rule` uses `--rse_expression`, `list-file-replicas` has both `--rse` and `--expression` (with different meaning), `get` has `--rse` but it actually takes an RSE expression. ------------ First it has to be decided whether it\u2019s worth breaking the current CLI interface to fix this. ", "1834": "---------- The tracer server accepts any kind of json send to it and expects clients to send the information using the correct schema. Sometimes a bug in a client can lead to an invalid trace that then can break any of the systems using the traces (monitoring, Kronos, etc.) ------------ Add a validator that listens on the ActiveMQ topic and constantly checks that necessary fields are set correctly or set at all. If something doesn't match the schema send out notifications. ", "1830": "---------- Separate logfile for dataset and file thread. ", "1829": "---------- A run-once unittest by mocking/pre-filling an activeMQ queue to consume from. ", "1817": "---------- The Flask API has problems with dealing exceptions when using JSON streaming. This should be fixed as it is essential for handling lot of data. ------------ See  for implementing JSON streaming to change the Flask API. Investigate how to deal with exceptions. ", "1808": "The LIGO and Virgo communities are stepping into Rucio to manage ste storage elements of their infrastructures. While LIGO is provided of proprietary HPC clusters, Virgo isn't and instead relies on a set of academic computing centers. The characteristics of the latter, as well as some previous choices, strongly point to a wide adoption of DIRAC in Virgo, while the interoperability with LIGO requires Rucio as storage manager/orchestrator. After some discussion we (maybe) found a nice solution. Instead of developing a DIRAC plugin able to interface it to Rucio in a POSIX-like manner, we think the best option is to create a \"DIRAC-mode\" for the Rucio catalog. This argument follows a comment of many people: DIRAC was born to create a uniform interface to different GRID implementations, but ended up managing both Computing Elements (CE) and Storage Elements (SE). This choice was taken to make DIRAC aware of the geographical data position in order to minimize the data transfers. Since DIRAC is used by a rather small community not a lot of organizations might be interested in stepping in the development of an integration. Instead Rucio is much more appealing and finding a way to keep in the Rucio's external catalog enough topological information to make DIRAC happy and efficient might be the way. In addition we discovered that DIRAC allows to pass it a custom catalog and some Virgo people has already performed some tests in that sense, creating an LFC catalog dump and running a DIRAC instance on that data. In fact such solution might scale up to decoupling the DIRAC storage function from the computing function, giving lots of benefits even to the DIRAC product. This could bring in some of their developers to assist in the process. Worth mentioning that DIRAC jobs can be any kind of executable, from an `sh` script to an executable available cluster(s)-wide (e.g. firefox...). Since the read from Rucio-managed files should be supported OOTB by DIRAC, the need of a plugin is needed only to register on Rucio the output files of the jobs. However, since DIRAC can run basic scripts, at first the registration of output files on Rucio might be handled by by-hand calls to the Rucio API from within the DIRAC job. If we think about a C++ compiled executable which produces a ```myfile.root``` file running on a shell ```myexe myfile.root```. Wrapping the same in something like ```myexe myfile.root && <rucio_API_call> myfile.root``` should do the basic tricks. ", "1806": "---------- The default transfer submission flow is based on the assumption that the proxy has already been delegated by an external script. For user transfers an integrated delegation is already in like  And a standalone method in the transfertool is already available  A similar solution can be implemented also for the default submission  avoiding to keep a cron/script doing the delegation externally. This new behavior must be optional and available through a configuration parameter. ------------ 2 steps needed here: - align the fts transfertool code to use python bindings. Followed on #1805 - check for response equivalence w.r.t. REST response - mock_fts3 may be needed for a proper testing - do FTS job submission with the integrated delegation check available by a configuration parameter ", "1801": "---------- The defaults we have for a couple of options could be different between VOs (e.g., `rucio list-dids --filter type=ALL`). Investigate VO-specific defaults for the clients. ", "1797": "---------- Concern by CMS that data which is only on the tape buffer may not be migrated to tape before additional disk copies are deleted which could result in the loss of data. ------------ Undetermined, but as I understand it - [ ] Add a new, optional, state \"Waiting for tape check\" - [ ] FTS transfers to tape systems are put in this state when the transfer finishes - [ ] A new daemon checks the state with the storage system, moves state to the one which can be picked up by the finisher ", "1773": "---------- For some use-cases the default of exracting metadata for specific file extensions is undesired. E.g. to use `rucio upload` with ROOT files within an standalone container image that does not have the software of a specific experiment installed. (partly discussed here:  ------------ allow to add configuration optiosn in the cfg file that list the file extensions for which metadata extraction should be performed. the defaults would stay as it is (extract GUID for root files) but it could be switched off ", "1771": "---------- Sorting of column of type datetime is broken datatable. The sorting is done alphabetically and not based on the date ", "1760": "---------- during last days, always when bb8 statrted to rebalanced, othe activities were completely suppressed ------------ 1) lower the share:  rebalancing is of 3rd highest priority now 2) distribute the rebalancing over a day. **Use case for delayed rule.** ", "1759": "---------- I've recently started seeing various mysql exceptions. First, listing file replicas in a dataset gives a database exception message in the client: ``` (ligo-rucio) \u279c ~ rucio list-file-replicas O1:L-L1_R-11266 2018-11-06 14:16:35,616 ERROR Database exception. Details: (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'SELECT rses.id AS rses_id, rses.rse AS rses_rse, rses.rse_type AS rses_rse_type, rses.`deterministic` AS rses_deterministic, rses.volatile AS rses_volatile, rses.staging_area AS rses_staging_area, rses.city AS rses_city, rses.region_code AS rses_region_code, rses.country_name AS rses_country_name, rses.continent AS rses_continent, rses.time_zone AS rses_time_zone, rses.`ISP` AS `rses_ISP`, rses.`ASN` AS `rses_ASN`, rses.longitude AS rses_longitude, rses.latitude AS rses_latitude, rses.availability AS rses_availability, rses.updated_at AS rses_updated_at, rses.created_at AS rses_created_at, rses.deleted AS rses_deleted, rses.deleted_at AS rses_deleted_at \\nFROM rses \\nWHERE rses.deleted = false AND rses.rse = %s'] [parameters: ('LIGO-WA',)] (Background on this error at:  ``` Next, the judge-cleaner log shows the following message several times a second: ``` 2018-11-06 11:20:17,659 3158 DEBUG rule_cleaner[0/0] index query time 0.002273 fetch size is 1 2018-11-06 11:20:17,660 3158 INFO rule_cleaner[0/0]: Deleting rule eb8794198a854dcd88846ca95998385c with expression UNL 2018-11-06 11:20:17,703 3158 DEBUG Deleting lock O1:H-H1_R-1126621184-64.gwf for rule eb8794198a854dcd88846ca95998385c Exception _mysql_exceptions.InterfaceError: (0, '') in <bound method SSCursor.__del__ of <MySQLdb.cursors.SSCursor object at 0x7f8c531e8950>> ignored (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('O1', 'H-H1_R-1126621184-64.gwf', '\\xeb\\x87\\x94\\x19\\x8a\\x85M\\xcd\\x88\\x84l\\xa9Y\\x988\\\\', 'NI\\t\\xdc\\xf5\\x13I\\xb0\\x93\\xb0\\x1a\\xf41\\xb6\\xf5\\xc6')] (Background on this error at:  2018-11-06 11:20:17,710 3158 ERROR Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/judge/cleaner.py\", line 104, in rule_cleaner delete_rule(rule_id=rule_id, nowait=True) File \"/usr/lib/python2.7/site-packages/rucio/db/sqla/session.py\", line 360, in new_funct raise DatabaseException(str(error)) DatabaseException: Database exception. Details: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely) (_mysql_exceptions.ProgrammingError) (2014, \"Commands out of sync; you can't run this command now\") [SQL: u'DELETE FROM locks WHERE locks.scope = %s AND locks.name = %s AND locks.rule_id = %s AND locks.rse_id = %s'] [parameters: ('O1', 'H-H1_R-1126621184-64.gwf', '\\xeb\\x87\\x94\\x19\\x8a\\x85M\\xcd\\x88\\x84l\\xa9Y\\x988\\\\', 'NI\\t\\xdc\\xf5\\x13I\\xb0\\x93\\xb0\\x1a\\xf41\\xb6\\xf5\\xc6')] (Background on this error at:  ``` Finally, the reaper log shows the following message which may or may not be related: ``` 2018-11-02 13:59:22,418 2582 INFO Reaper 0-1: Running on RSE LIGO-CIT-ARCHIVE None 2018-11-02 13:59:22,441 2582 DEBUG Reaper 0-1: list_unlocked_replicas on LIGO-CIT-ARCHIVE for None bytes in 0.0183210372925 seconds: 1 replicas 2018-11-02 13:59:22,442 2582 CRITICAL Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/rucio/daemons/reaper/reaper.py\", line 215, in reaper prot = rsemgr.create_protocol(rse_info, 'delete', scheme=scheme) File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 171, in create_protocol protocol_attr = select_protocol(rse_settings, operation, scheme, domain) File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 146, in select_protocol candidates = _get_possible_protocols(rse_settings, operation, scheme, domain) File \"/usr/lib/python2.7/site-packages/rucio/rse/rsemanager.py\", line 122, in _get_possible_protocols ' found : %s.' % str(rse_settings)) RSEProtocolNotSupported: RSE does not support requested protocol. Details: No protocol for provided settings found : {'third_party_copy_protocol': 1, 'rse_type': 'DISK', 'domain': ['lan', 'wan'], 'availability_delete': True, 'delete_protocol': 1, 'rse': u'LIGO-CIT-ARCHIVE', 'deterministic': True, 'write_protocol': 1, 'read_protocol': 1, 'staging_area': False, 'lfn2pfn_algorithm': 'ligo', 'availability_write': True, 'volatile': False, 'availability_read': True, 'credentials': None, 'verify_checksum': True, 'id': '65b733dd9afb4ec3beee2ac5c0fbc5c6', 'protocols': [{'extended_attributes': None, 'hostname': u'ldas-pcdev5.ligo.caltech.edu', 'prefix': u'/hdfs/frames', 'domains': {'wan': {'read': 1L, 'write': 0L, 'third_party_copy': 1L, 'delete': 0L}, 'lan': {'read': 0L, 'write': 0L, 'delete': 0L}}, 'scheme': u'gsiftp', 'port': 2811L, 'impl': u'rucio.rse.protocols.gfalv2.Default'}]}. ``` Note that these errors only seem to be affecting deletion of rules and replicas; actual transfer operations appear to be unaffected. ------------ ", "1752": "---------- While working with Rucio data records I found that their structure is not static in terms of data attributes. By that I mean that returned dictionary contains dynamic set of keys, plus it mixed pre-defined attributes names with attribute values. To demonstrate this I'll take one particular record from replicas API, e.g. ``` curl -H \"Accept: application/json\" -H \"X-Rucio-Auth-Token: $token\"  ``` This API returns the following records ``` {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {}, \"adler32\": \"06273a48\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/F8AD7081-A037-E811-BF64-0CC47AC52BAE.root\", \"rses\": {\"T2_US_Nebraska\": []}, \"scope\": \"cms\", \"bytes\": 2877629555, \"md5\": null} {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {}, \"adler32\": \"881ec8cf\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\", \"rses\": {}, \"scope\": \"cms\", \"bytes\": 3890770828, \"md5\": null} ``` These records contain static attribute names, such as `states`, `md5`, `scope`, etc. and within `states` part of the records they contain dynamic keys, in this case both records return `T2_US_Nebraska`, but in general it would be name of the rse. As you can see, the record keys (attributes) will be different in different type of queries since you used `rse` values as record key (attribute) of states data-structure. This is represent certain problem to treat these records in generic way, e.g. I can't map a record into static data-structure since records keys will be dynamic. Instead, rucio should return static data-strictures all the times. For instance the states sub-structure may be represented as following: ``` \"states\":{\"rse\":\"T2_US_Nebraska, \"state\":\"AVAILABLE\"} or to be generic for multiple rses as \"states\":[{\"rse\":\"T2_US_Nebraska, \"state\":\"AVAILABLE\"}, {\"rse\":\"T2_XXX\", \"state\":\"STATE_OF_RSE\"}] ``` In this case the states has static data-structure where all keys (attributes) are well defined, i.e. in this example there are two keys: `rse` and `state`, and such structures will remain static for infinitive number of RSE/STATE values. I wonder how many APIs inherit this behavior and how hard/easy will be to fix them. I also need to know how many of such dynamic structures exists in Rucio APIs output. Another issue with such dynamic data-structures affects long-term analytics (if we plan to do it) where records keys will grow over time and will not represent finite set of attributes. For the record, this issue is very critical for CMS Data Aggregation System which will consume this data and need to aggregate it with rest of CMS data-service records. In order to do aggregation several issues should be solve, including this particular one. I'm CCing @ericvaandering Eric to follow up this issue. Eric, Rucio falls into the same trap as many other CMS data-services by using key/attribute values as keys of the underlying python dict keys. This represent issue with developing generic parser of Rucio records. ", "1746": "---------- I'm struggle to understand the difference between these two API calls: - /replicas/scope/name - /replicas/scope/name/datasets In both cases the output seems identical, while the API description is different. The former list all replicas for data identifier, while the later is list dataset replicas. What's the difference? Here is my calls (to CMS rucio server): ``` # this API /replicas/cms/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570 returns {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {}, \"adler32\": \"881ec8cf\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\", \"rses\": {}, \"scope\": \"cms\", \"bytes\": 3890770828, \"md5\": null} # this API /replicas/cms/Charmonium/Run2017D-31Mar2018-v1/MINIAOD#56f1f7ec-3789-11e8-a513-02163e018570/datasets returns {\"states\": {\"T2_US_Nebraska\": \"AVAILABLE\"}, \"pfns\": {}, \"adler32\": \"881ec8cf\", \"name\": \"/store/data/Run2017D/Charmonium/MINIAOD/31Mar2018-v1/90000/FA607615-8E37-E811-B2DE-0CC47AC52C8E.root\", \"rses\": {}, \"scope\": \"cms\", \"bytes\": 3890770828, \"md5\": null} ``` in both cases we have 283 returned documents. ------------ Also, I want to get replicas only for a given name and RSE pair and so far I can't find API I need. The only way to do this is to get list of all replicas for a given name and filter the RSE I'm interested in. Would it be more efficient to pass RSE as a parameter to replicas API to fetch only subset of them on that given RSE? For example `/replicas/<scope>/<name>/<rse>` ", "1667": "With a volatile RSE that is outside of Rucio's control, it can happen that the whole cache is wiped or publishing breaks or for some other reason the files are deleted without the ability to tell Rucio what has been deleted. This means that replicas will stay forever in the catalog. In this case it would be good to tell Rucio to wipe all replicas from the RSE then the site can start the publishing from scratch. Of course this is rather dangerous so care should be taken to restrict this operation to volatile RSEs, admin accounts etc. ", "1648": "---------- The probe doesn't set properly the `RSE_TYPE` (need first to implement #1647). Moreover it should set a default distance for all the new RSEs. ", "1636": "---------- Email discussion with ATLAS T0: \"\"\"btw, the double-slash is always necessary for root. it works for add_replica, so there must be something different in the method add_files_to_dataset() On 9/14/18 3:52 PM, Tomas Javurek wrote: > Trying to register with add add_files_to_dataset that Luc is using: > RSE does not support requested protocol. > > while when I set the protocol, it works fine. So, we need to keep it there for now. *However, please keep it with srm* for now, because it did work for me only with srm, when I tried with root protocol, it failed with the same error. And I tried both: > > 'root://sedoor1.bfg.uni-freiburg.de:1094/pnfs/bfg.uni-freiburg.de/data/atlasdatadisk/rucio/user/ddmadmin/\u2018 > 'root://sedoor1.bfg.uni-freiburg.de:1094//pnfs/bfg.uni-freiburg.de/data/atlasdatadisk/rucio/user/ddmadmin/' > > > >> On 14 Sep 2018, at 15:21, Tomas Javurek <Tomas.Javurek@cern.ch <mailto:Tomas.Javurek@cern.ch>> wrote: >> >> I couldn\u2019t find the place where we subtract the protocol part from path when registering to replicas. But I\u2019ll try brute-force. Just preparing a script that would imitate Lucs needs, and see whether I can register without protocol part. \"\"\" ", "1583": "---------- \"\"\" Request from dCache via Paul Millar. Use case is for Atlas to pass the dataset of a file to dCache, which would enable optimisations (colocation on a small set of tapes) and bulk operations (bringonline dataset) amongst other possibilities. \"\"\" Associated FTS ticket:  ", "1558": "---------- To protect from deletions for files which might still be in the processing in other daemons. ", "1504": "---------- Contention between the `undertaker` and the `conveyor` can occur in case the `undertaker` tries to delete some files that are being transfered, in particular if the transfers have been submitted to a slow responding FTS server. Indeed the `undertaker` locks the corresponding rows in replicas and requests table before sending the cancel request to FTS. ------------ The `cancel_request_did` must propagate a timeout to FTS, right now there is none. In addition I would propose to add a new state for the request `BeingDeleted` similar to the one we have for the replica. Doing that way, the `conveyor` would not try to update the requests that are being deleted by the `undertaker`. ", "1431": "---------- When deleting the last replica of a file, reaper deletes the file logically as well but does not delete the contents in the archive_contents table. This causes a foreign key constraint error and the reaper gets stuck. There are three changes needed in total for this. - [x] #1433 When deleting the file logically, the rows from the archive_contents table need to be deleted too - [ ] The just deleted archive_content rows have to be added to the archive_contents_history table - [ ] For the files whose archive just got deleted, it needs to be checked if they do not have any other replicas, in which case they need to be removed logically (If there are no rules on them, which is actually a bit pointless) - [ ] #2213 When the last replica of a file is removed in the reaper, it should only be removed from the dataset if it is not a constituent of an existing archive replica. This also implies, if a archive replica is removed and there are no replicas on the constituents, the constituents need to be removed from their parent datasets. ", "1393": "---------- Trailing \"/\" in Rucio are not part of the DID name, however, the historic ATLAS naming did include them, such as: `mc16_13TeV:mc16_13TeV.301024.PowhegPythia8EvtGen_AZNLOCTEQ6L1_DYmumu_600M800.deriv.DAOD_EXOT0.e3649_s3126_r9364_r9315_p3374/` For most commands REST endpoints including the DID are used, where this trailing `/` resulted in a `//` in the URL. This `//` is interpreted as one `/` in  and thus the command did still deliver a result. However, since the 1.16.* clients, both scope and name get escaped using `urllib.quote_plus()`. This way the name gets transformed in e.g.: `mc16_13TeV.301024.PowhegPythia8EvtGen_AZNLOCTEQ6L1_DYmumu_600M800.deriv.DAOD_EXOT0.e3649_s3126_r9364_r9315_p3374%2F` and the lookup fails. ------------ **Possible solutions:** 1. Use `urllib.quote_plus(s, '/')` to declare '/' as a safe character. This should work, but for commands which do not use the did in the REST call, the trailing / of a container was and will still be a problem. E.g.: list_dids with a filter; attach_dids (container to container), detach_dids (container from container) etc. - This also fails for communites using '/' in their naming scheme 2. Filter trailing / completely in the client. - Might be problematic for communities 3. Ask external applications to filter trailing `/` on their side. ", "1382": "---------- The Replicas controller creates the metalink output differently than the ListReplicas controller. ", "1351": "---------- the rucio/rucio-atlas docker images are excellent to do basic get/put operations within a minimal environment. However uploading ROOT files fails because the pool_extractFileIdentifier is not found. I'm cross-referencing an issue on the ATLAS JIRA were I asked whether we can provide a statically linked version of it:  ------------ If we can get a statically linked binary presumably one could just add it to the image. ", "1304": "---------- If the UploadClient is used to upload a new replica of a file DID (no dataset given) that is already registered, no rule will be updated or added. ------------ ", "1248": "---------- At the moment it tries to insert replicas to RSEs but has PK failures due to the already existing replicas in state BEING_DELETED. ", "1245": "---------- when I declare bad a huge amount of files ( for example when a machine cannot be recovered) it takes several hours and it is not possible to understand at what stage the declaration is. On occasion not all the files get declared as it has happened with Glasgow. Files were declared lost in December but some of them were still there yesterday and deletion failed and I had to reduclare lost >240,000 files.  ------------ It would be useful if this tool left wrote a log file with the number of files declared if the admin wants to or wrote to the output as part of the verbose when it does a commit. So if it declares lost 1000 files at the time it writes it has done those and if something fails it logs that too. So that it is possible to rerun only on the missing files instead of going through the whole lot. A summary at the end would also be useful. For example as I imagine yesterday went from the output 241000 processed 233000 uknown replicas 3000 already declared 5000 declared bad ", "1213": "---------- The recovery doesn't take into account the replicas on sites blacklisted for read. If a file is declared bad, and is not recoverable because the only remaining site where it is located is blacklisted for read, the necromancer should declare the file as recoverable and not as definitely lost. ", "1202": "---------- Currently it is possible to directly download single files from RSEs using the webui if the RSE is webdav enabled. Furthermore, the webui can also create metalink files for datasets. This metalink file can then be used in a download manager to download the full dataset. But the problem is that there are no extensions that are able to handle the certificate authentication. So the browser will ask the user every time to provide a valid certificate. ------------ Development of a browser extension that is able to handle the metalink files from rucio and the necessary authentication to the storage elements using a certificate installed in the browser. ", "1157": "---------- * Avoid errors when `alembic_version` is not set in the DB * Make sure that the server code can understand the DB schema. ------------ At start up the RESTful API checks whether the `alembic_version` is in a whitelist, or whether the code version is supported. ", "1152": "---------- It looks like when the server times out, no exception is raised to the client ------------ Raise a new TimeOut exception ", "1114": "---------- In case of open containers the length and #events is not recorded as metadata (yet), but could be recursively calculated from the constituent datasets (or even files). The idea is to add a `--recursive` option to `rucio get-metadata`. ", "1109": "---------- The error message shown to unregistered users in the webui always points to ATLAS voms. ------------ Make the message configurable. ", "1091": "# Archive missing features tracker In this ticket we are tracking/discussing the features missing from the Rucio archive support. - [x] Download full archive and extract a single file #46 - [x] xrdcp transfer a single file part of a zip file #1137 - [x] Transparent list-replicas support of zip contents #1138 - [x] Forward of rule creation from constituent to archive #1376 - [x] list_dataset_replicas should resolve archives as well #1375 - [x] extract flag needs to be added to metalink #1353 - [x] Bug with checksums on xrdcp streamed replicas needs to be fixed #1613 - [x] Checksum checking needs to be enabled again due to fix in #1613 - [x] Forward of constituent rule to archive in case of tape replica #1663 - [x] Client needs to extract files from archive #1354 - [x] Revert xrdcp workaround #1598 - [ ] Handling archives in the reaper #1431 - [ ] Handling of lost files in the necromancer (Should not be removed if there is a zip replica) - [ ] Full transparent handling of constituents in the judge/rules - [x] list_replicas should re-order zips and prioritise root protocol over everything else #2313 The advantage of the transparent support of replica listing of content archives (with the # format root supports) would be that there is little to do in the actual rucio client, as there is no protocol difference between downloading a normal file or a file in a zip file. ## Other discussions -  ", "1026": "---------- It would be good to have the tests running against the flask backend in travis (with the mode allow failures) to expose the errors. With python 3, it can be even better but would require to have fixed the syntax first. ", "1021": "---------- Install rucio with setup.py/pip: `pip install .[oracle,postgresql,mysql,kerberos,dev]` Instead of manual copying and linking. ", "857": "---------- There are some fts3 specific parts in the conveyor, mostly job dictionary building and processing of activemq message results which needs to be disentangled from the code and moved to the tranfertool. ", "781": "Right now, the status of a replication rule only provides a single high-level error summary. For example: ``` $ rucio rule-info --examine db3277cdd4324dcc87074cc8c84ee324 Status of the replication rule: TRANSFER_FAILED:TRANSFER [5] TRANSFER CHECKSUM MISMATCH USER_DEFINE and SRC checksums are different. 09e3c4ef != 8f14f45a STUCK Requests: cms:/store/mc/RunIISummer17PrePremix/Neutrino_E-10_gun/GEN-SIM-DIGI-RAW/MCv2_correctPU_94X_mc2017_realistic_v9-v1/20001/2E63DAAA-930C-E811-867F-0242AC130002.root RSE: T3_US_NERSC_REAL Attempts: 40 Last Retry: 2018-02-26 07:52:37 Last error: FAILED Last source: T2_US_Nebraska_REAL Available sources: T2_US_Nebraska_REAL Blacklisted sources: cms:/store/mc/RunIISummer17PrePremix/Neutrino_E-10_gun/GEN-SIM-DIGI-RAW/MCv2_correctPU_94X_mc2017_realistic_v9-v1/00032/2ABA00E8-A610-E811-ADDB-0242AC130002.root RSE: T3_US_NERSC_REAL Attempts: 40 Last Retry: 2018-02-26 13:38:41 Last error: FAILED Last source: T2_US_Nebraska_REAL Available sources: T2_US_Nebraska_REAL Blacklisted sources: ``` (this particular case goes on for another 200 files or so...) However, it's not practical to debug this rule because I have no clue what error corresponds to which file. s -------------- Two suggestions: 1. In the output of `rucio rule-info --examine ....`, record the last error message per stuck file. 2. Also record the last FTS job ID where the file failed (will probably also need the FTS server URL). ", "736": "---------- Add doc how to configure Rucio: rses, accounts, etc ", "713": "---------- When sorting the rules in R2D2 by creation date the result is wrong. Currently this is done using a simple string sorting which is wrong. ------------ Change the sorting to correctly sort the dates chronologically. ", "704": "---------- The conveyor receiver incorrectly handles multi source jobs. This is the case for jobs where the first source FAILS and the second one is a success. The poller requests jobs via the transfer tool which does quite a bit of response formatting and checking. In the case of multi source jobs, it basically iterates the sources and if one of the sources is in state FTSState.FINISHED it returns the job as SUCCESS. The receiver however does not go via the transfertool as it gets the response directly from activemq. In the case of a job where the first source fails but the second one is a succes, the receiver only looks at the `t_final_transfer_state` which is FTSCompleteState.ERROR. However, what it actually should do is to iterate the sources like the conveyor poller. It seems that the `t_final_transfer_state` field can only be used for bulk-transfers, not multi-source transfers. ------------ Factorize the formatting used in the transfer tool and re-apply to conveyor receiver code. ", "689": "---------- Bulk updates of: - [ ] Conveyor poller/receiver: bulk request update - [ ] Conveyor finisher: bulk update requests, replicas, locks and rules ", "630": "---------- Introduce a backward compatible directory struture to have severl REST backends (webpy, flask, pyramide, django) ``` lib/rucio/web/rest/webpy/v1/__init__.py lib/rucio/web/rest/flask/v1/__init__.py ``` ", "609": "---------- The SciTokens project aims to build a federated ecosystem for authorization on distributed scientific computing infrastructures. Rucio should be made a full-fledged SciTokens issuer to make it easy to integrate with SciTokens-enabled storage and SciTokens-enabled transfer systems. Since Rucio already supports token-based authentication, issuing SciTokens would not be too difficult. Expected behavior ----------------- Clients should be able to ask Rucio for a SciToken next to the X-Rucio-Auth-Token. ", "595": "---------- Currently the web UI needs access to the DB connection to not crash. This may be simply a problem of side effects of importing modules Expected behavior ----------------- Stand alone web UI with a minimal rucio config and  config should be possible. ", "583": "---------- If a user specifies only files in the list of DIDs field R2D2 will still create a container and will try to put the files in the container. ------------ Check the type of DIDs and create a container of dataset accordingly. ", "536": "---------- For example, when decomissioning a site the RSE is already blacklisted, so all the rules could be \"re-evaluated\". ", "534": "---------- Docstrings and Copyright.   ", "528": "---------- If a RSE performs badly on deletion, exclude it temporarily to not impact the general deletion rate. ", "527": "---------- Change C3PO algorithm to use distance metrics. ", "526": "---------- Study showed that data is replicated mostly to sites without the necessary computing resources to run jobs there. ------------ Rewrite algorithm to also take into account computing resources ", "485": "---------- It seems like things are wrong, but in the opposite direction. :-) ``` [root@5a7c89cdab88 /]# rucio add-dataset user.ewv:MyDataset1 Added user.ewv:MyDataset1 [root@5a7c89cdab88 /]# rucio list-dids user.ewv:\\* +----------------------+--------------+ | SCOPE:NAME | [DID TYPE] | |----------------------+--------------| | user.ewv:/MyDataset1 | DATASET | +----------------------+--------------+ ``` I'm using the latest master for the client and whatever is installed at U Chicago. ------------ Expected behavior ----------------- / should not be prepended to the dataset name in this case. ", "479": "---------- One identity can map to multiple accounts. Therefore banning just one account is not enough. Identities that correspond to people banned in voms must be removed. _Migrated from  ", "461": "---------- I spotted three things wrong with the FTS3 transfer tool: * The FTS3 transfer tool code disables TLS verification everywhere. See  as an example. * The FTS3 transfer tool creates a new session (TCP connection + SSL) for every operation and utilizes no session caching. * It doesn't seem to actually create any delegation, which is necessary for using the JSON interface directly. ------------ We absolutely should not disable TLS verification. That should be improved ASAP. The transfer tool should additionally cache / reuse / keep-alive SSL sessions in order to efficiently interact with FTS. Finally, the delegation should be done to avoid a cronjob that must be run by the sysadmin. ", "421": "---------- As GSI is starting the long road of phasing out, there's increased interest in widening work around capability-based authorization for transfers. Capabilities are authorization mechanisms that describe what the bearer is _allowed to do_, rather than _who it is_. The most common implementation of capabilities is a HTTP bearer token. A token is added to the `Authorization` header of a HTTP request and the contents of this token prove that the requester is authorized. Done via `curl`, it might look something like this: ``` curl -H \"Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RlbW8uc2NpdG9rZW5zLm9yZyIsImV4cCI6MTUxNDk5ODg1MywiaWF0IjoxNTE0OTk4MjUzLCJuYmYiOjE1MTQ5OTgyNTN9.JWXvRpy1BRGHYckDdvP9FjdGrSHZgMjqi1OzlGJfxFtAWHRQ7nOm2gLJyV9LO16vPDOJnfy0fDzOLGOvKbGXFJ4KVoX6rvl8pbKkSPe9IumE4hzN8gwo1x0IZXW-oFDqF8mFw8xSPoBs0Dvj3QArjsM0dl2Uwk0AmCx-d2U1ysMUJgrcvtCeS2rnndEs_uVY7cAHxNSyYRQoWjWFavZ3CzWHeX2hH8ZRdpQsxie1D4s4Vyk6IEjgjxNGNwtdBCdvnhVW2_mx2nLfcsuuD82Jma0wafg_nzfKbxPoP9sDujy_90b2kDpdLthPbYYmby8orEitZRXVnp0jBsiqvalV9g\"  ``` For the WLCG, I'm aware of three different models around capabilities being investigated: 1. *VO-issued bearer tokens*: The VO creates a token, authorizing the bearer to perform a certain operation at a site storage service. The storage service verifies the token was signed by the VO and honors the token within the VO's storage area (allowing CMS to issue authorizations for CMS data, but not ATLAS data). How the VO determines the authorizations and how the VO user retrieves the token is still a bit implementation-defined. The token itself is formatted as a JWT; the SciTokens implementation of this scheme additionally uses the `scp` (scope) claim to specify the authorizations. The WLCG Authorization Task Force is working on an agreed-upon profile. * Here, Rucio would issue a fine-grained token signed with Rucio's public key and return the token to the client. The storage services would be configured to trust Rucio as an issuer for a certain area (using a standardized REST API to download the set of public keys that the issuer has blessed). 2. *Storage-issued bearer tokens*: A client authenticates with a storage, requesting token to access a particular resource. The storage creates the token and returns it to the client. The client can then hand the token to any third party; the third party can only perform the specific operation listed in the token. The example implementation here is dCache-issued macaroons: one would authenticate with dCache via GSI, request a macaroon via a dCache-specific (but relatively generic) REST API, and hand the returned token to a third party. * Here, Rucio would retrieve the macaroons from dCache and return it to the client. 3. *Signed URLs*: AWS allows object owners to pre-sign a URL to have a specific authorization. Anyone with possession of the URL would automatically get authorized by the storage service. Note that this is somewhat problematic as URLs are typically treated as public within the WLCG -- for example, FTS3 monitoring allows any user to view anyone else's transfer URLs. In the other two schemes, the resource being accessed and the capability authorizing the access are managed separately. * Rucio would create the pre-signed URL with its credentials and return this to the client. In all three cases, the user interacts with the storage utilizing non-GSI and interaction with Rucio can be done via any of the Rucio-supported methods. ------------ This is a significant piece of work and ties into lots of external activity (WLCG Authorization Task Force and development cycles of various storage projects). It's not a \"one pull request and done\" -- in fact, it's likely a bit of a moving target (token formats may change over the next 6 months). However, I think there's some commonalities between the approaches that allow us to lay the groundwork: 1. Token approaches are done per-RSE protocol. That is, each RSE will have a particular \"style\" of capabilities they support that can be done as an attribute. This suggests we should reserve an attribute name (`token-type`?); unfortunately, as we want to support non-WLCG storage, it is unlikely we'll ever converge to a single token type. 2. Server-side handlers for `rucio upload` and `rucio download` should get access to the token type and be able to invoke plugins for each type that issue the corresponding capabilities. 3. Client code for `rucio upload` and `rucio download` should be able to handle both a returned URL and a bearer token in the server response, using the bearer token as appropriate. 4. For *VO-issued bearer tokens*, it would make the most sense to have Rucio's current token issuing endpoint issue the tokens. For a python prototype, see  I have a parallel set of patches for FTS3 that allow third-party-copies using tokens issued with that API. ", "372": "``` There is a foreign key constraint error in the reaper: 2017-07-25 15:47:41,494 28552 WARNING Reaper 0-2: DatabaseException Database exception. Details: (cx_Oracle.IntegrityError) ORA-02292: integrity constraint (ATLAS_RUCIO.DATASET_LOCKS_DID_FK) violated - child record found [SQL: 'DELETE /_+ INDEX(DIDS DIDS_PK)_ / FROM atlas_rucio.dids WHERE atlas_rucio.dids.scope = :scope_1 AND atlas_rucio.dids.name = :name_1 OR atlas_rucio.dids.scope = :scope_2 AND atlas_rucio.dids.name = :name_2 OR atlas_rucio.dids.scope = :scope_3 AND atlas_rucio.dids.name = :name_3 OR atlas_rucio.dids.scope = :scope_4 AND atlas_rucio.dids.name = :name_4'] [parameters: \\{'name_2': '****', 'name_3': '****', 'name_1': '****', 'name_4': '****', 'scope_4': '****', 'scope_2': '****', 'scope_3': '****', 'scope_1': '****'}] ``` This basically happens when the reaper trys to delete a dataset, which is in a container, and there is a rule on the container. Due to this rule, there is also a dataset_lock for the dataset. The reaper now trys to delete the dataset from the table, but it fails because there is a foreign key constraint from dataset_locks to dids. The question is, if the reaper should just also delete the dataset_lock. This might be problematic, as then the rule on the container is out of sync. The same issue might also exist for the undertaker, but this has to be checked. _Migrated from  ", "366": "---------- As the paths of tape endpoints don't follow the same conventions used by disk, is necessary to test and maybe modify the consistency check module to support tape endpoints. _Migrated from  ", "354": "---------- ``` rucio.tests.test_dumper_path_parsing.TestPathParsing.test_remove_prefix ... ok rucio.tests.test_forecast.TestForecast.test_predict ... /usr/lib64/python2.7/site-packages/sqlalchemy/sql/default_comparator.py:161: SAWarning: The IN-predicate on \"requests.source_rse_id\" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate. Consider alternative strategies for improved performance. 'strategies for improved performance.' % expr) /usr/lib64/python2.7/site-packages/sqlalchemy/sql/default_comparator.py:161: SAWarning: The IN-predicate on \"requests.dest_rse_id\" was invoked with an empty sequence. This results in a contradiction, which nonetheless can be expensive to evaluate. Consider alternative strategies for improved performance. 'strategies for improved performance.' % expr) ok ``` ------------ Please fix this ;) ", "112": "If we fail on importing necessary dependencies like gfal, we end up in very long timeouts later in the workflow. We should exit quickly if dependencies are not available. ", "57": "Right now, these stuck rules are repaired in Hard mode, meaning all locks are selected, while it is actually enough to just select all STUCK locks and their sister locks. _Migrated from JIRA  ", "54": "_Migrated from JIRA  ", "53": "_Migrated from JIRA  ", "43": "_Migrated from JIRA  ", "38": "_Migrated from JIRA  ", "36": "_Migrated from JIRA  ", "35": "Hi A user noticed the following problem when submitting transfer request via R2D2: \"I used  with \"List of DIDs\" to submit a transfer to CERN-PROD_PERF-IDTRACKING. I included a bunch of containers and one dataset. I submitted this for approval (see email below). The R2D2 web page gave no clear indication of a problem, except I noticed an error message that flashed up after I pressed submit. But it was quickly replaced with something like \"submitted OK\". Once the request had been approved, it seems to have disappeared. I therfore resubmitted as two requests, one for the containers and one for the dataset, and it all worked fine. This was annoying, as each request needs to be manually approved, which understandably can take some time. If R2D2 really can't handle datasets and containers together, then it should give an error when selecting DIDs, not a brief flash and then nothing when submitting.\" Thanks Alastair _Migrated from JIRA  ", "31": "_Migrated from JIRA  "}