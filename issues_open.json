{"3395": "With rucio.rse.protocols.posix.Default implementation the replicas with file schema always have port in them eg file tmp testfile Override in the posix protocol. I will submit a pull request for this shortly. ", "3388": "Chained subscription use case for Belle II RAW data produce at subscription exports the RAW to one DATADISK at site A subscription exports the RAW from DATADISK at site A to DATATAPE at the same site ", "3383": "Currently Rucio WebUI does not offer a user the possibility to choose how they authenticate before authorized to use the WebUI. The WebUI authentication method has to be pre defined in the rucio.cfg file. changes in code under rucio lib rucio web ui ", "3377": "The current ways of specifying a client config file are not very flexible and don t make it easy to use two different rucio systems from the same machine. Use an environment variable like RUCIO CFG to specify a config file and add a command line option like c CONFFILE . ", "3365": "rucio get metadata K llhoft leads to the following rucio list file replicas K llhoft SCOPE NAME FILESIZE RSE REPLICA K llhoft MB . Because there are no letters the client interprets the string as a number in scientific notation next to last character is an e . ", "3360": "Upgrade of dependencies. pip requires client setuptools change to setuptools only compatible argcomplete change to argcomplete requests change to requests change to dogpile.cache comment that is only change to six change to six change to pip requires SQLAlchemy change to SQLAlchemy alembic change to alembic jsonschema change to jsonschema python dateutil change to python dateutil stomp.py change to stomp.py change to redis change to redis numpy change to numpy only did not work. Using again since numpy is not that critical paramiko change to paramiko ", "3350": "Move the config.yml file from its current place at . lib rucio transfertool to . etc where the other Rucio configuration files are stored. Modify the code and documentation that references the location of config.yml . lib rucio transfertool globusLibrary.py . doc source configure globus transfertool.rst ", "3347": "The setup.py for the main Rucio package is not Python compatible. Fix compatibility Add Test for server to travis ", "3346": "Allow client to query globus storage via rse protocol wrapper Adding list and exists methods to GlobusRSEProtocol ", "3340": "This should not be done in reverse but in a normal sort. Remove reverse True ", "3311": "The reaper seems to skip the deletion of the collection replica when the last replica of a dataset is deleted on a RSE ", "3283": "The configuration set method doesn t work properly if . is provided as a value rucio admin config set section automatix option separator value . But when trying to retrieve it Wed Feb info pid tid client mod wsgi pid process application rucio server int config Loading WSGI script usr lib site packages rucio web rest config.py . Wed Feb error pid tid NoneType object has no attribute lower Wed Feb error pid tid type exceptions.AttributeError Wed Feb error pid tid Traceback most recent call last Wed Feb error pid tid File usr lib site packages rucio web rest common.py line in decorated Wed Feb error pid tid return f args kwargs Wed Feb error pid tid File usr lib site packages rucio web rest common.py line in decorated Wed Feb error pid tid return f args kwargs Wed Feb error pid tid File usr lib site packages rucio web rest config.py line in GET Wed Feb error pid tid for item in config.items section issuer ctx.env.get issuer Wed Feb error pid tid File usr lib site packages rucio api config.py line in items Wed Feb error pid tid return config.items section Wed Feb error pid tid File usr lib site packages retrying.py line in wrapped f Wed Feb error pid tid return Retrying dargs dkw .call f args kw Wed Feb error pid tid File usr lib site packages retrying.py line in call Wed Feb error pid tid return attempt.get self. wrap exception Wed Feb error pid tid File usr lib site packages retrying.py line in get Wed Feb error pid tid six.reraise self.value self.value self.value Wed Feb error pid tid File usr lib site packages retrying.py line in call Wed Feb error pid tid attempt Attempt fn args kwargs attempt number False Wed Feb error pid tid File usr lib site packages rucio db sqla session.py line in new funct Wed Feb error pid tid return function args kwargs Wed Feb error pid tid File usr lib site packages rucio core config.py line in items Wed Feb error pid tid return item convert type item for item in items Wed Feb error pid tid File usr lib site packages rucio core config.py line in convert type Wed Feb error pid tid if value.lower in true yes on Wed Feb error pid tid AttributeError NoneType object has no attribute lower The value is converted to None. The value is set properly if one use directly the set value in core. Problem probably at the REST level ", "3261": "The resolve query takes a lot of time in cases where the RSE is not connected. Need to introduce thresholds to speed up the query. ", "3260": "It s impossible to figure out if rucio clients are hanging due to external libraries e.g. libgfal.so . Make the client forcefully kill itself upon repeated presses of CTRL C. ", "3254": "This needs to be changed in various parts of the code from third party copy to use third party copy read and third party copy write . In a later feature release the third party copy column can then be removed. ", "3250": "Following an example of download add tests of upload to the list of tests add site name to the test rses important for client location and lan wan decissions almost copy paste download it is already using upload functionality enhance that for several test settings fake file fake site corrupted file etc. ensure that it is running under both travis docker and dev docker env. ", "3249": "In order to avoid an upload with .rucio.upload tmp name extension we need to be able to set it in protocol extended attributes at rse level. It is not causing direct issue now since sets renaming attr. by default but this is bit implicit. Once somebody changes this all switch impl. to gfal.py uploads to rses where overwrite is not allowed will fail. if extended attributes in protocol attr self.renaming protocol attr extended atributes .get renaming False ", "3235": "I would like to make the following suggestions Use the synchronous API in case the number of files is below some thresholds. We had a case where a request to mark a large amount of files as temporarily unavailable was causing long delays for single file requests which should have had higher priority. Change the message to something other than replicas successfully declared . This is misleading as the PFNs could be invalid and the user will move on thinking that everything is done. Do some checking on the PFNs at the very least that they are valid URLs. It should be noted that on the side of Minos this isn\u2019t handled gracefully. Traceback most recent call last File usr lib site packages rucio daemons badreplicas minos.py li ne in minos dict rse rse id .extend tmp dict rse rse id File usr lib site packages retrying.py line in wrapped f return Retrying dargs dkw .call f args kw File usr lib site packages retrying.py line in call return attempt.get self. wrap exception File usr lib site packages retrying.py line in get six.reraise self.value self.value self.value File usr lib site packages retrying.py line in call attempt Attempt fn args kwargs attempt number False File usr lib site packages rucio db sqla session.py line in new funct return function args kwargs File usr lib site packages rucio core replica.py line in ge t pfn to rse split se surl.split .split ", "3200": "I have set myself quota on type DATADISK but the CLI cannot show it. rucio list account usage dcameron RSE USAGE LIMIT QUOTA LEFT CERN PROD SCRATCHDISK kB TB TB NDGF DATADISK MB MB MB NO NORGRID LOCALGROUPDISK MB TB TB SE SNIC LUND LOCALGROUPDISK kB B B ERROR Access to the requested resource denied. Details Account dcameron can not list global account usage. ", "3199": "Running as an unprivileged user it is expected that this fails but the exception should not be an internal server error c.set global account limit dcameron type DATADISK Traceback most recent call last File stdin line in module File home dcameron dev ddm rucio lib rucio client accountlimitclient.py line in set global account limit raise exc cls exc msg rucio.common.exception.RucioException An unknown exception occurred. Details no error information passed status code internal server error server error o The exception should be the same as I see when setting local account limits c.set local account limit dcameron NDGF DATADISK Traceback most recent call last File stdin line in module File home dcameron dev ddm rucio lib rucio client accountlimitclient.py line in set local account limit raise exc cls exc msg rucio.common.exception.AccessDenied Access to the requested resource denied. Details Account dcameron can not set account limits. ", "3164": "The Bring online parameter is the timeout for the FTS polling to check that the file is online. The current default value defined in Rucio is h. At the last data carousel exercice many transfers reached this timeout and the corresponding FTS requests failed error message in Rucio Stage was cancelled . Such staging request are then rescheduled on the same site participating to the long tail effect. Based on the values in FTS about the max staging request pers site e.g. requests the avg file size GB and the observed throughput e.g. from MB s to s the timeout value of hours is too small and lead to some unnecessary overheads and scheduling operations. I would suggest either to Increase the bring online timeout to reflect the storage tape performance wrt the number of queued staging request or set to the rule lifetime with some potential renormalisation or set it to a bigger constant value for all staging requests. ", "3148": "decommission mode not fully correctly working DB access vs. dumps configuration several things hardcoded at the time being logging needs to be correctly implemented oop evolved into quite complex tool. We might start to think about upgrading it. add Difficult issue see some limiting factors here ", "3147": "we are maintaing two independent codes at the moment clients and rsemgr. The aim is to have as much as possible at single place clients. the rsemgr evolved organically cleanup is needed. preparation for new features like bulk mode moving upload functionality to clients several small updates commissioning and testing in parallel with the old rsemgr checking if functionality is conserved. ", "3135": "rucio list rules history cms ttZJets madgraphMLM asymptotic NANOAODSIM ERROR expected string or buffer Martin Barisits AM I think this is due to missing quote plus on Maybe you can try to do this similar to quote plus around scope and name ", "3124": "Currently the monitor module only supports statsd but the CERN Kubernetes clusters come with a Prometheus server that can also be used for auto scaling. Make the monitor module configurable to either report to statd or to prometheus. ", "3115": "The decoder requires width padded strings i.e. some strings need extra padding elements in the string. Some keygens don t add them. Properly handle this corner case. ", "3103": "Decommissioning of sites with problems take too much human intervention. Automation is needed to cover the corner cases around of rules fail not because the site to be decommissioned but for other factors . New daemon service that keep track of the rules and transfers exiting the site to be decommissioned. ", "3101": "As of today one can pass a reference to a list where the traces from Rucio can be uploaded. If the other party already provided some traces we have no way to update them so far they would need to update the traces according to the information we provided. There is an option for trace pattern that can be used for traces that Rucio is producing but thah seems to be an option only for a single trace. This is matter of discussion. Either the other party provides list of references that they created and Rucio just updates them or Rucio send a traces and the other party merges them with the parts of traces coming from Rucio. ", "3097": "Quite often people ask DDM support what happened to my dataset? for datasets that have been deleted. It would be good to be able to query the metadata of these datasets eg time it was deleted. Add an option to the CLI like deleted to query the deleted DIDs table. ", "3096": "Currently there are several interfaces to address metadata in Rucio. This should be unified into one interface which intelligently selects which backend to use. Currently Hardcoded columns or JSON columns In the future this enables us to extend Rucio to external metadata catalogs which are accessed via the unified frontend. ", "3084": "Some usless duplicate functionality. Rather keeping in the Client. Keeping basic functinalities but removing the file loop duplicate check for existence of a file renaming we can do better duplicate code adding proper exceptions input validation ", "3076": "Error is thrown when fts hosts is empty string. The fts attribute is empty for Globus transfers resulting in an error. I made changes to fix here but cannot sure if they re ideal for FTS transfers. ", "3042": "Currently DESY ZN DATADISK is blacklisted for deleting but the Dark Reaper does not take that information into account. WARNING Dark Reaper Deletion NOACCESS of as davs lcg pnfs ifh.de data atlas atlasdatadisk rucio on DESY ZN DATADISK The requested service is not available at the moment. Mimic the behaviour of the regular Reaper. ", "3031": "rucio download is not able to download files if the scope or name contains ", "3023": "Transfers with the source files on tape are done in several steps by FTS The staging and then the transfers to the final destinations. Now only the later it is reported in Rucio. It might be be good to add the beginning and end of the staging as reported by FTS in order to do further analysis e.g. staged at staging at. Both values are reported as staging finished and staging start by FTS when querying the file transfer statuses. ", "2983": "is still the default version on many distributions so we cannot force upgrade to Make it compatible with both version. ", "2973": "The implementation here can be simplified This should not be done until GFAL reaches EPEL. Do not make a separate call to GFAL to create the destination path. Use the create parent transfer parameter instead. ", "2961": "From Rod See This file CosmicCalo.merge.RAW. SFO ALL. has tape replicas plus in an archive on tape and disk. rucio list file replicas shows only the replica from the staged zip file on CC DATADISK not the archive on CC DATATAPE and not the constituent replicas on tape. The see the latter again no resolve archives The python client shows all replicas. I don t think it will break prod just confusing. I had another example with no replicas before adding no resolve archives Background.merge.RAW. SFO Cheers Rod. ", "2926": "rucio upload rse scope mock file mock ds rucio upload rse scope mock mock ds rucio download mock ds mock The replica of mock seems to be added to list replicas twice. ", "2915": "Traceback Thu Sep error pid tid client Traceback most recent call last Thu Sep error pid tid client File usr lib site packages web application.py line in process Thu Sep error pid tid client return self.handle Thu Sep error pid tid client File usr lib site packages web application.py line in handle Thu Sep error pid tid client return self. delegate fn self.fvars args Thu Sep error pid tid client File usr lib site packages web application.py line in delegate Thu Sep error pid tid client return handle class cls Thu Sep error pid tid client File usr lib site packages web application.py line in handle class Thu Sep error pid tid client return tocall args Thu Sep error pid tid client File usr lib site packages rucio web rest account.py line in POST Thu Sep error pid tid client raise Duplicate error.args Thu Sep error pid tid client File usr lib site packages rucio common utils.py line in Thu Sep error pid tid client if len str exc msg Thu Sep error pid tid client UnicodeEncodeError ascii codec can t encode character u xed in position ordinal not in range Thu Sep error pid tid client ", "2881": "After recreating an Identity using the following steps the RUCIO allows the token creation using Creating the test identity fernando rucio admin identity add account carlos type USERPASS id fernando email cgamboa bnl.gov password secret Added new identity to account fernando carlos Listing identities for account carlos root rucio rucio admin account list identities carlos Identity fernando type USERPASS Deleting the Identity recently created root rucio rucio admin identity delete account carlos type USERPASS id fernando Deleted identity fernando Creating same Identity with a different password rucio admin identity add account carlos type USERPASS id fernando email cgamboa bnl.gov password othersecret I can still connect using the old password the one set when the identity was created. Token is created for the this password. The token is not created for the identity that was deleted and then reused recreated with a different password. We are using RUCIO ", "2830": "", "2827": "psycopg throws new kinds of exceptions when running on ", "2770": "All three methods below should work equivalently root rucio rucio S userpass u asdf pwd zxcv whoami cannot get auth token ERROR Cannot authenticate. Details userpass authentication failed root rucio RUCIO ACCOUNT mario rucio S userpass u asdf pwd zxcv whoami cannot get auth token ERROR Cannot authenticate. Details userpass authentication failed root rucio rucio account mario S userpass u asdf pwd zxcv whoami status ACTIVE account mario account type USER created at updated at suspended at None deleted at None email None ", "2714": "Some function based indexes are missing in models.py. For example the index for REPLICAS STATE IDX is missing while it is defined in the schema.sql for oracle. For other indices this exists. e.g. REPLICAS TOMBSTONE IDX is available although it is function based. Need to check which function based indices are missing in the models.py They need at least to be added non function based to the models.py Investigate if it is possible to add function based indices with SQLAlchemy if yes all of them need to be re written. ", "2706": "To help tape systems with colocation we will pass two metadata hints per file storage class classname as a single value and group hints mostimportant . leastimportant as an ordered list of string values. ", "2703": "Would be great to support text file upload or command line method to submit exceptions for the lifetime model. Currently it requires user to pick up samples one by one with wildcard but still not that convenient which is low efficient and very easily to introduce mistakes there. ", "2686": "One cannot see the parameters used in the POST PUT requests making it very difficult for debugging ", "2639": "Creation date of rule in the future injector will only inject it then. ", "2637": "A way to track visualise what happened to a did. Check historic information Transfers etc. ", "2636": "As discussed at the Community workshop. We want to establish some kind of repository for operator documentation recipes which allows convenient ways of contribution Wiki or similar ", "2635": "Overview ticket for Multi VO development. ", "2634": "I have noticed that in several places on CERN intranet and presentations slides there is a link to Rucio HOWTO page which does not exist anymore. If the page has been moved or replaced it would be nice to have a redirect to a new location. Also the output from rucio list file replicas help shows a link to that page is not there too. ", "2631": "Documentation page listing all possible config table and RSE attribute functional parameters. ", "2630": "Need to specify a policy about which information goes where. Needs to be enforced an current inconsistencies need to be removed. We need to write down in a concise documentation page what are the optional configuration parameters for both .cfg and config table Also need a page for functional RSE attributes ", "2621": "Most likely because activity shares is missing here ", "2613": "From Rod Hi I was recently introduced to where I can manually declare suspicious files bad which is required for last replicas. It is nice but I have a couple of feature requests. select all button and ideally select range with shift key. prodtask dev can do this. link to dashbd transfer errors for each file example below. a filed with the last data.reason text from the dashbd Cheers Rod. ", "2582": "ericvaandering I am using the CMS rucio dev instance. I have a couple of test RSEs which are not controlled by any centralised scripts. The only changes to these RSEs should be manual changes. However the distance ranking values between my RSEs and others seem to be lost in between sessions. I cannot view them using get distance or add a rule making a file transfer between them until I have used update distance to redefine the values. rucio admin rse get distance FR Disk Test CH CERN EOS Test d format a number is required not NoneType This means the parameter you passed has a wrong type. Nevertheless the distance ranking is still set in some form as I cannot use add distance . rucio admin rse add distance distance ranking FR Disk Test CH CERN EOS Test An object with the same identifier already exists. Details Distance from to already exists! This means that you are trying to add something that already exists. Within the same session these values seem to persist but next time they need redefining again. ", "2543": "Upgrade version of pyflakes pycodestyle and pylint ", "2542": "There are multiple throttler tickets for different use cases FIFO x throttle per activity source RSE x throttle per activity destination RSE throttle per activity destination and source RSE x throttle per all activities destination RSE x throttle per all activities source RSE throttle per all activities source and destination RSE Grouped FIFO x throttle per all activities destination RSE x throttle per all activities source RSE throttle per all activities destination and source RSE ", "2517": "The list replica command when no replicas are there matching the requirements returns a generator with element which is an empty string list rcli.list replicas name ST t channel top InclusiveDecays PSweights powheg realistic AODSIM scope cms rse expression FR GRIF IRFU u I may be wrong but I think it used to return an empty generator in previous versions current return an empty list ", "2459": "Given that a Rucio dataset corresponds to a CMS block it would be extremely helpful if the rucio clients list dataset replicas API would accept a list of dataset names such that we can fetch all CMS block replicas that belong to a CMS dataset in a single call thus having a better performance at multiple layers . For the record I ve placed the same issue under the CMSRucio project FYI ericvaandering Support a list of datasets in the list dataset replicas API from the rucio clients package. ", "2417": "The query to get suspicious files is not optimal. Sometimes Oracle switch from Nested Loop to Hash Join that can result to a huge slowing down of the query. Use the following query WITH bad repl AS SELECT scope name rse id COUNT FROM atlas rucio.bad replicas WHERE created at created at GROUP BY scope name rse id HAVING COUNT count SELECT cardinality bad repl INDEX replicas replicas pk atlas rucio.replicas.scope AS atlas rucio replicas sco atlas rucio.replicas.name AS atlas rucio replicas nam atlas rucio.replicas.rse id AS atlas rucio replicas rse MIN atlas rucio.replicas.created at AS min FROM atlas rucio.replicas bad repl WHERE atlas rucio.replicas.scope bad repl.scope AND atlas rucio.replicas.name bad repl.name AND atlas rucio.replicas.rse id bad repl.rse id GROUP BY atlas rucio.replicas.scope atlas rucio.replicas.name atlas rucio.replicas.rse id ", "2414": "The current configuration template for atlas at etc rucio.cfg.atlas.client.template doesn t work out of the box. A working template client section only client rucio host auth host ca cert client cert USER PROXY client key USER PROXY client proxy USER PROXY request retries auth type It would also be good to explain the configuration in the docs I couldn t find anything about it there. ", "2410": "Right now Rucio only supports and checksums via two specific columns for these checksums. This creates problems if a community has different checksums e.g. CMS historically used crc checksums and wants to store them. On the dev meeting on it was discussed to extend Rucio with a generic checksum column of string type. Multiple checksums can be added comma separated to this column always specifying the checksum type. E.g. crc We would have to specify a strategy in migrating to this new checksum column. ", "2393": "It might be desirable to recreate the original dataset from the zip files or at least amusing in a low priority kind of way. All the metadata for constituent files and dataset exists. rucio client pulls the zip file unpacks verifies and then stores the constituent replicas. It would take the constituent dataset as the argument and cycle over the zip files so as to not use too much disk space. It should be recursive so only recovering the files without a replica. To do this at scale would need some prodsys development but I am not sure we will ever need that. Some way to do it manually might come in handy though. Cheers Rod ", "2387": "The tests right now the database testing workflow foresee the following Setup database with current models.py Do full downgrade all revisions with alembic Do full upgrade all revisions with alembic Run the actual test cases Now the fact that the tests run through point to the fact that the database is actually equal to the one defined in the current models.py . However it is not guaranteed. What we would need is an extra step which compares the database schema before and after. How this actual check works is a bit difficult though. It could be a table describe for each table and then just compare strings but this would require the order of constraints indices etc. to be kept the same. ", "2356": "We sometimes have problem with sites running dual stack storage. Sometimes the interface is broken whereas the works. One would need a switch e.g. RSE attribute to force the use of ", "2330": "Default alembic.ini location should not be main rucio directory but should point to etc alembic.ini like the rest of the configuration. Remove tox everything is properly covered by Travis Some hosts are pointing to cern.ch servers in the default template replace with fake hostnames Remove useless scripts from tools e.g. sweep srm conftest ", "2325": "The current main page of the WebUI is a generic one. It would be useful to have a main page that shows the most interesting informations for the end user. Non exhaustive list Table with available space and quotas and or bar chart showing the available space by RSE List of replicating stuck rules List of requests for exceptions to the lifetime model ", "2318": "DPA needs a too to get the history of bad files ", "2312": "This is an overview ticket for the Rucio redirector. Need to assess the status of the service Need to expand tests to ensure usability ", "2311": "At the moment we only have very limited upload download tests either using mock calls or direct posix protocols. One possibility to expand these to actualy functional tests would be to use xrootd docker containers and start up two xrootd storages in the test environment. This would allow us to test upload download also with different protocols such as gfal which would also help enormously to validate gfal for python ", "2310": "Some users provided feedback to improve the suspicious files view in the WebUI If the RSE pattern is not OK do not run the loop forever. Instead break with clear pattern that RSE expression is wrong I have declared some files as bad. After Could you put a flag to declare that the file was declared lost. ", "2282": "The log files that have been declared suspicious a lot of times can be safely declared BAD . For now this is done as a probe but eventually it must be done by a dedicated daemon that apply different policies based on the project datatype nb replicas. ", "2264": "The demo image contains reference to service hosted at CERN. Need to be removed ", "2263": "Currently the Auditor expects to be able to list directory entries in order to identify the latest dump. This is not possible on object storages. Investigate and implement an alternative such as Try the last n days based on the pattern Use another file as an index Use a static location and obtain the date from its metadata ", "2258": "Since the merge of rucio list rse attributes crashes for staging endpoints twegner rucio version rucio twegner rucio v list rse attributes TAIWAN TAPE STAGING DEBUG Traceback most recent call last File cvmfs atlas.cern.ch repo ATLASLocalRootBase rucio clients bin rucio line in new funct return function args kwargs File cvmfs atlas.cern.ch repo ATLASLocalRootBase rucio clients bin rucio line in list rse attributes print tabulate.tabulate table tablefmt plain File cvmfs atlas.cern.ch repo ATLASLocalRootBase rucio clients lib site packages tabulate.py line in tabulate for c ct fl fmt miss v in zip cols coltypes float formats missing vals File cvmfs atlas.cern.ch repo ATLASLocalRootBase rucio clients lib site packages tabulate.py line in format return format float val floatfmt ValueError invalid literal for float True ERROR invalid literal for float True ERROR Strange error No section policy ", "2254": "We tried to switch implementation for xrootd protocol at ral echo scratchdisk and it didn t work from obvious reasons. and not startswith root needs to be added to line stat method returning checksum needs to be implemented mlassnig xrdfs root eosatlas.cern.ch query checksum eos atlas atlasdatadisk rucio ", "2232": "Whenever a lock is being repaired this lock repair cnt is incremented by As a second condition Next to the week timeout the repairer can then also SUSPEND rules based on the lock repair cnt . ", "2218": "This would be a possibility to test new releases without having to deploy them on CVMFS. ", "2216": "from rucio.daemons.auditor import process output process output \u2026 Traceback most recent call last File stdin line in module File usr lib site packages rucio daemons auditor init .py line in process output lost pfns r rses rse for r in list replicas lost replicas File usr lib site packages rucio db sqla session.py line in new funct raise DatabaseException str error DatabaseException Database exception. Details cx Oracle.DatabaseError ORA internal error code arguments SQL u SELECT INDEX DIDS DIDS PK atlas rucio.dids.scope AS atlas rucio dids scope \u2026 Note that this is not an ordinary case. The file used in this example has about lost files. Originally I was chunking on the Auditor\u2019s side but I was recommended against that. ", "2213": "When the last replica of a file is removed in the reaper it should only be removed from the dataset if it is not a constituent of an existing archive replica. This also implies if a archive replica is removed and there are no replicas on the constituents the constituents need to be removed from their parent datasets. ", "2159": "Currently it is not clear why a certain request is REPLICATING but did not issue a FTS transfer yet. We should expose that a request is in WATING or even MISMATCH state. What would also be good to maybe show the queue length although this might be more difficult as it is depending on throttle configuration. ", "2143": "Zenodo is a research data repository especially nice for linking publications to the datasets they used. Investigating integration interfacing with Zenodo might be useful. ", "2104": "There are double counts in the lost files reports when the same lost file is in many datasets. easy ", "2063": "There are currently history tables which are defined by using the SQLA model base Versioned . The advantage of this is that the history table does not have to be created by hand but it is just mirrored by the primary base. However by doing this we loose full control about the indices and primary keys of the history table as it is also just mirrored from the primary model. This already creates inconsistencies as the history tables we define on oracle do not have indices and primary keys. Two possibilities Find a way to adapt indices PKs for versioned tables AFAIK not possible Do not use Versioned at all and define the history tables explicitly This is already done for some tables anyway. ", "2029": "Assuming that the DID exists this command returns a broken XML where the closing tag is missing. It is caused by using a RSE expression that returns no RSEs shell ReplicaClient .list replicas name file LMXZHCXRHS scope hip rse expression test True metalink True ?xml version encoding UTF ? n metalink xmlns urn ietf params xml ns metalink n ", "1923": "Wrong title in lifetime exception should be fixed Need a button to bulk update the WAITING requests ", "1911": "Even on a bad installation or typos in the configuration system rucio help ought to work. The current version of rucio imports the DownloadClient at the module level. If there is an error in the rucio.cfg that causes the DownloadClient constructor to throw an exception you can t get to help . Example rucio help Traceback most recent call last File home bbockelm software rucio doma bin rucio line in module from rucio.client.downloadclient import DownloadClient File home bbockelm projects rucio lib rucio client downloadclient.py line in module client Client stack trace goes on for miles File home bbockelm software rucio doma lib site packages requests adapters.py line in send self.cert verify conn request.url verify cert File home bbockelm software rucio doma lib site packages requests adapters.py line in cert verify invalid path .format cert loc IOError Could not find a suitable TLS CA certificate bundle invalid path etc ca.crt In my case the problem was caused by an invalid path in rucio.cfg for the CA bundle. Failures to create the clients should be swallowed until they are actually used. Potentially we shouldn t even import the clients until needed. ", "1895": "This is needed to let the development of user data transfers code separated from the RUCIO one. Talking in particular of using importlib using configuration param for the following methods user to DN mapping myproxy interactions I would go with importing just the methods since the case of mapping a user to a DN and then retrieving the corresponding proxy may have a more general use. In any case a feedback on this is very welcome. ", "1867": "The calculation does not take into account the replicas that are already available on the destination. Use rucio.core.replica.list replicas . The operation can be expensive but since it\u2019s not done frequently it should be fine. ", "1860": "For rules on containers when datasets are detached from the container the respective DATASET LOCK does not seem to be removed properly. There were some cases found where this was the case. Needs to be investigated and fixed. TBD ", "1839": "It\u2019s difficult memorising command line options when they are named differently across commands. delete rule uses rse expression list file replicas has both rse and expression with different meaning get has rse but it actually takes an RSE expression. First it has to be decided whether it\u2019s worth breaking the current CLI interface to fix this. ", "1834": "The tracer server accepts any kind of json send to it and expects clients to send the information using the correct schema. Sometimes a bug in a client can lead to an invalid trace that then can break any of the systems using the traces monitoring Kronos etc. Add a validator that listens on the ActiveMQ topic and constantly checks that necessary fields are set correctly or set at all. If something doesn t match the schema send out notifications. ", "1830": "Separate logfile for dataset and file thread. ", "1829": "A run once unittest by mocking pre filling an activeMQ queue to consume from. ", "1817": "The Flask API has problems with dealing exceptions when using JSON streaming. This should be fixed as it is essential for handling lot of data. See for implementing JSON streaming to change the Flask API. Investigate how to deal with exceptions. ", "1808": "The LIGO and Virgo communities are stepping into Rucio to manage ste storage elements of their infrastructures. While LIGO is provided of proprietary HPC clusters Virgo isn t and instead relies on a set of academic computing centers. The characteristics of the latter as well as some previous choices strongly point to a wide adoption of DIRAC in Virgo while the interoperability with LIGO requires Rucio as storage manager orchestrator. After some discussion we maybe found a nice solution. Instead of developing a DIRAC plugin able to interface it to Rucio in a POSIX like manner we think the best option is to create a DIRAC mode for the Rucio catalog. This argument follows a comment of many people DIRAC was born to create a uniform interface to different GRID implementations but ended up managing both Computing Elements CE and Storage Elements SE . This choice was taken to make DIRAC aware of the geographical data position in order to minimize the data transfers. Since DIRAC is used by a rather small community not a lot of organizations might be interested in stepping in the development of an integration. Instead Rucio is much more appealing and finding a way to keep in the Rucio s external catalog enough topological information to make DIRAC happy and efficient might be the way. In addition we discovered that DIRAC allows to pass it a custom catalog and some Virgo people has already performed some tests in that sense creating an LFC catalog dump and running a DIRAC instance on that data. In fact such solution might scale up to decoupling the DIRAC storage function from the computing function giving lots of benefits even to the DIRAC product. This could bring in some of their developers to assist in the process. Worth mentioning that DIRAC jobs can be any kind of executable from an sh script to an executable available cluster s wide e.g. firefox . Since the read from Rucio managed files should be supported OOTB by DIRAC the need of a plugin is needed only to register on Rucio the output files of the jobs. However since DIRAC can run basic scripts at first the registration of output files on Rucio might be handled by by hand calls to the Rucio API from within the DIRAC job. If we think about a C compiled executable which produces a myfile.root file running on a shell myexe myfile.root . Wrapping the same in something like myexe myfile.root rucio API call myfile.root should do the basic tricks. ", "1806": "The default transfer submission flow is based on the assumption that the proxy has already been delegated by an external script. For user transfers an integrated delegation is already in like And a standalone method in the transfertool is already available A similar solution can be implemented also for the default submission avoiding to keep a cron script doing the delegation externally. This new behavior must be optional and available through a configuration parameter. steps needed here align the fts transfertool code to use python bindings. Followed on check for response equivalence w.r.t. REST response mock may be needed for a proper testing do FTS job submission with the integrated delegation check available by a configuration parameter ", "1801": "The defaults we have for a couple of options could be different between VOs e.g. rucio list dids filter type ALL . Investigate VO specific defaults for the clients. ", "1797": "Concern by CMS that data which is only on the tape buffer may not be migrated to tape before additional disk copies are deleted which could result in the loss of data. Undetermined but as I understand it Add a new optional state Waiting for tape check FTS transfers to tape systems are put in this state when the transfer finishes A new daemon checks the state with the storage system moves state to the one which can be picked up by the finisher ", "1773": "For some use cases the default of exracting metadata for specific file extensions is undesired. E.g. to use rucio upload with ROOT files within an standalone container image that does not have the software of a specific experiment installed. partly discussed here allow to add configuration optiosn in the cfg file that list the file extensions for which metadata extraction should be performed. the defaults would stay as it is extract GUID for root files but it could be switched off ", "1771": "Sorting of column of type datetime is broken datatable. The sorting is done alphabetically and not based on the date ", "1760": "during last days always when statrted to rebalanced othe activities were completely suppressed lower the share rebalancing is of highest priority now distribute the rebalancing over a day. Use case for delayed rule. ", "1759": "I ve recently started seeing various mysql exceptions. First listing file replicas in a dataset gives a database exception message in the client ligo rucio \u279c rucio list file replicas L R ERROR Database exception. Details mysql exceptions.ProgrammingError Commands out of sync you can t run this command now SQL u SELECT rses.id AS rses id rses.rse AS rses rse rses.rse type AS rses rse type rses. deterministic AS rses deterministic rses.volatile AS rses volatile rses.staging area AS rses staging area rses.city AS rses city rses.region code AS rses region code rses.country name AS rses country name rses.continent AS rses continent rses.time zone AS rses time zone rses. ISP AS rses ISP rses. ASN AS rses ASN rses.longitude AS rses longitude rses.latitude AS rses latitude rses.availability AS rses availability rses.updated at AS rses updated at rses.created at AS rses created at rses.deleted AS rses deleted rses.deleted at AS rses deleted at nFROM rses nWHERE rses.deleted false AND rses.rse s parameters LIGO WA Background on this error at Next the judge cleaner log shows the following message several times a second DEBUG rule cleaner index query time fetch size is INFO rule cleaner Deleting rule with expression UNL DEBUG Deleting lock H R for rule Exception mysql exceptions.InterfaceError in bound method SSCursor. del of MySQLdb.cursors.SSCursor object at ignored raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters H R xeb xcd NI t xdc Background on this error at ERROR Traceback most recent call last File usr lib site packages rucio daemons judge cleaner.py line in rule cleaner delete rule rule id rule id nowait True File usr lib site packages rucio db sqla session.py line in new funct raise DatabaseException str error DatabaseException Database exception. Details raised as a result of Query invoked autoflush consider using a session.no autoflush block if this flush is occurring prematurely mysql exceptions.ProgrammingError Commands out of sync you can t run this command now SQL u DELETE FROM locks WHERE locks.scope s AND locks.name s AND locks.rule id s AND locks.rse id s parameters H R xeb xcd NI t xdc Background on this error at Finally the reaper log shows the following message which may or may not be related INFO Reaper Running on RSE LIGO CIT ARCHIVE None DEBUG Reaper list unlocked replicas on LIGO CIT ARCHIVE for None bytes in seconds replicas CRITICAL Traceback most recent call last File usr lib site packages rucio daemons reaper reaper.py line in reaper prot rsemgr.create protocol rse info delete scheme scheme File usr lib site packages rucio rse rsemanager.py line in create protocol protocol attr select protocol rse settings operation scheme domain File usr lib site packages rucio rse rsemanager.py line in select protocol candidates get possible protocols rse settings operation scheme domain File usr lib site packages rucio rse rsemanager.py line in get possible protocols found s. str rse settings RSEProtocolNotSupported RSE does not support requested protocol. Details No protocol for provided settings found third party copy protocol rse type DISK domain lan wan availability delete True delete protocol rse u LIGO CIT ARCHIVE deterministic True write protocol read protocol staging area False algorithm ligo availability write True volatile False availability read True credentials None verify checksum True id protocols extended attributes None hostname u ldas prefix u hdfs frames domains wan read write third party copy delete lan read write delete scheme u gsiftp port impl u . Note that these errors only seem to be affecting deletion of rules and replicas actual transfer operations appear to be unaffected. ", "1752": "While working with Rucio data records I found that their structure is not static in terms of data attributes. By that I mean that returned dictionary contains dynamic set of keys plus it mixed pre defined attributes names with attribute values. To demonstrate this I ll take one particular record from replicas API e.g. curl H Accept application json H X Rucio Auth Token token This API returns the following records states US Nebraska AVAILABLE pfns name store data Charmonium MINIAOD rses US Nebraska scope cms bytes null states US Nebraska AVAILABLE pfns name store data Charmonium MINIAOD rses scope cms bytes null These records contain static attribute names such as states scope etc. and within states part of the records they contain dynamic keys in this case both records return US Nebraska but in general it would be name of the rse. As you can see the record keys attributes will be different in different type of queries since you used rse values as record key attribute of states data structure. This is represent certain problem to treat these records in generic way e.g. I can t map a record into static data structure since records keys will be dynamic. Instead rucio should return static data strictures all the times. For instance the states sub structure may be represented as following states rse US Nebraska state AVAILABLE or to be generic for multiple rses as states rse US Nebraska state AVAILABLE rse XXX state STATE OF RSE In this case the states has static data structure where all keys attributes are well defined i.e. in this example there are two keys rse and state and such structures will remain static for infinitive number of RSE STATE values. I wonder how many APIs inherit this behavior and how hard easy will be to fix them. I also need to know how many of such dynamic structures exists in Rucio APIs output. Another issue with such dynamic data structures affects long term analytics if we plan to do it where records keys will grow over time and will not represent finite set of attributes. For the record this issue is very critical for CMS Data Aggregation System which will consume this data and need to aggregate it with rest of CMS data service records. In order to do aggregation several issues should be solve including this particular one. I m CCing ericvaandering Eric to follow up this issue. Eric Rucio falls into the same trap as many other CMS data services by using key attribute values as keys of the underlying python dict keys. This represent issue with developing generic parser of Rucio records. ", "1746": "I m struggle to understand the difference between these two API calls replicas scope name replicas scope name datasets In both cases the output seems identical while the API description is different. The former list all replicas for data identifier while the later is list dataset replicas. What s the difference? Here is my calls to CMS rucio server this API replicas cms Charmonium MINIAOD returns states US Nebraska AVAILABLE pfns name store data Charmonium MINIAOD rses scope cms bytes null this API replicas cms Charmonium MINIAOD datasets returns states US Nebraska AVAILABLE pfns name store data Charmonium MINIAOD rses scope cms bytes null in both cases we have returned documents. Also I want to get replicas only for a given name and RSE pair and so far I can t find API I need. The only way to do this is to get list of all replicas for a given name and filter the RSE I m interested in. Would it be more efficient to pass RSE as a parameter to replicas API to fetch only subset of them on that given RSE? For example replicas scope name rse ", "1667": "With a volatile RSE that is outside of Rucio s control it can happen that the whole cache is wiped or publishing breaks or for some other reason the files are deleted without the ability to tell Rucio what has been deleted. This means that replicas will stay forever in the catalog. In this case it would be good to tell Rucio to wipe all replicas from the RSE then the site can start the publishing from scratch. Of course this is rather dangerous so care should be taken to restrict this operation to volatile RSEs admin accounts etc. ", "1648": "The probe doesn t set properly the RSE TYPE need first to implement . Moreover it should set a default distance for all the new RSEs. ", "1636": "Email discussion with ATLAS btw the double slash is always necessary for root. it works for add replica so there must be something different in the method add files to dataset On PM Tomas Javurek wrote Trying to register with add add files to dataset that Luc is using RSE does not support requested protocol. while when I set the protocol it works fine. So we need to keep it there for now. However please keep it with srm for now because it did work for me only with srm when I tried with root protocol it failed with the same error. And I tried both root freiburg.de pnfs bfg.uni freiburg.de data atlasdatadisk rucio user ddmadmin \u2018 root freiburg.de pnfs bfg.uni freiburg.de data atlasdatadisk rucio user ddmadmin On Sep at Tomas Javurek Tomas.Javurek cern.ch mailto Tomas.Javurek cern.ch wrote I couldn\u2019t find the place where we subtract the protocol part from path when registering to replicas. But I\u2019ll try brute force. Just preparing a script that would imitate Lucs needs and see whether I can register without protocol part. ", "1583": "Request from dCache via Paul Millar. Use case is for Atlas to pass the dataset of a file to dCache which would enable optimisations colocation on a small set of tapes and bulk operations bringonline dataset amongst other possibilities. Associated FTS ticket ", "1558": "To protect from deletions for files which might still be in the processing in other daemons. ", "1504": "Contention between the undertaker and the conveyor can occur in case the undertaker tries to delete some files that are being transfered in particular if the transfers have been submitted to a slow responding FTS server. Indeed the undertaker locks the corresponding rows in replicas and requests table before sending the cancel request to FTS. The cancel request did must propagate a timeout to FTS right now there is none. In addition I would propose to add a new state for the request BeingDeleted similar to the one we have for the replica. Doing that way the conveyor would not try to update the requests that are being deleted by the undertaker . ", "1431": "When deleting the last replica of a file reaper deletes the file logically as well but does not delete the contents in the archive contents table. This causes a foreign key constraint error and the reaper gets stuck. There are three changes needed in total for this. x When deleting the file logically the rows from the archive contents table need to be deleted too The just deleted archive content rows have to be added to the archive contents history table For the files whose archive just got deleted it needs to be checked if they do not have any other replicas in which case they need to be removed logically If there are no rules on them which is actually a bit pointless When the last replica of a file is removed in the reaper it should only be removed from the dataset if it is not a constituent of an existing archive replica. This also implies if a archive replica is removed and there are no replicas on the constituents the constituents need to be removed from their parent datasets. ", "1393": "Trailing in Rucio are not part of the DID name however the historic ATLAS naming did include them such as DYmumu For most commands REST endpoints including the DID are used where this trailing resulted in a in the URL. This is interpreted as one in and thus the command did still deliver a result. However since the clients both scope and name get escaped using urllib.quote plus . This way the name gets transformed in e.g. DYmumu and the lookup fails. Possible solutions Use urllib.quote plus s to declare as a safe character. This should work but for commands which do not use the did in the REST call the trailing of a container was and will still be a problem. E.g. list dids with a filter attach dids container to container detach dids container from container etc. This also fails for communites using in their naming scheme Filter trailing completely in the client. Might be problematic for communities Ask external applications to filter trailing on their side. ", "1382": "The Replicas controller creates the metalink output differently than the ListReplicas controller. ", "1351": "the rucio rucio atlas docker images are excellent to do basic get put operations within a minimal environment. However uploading ROOT files fails because the pool extractFileIdentifier is not found. I m cross referencing an issue on the ATLAS JIRA were I asked whether we can provide a statically linked version of it If we can get a statically linked binary presumably one could just add it to the image. ", "1304": "If the UploadClient is used to upload a new replica of a file DID no dataset given that is already registered no rule will be updated or added. ", "1248": "At the moment it tries to insert replicas to RSEs but has PK failures due to the already existing replicas in state BEING DELETED. ", "1245": "when I declare bad a huge amount of files for example when a machine cannot be recovered it takes several hours and it is not possible to understand at what stage the declaration is. On occasion not all the files get declared as it has happened with Glasgow. Files were declared lost in December but some of them were still there yesterday and deletion failed and I had to reduclare lost files. It would be useful if this tool left wrote a log file with the number of files declared if the admin wants to or wrote to the output as part of the verbose when it does a commit. So if it declares lost files at the time it writes it has done those and if something fails it logs that too. So that it is possible to rerun only on the missing files instead of going through the whole lot. A summary at the end would also be useful. For example as I imagine yesterday went from the output processed uknown replicas already declared declared bad ", "1213": "The recovery doesn t take into account the replicas on sites blacklisted for read. If a file is declared bad and is not recoverable because the only remaining site where it is located is blacklisted for read the necromancer should declare the file as recoverable and not as definitely lost. ", "1202": "Currently it is possible to directly download single files from RSEs using the webui if the RSE is webdav enabled. Furthermore the webui can also create metalink files for datasets. This metalink file can then be used in a download manager to download the full dataset. But the problem is that there are no extensions that are able to handle the certificate authentication. So the browser will ask the user every time to provide a valid certificate. Development of a browser extension that is able to handle the metalink files from rucio and the necessary authentication to the storage elements using a certificate installed in the browser. ", "1157": "Avoid errors when alembic version is not set in the DB Make sure that the server code can understand the DB schema. At start up the RESTful API checks whether the alembic version is in a whitelist or whether the code version is supported. ", "1152": "It looks like when the server times out no exception is raised to the client Raise a new TimeOut exception ", "1114": "In case of open containers the length and events is not recorded as metadata yet but could be recursively calculated from the constituent datasets or even files . The idea is to add a recursive option to rucio get metadata . ", "1109": "The error message shown to unregistered users in the webui always points to ATLAS voms. Make the message configurable. ", "1091": "Archive missing features tracker In this ticket we are tracking discussing the features missing from the Rucio archive support. x Download full archive and extract a single file x xrdcp transfer a single file part of a zip file x Transparent list replicas support of zip contents x Forward of rule creation from constituent to archive x list dataset replicas should resolve archives as well x extract flag needs to be added to metalink x Bug with checksums on xrdcp streamed replicas needs to be fixed x Checksum checking needs to be enabled again due to fix in x Forward of constituent rule to archive in case of tape replica x Client needs to extract files from archive x Revert xrdcp workaround Handling archives in the reaper Handling of lost files in the necromancer Should not be removed if there is a zip replica Full transparent handling of constituents in the judge rules x list replicas should re order zips and prioritise root protocol over everything else The advantage of the transparent support of replica listing of content archives with the format root supports would be that there is little to do in the actual rucio client as there is no protocol difference between downloading a normal file or a file in a zip file. Other discussions ", "1026": "It would be good to have the tests running against the flask backend in travis with the mode allow failures to expose the errors. With python it can be even better but would require to have fixed the syntax first. ", "1021": "Install rucio with setup.py pip pip install . oracle postgresql mysql kerberos dev Instead of manual copying and linking. ", "857": "There are some specific parts in the conveyor mostly job dictionary building and processing of activemq message results which needs to be disentangled from the code and moved to the tranfertool. ", "781": "Right now the status of a replication rule only provides a single high level error summary. For example rucio rule info examine Status of the replication rule TRANSFER FAILED TRANSFER TRANSFER CHECKSUM MISMATCH USER DEFINE and SRC checksums are different. ! STUCK Requests cms store mc Neutrino E gun GEN SIM DIGI RAW correctPU realistic RSE US NERSC REAL Attempts Last Retry Last error FAILED Last source US Nebraska REAL Available sources US Nebraska REAL Blacklisted sources cms store mc Neutrino E gun GEN SIM DIGI RAW correctPU realistic ADDB RSE US NERSC REAL Attempts Last Retry Last error FAILED Last source US Nebraska REAL Available sources US Nebraska REAL Blacklisted sources this particular case goes on for another files or so However it s not practical to debug this rule because I have no clue what error corresponds to which file. s Two suggestions In the output of rucio rule info examine . record the last error message per stuck file. Also record the last FTS job ID where the file failed will probably also need the FTS server URL . ", "736": "Add doc how to configure Rucio rses accounts etc ", "713": "When sorting the rules in by creation date the result is wrong. Currently this is done using a simple string sorting which is wrong. Change the sorting to correctly sort the dates chronologically. ", "704": "The conveyor receiver incorrectly handles multi source jobs. This is the case for jobs where the first source FAILS and the second one is a success. The poller requests jobs via the transfer tool which does quite a bit of response formatting and checking. In the case of multi source jobs it basically iterates the sources and if one of the sources is in state FTSState.FINISHED it returns the job as SUCCESS. The receiver however does not go via the transfertool as it gets the response directly from activemq. In the case of a job where the first source fails but the second one is a succes the receiver only looks at the t final transfer state which is FTSCompleteState.ERROR. However what it actually should do is to iterate the sources like the conveyor poller. It seems that the t final transfer state field can only be used for bulk transfers not multi source transfers. Factorize the formatting used in the transfer tool and re apply to conveyor receiver code. ", "689": "Bulk updates of Conveyor poller receiver bulk request update Conveyor finisher bulk update requests replicas locks and rules ", "630": "Introduce a backward compatible directory struture to have severl REST backends webpy flask pyramide django lib rucio web rest webpy init .py lib rucio web rest flask init .py ", "609": "The SciTokens project aims to build a federated ecosystem for authorization on distributed scientific computing infrastructures. Rucio should be made a full fledged SciTokens issuer to make it easy to integrate with SciTokens enabled storage and SciTokens enabled transfer systems. Since Rucio already supports token based authentication issuing SciTokens would not be too difficult. Expected behavior Clients should be able to ask Rucio for a SciToken next to the X Rucio Auth Token. ", "595": "Currently the web UI needs access to the DB connection to not crash. This may be simply a problem of side effects of importing modules Expected behavior Stand alone web UI with a minimal rucio config and config should be possible. ", "583": "If a user specifies only files in the list of DIDs field will still create a container and will try to put the files in the container. Check the type of DIDs and create a container of dataset accordingly. ", "536": "For example when decomissioning a site the RSE is already blacklisted so all the rules could be re evaluated . ", "534": "Docstrings and Copyright. ", "528": "If a RSE performs badly on deletion exclude it temporarily to not impact the general deletion rate. ", "527": "Change algorithm to use distance metrics. ", "526": "Study showed that data is replicated mostly to sites without the necessary computing resources to run jobs there. Rewrite algorithm to also take into account computing resources ", "485": "It seems like things are wrong but in the opposite direction. root rucio add dataset user.ewv Added user.ewv root rucio list dids user.ewv SCOPE NAME DID TYPE user.ewv DATASET I m using the latest master for the client and whatever is installed at U Chicago. Expected behavior should not be prepended to the dataset name in this case. ", "479": "One identity can map to multiple accounts. Therefore banning just one account is not enough. Identities that correspond to people banned in voms must be removed. Migrated from ", "461": "I spotted three things wrong with the transfer tool The transfer tool code disables TLS verification everywhere. See as an example. The transfer tool creates a new session TCP connection SSL for every operation and utilizes no session caching. It doesn t seem to actually create any delegation which is necessary for using the JSON interface directly. We absolutely should not disable TLS verification. That should be improved ASAP. The transfer tool should additionally cache reuse keep alive SSL sessions in order to efficiently interact with FTS. Finally the delegation should be done to avoid a cronjob that must be run by the sysadmin. ", "421": "As GSI is starting the long road of phasing out there s increased interest in widening work around capability based authorization for transfers. Capabilities are authorization mechanisms that describe what the bearer is allowed to do rather than who it is . The most common implementation of capabilities is a HTTP bearer token. A token is added to the Authorization header of a HTTP request and the contents of this token prove that the requester is authorized. Done via curl it might look something like this curl H Authorization Bearer For the WLCG I m aware of three different models around capabilities being investigated VO issued bearer tokens The VO creates a token authorizing the bearer to perform a certain operation at a site storage service. The storage service verifies the token was signed by the VO and honors the token within the VO s storage area allowing CMS to issue authorizations for CMS data but not ATLAS data . How the VO determines the authorizations and how the VO user retrieves the token is still a bit implementation defined. The token itself is formatted as a JWT the SciTokens implementation of this scheme additionally uses the scp scope claim to specify the authorizations. The WLCG Authorization Task Force is working on an agreed upon profile. Here Rucio would issue a fine grained token signed with Rucio s public key and return the token to the client. The storage services would be configured to trust Rucio as an issuer for a certain area using a standardized REST API to download the set of public keys that the issuer has blessed . Storage issued bearer tokens A client authenticates with a storage requesting token to access a particular resource. The storage creates the token and returns it to the client. The client can then hand the token to any third party the third party can only perform the specific operation listed in the token. The example implementation here is dCache issued macaroons one would authenticate with dCache via GSI request a macaroon via a dCache specific but relatively generic REST API and hand the returned token to a third party. Here Rucio would retrieve the macaroons from dCache and return it to the client. Signed URLs AWS allows object owners to pre sign a URL to have a specific authorization. Anyone with possession of the URL would automatically get authorized by the storage service. Note that this is somewhat problematic as URLs are typically treated as public within the WLCG for example monitoring allows any user to view anyone else s transfer URLs. In the other two schemes the resource being accessed and the capability authorizing the access are managed separately. Rucio would create the pre signed URL with its credentials and return this to the client. In all three cases the user interacts with the storage utilizing non GSI and interaction with Rucio can be done via any of the Rucio supported methods. This is a significant piece of work and ties into lots of external activity WLCG Authorization Task Force and development cycles of various storage projects . It s not a one pull request and done in fact it s likely a bit of a moving target token formats may change over the next months . However I think there s some commonalities between the approaches that allow us to lay the groundwork Token approaches are done per RSE protocol. That is each RSE will have a particular style of capabilities they support that can be done as an attribute. This suggests we should reserve an attribute name token type ? unfortunately as we want to support non WLCG storage it is unlikely we ll ever converge to a single token type. Server side handlers for rucio upload and rucio download should get access to the token type and be able to invoke plugins for each type that issue the corresponding capabilities. Client code for rucio upload and rucio download should be able to handle both a returned URL and a bearer token in the server response using the bearer token as appropriate. For VO issued bearer tokens it would make the most sense to have Rucio s current token issuing endpoint issue the tokens. For a python prototype see I have a parallel set of patches for that allow third party copies using tokens issued with that API. ", "372": "There is a foreign key constraint error in the reaper WARNING Reaper DatabaseException Database exception. Details cx Oracle.IntegrityError ORA integrity constraint ATLAS RUCIO.DATASET LOCKS DID FK violated child record found SQL DELETE INDEX DIDS DIDS PK FROM atlas rucio.dids WHERE atlas rucio.dids.scope scope AND atlas rucio.dids.name name OR atlas rucio.dids.scope scope AND atlas rucio.dids.name name OR atlas rucio.dids.scope scope AND atlas rucio.dids.name name OR atlas rucio.dids.scope scope AND atlas rucio.dids.name name parameters name name name name scope scope scope scope This basically happens when the reaper trys to delete a dataset which is in a container and there is a rule on the container. Due to this rule there is also a dataset lock for the dataset. The reaper now trys to delete the dataset from the table but it fails because there is a foreign key constraint from dataset locks to dids. The question is if the reaper should just also delete the dataset lock. This might be problematic as then the rule on the container is out of sync. The same issue might also exist for the undertaker but this has to be checked. Migrated from ", "366": "As the paths of tape endpoints don t follow the same conventions used by disk is necessary to test and maybe modify the consistency check module to support tape endpoints. Migrated from ", "354": "rucio.tests.test dumper path parsing.TestPathParsing.test remove prefix ok rucio.tests.test forecast.TestForecast.test predict usr site packages sqlalchemy sql default comparator.py SAWarning The IN predicate on requests.source rse id was invoked with an empty sequence. This results in a contradiction which nonetheless can be expensive to evaluate. Consider alternative strategies for improved performance. strategies for improved performance. expr usr site packages sqlalchemy sql default comparator.py SAWarning The IN predicate on requests.dest rse id was invoked with an empty sequence. This results in a contradiction which nonetheless can be expensive to evaluate. Consider alternative strategies for improved performance. strategies for improved performance. expr ok Please fix this ", "112": "If we fail on importing necessary dependencies like gfal we end up in very long timeouts later in the workflow. We should exit quickly if dependencies are not available. ", "57": "Right now these stuck rules are repaired in Hard mode meaning all locks are selected while it is actually enough to just select all STUCK locks and their sister locks. Migrated from JIRA ", "54": "Migrated from JIRA ", "53": "Migrated from JIRA ", "43": "Migrated from JIRA ", "38": "Migrated from JIRA ", "36": "Migrated from JIRA ", "35": "Hi A user noticed the following problem when submitting transfer request via I used with List of DIDs to submit a transfer to CERN PROD PERF IDTRACKING. I included a bunch of containers and one dataset. I submitted this for approval see email below . The web page gave no clear indication of a problem except I noticed an error message that flashed up after I pressed submit. But it was quickly replaced with something like submitted OK . Once the request had been approved it seems to have disappeared. I therfore resubmitted as two requests one for the containers and one for the dataset and it all worked fine. This was annoying as each request needs to be manually approved which understandably can take some time. If really can t handle datasets and containers together then it should give an error when selecting DIDs not a brief flash and then nothing when submitting. Thanks Alastair Migrated from JIRA ", "31": "Migrated from JIRA "}